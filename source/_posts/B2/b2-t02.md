title: B2-T02
date: 2019-01-16 16:11:23
---
Conceptos de sistemas operativos: Características, evolución y tendencias. Estructuras, componentes y funciones. Sistemas operativos multiprocesador.
=====================================================================================================================================================

Conceptos de Sistemas Operativos
--------------------------------

### Introducción

Un Sistema Operativo es un programa que administra el hardware de una computadora. También proporciona las bases para los programas de aplicación, y actúa como intermediario entre el usuario y el hardware. Estas tareas pueden ser llevadas a cabo de varias formas, lo que permite que algunos Sistemas Operativos se diseñen para ser prácticos, otros eficientes y otros para ser ambas cosas.

### ¿Qué hace un Sistema Operativo?

Un sistema informático puede dividirse en cuatro componentes: el hardware, el Sistema Operativo, los programas de aplicación y los usuario. El Sistema Operativo controla y coordina el uso del hardware entre los diversos programas de aplicación por parte de los distintos usuarios.

También podemos ver un sistema informático como hardware, software y datos. El Sistema Operativo proporciona los medios para hacer un uso adecuado de estos recursos durante el funcionamiento del sistema informático.

### Definición Sistema Operativo

Un Sistema Operativo es un programa, o conjunto de programas eficiente y productivo en el uso de un computador (hardware), permitiendo la ejecución de aplicaciones de usuario. Es el intermediario entre las aplicaciones de usuario y el hardware.

Metas:

-   Brindar un ambiente de realización y ejecución de aplicaciones.
-   Proveer un entorno sin interferencias a cada usuario (interferencia: lo que un usuario modifica en su entorno, no interfiera ni modifique lo de otro usuario).
-   Administrar de forma equitativa los recursos (hardware y software).
-   Hacerlo de la forma más amigable e intuitiva posible.

Todas las aplicaciones de usuario requieren un conjunto común de operaciones que son incorporadas al Sistema Operativo.

Tareas principales:

-   Implementar diferentes entornos para diferentes usos (interfaz gráfica, shells, tipo web, etc).
-   Proveer una o más interfaces con el usuario.
-   Proveer a las aplicaciones un conjunto de servicios (a través de los “system services”).
-   Eficiencia y equidad en la administración de recursos.

Se puede decir que el Sistema Operativo es un:

-   Administrador de recursos. Sus tareas consisten en administrar los recursos disponibles y decidir como asignar estos recursos según los pedidos y asignaciones que tenga.
-   Programa de control. Controla la ejecución de los programas para la prevención de errores y mal uso del sistema.

Frecuentemente la porción residente del propio Sistema Operativo se denomina _núcleo del sistema (Kernel)_ .

Evolución de los Sistemas Operativos
------------------------------------

La informática tal y como se le conoce hoy día, surgió a raíz de la II Guerra Mundial, en la década de los 40. En esos años no existía siquiera el concepto de “Sistema Operativo” y los programadores interactuaban directamente con el hardware de las computadoras trabajando en lenguaje máquina (es decir, en binario, programando únicamente 0s y 1s).

![](https://gsitic.files.wordpress.com/2017/12/evolucionso.png?w=825)

#### Sistemas Batch o por Lotes (Años 70 y comienzo de los 80)

En las primeras épocas los sistemas eran grandes y costosos. Constaban de una entrada de trabajos y una salida impresa, por lo cual la interacción con el usuario era prácticamente nula. Las principales características eran que el sistema soportaba un único trabajo a la vez, y que las tareas relacionadas se agrupaban en conjuntos o lotes, para su procesamiento más eficiente.

A comienzo de los 80, utilizando las técnicas de Spooling (proceso mediante el cual la computadora introduce trabajos en un buffer, de manera que un dispositivo pueda acceder a ellos cuando esté listo) y multiprogramación (ejecución de múltiples tareas compartiendo recursos) se pudo comenzar a desarrollar técnicas de planificación de despacho.

Esta técnica consistía en seleccionar un lote de trabajos que estaban en memoria secundaria para cargarlos en memoria principal. Luego, el SO seleccionaba uno de ellos para ejecutar, y si este debía esperar por alguna tarea (por ejemplo ejecución de E/S) el sistema elegía otro del lote para utilizar el procesador. Esto incrementó el uso del procesador.

#### Sistemas de Tiempo Compartido (Finales de los 80)

Estos sistemas eran multiusuarios. Ejecutaban programas de forma concurrente con una elevada tasa de procesos, de forma tal que permitía a los usuarios interactuar directamente con el sistema como si fuera un único usuario.

La necesidad de acceder y actualizar datos de forma concurrente, creó la necesidad de evolucionar el sistema de archivos a uno multiusuario, incorporando técnicas de protección de accesos.

#### Sistemas de Computadores Personales (Años 80)

Con costos de hardware decrecientes, fue posible el diseño y uso de computadores personales. Los Sistemas fueron diseñados en base a que serían utilizados por un único usuario, y todo el énfasis en el desarrollo estuvo en mejorar la interacción con el usuario. Se desarrolló la interfaz de ventanas que conocemos hoy.

#### Sistemas Paralelos (Comienzo de los 90)

Son sistemas donde se dispone de más de un procesador, permitiendo ejecución simultánea y sincronizada de procesos. Se clasifican en:

-   Altamente integrados: “tightly coupled”. Son sistemas en donde los canales de interconexión son de alta velocidad (bus común o memoria compartida).
-   Poco integrados: “closely coupled”. Son sistemas en donde los canales de interconexión son de baja velocidad (sistemas en red).

![](https://gsitic.files.wordpress.com/2017/12/arquitectura_memorias.png?w=825)

Veamos ahora otra clasificación de los Sistemas Paralelos:

-   **Asimétricos** : se designa una CPU (master) para ejecutar el código del núcleo, para no lidiar con la concurrencia, los demás (slaves) ejecutarán lo que éste les designe.
-   **Simétricos** : todos los procesadores son considerados iguales, el código del núcleo se dispone en memoria común y es ejecutado por cualquier procesador.

Y otra clasificación más:

-   **UMA (Uniform Memory Access)** : cada CPU accede a cualquier lugar de la memoria en el mismo tiempo.
-   **NUMA (Non-Uniform Memory Access)** : las CPU tienen áreas de memoria a las que acceden más rápido que el resto.

Veamos ahora una clasificación de Arquitecturas (Taxonomía de Flynn):

-   **SISD (Single Instruction, Single Data)** : Arquitectura secuencial, no hay paralelismo, son arquitecturas monoprocesadores.
-   **SIMD (Single Instruction, Multiple Data)** : Son sistemas que ejecutan la misma instrucción sobre un conjunto de datos (Arquitectura Vectorial).
-   **MISD (Multiple Instruction, Single Data)** : Paralelismo redundante.
-   **MIMD (Multiple Instruction, Multiple Data)** : Varios procesadores autónomos que ejecutan en forma simultanea varias instrucciones sobre datos diferentes
    -   **memoria compartida** : escalan poco, acceso a memoria es cuello de botella
    -   **memoria distribuida** : escalan a miles de procesadores, conectados en una red de alta velocidad.

Como ejemplo de sistemas computacionales que utilizan sistemas paralelos tenemos los clusters. Estos son sistemas en la cual participan varias computadoras. Los clusters brindan alta disponibilidad (mantiene una serie de servicios, a pesar de posibles fallos), alto rendimiento (en cuanto a capacidad de cálculo) y balance de carga (técnica usada para compartir el trabajo a realizar entre varios procesos, ordenadores, etc)

Se clasifican en:

-   Simétricos: todos los nodos ejecutan tareas y asumen las de otros ante fallos.
-   Asimétricos: nodos primarios ejecutan tareas y nodos secundarios esperan fallos.

#### Sistemas de Tiempo Real

Son sistemas en los cuales todo resultado debe producirse en un cierto tiempo. De lo contrario se considera que el sistema ha fallado.

Estructura de los Sistemas Operativos
-------------------------------------

### Componentes de un Sistema Operativo

-   Gestión de procesos
-   Gestión de memoria
-   Gestión de Entrada/Salida
-   Administración de Almacenamiento Secundario
-   Gestión de Archivos
-   Sistema de Protección

**Gestión de Procesos**

Un proceso es un programa en memoria + CPU + acceso a dispositivos + otros recursos. Un proceso necesita de ciertos recursos (CPU, memoria, archivos, dispositivos E/S, etc) para realizar su tarea.

Podemos ver entonces que un proceso es una entidad activa, mientras que un programa es una entidad pasiva.

Sabiendo entonces qué es un proceso, podemos decir entonces que el sistema operativo es el encargado de su administración. Es el encargado de proveer servicios para que cada proceso pueda realizar su tarea. Entre los servicios se encuentran:

-   Crear y destruir procesos
-   Suspender y reanudar procesos
-   Proveer mecanismos para la sincronización y comunicación entre procesos
-   Proveer mecanismos para prevenir dead-locks o lograr salir de ellos

**Gestión de Memoria**

La memoria es un área de almacenamiento común a los procesadores y dispositivos, donde se almacenan programas, datos, etc. El sistema deberá administrar el lugar libre y ocupado, y será el encargado de las siguientes tareas:

-   Mantener qué partes de la memoria están siendo usadas, y por quién.
-   Decidir qué procesos serán cargados a memoria cuando exista espacio de memoria disponible, pero no suficiente para todos los procesos que deseamos.
-   Asignar y quitar espacio de memoria según sea necesario.

**Gestión de Entrada/Salida**

El sistema operativo deberá ocultar las características específicas de cada dispositivo y ofrecer servicios comunes a todos. Estos servicios serán, entre otros:

-   Montaje y desmontaje de dispositivos
-   Una interfaz entre el cliente y el sistema operativo para los device drivers
-   Técnicas de caché, buffering y spooling
-   Device drivers específicos

**Administración de Almacenamiento Secundario**

Dado que la memoria RAM es volátil y pequeña para todos los datos y programas que se precisan guardar, se utilizan discos para guardar la mayoría de la información. El sistema operativo será el responsable de:

-   Administrar el espacio libre
-   Asignar la información a un determinado lugar
-   Algoritmos de planificación de disco (estos algoritmos deciden quien utiliza un determinado recurso del disco cuando hay competencia por él)

**Gestión de Archivos**

Proporciona una vista uniforme de todas las formas de almacenamiento, implementando el concepto de archivo como una colección de bytes. El Sistema Operativo deberá proveer métodos para:

-   Abrir, cerrar y crear archivos
-   Leer y escribir archivos
-   Organización de directorios

**Sistema de Protección**

Por Protección nos referimos a los mecanismos por los que se controla el acceso de los procesos a los recursos.

En un sistema multiusuario donde se ejecutan procesos de forma concurrente se deben tomar medidas que garanticen la ausencia de interferencia entre ellos. Estas medidas deben incorporar la posibilidad de definir reglas de acceso, entre otras cosas.

### Servicios del Sistema Operativo

El sistema brindará un entorno de ejecución de programas donde se dispondrá de un conjunto de servicios. Los servicios principales serán:

-   **Ejecución de programas** : el SO deberá ser capaz de cargar un programa a memoria y ejecutarlo. El programa deberá poder finalizar, de forma normal o anormal.
-   **Operaciones de E/S** : el SO deberá proveer un mecanismo de acceso ya que por eficiencia y protección los usuarios no accederán directamente al dispositivo.
-   **Manipulación del Sistema de Archivos** : se deberá tener acceso al sistema de archivos y poder, como mínimo, leer, escribir, borrar y crear.
-   **Comunicación entre procesos** : los procesos deberán poder comunicarse, ya sea que estén en el mismo computador o en diferentes.
-   **Manipulación de errores** : el sistema deberá tomar decisiones adecuadas ante eventuales errores que ocurran, como fallo de un dispositivo de memoria, fallo en un programa, etc.

### Estructura del Sistema

La estructura interna de los sistemas operativos pueden ser muy diferentes, ya que se debe tener en cuenta las metas de los usuarios (fácil, uso, confiable, rápido, etc) y las del sistema (fácil de diseñar, implementar y mantener, eficiente, etc).

Veremos 3 posibles diseños del sistema: sistema monolítico, sistema en capas, sistema con micronúcleo.

**Sistema Monolítico**

Estos sistemas no tienen una estructura definida, sino que son escritos como una colección de procedimientos donde cualquier procedimiento puede invocar a otro.

Ejemplo de estos sistemas pueden ser MS-DOS. Es importante tener en cuenta que ningún sistema es puramente de un tipo.

**Sistema en Capas o Niveles**

El diseño se organiza en una jerarquía de capas, donde los servicios que brinda una capa son consumidos solamente por la capa superior. La capa 0 es del Hardware y la N es la de los procesos de Usuario.

![](https://gsitic.files.wordpress.com/2017/12/capas.png?w=825)

Estos sistemas tienen como ventaja que son modulares y la verificación se puede hacer a cada capa por separado (son más mantenibles). Sin embargo el diseño es muy costoso y es menos eficiente que el sistema monolítico ya que pierde tiempo pasando por cada capa.

**Sistema con micronúcleo (microkernels)**

La idea consiste en tener un núcleo que brinde los servicios mínimos de manejo de procesos, memoria y que provea la comunicación entre procesos. Todos los restantes servicios se construyen como procesos separados del micronúcleo, que ejecutan en modo usuario.

Estos sistemas tienen como ventaja un diseño simple y funcional, que aumenta la portabilidad y la escalabilidad. Para agregar un nuevo servicio no es necesario modificar el núcleo, y es más seguro ya que los servicios corren en modo usuario.

**Cliente/Servidor**

Los procesos se diferencian en servidores, que proporcionan ciertos servicios y clientes que disponen de esos servicios.

**Máquinas Virtuales**

Se ejecuta un monitor de máquinas virtuales que proporciona copias virtuales del hardware al resto de procesos. En cada una de las máquinas se ejecuta un SO.

**Exokernels**

Un programa se ejecuta en modo kernel, asignando los recursos a las máquinas virtuales.

**Híbrido**

Implica que el núcleo en cuestión usa conceptos de arquitectura o mecanismos tanto del diseño monolítico como del micronúcleo.

### Tipos de Sistemas Operativos

Se pueden clasificar los SO en función de:

-   **Nº de usuarios**
    -   **Monousuario** : solo 1 usuario puede usar los recursos del sistema simultáneamente.
    -   **Multiusuario** : varios usuarios pueden usar los recursos del sistema simultáneamente. Por tanto, aunque haya más de un usuario dado de alta en el sistema, si no pueden trabajar de forma simultánea, el SO no es multiusuario.
-   **Nº de procesos o tareas**
    -   **Monotarea** : solo puede ejecutar 1 tarea a la vez.
    -   **Multitarea o multiprogramación** : puede ejecutar varios programas a la vez.
-   **Nº de procesadores**
    -   **Monoproceso / monoprocesador** : el SO es capaz de gestionar solo 1 procesador, de manera que si tuviese más sería inútil. En estos SO los procesos irán alternando su ocupación en la CPU.
    -   **Multiproceso / multiprocesador** : el SO es capaz de gestionar varios procesadores, de modo que puede usarlos simultáneamente para distribuir su carga de trabajo. Estos sistemas trabajan de dos formas:
        -   **Asimétrica** : el SO reparte las tareas, que está realizando, entre los procesadores. Determinados procesos los ejecutará siempre un procesador, y el otro procesador sólo se utilizará para realizar procesos de usuario. En este caso, es posible que un procesador esté siempre trabajando y el otro, en ocasiones, sin actividad.
        -   **Simétrica** : los procesos son enviados indistintamente a cualquiera de los procesadores disponibles.
-   **Tiempo de respuesta** (tiempo que tarda el usuario en obtener los resultados después de iniciar la ejecución de un programa):
    -   **Procesamiento por lotes** : el tiempo de respuesta no es importante y suele ser alto. Los procesos se ejecutan secuencialmente unos tras otro. No existe interacción con el usuario. Ejemplo: copias de seguridad.
    -   **Tiempo compartido** : el procesador divide su tiempo entre todos los procesos (usando algoritmos de planificación como Round Robin). Ejemplo: sistemas multiusuarios interactivos (los usuarios interactúan con el sistema).
    -   **Tiempo real** : en estos SO, los procesos requieren un tiempo de respuesta muy bajo o inmediato. Ejemplos donde esto es especialmente importante: sistema donde el tiempo de respuesta es crucial como sistemas médicos de monitorización de pacientes, sistemas bancarios, tráfico aéreo…

Administración de Memoria
-------------------------

### Memoria Principal

**Introducción**

En sistemas multiprogramados, para sacarle jugo a la multiprogramación, se necesita tener varios procesos cargados en memoria a la vez.

Recordemos, que con respecto a la administración de memoria, el SO es responsable de:

-   Mantener qué partes de la memoria están en uso y por quién.
-   Decidir qué procesos cargar cuando haya memoria libre.
-   Asignar y quitar espacio de memoria según sea necesario.

**Preparación de un programa para ejecutar**

Los programas son normalmente escritos en lenguajes de alto nivel, y deben pasar por distintas etapas antes de ser ejecutados:

-   Compilación (compile): traducción de código fuente a código objeto.
-   Ensamblaje (linker): ensamblar varios códigos objeto en un archivo ejecutable. Surge ante la necesidad de modularizar los programas y reutilizar código.
-   Carga (load): asigna el archivo ejecutable a la memoria principal del sistema (crea en memoria el espacio necesario para diferentes áreas y las carga con la información).

El tamaño de un proceso en memoria principal está limitado por la cantidad de memoria física que exista. Para aprovechar mejor la memoria, se puede utilizar la carga dinámica, la cual no cargará en memoria principal una rutina hasta que ésta no sea invocada.

La gran ventaja es que las rutinas que no son utilizadas, no son cargadas a memoria física, y por lo tanto no consumen este recurso.

**Direcciones Relativas y Absolutas**

La mayoría de los SO permiten que un proceso de usuario resida en cualquier parte de la memoria principal. Es así que, aunque el espacio de direcciones comience en el 0000, la primera dirección de usuario no tiene porqué ser 0000. Esta posibilidad afecta a las direcciones que el programa de usuario puede utilizar. En cada una de las etapas que hemos visto para poder ejecutar un programa, las direcciones pueden representarse de diferentes formas.

Las direcciones de un programa fuente son normalmente simbólicas. Al compilar, el compilador se encarga de reasignar estas direcciones simbólicas a direcciones relativas. El cargador, se encargará, a su vez, de reasignar direcciones relativas a direcciones absolutas.

**Asociación de direcciones (address binding)**

¿En qué momento el SO reasigna las instrucciones y los datos a direcciones de memoria?:

-   Tiempo de Compilación: Si sabemos en el momento de la compilación donde va a residir el proceso en memoria, podemos generar código absoluto (con direcciones absolutas). Ahora, si en algún momento deseamos cambiar su ubicación, deberemos recompilar el código.
-   Tiempo de Carga: El compilador deberá generar código reubicable (con direcciones relativas), y en este caso se retarda la reasignación a direcciones absolutas hasta el momento de la carga. Si en algún momento deseamos cambiar su ubicación, deberemos solamente volver a cargarlo.
-   Tiempo de Ejecución: Si el proceso puede variar su ubicación en memoria durante su ejecución, entonces es necesario retardar su asignación a direcciones absolutas hasta el momento de ejecución. Para que este esquema pueda funcionar, se requiere soporte de hardware.

**Espacios de direcciones lógico y físico**

Una dirección generada por la CPU se denomina normalmente dirección lógica, mientras que una dirección vista por la unidad de memoria se denomina dirección física.

Los métodos de reasignación en tiempo de compilación y de carga generan direcciones físicas y lógicas idénticas; no es el caso para el tiempo de ejecución. En este caso decimos que la dirección lógica es una dirección virtual.

Al conjunto de todas las direcciones lógicas de un programa se le denomina espacio de direcciones lógicas; mientras que al conjunto de todas las direcciones físicas de un programa se le denomina espacio de direcciones físicas.

La correspondencia entre direcciones virtuales y físicas en tiempo de ejecución es establecida por un dispositivo de hardware que se denomina Unidad de Gestión de Memoria (MMU = Memory Management Unit).

![](https://gsitic.files.wordpress.com/2017/12/gestion_memoria.png?w=825)

**Estrategia de asignación o reubicación**

¿Cómo elige el SO en que porción de memoria colocaremos un proceso? Existen varias estrategias:

-   First fit: Asigna el primer “agujero”de memoria libre que satisface la necesidad.
-   Best fit: Asigna el mejor “agujero” de memoria libre que exista en la memoria principal.
-   Worst fit: Asigna en el “agujero” más grande que exista en la memoria principal.

Estudios de simulación han mostrado que _first-fit_ y _best-fit_ lograron mejores rendimientos en tiempo de asignación y utilización de la memoria que la estrategia _worst-fit_ .

Veamos un ejemplo:

Si quisiéramos asignar a memoria un proceso de 212 kb, y tenemos los siguientes espacios libres (espacios en blanco):

![](https://gsitic.files.wordpress.com/2017/12/asignacion.png?w=825)

Veamos en qué hueco asigna al proceso cada estrategia:

![](https://gsitic.files.wordpress.com/2017/12/asignacion2.png?w=825)

**Problema de asignación de memoria**

La memoria física puede ser asignada a los diversos procesos en ejecución siguiendo diversas técnicas:

-   Asignación contigua
-   Asignación dispersa

-   **Asignación contigua**

El espacio de direcciones lógicas de un proceso se mapea sobre una única zona de la memoria física: las direcciones de memoria son contiguas.

Métodos:

-   Particiones fijas
-   Particiones variables

![](https://gsitic.files.wordpress.com/2017/12/asignacion_contigua.png?w=825)

-   **Asignación dispersa**

La memoria lógica se divide en fragmentos (páginas o segmentos), que se mapean sobre zonas no contiguas de la memoria física.

Técnicas de asignación dispersa:

-   Paginación
-   Paginación multinivel
-   Segmentación
-   Segmentación paginada

![](https://gsitic.files.wordpress.com/2017/12/asignacion_dispersa.png?w=825)

Para implementar estas técnicas se necesita el apoyo de la MMU.

**Fragmentación**

Existen dos tipos de fragmentación:

-   Fragmentación Interna: Es la pérdida de espacio en disco debido al hecho de que el tamaño de un determinado archivo sea inferior al tamaño del clúster, ya que teóricamente el archivo estaría obligado a ser referenciado como un clúster completo.
-   Fragmentación Externa: Se da cuando existe suficiente memoria libre en el sistema para satisfacer un requerimiento de memoria, pero no es posible asignarlo debido a que no es un espacio contiguo.

Dicho lo anterior, vemos que las estrategias presentadas en el ejemplo anterior muestran problemas de fragmentación externa, ya que en la memoria van quedando una gran cantidad de espacios pequeños que no son asignados.

**Intercambio (Swapping)**

Como ya vimos, un proceso debe estar en memoria principal para ser ejecutado. Sin embargo, los procesos pueden ser intercambiados temporalmente, sacándolos de memoria y almacenándolos en el disco, y volviéndolos a llevar a memoria para continuar su ejecución.

Al mecanismo de llevar un proceso desde memoria principal a disco se le denomina s _wap-out._ Al inverso se le denomina _swap-in_ . El mayor tiempo consumido en el _swaping_ es el tiempo de transferencia.

![](https://gsitic.files.wordpress.com/2017/12/swapping.png?w=825)

### Memoria Virtual

**Introducción**

La memoria virtual permite ejecutar procesos que requieren más memoria que la disponible en el sistema, manteniendo en memoria principal solo aquella memoria que el proceso esté utilizando y el resto en el disco. De esta forma el usuario ya no debe preocuparse por las limitaciones de memoria física.

Cada proceso tiene su propio espacio de direccionamiento virtual (o lógico) y la MMU es la encargada de mapear las direcciones virtuales (o lógicas) a físicas.

**Implementación**

La implementación de memoria virtual es realizada a través de la técnica de paginación bajo demanda. En la paginación bajo demanda los procesos residen en un dispositivo de disco y son puestos en memoria principal cuando es necesario cargarlos para ejecutar. La carga del proceso en memoria no es total, sino que implementa un cargador “perezoso” (lazy swapper), que cargará las páginas según se vayan necesitando.

Utilizar un esquema de este tipo requiere el conocimiento de las páginas que están activas en memoria. Para ello se utiliza el valid-invalid bit, que consiste en agregar a la tabla de páginas un nuevo campo (bit de validez), que indique para cada entrada, si la página se encuentra o no en memoria. Al inicio, la tabla de páginas indicará que ninguna página está en memoria (todos los bits de validez se encontrarán en **i** (invalid)).

![](https://gsitic.files.wordpress.com/2017/12/memvirtual.png?w=825)

En este ejemplo tenemos que el proceso tiene para usar 8 páginas, de las cuales solo usa 6, y de las cuales solo 3 están en memoria principal (A, C, F). Todas las páginas estarán el el disco (incluidas aquellas que también están en memoria principal).

**Fallo de página**

La memoria cargada en memoria principal se le denomina memoria residente. El acceso a memoria residente por parte de un proceso es tomado como un acceso normal, pero el acceso a memoria no residente genera un fallo de página.

El fallo de página genera un trap a nivel del SO, que activa una rutina de atención que carga la página en memoria principal.

**Acceso a Memoria**

El acceso a memoria genera la siguiente secuencia de pasos:

-   Verificar que el proceso referencia una página correcta dentro de su espacio virtual, ya que no todas las direcciones dentro de su espacio son válidas. Por ejemplo, el acceso fuera de un array puede generar un acceso a una página virtual que no fue asignada al proceso. Si el proceso referencia a una página incorrecta, se genera un errar y el proceso termina.
-   Si el acceso fue correcto, se busca en la tabla de páginas el frame correspondiente, verificando el bit de validez-invalidez.
-   Si el bit es de validez se accede al frame correspondiente y se termina el acceso.
-   Si no es válido se genera un trap de page fault, que involucra los siguientes pasos:
    1.  Se busca frame libre en memoria principal, si no hay se ejecuta el algoritmo de reemplazo.
    2.  Se lee de disco la página a cargar, y se carga en el frame obtenido en el paso anterior.
    3.  Se actualiza la tabla de páginas, indicando que la página está disponible en memoria principal.
    4.  Se devuelve el control a la instrucción que fue interrumpida por el PF (page fault).

Si se aplica este método se tendrá un sistema puro de paginación bajo demanda. Tener en cuenta que para poder llevarlo a cabo se precisa una tabla de páginas y espacio swap de disco.

**Algoritmos de reemplazo**

La necesidad de traer a memoria principal una página en una memoria principal llena, genera la búsqueda de un frame a reemplazar, mediante un algoritmo de reemplazo. Un mal algoritmo de reemplazo puede generar un impacto significativo de degradación del sistema.

Cuando se elige un frame a reemplazar, este será puesto en memoria swap, y ante un eventual uso futuro, volverá a memoria principal a través de un page fault.

Los pasos a seguir cuando reemplazamos frames son los siguientes:

-   Elegir el frame mediante algún algoritmo de reemplazo.
-   Escribir el frame en memoria swap (swap out) y ajustar la tabla de páginas.
-   Cargar la página en el frame correspondiente (swap in).
-   Ajustar la tabla de página.

![](https://gsitic.files.wordpress.com/2017/12/reemplazo.png?w=455&h=344)

Veamos ahora algunos algoritmos:

-   **_FIFO (First in First out)_**

El algoritmo reemplaza la página que lleva más tiempo en memoria principal. Es un algoritmo fácil de implementar ya que requiere únicamente de una estructura tipo cola, pero reemplaza las páginas sin tener en cuenta las referencias que tuvo.

-   **_Segunda Oportunidad_**

Este algoritmo intenta disminuir la cantidad de fallos de páginas del algoritmo FIFO, teniendo en cuenta las referencias a las páginas.

El algoritmo será igual al anterior, salvo que cada página tendrá un bit que indicará si fue o no referenciada luego de ser cargada a memoria. Al momento del reemplazo, se verifica el bit de referencia; si está encendido, a la página se le da una segunda oportunidad y es puesta al final de la cola. Luego se continúa con la siguiente página que está al principio de la cola. Si el bit está apagado, esta página será seleccionada para ser reemplazada.

Es un tanto ineficiente, pero disminuye la cantidad de fallos de páginas.

-   **_Óptimo_**

En este algoritmo se reemplaza la página que no va a ser usada por el mayor periodo de tiempo. Es imposible de implementar porque requiere conocer a qué páginas accederá el proceso.

-   **_LRU (Least Recently Used – Recientemente Menos Usada)_**

Este algoritmo asocia a cada página el tiempo en que fue referenciada. La página elegida por el algoritmo de reemplazo será la que fue accedida hace más tiempo. Este algoritmo es el que más se aproxima al óptimo y es bastante utilizado por los SO.

-   _**NRU (No Recientemente Usada)**_

En este algoritmo a las páginas se les asigna un bit de referencia y otro de modificación. El bit de referencia se enciende cada vez que se lee o escribe la página, mientras que el de modificación solo se enciende cada vez que se escribe. Cada cierto tiempo el bit de referencia es apagado.

Al ocurrir un fallo de página, los frames son divididos en 4 clases. Se reemplazará un frame al azar de la clase más baja que no esté vacía:

-   Clase 0: No referenciada, no modificada
-   Clase 1: No referenciada, modificada
-   Clase 2: Referenciada, no modificada
-   Clase 3: Referenciada, modificada

Al ejecutar el algoritmo de reemplazo, existen dos opciones de páginas a reemplazar:

-   Reemplazo global: Un proceso puede reemplazar un frame utilizado por otro. Aunque los PF de un proceso afectan a otros, es el método más usado.
-   Reemplazo local: Un proceso reemplaza únicamente los frames que tiene asignado, es por eso que la cantidad de frames de un proceso no varía. La desventaja es que hay marcos que se pueden desperdiciar.

**Asignación de frames a procesos e hiperpaginación**

Si el SO no implementa una estrategia de asignación de memoria, un proceso que requiera mucha memoria puede hacer colapsar el sistema.

Una forma de asignar frames a procesos podría ser dividir la cantidad de frames del sistema en partes iguales para cada proceso. Este método puede ser ineficiente ya que no todos los procesos consumen la misma cantidad de memoria.

Si un proceso utiliza en forma activa una cantidad mayor de frames de los asignados por el sistema, tendrá un algo porcentaje de fallos de página, dando lugar a que el proceso esté continuamente realizando PF, pasando más tiempo paginando que ejecutando, lo que se conoce como **hiperpaginación** . Se degrada significativamente el rendimiento del sistema.

Procesos
--------

**Definición de proceso**

Un proceso es un programa en ejecución que necesita estar cargado en memoria y disponer de recursos (CPU, memoria, archivos, dispositivos de E/S) para cumplir su objetivo. Se trata de una entidad activa. Mientras que los programas son un conjunto de archivos que están almacenados en algún dispositivo de almacenamiento (disco duro, pendrive …) y cuyo código fuente está escrito en algún lenguaje de programación. Cuando este conjunto de archivos se ejecutan, entonces pasa a ser un proceso.

**Procesos en memoria**

Un proceso en memoria se constituye de varias secciones:

-   **Código (text)** : instrucciones del proceso.
-   **Datos (data)** : variables globales del proceso.
-   **Memoria dinámica (Heap)** : Memoria dinámica que genera el proceso.
-   **Pila (Stack)** : utilizado para preservar el estado en la invocación anidada de procedimientos y funciones.

**Estados de los procesos**

El estado de un proceso se define por su actividad actual, cambiando a medida que se ejecuta. La ejecución de un proceso alterna una serie de ráfagas de CPU y E/S.

Los estados de un proceso son:

-   **Nuevo** : Cuando el proceso es creado.
-   **En Ejecución** : El proceso tiene asignado un procesador y está ejecutando sus instrucciones.
-   **Bloqueado** : El proceso está esperando por un evento (que se complete un pedido de E/S o una señal).
-   **Preparado** : El proceso está listo para ejecutar, solo necesita del recurso procesador.
-   **Terminado** : El proceso finalizó su ejecución.

**Transiciones entre los estados**

Veamos ahora como los procesos pueden cambiar de estados a partir de determinados hechos. A continuación se muestra el diagrama de estados y transiciones de los procesos:

![](https://gsitic.files.wordpress.com/2017/12/estados_procesos1.png?w=825)

-   **Nuevo -> Preparado** : el SO está preparado para admitir un proceso más.
-   **Preparado -> Ejecución** : el planificador escoge un proceso para la ejecución.
-   **Ejecución -> Preparado** : el proceso en ejecución es interrumpido y expulsado del procesador porque ya ha consumido su tiempo asignado o porque otro proceso de mayor prioridad está esperando.
-   **Ejecución -> Bloqueado** : el proceso abandona voluntariamente la CPU y espera a un evento externo.
-   **Bloqueado -> Preparado** : finaliza el evento que estaba esperando el proceso y pasa al estado preparado.
-   **Ejecución -> Terminado** : el proceso termina su ejecución (terminación normal).
-   **Preparado/Bloqueado -> Terminado** : el proceso es eliminado (terminación anormal).

**Listas y colas de procesos**

Los procesos, según su estado, deberán esperar por determinados eventos, como ya vimos. Puede suceder, que más de un proceso esté esperando por el mismo evento, es por eso que se deben organizar en diferentes colas o listas.

-   **Lista de procesos del sistema (job queue)** : Esta será una lista especial, porque los procesos que están en ella no esperan por nada en particular, sino que es la lista de todos los procesos del sistema. Al crearse un nuevo proceso se agrega el PCB a esta lista. Cuando el proceso termina su ejecución es borrado.
-   **Cola de procesos listos (ready queue)** : Esta cola se compondrá de los procesos que estén en estado listo. La estructura de esta cola dependerá de la estrategia de planificación utilizada.
-   **Cola de espera de dispositivos (device queue)** : Los procesos que esperan por un dispositivo de E/S particular son agrupados en una lista específica al dispositivo. Cada dispositivo de E/S tendrá su cola de espera, por lo que existirán varias device queue.

**Bloque de Control de Proceso (PCB)**

Cuando un proceso se ejecuta, el SO le asigna un espacio de direcciones de memoria (que contiene las instrucciones, los datos y la pila que es una estructura para almacenar y recuperar datos del proceso) y lo añade a la tabla de procesos.

El SO guarda en la tabla de procesos por cada proceso una estructura de datos llamada Bloque de Control de Proceso (PCB) que almacena la siguiente información:

-   **Identificación** de proceso: del proceso en sí (PID), del proceso padre (PPID) y de usuario.
-   **Información de estado** del proceso: preparado, en ejecución, bloqueado, ….
-   **Prioridad** del proceso.
-   **Dirección de memoria** donde se ha cargado el proceso
-   **Otros** : recursos utilizados, valores de los registros del procesador, propietarios, permisos.

**Cambio de contexto (context switch)**

Para dar sensación de ejecución simultánea o multiprogramación, el tipo de CPU debe repartirse entre los procesos. Esto implica **cambios de contexto** que consisten en quitarle la CPU al proceso “en ejecución” y asignársela a otro estado “preparado”. Esta operación la realiza un componente del SO llamado **dispatcher** o **planificador a corto** **plazo** y en ella se guarda el contexto del proceso en ejecución en su PCB y se restaura el contexto del nuevo proceso a ejecutar mediante su PCB.

![](https://gsitic.files.wordpress.com/2017/12/cambio_contexto.png?w=825)

Los cambios de contexto pueden suponer una sobrecarga si se utilizan con mucha frecuencia. En general, suelen producirse cuando un proceso finaliza, es expulsado o se suspende.

**Comunicación entre procesos**

Procesos que se ejecutan concurrentemente pueden ser procesos independientes o cooperativos. Un proceso es cooperativo si puede afectar o verse afectado por los restantes procesos que se ejecuten en el sistema, y es independiente si no.

Evidentemente, cualquier proceso que comparta datos con otro será cooperativo. Veamos algunas razones por las cuales es bueno tener un entorno que permita la cooperación de procesos:

-   Compartir información. Dado que varios usuarios pueden estar interesados en la misma información, se debe proveer un acceso concurrente a ella.
-   Acelerar cálculos. Si deseamos que una determinada operación se ejecute rápidamente, debemos dividirla en subtareas ejecutándose cada una de ellas en paralelo. Esto se consigue solo si hay múltiples CPU o varios canales de E/S.

El mecanismo que provee esto es IPC (InterProcess Comunication), que permite intercambiar datos e información.

Hilos (Thread)
--------------

La mayoría de los SO proporcionan caracteríscas que permiten que un proceso tenga múltiples hilos de control.

**¿Qué es un hilo?**

Un hilo es una unidad básica de utilización de CPU, la cual contiene un id de hilo, su propio program counter, un conjunto de registros y una pila; se representa a nivel del SO con una estructura llamada TCB (Thread Control Block)

Los hilos comparten con otros hilos que pertenecen al mismo proceso la sección de código, la sección de datos, entre otras cosas. Si un proceso tiene múltiples hilos, puede realizar más de una tarea a la vez (esto es real cuando se posee más de una CPU).

**Ventajas de usar hilos**

-   **Respuesta** : el tiempo de respuesta mejora, ya que el programa puede continuar ejecutándose, aunque parte de él esté bloqueado.
-   **Compartir recursos** : los hilos comparten la memoria y los recursos del proceso al que pertenecen, por lo que se puede tener varios hilos de ejecución dentro del mismo espacio de direcciones.
-   **Economía** : es más fácil la creación, cambio de contexto y gestión de hilos que de procesos.
-   **Utilización múltiples CPUs** : permite qué hilos de un mismo proceso ejecuten en diferentes CPUs a la vez. En un proceso mono-hilo, un proceso ejecuta en una única CPU, independientemente de cuantas tenga disponibles.

**Hilos a nivel de usuario y de kernel**

-   **Hilos a nivel de usuario** : son implementados en alguna librería. Estos hilos se gestionan sin soporte del SO, el cual solo reconoce un hilo de ejecución.
-   **Hilos a nivel de kernel** : el SO es quien crea, planifica y gestiona los hilos. Se reconocen tantos hilos como se hayan creado.

Los hilos a nivel de usuario tienen como beneficio que su cambio de contexto es más sencillo que el cambio de contexto entre hilos de kernel. Además, se pueden implementar aún si el SO no utiliza hilos a nivel de kernel. Otro de los beneficios consiste en poder planificar diferente a la estrategia del SO.

Los hilos a nivel de kernel tienen como gran beneficio poder aprovechar mejor las arquitecturas multiprocesadores, y que proporcionan un mejor tiempo de respuesta, ya que si un hilo se bloquea, los otros puedes seguir ejecutándose.

Planificación
-------------

La planificación es la base para lograr la multiprogramación.

En un sistema multiprogramado, generalmente en un determinado instante existirán varios procesos que requieren el procesador a la vez, el componente del SO que realiza la operación de elegir el proceso a utilizar es el planificador.

**Principales planificadores de CPU**

-   **Planificador a largo plazo** :
    -   Selecciona procesos de la cola de esperando ejecución y los carga a memoria
    -   Controla el grado de multiprogramación. Es importante que elija un conjunto equilibrado de procesos.
    -   Se ejecuta con poca frecuencia.
-   **Planificador a corto plazo** :
    -   Selecciona entre los procesos preparados en memoria y les asigna la CPU.
    -   Se ejecuta con mucha frecuencia.
-   **Planificador a medio plazo** :
    -   Decide qué proceso pasa de la memoria principal a la secundaria (memoria virtual) o viceversa.

![](https://gsitic.files.wordpress.com/2017/12/planificadores.png?w=825)

El SO enlaza losPCB’s de los procesos que están en el mismo estado a las diversas colas que puedan existir.

**Esquemas de planificación**

Se invoca al planificador cuando:

1.  Cuando un proceso cambia de ejecutando a bloqueado.
2.  Cuando un proceso finaliza.
3.  Cuando un proceso cambia de ejecutando a listo.
4.  Cuando un proceso cambia de bloqueado a listo.
5.  Cuando se crea un nuevo proceso.

Cuando ocurren los dos primeros casos, el planificador es invocado debido a que el proceso en ejecución libera el procesador.

Los últimos tres casos se dan solamente cuando el planificador es expropiativo, ya que puede quitar el procesador a un proceso que estaba ejecutando para dárselo a otro.

**Planificación no apropiativa y apropiativa**

-   Planificación no apropiativa (non-preemptive):
    -   Algoritmos no expulsivos.
    -   Los procesos se ejecutan hasta que terminan o se bloquean.
    -   Sencillo de implementar.
    -   Rendimiento negativo en general.
-   Planificación aproviativa (preemptive):
    -   Algoritmos expulsivos.
    -   Los procesos puedes ser expulsados de la CPU.
    -   Mayor coste de implementación. Necesitan soporte hardware adicional (relojes).
    -   Mejora el servicio y evita monopolización de la CPU.

**Medidas para poder evaluar los algoritmos de planificación**

-   **Utilización de CPU** : es el porcentaje de uso útil que tiene un procesador.
-   **Rendimiento (Throughput)** : número de procesos terminados por unidad de tiempo.
-   **Tiempo de retorno** : tiempo desde que un proceso se carga hasta que finaliza su ejecución.
-   **Tiempo de espera** : es la suma de los tiempos que un proceso estuvo en la cola de procesos listos.
-   **Tiempo de respuesta** : tiempo desde la carga hasta que el proceso da su primera respuesta.

**Algoritmos de planificación**

-   **FCFS (First Come First Served)**

La CPU es asignada a los procesos en el mismo orden que lo solicitan. Es un algoritmo no expulsivo.

-   Ventajas
    -   Sencillo de implementar (cola FIFO).
-   Inconvenientes
    -   Mal tiempo de espera
    -   Efecto convoy (procesos con largar ráfagas de CPU retrasan a procesos con ráfagas cortas).
    -   No válido para procesos interactivos.

![](https://gsitic.files.wordpress.com/2017/12/fcfs1.png?w=825)

-   **SJF (Shortest Job First)**

Primero el que menos tiempo total de CPU requiere.

Se escoge el proceso de la cola de preparados con una próxima racha de CPU más corta y se ejecuta hasta que se termine o se suspenda. Si hay varios procesos con rachas de CPU iguales, se puede aplicar FIFO. Algoritmo no expulsivo.

-   Ventajas
    -   Optimiza el tiempo de espera
    -   Favorece los procesos orientados a E/S
-   Desventajas
    -   Es costoso averiguar cuándo dura la siguiente racha de CPU
    -   Inanición de los procesos con rachas de CPU largas

![](https://gsitic.files.wordpress.com/2017/12/sjf.png?w=825)

-   **SRTF (Shortest Remaining Time First)**

Primero al que menos tiempo de CPU le queda para acabar.

Versión apropiativa de SJF (como el SJF, solo que puede echar a los procesos)

![](https://gsitic.files.wordpress.com/2017/12/srtf.png?w=825)

-   **Planificación por prioridades**

Primero el que tiene más prioridad.

Cada proceso tiene asignada una prioridad. El planificador selecciona el proceso con prioridad más alta (a igual prioridad se selecciona con FCFS).

Las prioridades pueden ser dinámicas (cambian con el tiempo) o estáticas (se mantienen).

-   Inconvenientes
    -   Riesgo de inanición de procesos con prioridad baja. Una solución sería aumentar la prioridad con el incremento del tiempo de espera.

![](https://gsitic.files.wordpress.com/2017/12/por_prioridades.png?w=825)

-   **Planificación Circular (Round Robin)**

Todos el mismo tiempo por turnos.

A cada proceso se le asigna una cantidad de tiempo de CPU llamada “quantum”. Si el proceso tiene un intervalo de CPU mayor que el quantum es expulsado de la CPU.

La cola de preparados se gestiona con una política FIFO.

Si el valor del quantum es grande el algoritmo degenera en FCFS. Si es pequeño se generará sobrecarga debido a cambios de contexto.

Es Equitativo.

![](https://gsitic.files.wordpress.com/2017/12/roundrobin.png?w=825)

-   **Multilevel Queue**

Este algoritmo propone dividir la lista de procesos listos en varias colas, una para cada tipo de proceso. Cabe destacar que los procesos no podrán cambiar de cola, que cada cola tendrá su propio algoritmo de planificación, y que existirá un algoritmo de planificación entre colas.

-   **Multilevel Feedback Queue**

Este algoritmo se diferencia con el anterior en que los procesos si pueden cambiar de nivel, dependiendo del uso del CPU que tengan. La cola de más alta prioridad corresponderá a los I/O bound, la más baja a los CPU-bound.

Un algoritmo así se define por:

-   Cantidad de colas
-   Algoritmo de cada cola
-   Criterio para subir de nivel un proceso
-   Criterio para bajar de nivel un proceso
-   Criterio para asignar un proceso nuevo a una de las colas

![](https://gsitic.files.wordpress.com/2017/12/multilevel-feddback.png?w=361&h=228)

-   **Sistemas Multiprocesador**

En un sistema simétrico, cualquier procesador ejecuta procesos de usuario. Se puede asignar una cola de listos a cada CPU, lo cual es conveniente para el uso de la caché. Pueden haber desbalances de trabajo entre procesadores, por lo cual se pueden migrar procesos de cola para balancear la carga nuevamente.

Programación Concurrente
------------------------

**¿Qué es la programación concurrente?**

Se conoce por programación concurrente a la rama de la informática que trata de las técnicas de programación que se usan para expresar el paralelismo entre tareas y para resolver los problemas de comunicación y sincronización entre procesos.

El principal problema de la programación concurrente corresponde a no saber en qué orden se ejecutan los programas (en especial los programas que se comunican). Se debe tener especial cuidado en que este orden no afecte el resultado de los programas.

**Sección crítica y exclusión mutua**

El método más sencillo de comunicación entre procesos de un programa concurrente es el uso común de unas variables de datos. Esta forma tan sencilla de comunicación puede llevar, no obstante, a errores en el programa ya que el acceso concurrente puede hacer que la acción de un proceso interfiera en las acciones de otro de una forma no adecuada.

Para evitar este tipo de errores se pueden identificar aquellas regiones de los procesos que acceden a variables compartidas y dotarlas de la posibilidad de ejecución como si fueran una única instrucción.

Se denomina **Sección Crítica** a aquellas partes de los procesos concurrentes que no pueden ejecutarse de forma concurrente o, también, que desde otro proceso se ven como si fueran una única instrucción. Esto quiere decir que si un proceso entra a ejecutar una sección crítica en la que se accede a unas variables compartidas, entonces otro proceso no puede entrar a ejecutar una región crítica en la que acceda a variables compartidas con el anterior.

Las secciones críticas se pueden **excluir mutuamente** . Para conseguir dicha exclusión se deben implementar protocolos software que impidan el acceso a una sección crítica mientras está siendo utilizada por un proceso.

**Semáforos**

Dijkstra dio en 1968 una solución al problema de la exclusión mutua con la introducción del concepto de semáforo binario. Esta técnica permite resolver la mayoría de los problemas de sincronización entre procesos y forma parte del diseño de muchos SO.

Un semáforo binario es un indicador (S) de condición que registra si un recurso está disponible o no. Un semáforo binario solo puede tomar dos valores: 0 y 1. Si, para un semáforo binario S=1 entonces el recurso está disponible y la tarea lo puede utilizar; si S=0 el recurso no está disponible y el proceso debe esperar.

Los semáforos se implementan con una cola de tareas a la cual se añaden los procesos que están en espera del recurso.

Un semáforo binario se puede definir como un tipo de datos especial que sólo puede tomar los valores 0 y 1, con una cola de tareas asociada y con sólo tres operaciones para actuar sobre él:

![](https://gsitic.files.wordpress.com/2017/12/semaforo.png?w=825)

1.  La operación **INIT** se debe sellar a cabo antes de que comience la ejecución concurrente de los procesos ya que su función exclusiva es dar una valor inicial al semáforo.
2.  Un proceso que corre la operación **P** y encuentra el semáforo a 1, lo pone a 0 y prosigue su ejecución. Si el semáforo está a 0 el proceso queda en estado de _bloqueado_ hasta que el semáforo se libera.
3.  Cuando se ejecuta la operación **V** puede haber varios procesos en la lista o cola. El proceso que la dejará para pasar al estado listo dependerá del esquema de gestión de la Cola. Si no hay ningún proceso en espera el semáforo se deja libre para el primero que lo requiera.

El semáforo binario resulta adecuado cuando hay que proteger un recurso que pueden compartir varios procesos, pero cuando lo que hay que proteger es un conjunto de recursos similares, se puede usar una versión más general de semáforo que lleve la cuenta del número de recursos disponibles. En este caso el semáforo se inicializa con el número total de recursos disponibles (N) y las operaciones P y V se diseñan de modo que se impida el acceso al recurso protegido por el semáforo cuando el valor de éste es meno o igual que cero. Cada vez que se solicita y obtiene un recurso, el semáforo se decrementa y se incrementa cuando uno de ellos se libera.

Las operaciones que tenemos son las mismas, con algunas diferencias en su semántica:

![](https://gsitic.files.wordpress.com/2017/12/semaforos1.png?w=825)

**Semáforos: mutua exclusión**

La exclusión mutua se realiza fácilmente utilizando semáforos. La operación P se usará como procedimiento de bloqueo antes de acceder a una sección crítica y la operación V como procedimiento de desbloqueo. Se utilizarán tantos semáforos como clases de secciones críticas se establezcan.

Gestión de Entrada/Salida
-------------------------

La **gestión de entrada/salida** es una de las funciones más importantes del SO, ya que el SO debe ser capaz de manejar los diferentes periféricos existentes.

Para ello **debe** :

-   Enviar órdenes a los dispositivos de E/S
-   Determinar el dispositivo que necesita la atención del procesador
-   Detectar las interrupciones
-   Controlar los errores
-   Proporcionar una interfaz entre los dispositivos y el resto del sistema. Esta interfaz debe ser:
    -   Sencilla y fácil de usar
    -   Debe ser la misma para todos los dispositivos

El SO tiene varias maneras de llevar a cabo la E/S:

-   **E/S programada** : el procesador ejecuta un programa que controla las operaciones de E/S. El problema es que el procesador se tiene que quedar esperando (parado) a recibir respuesta.
-   **E/S controlada por interrupciones** : los dispositivos envían una señal de interrupción para llamar la atención del sistema.
-   **E/S mediante el uso de DMA (acceso directo a memoria)** : un chip se encarga de la transferencia y accede a la memoria para leer o escribir datos que recibe y envía el dispositivo sin pasar por el procesador.

Actualmente los discos duros, unidades de CD, DVD, Blueray, admiten DMA y la tienen activada por defecto.

Dado que la velocidad del procesador es muy superior a la de los dispositivos de E/S, se utilizan **técnicas de almacenamiento intermedio** para mejorar el rendimiento del sistema:

-   **Caching** : consiste en almacenar una caché temporal, de rápido acceso, los datos que se usan con más frecuencia.
-   **Buffering** : consiste en utilizar un área de memoria como buffer, simulando un dispositivo o un periférico lógico, que hará de dispositivo intermedio entre el periférico real y el procesador. El buffer es independiente del dispositivo de entrada y/o salida, por lo que permite que el procesador comience a trabajar leyendo o almacenando en el buffer mientras la información del periférico se va almacenando o extrayendo del buffer. Esto evita que un periférico lento afecte al rendimiento del equipo informático.
-   **Spooling** : técnica en la cual la computadora introduce trabajos en un buffer (un área especial en memoria o en un disco), de manera que un dispositivo pueda acceder a ellos cuando esté listo. El spooling es útil en caso de dispositivos que acceden a los datos a distintas velocidades. El buffer proporciona un lugar de espera donde los datos pueden estar hasta que el dispositivo (generalmente más lento) los procesa. Esto permite que la CPU pueda trabajar en otras tareas mientras que espera que el dispositivo más lento acabe de procesar el trabajo.

La aplicación más común del spooling es la impresión. En este caso, los documentos son cargados en un área de un disco, y la impresora los saca de éste a su propia velocidad. El usuario puede entonces realizar otras operaciones en el ordenador mientras la impresión tiene lugar en segundo plano. El spooling permite también que los usuarios coloquen varios trabajos de impresión en una cola de una vez, en lugar de esperar a que cada uno acabe para enviar el siguiente.

**Unidad de Entrada/Salida**

La Unidad de Entrada/Salida (chipset) permite la comunicación de la CPU y la Memoria Principal con el exterior: impresoras, monitor, teclado, etc.

Para que se pueda llevar a cabo el intercambio de información se deben realizar las siguientes tareas:

-   **Direccionamiento** : selección del dispositivo de E/S implicado en una transferencia determinada.
-   **Sincronización** de CPU y periféricos: es necesario coordinar la actividad de la CPU con los periféricos, ya que sus velocidades de trabajo son distintas.
-   **Transferencia** de datos desde o hacia el dispositivo seleccionado.

![](https://gsitic.files.wordpress.com/2017/12/es.png?w=825)

**Software de Entrada/Salida**

El software de E/S se organiza en niveles. Los del nivel inferior ocultan las particularidades del hardware a los del nivel superior que presentan una interfaz simple y uniforme al usuario.

![](https://gsitic.files.wordpress.com/2017/12/softes.png?w=825)

**Hardware de Entrada/Salida**

En general, las unidades de E/S constan de:

-   Un componente electrónico denominado controladora
-   Un componente mecánico, que es el dispositivo mismo

-   **Controladoras**

Las principales funciones de las controladoras son:

-   Comunicación en el periférico, intercambio de órdenes, información del estado.
-   Detección de errores.
-   Comunicación con el procesador, descodificadores de órdenes, datos, información de estado, reconocimiento de dirección.

Al código específico que el SO utiliza para programar una controladora se le conoce como manejador o driver de la controladora. De este modo, para llevar a cabo las tareas de E/S, el SO, usando el driver, se comunica con la controladora a través de una serie de registros específicos que cada controladora tiene. Cuando la orden ha sido cumplida, la controladora produce una interrupción con el fin de permitir que la CPU atienda al SO para comprobar los resultados de la operación de E/S. Para dicha comprobación se utilizan los valores de los registros de la controladora que informan sobre el estado final.

-   **Dispositivos de E/S**

Los periféricos se pueden clasificar en función de si gestionan la información por bloques o caracteres.

Gestión de Ficheros
-------------------

**Introducción**

-   Fichero o archivo: Conjunto de información de un determinado tipo que está almacenada en un dispositivo de almacenamiento. Ejemplo: documento de texto, sonido, imagen, …
-   Carpeta o directorio: Tipo especial de fichero que se utiliza para organizar ficheros (u otras carpetas).
-   Sistema de ficheros: Parte del SO que permite “administrar” la información almacenada de los dispositivos de E/S en forma de ficheros.
    -   Objetivos:
        -   Crear, modificar o borrar ficheros (o carpetas)
        -   Controlar el acceso a los ficheros (mediante permisos)
        -   Permitir intercambio de datos entre ficheros
        -   Permitir realizar copias de seguridad de los ficheros
        -   Permitir el acceso a los ficheros mediante nombres simbólicos

**Ficheros**

**\-> Nombre y extensión de los ficheros**

Los archivos generalmente se componen de:

-   Nombre: La mayoría de SO permiten usar nombres de hasta 255 caracteres y algunos SO, como Linux, distinguen entre mayúsculas y minúsculas.
-   Extensión: Sirve para saber el programa que permite ejecutar o abrir un fichero. Algunos SO como Linux no necesitan el uso de extensiones.

**\-> Tipos de ficheros:**

-   Ficheros normales o regulares: Aquellos ficheros que contienen datos (información).
-   Directorios: Fichero que se utiliza para organizar los ficheros (u otras carpetas).
-   Ficheros especiales de dispositivos: representan a dispositivos de E/S.

**\-> Información que contiene un fichero:**

-   Nombre
-   Tamaño
-   Fechas: de creación, modificación, …
-   Propietario
-   Permisos (lectura, escritura, ejecución, …)
-   Ubicación
-   Enlaces: puntos desde los que se puede acceder al fichero

**\-> Operaciones que se puedes hacer sobre un fichero:**

-   Crear
-   Abrir
-   Escribir
-   Cerrar
-   Borrar

**Directorios**

\-> Operaciones que se pueden hacer sobre un directorio:

-   Crear
-   Entrar
-   Salir
-   Leer su contenido
-   Añadir o Eliminar en él archivos o directorios
-   Borrar

La mayoría de los SO tienen un sistema de archivos de estructura jerárquica, en el que los directorios parten de uno llamado directorio raíz, y del que cuelgan todos los demás en forma de árbol, de ahí que se utilicen términos como árbol de subdirectorios.

**\-> Directorios especiales:**

-   Existen dos tipos:
    -   **.** directorio actual
    -   **..** directorio padre

**\-> Rutas:**

Concatenación de directorios y subdirectorios para llamar a un archivo en una estructura de directorios.

-   Tipos:
    -   Absolutas: se llama al archivo desde el directorio raíz hasta el archivo. Ejemplo: c:\\web\\imagenes\\logo.gif
    -   Relativas: se llama al archivo desde el directorio actual en el que estemos. Ejemplo: si estamos en la carpeta “web” la ruta hasta llegar al archivo “logo.gif” sería: imagenes\\logo.gif

**Métodos de asignación**

Métodos para asignar espacio a cada fichero dentro del disco.

-   Asignación contigua: los bloques de un fichero se encuentran de forma contigua en el disco.
-   Asignación enlazada: en cada bloque está parte de los datos del fichero y una pequeña parte para indicar el siguiente bloque que contiene los datos del fichero.

Bibliografía
------------

-   [Facultad de ingeniería. Universidad de la República. Uruguay.](https://www.fing.edu.uy/)
-   [Wikipedia](https://es.wikipedia.org/wiki/Desfragmentaci%C3%B3n)
-   [IES Serra Perenxisa](http://iesperenxisa.edu.gva.es/wordpress/)
