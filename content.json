{"pages":[{"title":"About","date":"2019-01-16T13:24:31.161Z","path":"about/index.html","text":""},{"title":"Categories","date":"2019-01-16T13:24:31.165Z","path":"categories/index.html","text":""},{"title":"Tags","date":"2019-01-16T13:24:31.172Z","path":"tags/index.html","text":""},{"title":"temario","date":"2019-01-16T15:22:58.000Z","path":"temario/index.html","text":"Temario GSI Organización del Estado y Administración electrónica. La Constitución Española de 1978. Derechos y deberes fundamentales. Su garantía y suspensión. La Jefatura del Estado. La Corona. Funciones constitucionales del Rey. Las Cortes Generales. Atribuciones del Congreso de los Diputados y del Senado. El Tribunal Constitucional: composición y atribuciones. El Defensor del Pueblo. El Gobierno. Su composición. Nombramiento y cese. Las funciones del Gobierno. Relaciones entre el Gobierno y las Cortes Generales. El Gobierno Abierto. Concepto y principios informadores: colaboración, participación, transparencia y rendición de cuentas. La Alianza para el Gobierno Abierto y los planes de acción de España. La Ley 19/2013, de 9 de diciembre, de transparencia, acceso a la información pública y buen gobierno. El Consejo de Transparencia y Buen Gobierno: Real Decreto 919/2014, de 31 de octubre, por el que se aprueba su estatuto. Funciones. El Portal de Transparencia. Las Unidades de Información y Transparencia (UITS). La Administración pública: principios constitucionales informadores. La Administración General del Estado: organización y personal a su servicio. El texto refundido del Estatuto Básico del Empleado Público y demás normativa vigente. Las Comunidades Autónomas y la Administración local: regulación constitucional. Las fuentes del derecho administrativo. La jerarquía de las fuentes. La ley. Las Disposiciones del Gobierno con fuerza de ley: decreto-ley y decreto legislativo. El reglamento: concepto, clases y límites. Otras fuentes del derecho administrativo. Políticas de igualdad de género. La Ley Orgánica 3/2007, de 22 de marzo, para la igualdad efectiva de mujeres y hombres. Políticas contra la violencia de género. La Ley Orgánica 1/2004, de 28 de diciembre, de medidas de protección integral contra la violencia de género. Discapacidad y dependencia. La sociedad de la información. La Agenda Digital para España. Identidad y firma electrónica: régimen jurídico. Reglamento eIDAS. El DNI electrónico. La protección de datos personales. Régimen jurídico. El Reglamento (UE) 2016/679, de 27 de abril, relativo a la protección de las personas físicas en lo que respecta al tratamiento de datos personales y a la libre circulación de estos datos. Principios y derechos. Obligaciones. El Delegado de Protección de Datos en las Administraciones Públicas. La Agencia Española de Protección de Datos. Las Leyes de Procedimiento Administrativo Común de las Administraciones Públicas y de Régimen Jurídico del Sector Público y su normativa de desarrollo. La gestión electrónica de los procedimientos administrativos. Esquema Nacional de Seguridad (ENS). Esquema Nacional de Interoperabilidad (ENI). Normas técnicas de interoperabilidad. Guías CCN-STIC serie 800. Instrumentos para el acceso electrónico a las Administraciones públicas: sedes electrónicas, canales y punto de acceso, identificación y autenticación. Datos abiertos. Normativa vigente de reutilización de la información del sector público. Instrumentos y órganos para la cooperación entre Administraciones públicas en materia de Administración electrónica. Infraestructuras y servicios comunes. Plataformas de validación e interconexión de redes. Tecnología básica. Tecnologías actuales de ordenadores: de los dispositivos móviles a los superordenadores y arquitecturas escalables y de altas prestaciones. Computación en la nube. Base tecnológica. Componentes, funcionalidades y capacidades. Conceptos de sistemas operativos: Características, evolución y tendencias. Estructura, componentes y funciones. Sistemas operativos multiprocesador. Características técnicas y funcionales de los sistemas operativos: Windows, Linux, Unix y otros. Sistemas operativos para dispositivos móviles. Características técnicas de los lenguajes y paradigmas actuales de programación. Inteligencia de negocios: cuadros de mando integral, sistemas de soporte a las decisiones, sistemas de información ejecutiva y almacenes de datos. OLTP y OLAP. Sistemas de gestión de bases de datos relacionales: características y elementos constitutivos. Antecedentes históricos. El lenguaje SQL. Estándares de conectividad: ODBC y JDBC. Arquitectura de sistemas cliente-servidor y multicapas: tipología. Componentes. Interoperabilidad de componentes. Ventajas e inconvenientes. Arquitectura de servicios web. El modelo TCP/IP y el modelo de referencia de interconexión de sistemas abiertos (OSI) de ISO: arquitectura, capas, interfaces, protocolos, direccionamiento y encaminamiento. Lenguajes de marca o etiqueta. Características y funcionalidades. SGML, HTML, XML y sus derivaciones. Lenguajes de script. Análisis y gestión de riesgos de los sistemas de información. La metodología MAGERIT: método, elementos y técnicas. Auditoría Informática: objetivos, alcance y metodología. Técnicas y herramientas. Normas y estándares. Auditoría del ENS y de protección de datos. Auditoría de seguridad física. Gestión de la atención a clientes y usuarios: centros de contacto, CRM. Arquitectura multicanal. Sistemas de respuesta de voz interactiva (IVR). Voice XML. Seguridad física y lógica de un sistema de información. Herramientas en ciberseguridad. Gestión de incidentes. Informática forense. Software libre y software propietario. Características y tipos de licencias. La protección jurídica de los programas de ordenador. Tecnologías de protección de derechos digitales. Técnicas de evaluación de alternativas y análisis de viabilidad. Personal, procedimientos, datos, software y hardware. Presupuestación y control de costes de un proyecto informático. Documática. Gestión y archivo electrónico de documentos. Sistemas de gestión documental y de contenidos. Sindicación de contenido. Sistemas de gestión de flujos de trabajos. Búsqueda de información: robots, spiders, otros. Posicionamiento y buscadores (SEO). Desarrollo de sistemas. Concepto del ciclo de vida de los sistemas y fases. Modelos de ciclo de vida. Gestión del proceso de desarrollo: objetivos, actores y actividades. Técnicas y prácticas de gestión de proyectos. Planificación del desarrollo. Técnicas de planificación. Metodologías de desarrollo. La metodología Métrica. Estrategias de determinación de requerimientos: entrevistas, derivación de sistemas existentes, análisis y prototipos. La especificación de requisitos de software. Análisis estructurado. Diagramas de flujo de datos. Diagramas de estructura. Diccionario de datos. Flujogramas. Modelización conceptual. El modelo Entidad/Relación extendido (E/R): elementos. Reglas de modelización. Validación y construcción de modelos de datos. Diseño de bases de datos. La arquitectura ANSI/SPARC. El modelo lógico relacional. Normalización. Diseño lógico. Diseño físico. Problemas de concurrencia de acceso. Mecanismos de resolución de conflictos. Tipos abstractos de datos y estructuras de datos. Grafos. Tipos de algoritmos: ordenación y búsqueda. Estrategias de diseño de algoritmos. Organizaciones de ficheros. Diseño de programas. Diseño estructurado. Análisis de transformación y de transacción. Cohesión y acoplamiento. Construcción del sistema. Entornos de construcción y generación de código. Estándares de documentación. Manuales de usuario y manuales técnicos. Formación de usuarios y personal técnico: métodos y materiales. Pruebas. Planificación y documentación. Utilización de datos de prueba. Pruebas de software, hardware, procedimientos y datos. Instalación y cambio. Estrategias de sustitución. Recepción e instalación. Evaluación post-implementación. Mantenimiento. Análisis y diseño orientado a objetos. Elementos. El proceso unificado de software. El lenguaje de modelado unificado (UML). Patrones de diseño. La arquitectura Java EE. Características de funcionamiento. Elementos constitutivos. Productos y herramientas. Persistencia. Seguridad. La plataforma.Net. Modelo de programación. Servicios. Herramientas. Persistencia. Seguridad. Aplicaciones web. Diseño web multiplataforma/multidispositivo. Desarrollo web front-end y en servidor. Tecnologías de programación: JavaScript, applets, servlets, ASP, JSP y PHP. Servicios web: estándares, protocolos asociados, interoperabilidad y seguridad. Internacionalización y localización. La calidad del software y su medida. Modelos, métricas, normas y estándares. Accesibilidad, diseño universal y usabilidad. Accesibilidad y usabilidad de las tecnologías, productos y servicios relacionados con la sociedad de la información. Experiencia de Usuario o UX. La Guía de comunicación digital de la Administración del Estado. Minería de datos. Aplicación a la resolución de problemas de gestión. Tecnología y algoritmos. Procesamiento analítico en línea (OLAP). Big data. Bases de datos NoSQL. Sistemas y comunicaciones. Administración del Sistema operativo y software de base. Funciones y responsabilidades. Administración de sistemas de gestión de bases de datos. Funciones Y responsabilidades. Administración de datos. Prácticas de mantenimiento de equipos e instalaciones. Tipos de mantenimiento. Monitorización y gestión de capacidad. Gestión de la configuración. Gestión de librerías de programas y de medios magnéticos. Control de cambios y de versiones. Los lenguajes de control de trabajos. Las técnicas y herramientas de operación automática. Control de la ejecución de los trabajos. Evaluación del rendimiento. Planificación de la capacidad. Análisis de la carga. Herramientas y técnicas utilizables. Almacenamiento masivo de datos. Sistemas SAN, NAS y DAS: componentes, protocolos, gestión y administración. Virtualización del almacenamiento. Gestión de volúmenes. Medios de transmisión guiados y no guiados (inalámbricos). Cables metálicos. Cable coaxial. Fibra óptica. Tipología de redes de cable. Sistemas de transmisión por satélite. Redes locales. Tipología. Técnicas de transmisión. Métodos de acceso. Dispositivos de interconexión. Administración de redes locales. Gestión de usuarios. Gestión de dispositivos. Monitorización y control de tráfico. Gestión SNMP. Gestión de incidencias. Principales protocolos de la arquitectura de comunicaciones TCP/IP. Planificación física de un centro de tratamiento de la información. Vulnerabilidades, riesgo y protección. Dimensionamiento de equipos. Factores a considerar. Virtualización de plataforma y de recursos. Virtualización de puestos de trabajo. Redes conmutadas y de difusión. Conmutación de circuitos y de paquetes. Integración voz-datos. Protocolos de encaminamiento. Ethernet conmutada. MPLS. Calidad de servicio (QOS). La seguridad en redes. Seguridad perimetral. Control de accesos. Técnicas criptográficas y protocolos seguros. Mecanismos de firma digital. Redes privadas virtuales. Seguridad en el puesto del usuario. La red Internet: arquitectura de red. Principios de funcionamiento. Servicios: evolución, estado actual y perspectivas de futuro. La web 2.0. La web semántica. Internet de las Cosas (IoT). Tecnología XDSL y telecomunicaciones por cable: concepto, características y normativa reguladora. de nueva generación y servicios convergentes (NGN/IMS). VoIP, ToIP y comunicaciones unificadas. Convergencia telefonía fija-telefonía móvil. Sistemas de comunicaciones móviles. Generaciones. Telefonía sin hilos y DECT. Paging. Radiotelefonía privada. Sistemas celulares. Trunking. Soluciones de gestión de dispositivos móviles (MDM). Redes inalámbricas. Protocolos. Características funcionales y técnicas. Sistemas de expansión del espectro. Sistemas de acceso. Modos de operación. Seguridad. Normativa reguladora. IP móvil y PLC (Power Line Comunications). Características técnicas. Modos de operación. Seguridad. Normativa reguladora. Ventajas e inconvenientes. Televisión digital. Servicios de televisión (IPTV y OTT). Radiodifusión sonora digital."},{"title":"bloque-4","date":"2019-01-16T15:22:00.000Z","path":"bloque-4/index.html","text":"B4 Sistemas y comunicaciones Prácticas de mantenimiento de equipos e instalaciones. Tipos de mantenimiento. Monitorización y gestión de capacidad.IntroducciónEl mantenimiento se puede definir como aquel conjunto de técnicas que aseguran el continuo funcionamiento del ordenador personal, se puede ver como el cuidado que se le da al ordenador para prevenir posibles fallos y para prolongarle la vida, teniendo en cuenta la ubicación física del equipo ya sea en la oficina o en casa. Tipos de MantenimientoHay tres tipos de mantenimiento: Mantenimiento preventivo. Mantenimiento correctivo. Mantenimiento perfectivo. Mantenimiento preventivo de ordenadores El mantenimiento preventivo consiste en crear un ambiente favorable para el sistema y conservar limpias todas las partes que componen un computador, por ejemplo, basta decir que la mayor parte de fallos que presentan los equipos se debe al exceso de polvo en los componentes internos, ya que éste actúa como aislante térmico. En definitiva se trata de conocer aquellos elementos que pueden disminuir la vida útil del equipo. Otro aspecto que junto al polvo perjudica a la vida del ordenador es que el calor generado por los componentes no pueda dispersarse adecuadamente porque es atrapado en la capa de polvo. Ni que decir tiene el hecho de junto con el polvo se mezclen las partículas de grasa y aceite que pueda contener el aire del ambiente, ya que crean una espesa capa aislante que refleja el calor hacia los demás componentes, con lo cual se reduce la vida útil del sistema en general. Por otro lado, el polvo contiene elementos conductores que pueden generar cortocircuitos entre las trayectorias de los circuitos impresos y tarjetas de periféricos. Si se quiere prolongar la vida útil del equipo y hacer que permanezca libre de reparaciones por muchos años se debe realizar una limpieza con frecuencia del entorno, o por lo menos evitar en la medida de los posible una serie de factores como son: El agua. La electricidad. Los golpes. El frío. El calor. etc. Mantenimiento correctivo y perfectivo para PCs El mantenimiento correctivo consiste en la reparación de alguno de los componentes del ordenador, que abarcan desde una soldadura pequeña, el cambio total de una tarjeta (sonido, video, memoria), o el cambio total de algún dispositivo periférico como el ratón, teclado, monitor, disco duro, etc. El mantenimiento perfectivo consiste en la ampliación de algún componente del equipo para aumentar su capacidad en base a una serie de requerimientos, por ejemplo, ampliar la capacidad del disco duro con uno nuevo, aumentar un módulo de memoria, etc. Generalmente resulta mucho más barato cambiar algún dispositivo que al tratar de repararlo pues muchas veces la complejidad del dispositivo nos limita ya no solo por la necesidad de tener aparatos especiales, sino por la complejidad técnica del componente en si mismo. Por ejemplo intentar reparar una placa base de una marca X modelo XXX puede ser completamente distinto de reparar una placa base de la misma marca X pero el modelo XXY. En estos casos es más sencillo sustituir la pieza y enviar la defectuosa al servicio técnico de la casa X. Asimismo, para realizar el mantenimiento debe considerarse lo siguiente: Revisión de los recursos del sistema, memoria, procesador y disco duro. Optimización de la velocidad del micro. Revisión de la instalación eléctrica (sólo para especialistas). Un informe de los problemas de cada equipo junto con el mantenimiento realizado a cada equipo. Observaciones que puedan mejorar el ambiente de funcionamiento. Criterios que se deben considerar para el mantenimiento del PCA la hora de realizar el mantenimiento basta con seguir dos criterios muy sencillos de aplicar: La periodicidad: debería de hacerse, con una periodicidad semestral, una limpieza física del ordenador para evitar tener que realizar un mantenimiento correctivo. La ubicación del equipo: afectará o beneficiará al ordenador, deben tenerse en cuenta varios factores: En casa: Alejar el ordenador de las ventanas para evitar que los rayos del sol dañen al equipo y para evitar que el polvo se acumule con mayor rapidez. Colocar el equipo en un mueble que se pueda limpiar con facilidad. Comprar enchufes protectores de corriente. Aspirar con frecuencia la alfombra o moqueta en la que se encuentra el equipo para evitar que se acumule el polvo. No colocar objetos sobre el monitor que le tapen la ventilación y por tanto no disipen bien el calor. No colocar el equipo muy pegado a la pared, dejando que el ventilado de la fuente de alimentación disipe bien el calor. Oficina, en el lugar de trabajo, el mantenimiento es más tedioso debido a que se genera más polvo que en casa, hay más vibraciones y seguramente descargas eléctricas, habrá aparatos que produzcan magnetismo y pueden provocar pérdidas de datos, hay más humo, etc. Consideraciones finales con respecto al equipo: No exponerlo a los rayos del sol. No colocarlo en lugares húmedos. Mantenerlo alejado de equipos electrónicos que produzcan campos magnéticos ya que pueden dañarlo. Limpiar con frecuencia. No fumar. Evitar comer y beber cuando se esté usando. Utilizar sistemas de alimentación ininterrumpida en los servidores para que si la energía se corta tener tiempo para guardar la información. Revisión de la instalación eléctrica de la casa u oficina, pero esto lo debe de hacer un especialista. Herramientas para Mantener el PCAntes de empezar a realizar un mantenimiento del PC, o a ensamblar uno nuevo a partir de componentes sueltos, es muy conveniente tener en cuenta una serie de recomendaciones en cuanto a las herramientas que posiblemente se vayan a precisar y alguna advertencia sobre la manipulación de los componentes y la electricidad estática. Para mantener y ampliar un ordenador, es necesario tener algunas herramientas, las cuales en un principio deberían de ser de una calidad aceptable para que duren más y ayuden en la facilidad de mantener. Por lo menos se tiene que quitar la carcasa de la torre e insertar o quitar tarjetas de expansión, ventiladores, fuente de alimentación, memoria, reemplazar un lector de disquetes de CDROM, un disco duro, etc. Se debe también ser capaz de configurar los jumpers y los conmutadores que se pueden encontrar en los dispositivos IDE y en la placa base. Hay dispositivos a los que se les conectan cables de corriente y por lo tanto hay que probar la continuidad en fusibles y cables. Para ejecutar tales tareas es necesario un equipo de herramienta básico que contenga los elementos siguientes: Destornillador de estrella de un tamaño mediano. Destornillador de cabeza plana de un tamaño mediano. Destornillador Torx (especial para tornillos con una cabeza en forma de estrella de 6 puntas). Por norma general, estos tornillos son típicos de PC de marca o se encuentran en lugares a donde el fabricante del PC no desea que se acceda. Pinza extractora: Permite recuperar un tornillo cuando se cae entre los componentes de la placa base, y también se utiliza para insertar y sacar jumpers. Alicates terminados en punta: Útiles para, por ejemplo, extraer los separadores de la placa base. Destornillador buscapolos: Para comprobar, por ejemplo, el interruptor de encendido del ordenador. Bridas: Para no dejar cables sueltos. Insertor / Extrator de chips: necesarios para llevar a cabo ampliaciones de memoria (las famosas tarjetas gráficas que se les podía expandir la memoria) y reemplazar chips defectuosos. Cortador de cables. Pelacables. Pulsera antiestática: Para evitar la electricidad estática. Un polímetro: Permite comprobar la continuidad de corriente. Tenacillas. Un disquete y una cinta para limpiar las unidades correspondientes. Piezas de recambio. La siguiente lista de elementos no son imprescindibles para realizar las tareas de mantenimiento de PC’s, pero sí que son útiles para determinadas tareas: Un soldador y su soporte. Un bote de aire comprimido: Quita el polvo que se acumula en el interior de la caja del ordenador y en el ventilador de la fuente de alimentación. Un pequeño limpiador de vacío: para las placas y el teclado. Materiales de limpieza. Elimina la suciedad que se “pega” en el exterior del ordenador, incluido el monitor. PolímetrosUn polímetro es una pieza extremadamente útil en un equipo de comprobación y debe formar parte del equipo de herramientas para realizar mantenimiento. Para mantener un equipo no es necesario comprar el mejor polímetro del mercado, simplemente es bueno comprar aquel que incluya protección a las sobrecargas, ya que de haberlas, un polímetro de protección de sobrecarga es más resistente a estos abusos y el costo extra que pueda suponer se amortiza rápidamente. Existen en el mercado polímetros analógicos y digitales, siendo estos últimos más caros y más precisos pero en el uso que se le da para el mantenimiento del equipo no son recomendables por su alto coste. Con un polímetro se comprueban los fusibles y la continuidad de los conectores, utilizando los rangos de resistencias para medir continuidad y determinar si hay alguna resistencia alta. Se puede usar en los rangos de voltios DC, entre otras cosas, para comprobar las salidas de la fuente de alimentación. Para comprobar un fusible, se utiliza el rango de resistencias. Primero se cortocircuitan las puntas de pruebas y luego se ajusta a cero la medida que da, utilizando el control que hay en el polímetro. A continuación se conectan las puntas una a cada extremo del fusible. Si no se obtiene una lectura de cero ohmios, el fusible está defectuoso. Para comprobar una batería se pone el polímetro en el rango de voltaje DC y en la escala más alta en la que se espera que se encuentre el voltaje de la batería. Luego se conectan las puntas de prueba a los contactos de la batería, teniendo cuidado de que la polaridad sea correcta observando el voltaje en la escala. Extracción y Cambio de chipsNo es normal que en las labores de mantenimiento se tengan que reparar las placas base y de las tarjetas de un PC cambiando algún chip por dos motivos: Es una tarea de los técnicos de las respectivas casas. No es habitual encontrar en el mercado un componente de esas características. Pero si que puede llegarse a dar el caso de tener que quitar y volver a instalar algún que otro chip. Por ejemplo, a las tarjetas gráficas de hace 3 años (que aún se encuentran en numerosos puntos de nuestra geografía), se les podía ampliar la memoria, y esta ampliación se realizaba insertándole un chip en un slot de la tarjeta. Piezas de RecambioPara realizar un buen mantenimiento, aparte de las herramientas también hace falta disponer de ciertas piezas de recambio, exactamente las piezas que más hacen falta en todo sistema informático son: Fuentes de alimientación. Discos Duros. Disqueteras. Son estándar, de 3’5″. Ratones. No ocupan mucho espacio y es habitual tener que cambiarlos. Teclados. Monitores: Tienen el problema de que son voluminosos para tenerlos almacenados. Estas piezas se pueden obtener por diversos cauces: Comprándolas: La compra de fuentes de alimentación, disqueteras, ratones y teclados son triviales y no son excesivamente caras a no ser que se deseen componentes especiales como p.ej. un ratón inalámbrico. Sin embargo los discos duros y los monitores suelen ser caros, y los monitores son voluminosos para tenerlos de recambios. Reutilizando: Cuando se retira un equipo se pueden aprovecha los componentes como la fuente, el teclado, etc, para en un momento dado volverlo a utilizar. Mantenimiento Preventivo: Elementos que afectan a la vida del equipoHoy en día los ordenadores (clónicos y de marca) junto con los periféricos son muy fiables. Funcionan durante mucho tiempo sin el más mínimo fallo, pero es prudente no dar ciertas cosas por supuestas. Si se acondiciona el entorno en el que se va a instalar un PC y se cuida durante el uso, se evitarán muchos problemas. En este apartado se hace un repaso a aquellos elementos que a veces parecen insignificantes, pero que tanto pueden afectar a la vida útil de un equipo. Conocerlos ayudará a evitar que se produzcan problemas o, al menos, reducir su importancia. Se verán los factores que hay que tener en cuenta a la hora de elegir un entorno de trabajo adecuado. También se comentarán los problemas causados por los picos de tensión y cómo solucionarlos. Y una serie de consejos a seguir para prolongar la vida de los ordenadores y de los cuidados que se deben tener durante el trabajo, para conseguirlo. Ubicación del EquipoDe todos es sabido que los mainframes requieren un entorno refrigerado con temperatura constante ya que sus complejos circuitos generan una gran cantidad de calor. El exceso de humedad y el polvo pueden reducir la fiabilidad de su gran número de interconexiones además los mainframes también necesitan fuentes de alimentación con filtros especiales y altamente fiables. Los ordenadores actuales tienen menos componentes y generan menos calor, siendo más silenciosos que sus predecesores (sin tener en cuenta el ruido que generan los ventiladores). No obstante, lo mejor sería cuidar el ambiente en el que está ubicado el equipo, en concreto sería correcto tener en cuenta las siguientes consideraciones: Tenerlo correctamente ventilado. No tenerlo con un exceso de calor ni de frío. Evitar los humos. Evitar colocarlo cerca de materiales magnéticos. Evitar ubicarlo cerca de fuentes de vibración (impresoras) pues pueden dañar los discos duros. No conectarlo en líneas eléctricas a la que estén conectados aparatos con gran consumo de energía como aparatos de aire acondicionado. Mantener el entorno limpio, libre de polvo. Frío y CalorLos componentes electrónicos consumen corriente y, por tanto, terminan por calentarse, pero hay que tener muy en cuenta que el exceso de calor perjudica. Para comprobar este consumo de energía basta con abrir la caja del equipo después de apagarlo y comprobar si componentes como el disco duro, los chips, etc, están calientes. Los componentes electrónicos tienen una vida limitada, que se reduce en la medida que el calor se incrementa, por lo que es muy importante tener en cuenta la temperatura ambiente a la que el PC funciona. Dentro del ordenador, se disipa una gran cantidad de calor gracias a la corriente de aire que el ventilador hace circular a través de la caja. Además de incrementar el riesgo de fallo de los componentes, el calor excesivo puede producir otros problemas. Si la temperatura se eleva considerablemente, habrá muchas posibilidades de que algún componente deje de funcionar bien o que funcione mal. Algunos dispositivos, como por ejemplo las fuentes de alimentación, en la actualidad disponen de diseños de circuitos protegidos contra el calor, de tal forma que si la temperatura de funcionamiento excede de ciertos niveles, se produce una parada en el funcionamiento. Todos los componentes se calientan cuando se conectan y se enfrían cuando se desconectan, lo que produce dilataciones y contracciones, que a su vez, producen tensiones mecánicas. El problema es más grave con los chips instalados sobre zócalos que con los soldados en la placa. Con el paso del tiempo, las dilataciones y las contracciones hacen que las patillas se salgan fuera del zócalo produciéndose un fallo. Este tipo de fallos es muy común en las tarjetas gráficas, de hecho en los nuevos zócalos AGP traen incorporado una especie de “amarra tarjetas AGP”, para evitar que se muevan. De igual forma se debe evitar un exceso de frío, porque fomenta la formación de condensación, lo que puede causar la oxidación de las superficies metálicas tanto en los conectores como en los zócalos de los chips. Con el tiempo, esto produce contactos eléctricos poco fiables. Si se ha dejado la máquina en un medio extremadamente frío durante un tiempo, no se debe conectarla hasta que pase el tiempo suficiente como para que alcance la temperatura de la habitación. la resistencia de los componentes eléctricos disminuye con la temperatura, lo que hace que por ellos circule más corriente, de este modo, al conectar el ordenador, se podría estropear algún fusible o causar daños más serios. Los ordenadores no habrían llegado a ser tan populares si hubiesen necesitado aire acondicionado, pero no deben estar sometidos a temperaturas extremas; por ejemplo, no se deben poner junto a una ventana que a veces reciba directamente la luz solar, sobre todo en verano, cuando la temperatura puede pasar de 40ºC. También se debe evitar poner junto a calentadores o fogones. Un punto importante que no se debe pasar por alto es permitir la obstrucción de las ranuras de ventilación de las cajas de la unidad y del monitor. El ventilador no funcionará de forma totalmente eficiente si no hay suficiente espacio detrás de la unidad. No se debe poner la máquina contra una pared u otra barrera, ni apilar papeles, manuales o discos en lo alto del ordenador o del monitor. HumedadUn exceso de humedad puede dañar al ordenador. Este problema puede ser particularmente serio a altas temperaturas. La condensación puede tener muchos efectos, como pueden ser la oxidación de las superficies de las partes metálicas, o de los contactos eléctricos, o la rotura de los materiales de aislamiento de la fuente de alimentación y del monitor. Este fenómeno es frecuente en zonas costeras españoles, donde el problema se agrava aún más debido a la salinidad del agua del mar que se mezcla con la humedad en las viviendas. La ausencia de humedad, por calor, o por tiempo seco, puede crear problemas con la electricidad estática. Polvo y SuciedadHay poco que se pueda hacer cuando se utiliza el ordenador en un entorno sucio y polvoriento. Sin embargo, se ha de tener en cuenta que la acumulación de polvo reduce la fiabilidad del ordenador. Una limpieza periódica podría ser necesaria para evitar problemas. El monitor es un buen indicador de la cantidad de polvo que hay en el entorno. Los monitores de ordenador, atraen el polvo, debido a la electricidad estática que hay en la pantalla. Si se acerca la mano a la pantalla se nota como los pelos del dorso son atraídos hacia el CTR. Se pueden incluso notar chasquidos de la electricidad estática. La pantalla actúa como un imán para el polvo, que se pega a ella o se mete en la unidad del sistema. Es asombrosa la cantidad de polvo que se puede acumular dentro de la unidad. Esto se debe a la corriente de aire del ventilador que actúa como una bomba de vacío. La mayor parte de los ventiladores funcionan soplando aire por la parte trasera de la unidad. Este aire entra a través de las ranuras de ventilación, así como por otras aberturas, como las ranuras de las unidades de disco. El polvo puede causar varios problemas en los ordenadores, ya que se amontona en las unidades de disquetes, ensuciando sus mecanismos y lo que es peor, pasándose a los discos, causando errores de lectura y otros fallos. El polvo se adhiere también a los componentes de la placa, reduciendo la capacidad que tienen de disipar calor. Se acumula, de la misma forma, en enchufes y zócalos, en los que las sustancias químicas que transporta pueden provocar que los contactos se enmohezcan, pudiendo producir conexiones poco fiables. Por último, puede introducirse dentro del motor, que en caso extremo, podría quemarse. Si hay que trabajar con el ordenador en un medio polvoriento, lo único y más importante que se puede hacer es aumentar la frecuencia del mantenimiento preventivo, por lo que se debería limpiar más a menudo. Sin embargo, se puede reducir la cantidad de polvo que puede entrar en el ordenador si se es cuidadoso al elegir su colocación. Las cajas tipo torre que se colocan en el suelo aumentan el nivel del polvo ya que los humanos al andar levantan gran cantidad de polvo. Es muy recomendable utilizar un aspirador o una mopa en las salas de ordenadores, ya que las escobas lo único que hacen es levantar el polvo, sin embargo el aspirador lo absorbe. El problema del polvo se vuelve peliagudo si se habla de los típicos servidores ubicados en una esquina y encendidos las 24 horas del día los 365 días del año, ya que suelen atraer gran cantidad de polvo, que puede repercutir en la disponibilidad. Los teclados son también vulnerables a los efectos de la suciedad y del polvo, ya que por su naturaleza, son propensos a recoger otras cosas como migajas y ceniza de cigarrillos. Con el paso del tiempo, estas cosas se acumulan en su interior y pueden impedir, por un atasco, el funcionamiento de las teclas. Se deben limpiar los teclados de forma periódica, como parte del mantenimiento preventivo. Sin embargo, en un medio particularmente sucio, se debe emplear un mayor nivel de protección, por lo que sería una buena idea comprar una membrana de plástico transparente y flexible que se adapta sobre las teclas y las protege de la suciedad, del polvo y de los líquidos. Esto es irrelevante en oficinas, pues se supone que se limpia todos los días, pero es un factor muy importante en las industrias o almacenes. Golpes y VibracionesLos ordenadores son bastante resistentes, pero hay un límite al que ellos pueden resistir. Las vibraciones constantes pueden ser la causa de que los chips se salgan de sus zócalos y de que los conectores se aflojen. Los golpes y los impactos repentinos pueden ser dañinos, sobre todo para los discos duros, que son especialmente vulnerables. Si se mueve un disco duro o se produce una vibración mientras está funcionando, se pueden producir serios daños y pérdida de datos. También hay que asegurarse que el ordenador no se instala en un lugar fácilmente desplazable. Si hay que instalar un PC donde los choques y las vibraciones son inevitables, sería conveniente disponer de una máquina especial, resistente a dichos golpes. Electricidad EstáticaAl moverse por una habitación, se genera una carga eléctrica cuya cantidad depende de varios factores: de las ropas que se tengan puestas, del tipo de cobertura del suelo, del nivel de humedad de la atmósfera y de la conductividad de los zapatos. Cuando se está cargado eléctricamente y se toca algo conectado a tierra seguramente se recibirá una descarga eléctrica. Estas descargas eléctricas son inofensivas para el individuo. Sin embargo, pueden ser la causa de un deterioro de la memoria del ordenador, para el micro o para la placa base (comúnmente se denomina quemar la memoria, el micro y la placa). Los generadores de electricidad estática son: La piel humana. El vidrio. El nylon. La lana. El pelo. El plomo. La seda. El aluminio. El algodón. El acero. El poliéster. El teflón. Se pueden utilizar pulverizadores antiestáticos para las alfombras y reducir la generación de electricidad estática. También, se pueden colocar felpudos antiestáticos, conectados a tierra, debajo del ordenador y de las sillas. Esto impedirá que se genere electricidad estática al moverse mientras trabaja en el ordenador. En climas secos será necesario un humidificador. La electricidad estática puede ser un serio problema para los empleados de servicio técnico y para todos aquellos que trabajan en ordenadores abiertos ya que al tocar un componente electrónico, puede quedar inutilizado fácilmente. Se puede evitar esto trabajando con pulseras antiestáticas conectadas a tierra a través del chasis del equipo, pero, a veces, es posible que no se pueda evitar ni usándolos. A continuación se dan una serie de indicaciones para eliminar el riesgo de dañar los componentes con descargas de electricidad estática: Tocar la caja del ordenador, o cualquier otra cosa que esté conectada a tierra, antes de tocar una placa del circuito o cualquier otra parte de la máquina. Esto dará la seguridad de que la electricidad estática se descarga inofensivamente a tierra. Evitar tocar los componentes electrónicos o los conectores laterales cuando se proceda a instalar, quitar o configurar placas del circuito. No tocar los pines de los módulos de memoria cuando se hagan ampliaciones. Las placas, tarjetas y memorias se deben de colocar en un embalaje antiestático. Utilizar calzado deportivo por ser un buen aislante. Problemas relacionados con la corrienteLos ordenadores están diseñados para funcionar con salidas normales de corriente, sin embargo, la calidad del voltaje de la línea de corriente alterna puede variar en momentos puntuales e incluso en algunos lugares, el suministro puede ser tan poco fiable como para impedir al ordenador funcionar correctamente. Los ordenadores mainframes son tan complejos que un leve fallo podría hacer caer a todo el sistema, destruyendo el trabajo de cientos de usuarios. En estos sistemas se emplean circuitos especiales de filtro para proteger el hardware de los efectos de las pequeñas fluctuaciones de la línea de voltaje. Los ordenadores personales son mucho más tolerantes con las irregularidades de la corriente. Sin embargo, si la calidad del suministro es pobre, también se producirán problemas. Hay dos tipos principales de perturbaciones que se pueden dar en cualquier edificio y por cualquier motivo que son: los picos de alto voltaje y las fluctuaciones de la línea de corriente. Picos de Voltaje Los picos de voltaje pueden causar el deterioro de la memoria y la caída del sistema. Ocurren con frecuencia cuando aparatos como los aires acondicionados, los calentadores y cafeteras se conectan y desconectan. Otros elementos del equipo de una oficina, que consumen mucha corriente, como los radiadores, son también culpables de estas perturbaciones. Cuando un dispositivo se conecta y desconecta, en vez de un incremento o disminución de la corriente a través de los cables, lo que se produce es una oscilación amortiguada de alta frecuencia. La oscilación es lo que comúnmente se llama pico . Algunas veces, ese pico no se elimina en los circuitos de filtro de la fuente de alimentación y aparece en las líneas de voltaje que alimentan los circuitos electrónicos de un PC, pudiendo causar deterioros en la memoria y la caída del sistema. Los picos pueden ser tan grandes como para causar el fallo de los componentes de la fuente de alimentación, e incluso llegar a afectar al micro y a la placa. Las fluctuaciones de voltaje, producidas por la conexión y desconexión de elementos del equipo, están presentes en cualquier línea eléctrica. Las condiciones que determinan si estas fluctuaciones son suficientemente fuertes como para hacer que el equipo funcione mal, son complejas y difíciles de analizar. Sin embargo, los pisos superiores de los edificios altos son particularmente propensos a ello. Fluctuaciones de la Línea de Corriente La fuente de alimentación de un ordenador está diseñada para funcionar a un cierto voltaje nominal de entrada. Las diseñadas para usarse en Estados Unidos y Canadá tienen una entrada de 110 V de corriente alterna, mientras que para la mayor parte de Europa, la entrada estándar es de 240 V de corriente alterna. Otros países usan 220 V de corriente alterna. Cuando se compre cualquier componente para el equipo (incluida la fuente de alimentación) hay que asegurarse de que están diseñadas para ser utilizadas en España y que han sido fabricados para adaptarse a las normas de seguridad eléctricas. La pérdida de corriente puede causar mayores inconvenientes, de tal forma que, aunque dure menos de un segundo, puede hacer que el ordenador vuelva a arrancar, con lo que se perderían todos los cambios que hayan hecho desde la última vez que se salvó el trabajo. Los archivos que no fueron cerrados correctamente, pueden hacer estragos en el sistema de archivos de la mayor parte de los SO, produciendo la pérdida de sectores y otros problemas. Para tratar los problemas de la línea de corriente se pueden tomar tres opciones: Se puede comprar un enchufe protector especial de onda en almacenes de componentes electrónicos o de ordenadores, que ofrecen protección limitada contra picos de corriente. Comprar un rectificador de corriente. su tamaño es como el de una caja de zapatos y se conecta al enchufe de la pared, teniendo, a su vez, dos o tres salidas. Se componen de circuitos que filtran la corriente y eliminan tanto los picos como los ruidos. Adquirir un sistema de alimentación continua o ininterrumpida SAI (UPS, Uninterrumpible Power Supply). Esta fuente, al igual que el rectificador, se conecta entre el enchufe de la pared y el ordenador. Durante el funcionamiento normal, la corriente alterna pasa por ella y por algún filtro que lleve incluido. La UPS tiene unas baterías que se cargan continuamente con el voltaje de la línea. También contienen componentes electrónicos llamados inversores que convierten el bajo voltaje de la salida de la batería en alto voltaje de corriente alterna. cuando la UPS detecta una rápida caída de voltaje, un relé se conecta sobre el circuito inversor que comienza a generar un voltaje de corriente alterna. Dependiendo del consumo de corriente y de la capacidad de las baterías de la UPS, un PC puede seguir funcionando desde unos minutos hasta hora y media. Esto permite al usuario salvar su trabajo y preparar ordenadamente una salida del sistema si la corriente no se restablece. Las UPS se pueden encontrar en una amplia gama de tipos y tamaños. Algunos más baratos generan una onda cuadrada de salida, en lugar de la senoidal de la corriente de la red. Esto es aceptable para alimentar las fuentes de alimentación conmutadas que usan los ordenadores, pero no en otros equipos. DeterioroLos ordenadores, como cualquier componente electrónico se deterioran con el paso del tiempo. Sin embargo, el modo en que se use, también afectará a la mayor o menor probabilidad de que ocurran fallos. Por ejemplo aunque no parezca cierto, el hecho de encender y apagar el equipo en intervalos de tiempo cortos hará que se deteriore antes. Si se quiere alargar la vida del PC, se debe conectar y desconectar tan espaciadamente como sea posible. Si se deja el ordenador conectado durante mucho tiempo, sin usarlo, el CRT puede sufrir daños, ya que, al estar presentando la misma imagen de forma continuada, se puede quemar el revestimiento de fósforo de la pantalla; para solucionar este problema se puede coger el hábito de habilitar el salvador de pantallas o simplemente apagar el monitor cuando uno se ausenta durante un espacio prolongado de tiempo. Los mayores problemas pueden venir por descuidar y abusar del equipo. Hay varias reglas básicas de buen comportamiento que se deben de conocer que son: Evitar fumar o comer mientras se está trabajando con el ordenador, ya que esto puede conducir a que la ceniza y las migajas se introduzcan en el teclado, pudiendo atascar las teclas. Mantener los disquetes aislados del polvo y de la suciedad, almacenándolos, metidos en sus fundas, cuando no se usan. Manejar los disquetes cuidadosamente, sin tocar la superficie de grabación. Esto ayudará a preservar los datos ya que el polvo y la grasa no se introduzcan dentro de mecanismo de la unidad. No se debe ser brusco en el manejo del ordenador. Para introducir un disquete en la unidad, sólo es necesario presionar suavemente con la punta de los dedos. También se debe mover el cierre de la unidad con delicadeza. No se deben machacar las teclas como si se tratara de una vieja máquina de escribir. Evitar presionar los cables. cuando se intente desconectar una regleta, se debe tirar del enchufe y no de los cables. Mantenimiento Correctivo y PerfectivoEl mantenimiento correctivo consiste en la reparación de alguno de los componentes del ordenador, que abarcan desde una soldadura pequeña, el cambo total de una tarjeta (sonido, video, memoria), o el cambio total de algún dispositivo periférico como el ratón, teclado, monitor, disco duro, etc. El mantenimiento perfectivo consiste en la ampliación de algún componente del equipo para aumentar su capacidad en base a una serie de requerimientos, por ejemplo, ampliar la capacidad del disco duro con uno nuevo, aumentar un módulo de memoria, etc. En ambos tipos de mantenimiento habrá que abrir el equipo y sustituir una pieza o simplemente añadir una pieza, por eso en los siguientes aparatados se va a ver cuales son las piezas más importantes de un ordenador y como realizar su sustitución o como insertar una nueva pieza en el mismo. Los componentes físicos de un ordenador que se pueden cambiar o ampliar son los siguientes: Placas Base. Micro. Memoria. Disco duro, lectores de CD, CD-RW, DVD, etc. Disqueteras. Abriendo la Caja del EquipoSe abrirá la caja del ordenador para limpiarle el polvo o sustituir o ampliar algún componente. Para abrir la caja será necesario un destornillador plano, de estrella o de Torx. Existen cajas que ni siquiera tienen tornillos sino que se abren pulsando alguna lengüeta y desplazando los laterales. También es posible encontrar cajas que llevan candados con lo cual antes habrá que abrir el candado con la llave respectiva. Cuando se habla de formatos de cajas hay que tener en cuenta dos aspectos básicos. Por una parte el formato exterior, que determina su tamaño, estilo, … Y por otra parte los formatos basados en especificaciones técnicas, que determinarán la futura distribución de los componentes internos. Formatos “técnicos”: Va a determinar el formado de la placa base del PC. Los elementos que más se van a ver influenciados por el formato de la caja son la placa base y la fuente de alimentación. A continuación se detallan cuales son estos formatos: XT: Es el formato de los primeros PC’s. Surge en 1981 con el IBM PC. Se caracterizaban por la existencia de grandes interruptores en su parte posterior y su gran tamaño y peso. AT: También diseñado por IBM, surge en 1984. Similar al formato anterior, pero con grandes diferencias en el interior de la caja que determinaba otra colocación de la placa. Tenía aspecto de torre y no era compatible con el anterior. En años posteriores fueron apareciendo formatos que apenas diferían de éste, como Baby AT que presentaba unas dimensiones menor y consecuentemente menos coste. ATX: Surge en 1995 y supone un gran cambio con respecto a sus predecesores. Sobre todo en el formato de la fuente de alimentación (que permite diferentes voltajes) y en que son mucho más fáciles de ampliar. NLX: Formato reciente que tiene por objetivo reducir el tamaño. Aspecto: Mini-Torre: 2 bahías 5 ¼ 2 bahías 3 1/5 FA: 200 W Midi-Torre: 3 bahías 5 ¼ 2 habías 3 1/5 (poseen itnernas) FA: 250-300W Torre Gran-Torre Semi-Torre Server Mini-ITX Pasos a dar: Desconectar el cable de alimentación del PC ya que no llega con tener apagado el equipo. Descargarse de la electricidad estática. Localizar los tornillos o buscar las lengüetas y quitarlos. Desplazar los laterales de la carcasa para tener acceso al interior Placas BasePara colocar la placa en la caja del equipo hace falta el siguiente material: Placa Tornillos arpones Herramientas: Destornillador Brazalete antiestático Cutter Antes de insertar la placa en la caja hay que realizar el siguiente trabajo previo: Anclar los buses a la placa antes de instalarla, además del procesador y la memoria, ya que si la caja es pequeña, o incomoda de trabajar, más tarde será más difícil. Antes de colocar la placa será necesario colocar la fuente de alimentación, si bien actualmente la mayor parte de las cajas ya traen la fuente de alimentación colocada de “fábrica”. Configurar los Jumpers ajustando el voltaje adecuado, la frecuencia de reloj del bus principal y el multiplicador. A la hora de colocar la placa en la caja hay que seguir los siguientes consejos: Evitar que la placa contacte con cualquier parte metálica de la caja. Para ello se deben de utilizar los arpones suministrados con la placa. Si la caja no dispusiera de bahías para poder enganchar los arpones, se cortará el enganche de éstos con un cutter. Para evitar que la placa toque la superficie metálica de la caja también se puede recurrir a un truco muy viejo que consiste en utilizar la esponja en la que viene embalada. Si el procesador alcanza temperaturas muy altas habrá que recortar la superficie de esponja que se aloje debajo del microprocesador ya que se podrá llegar a quemar. Al atornillas los tornillos hay que cerciorarse de que la placa está bien anclada pero sin pasarse. La circuitería es muy delicada. El siguiente paso es colocar los conectores de la fuente de alimentación en la placa. Para realizar esto hay que tener en cuenta que existen dos tipos de placa (que coincide con los tipos de caja): Placas AT: Para este tipo de placas de las fuentes de alimentación salen dos conectores hembras del mismo tamaño y para enchufarlos en los conectores machos de la placa hay que saber que los cables negros tienen que ir en el medio. De esta forma sólo un sentido es válido. Placas ATX: El conector sólo encaja perfectamente en un sentido. Lo último que queda por realizar es conectar los LEDs luminosos y el altavoz, estos son los cables que vienen incluidos en la caja. Habrá que enchufar los cables de la placa etiquetados con el led correspondiente a la toma correspondiente de la placa. Para realizar la correcta conexión hay que: Contar con el manual de la placa. En caso de no tenerlo, mirar las inscripciones de la placa. Si esto no funciona queda el método de “prueba-error”. El ProcesadorTécnicamente resulta imposible realizar un mantenimiento correctivo del procesador, por la imposibilidad de contar con las herramientas necesarias, como mucho se puede realizar una sustitución de un procesador por otro. Realizar un mantenimiento perfectivo del procesador consiste en sustituir un procesador por otro más avanzado siempre y cuando la placa base lo soporte. Hay que tener en cuenta que se distinguen distintos tipos de procesadores, aparte de por su fabricante y por su velocidad por el tipos de conexión. Según esta, se distinguen los siguientes: ZIF : Zero Insertion Force. Para encajar el microprocesador en la placa no hace falta hacer fuerza, simplemente basca con levantar una palanca del zócalo en la placa base, colocar el procesador encima y bajar la palanca. Es necesario decir que estos procesadores traen los pines alrededor de la carcasa de tal forma que estos fines encajan en el zócalo. En los micros los pines no se cierran en un cuadrado perfecto, sino que dejan unas esquinas que indican por qué lado encajar el micro en el zócalo. Vista de un micro Ejemplos de estos micros son: Socket 7: Pentium Socket 8: Pentium Pro de Intel Socket 370 o PPGA: Pentium III más modernos y Pentium celerón más modernos Socket 462/Socket A: Procesadores AMD Athlon, Duron, Thunderbird, XP y MP Socket 423 y 478: Pentium IV de Intel Zócalo donde se inserta el micro Inserción del micro en el zócalo SEC : En estos zócalos los micros se insertan como si fueran unos cartuchos y ya hay que hacer fuerza para que encaje el micro en su zócalo. Slot 1: Pentium II Slot II: Pentium III Slot A: Athlon Procesador SEC No ZIF : Antiguos zócalos donde se insertaban procesadores 486 y anteriores. Soldados : Antiguamente algunos procesadores estaban soldados a su placa base. Una vez colocado el micro es necesario colocarle el disipador para evitar que se queme. Antes de realizar este paso a la superficie del micro hay que echarle “pasta térmica” que elimina los huecos que quedan entre el disipador y el micro, consiguiendo una correcta refrigeración. El ventilador suele necesitar alimentación, que la suele proporcionar la placa a través de unos conectores etiquetados generalmente con “CPU FAN”, que será donde hay que colocar el enchufe del disipador. DisqueteraLa disquetera se suele estropear con más frecuencia de la necesaria debido al exceso de polvo que suele acumular. Para evitar hacer un mantenimiento correctivo o perfectivo basta con realizar un mantenimiento preventivo previo. La disquetera se conecta a la placa a través de un bus de datos que conecta la placa con la disquetera. El conector donde se enganchan los buses de datos de la disquetera a la placa se denomina FDC: Floppy Disk Controler o FDD: Floppy Disk Drive y está situado al lado de los conectores IDE’s. A continuación se exponen los pasos a seguir para realizar un mantenimiento perfectivo (añadir una nueva disquetera) o correctivo (sustituir la disquetera). Localizar la bahía de la caja correspondiente 3 ¼. Insertar la disquetera. Atornillarla en la bahía. Colocar Bus de Datos: Si solo hay una disquetera, insertar el extremo del cable que contiene una doblez a la disquetera y el otro extremo del bus al conector FDC o FDD de la placa. Si hay ya una disquetera colocar el conector que no tiene la doblez y que está más próximo a ésta en la segunda disquetera y el otro extremo del bus al conector FDC o FDD de la placa. La disquetera que tiene la doblez actuaría como maestra. La disquetera que no tiene la doblez actuaría como esclava. Enchufar la disquetera a la fuente de alimentación utilizando el conector pequeño de la misma. Conectar un dispositivo IDE (Integrated Drive Electronic)Las interfaces IDE es un estándar que define la interfaz para conectar periféricos como discos duros, cd-rom, cdrw, etc, a la placa. EIDE (Enhanced): soporta 4 dispositivos conectados, si bien coloquialmente se le sigue llamando por el nombre IDE. Una placa base convencional suele tener 2 canales IDE: el primario y el secundario. En la placa base se distinguen porque aparecen impresas las letras: IDE1 (primario) e IDE2 (secundario). A cada canal se pueden conectar dos dispositivos: Primario (Maestro). El maestro es el primero de los dos y se suele situar al final del cable. Tiene preferencia sobre el esclavo a la hora de transmitir datos. Secundario (Esclavo). El esclavo es el segundo, normalmente conectado en el centro del cable entre el maestro y la controladora. Si tenemos en cuenta el hecho de que en una placa base hay 2 canales IDE y que cada canal soporta 2 dispositivos, podemos tener en total 4 dispositivos. Los dispositivos (discos duros, CD-ROM, DVD-ROM, etc) tienen unos “puentes” llamados jumpers, situados generalmente en la parte posterior o inferior de los mismos, que permiten seleccionar su carácter de maestro, esclavo. Las posiciones de los jumpers vienen indicadas en una calcomanía en la superficie del disco, o bien en el manual o serigrafiadas en la placa de circuito del disco rígido, con las letras _M_ para designar “maestro” y _S_ para designar “esclavo”. La distribución “estándar” en un PC para un rendimiento óptimo es la siguiente: Canal o Puerto IDE 1: Maestro: Disco Duro principal (Contiene el SO). Esclavo: lector de CD-ROM/DVD. Canal o Puerto IDE 2: Maestro: Grabadora de CD/DVD. Esclavo: Segundo Disco Duro, unidad magneto óptica, … Los pasos para montar un dispositivo IDE son los siguientes: Localizar la bahía de la caja correspondiente: 3¼ para discos duros y 5½ para lectores de CD, DVD, etc. Insertar el dispositivo. Atornillarla en la bahía. Colocar el Bus de Datos. Si el dispositivo está solo en el IDE, configurarlo como maestro. Si no está, solo se siguen las recomendaciones anteriores. En cuanto al Bus de Datos, se observará que uno de los extremos tiene un color rojo o rosa, este hilo tiene que encajarse con la pata del dispositivo donde está serigrafiado un 1 que suele ser el de la derecha del conector macho del dispositivo. Solo queda conectarle el cable de alimentación. MemoriaEste es el componente que con más frecuencia se necesita mantener, no porque se estropee, sino porque se necesita ampliar debido a las necesidades de las aplicaciones y SO actuales. Cuanta más memoria se pueda insertar es mejor. Formatos La memoria RAM del PC, en cualquiera de sus tipos, es físicamente un módulo o pastilla (placa de circuito impreso que agrupa varios chips de memoria) que se acaba insertando en los zócalos correspondientes de la placa. Estos módulos pueden tener diferentes formatos que a continuación se describen. DIPS (Cápsula Dual en Línea, Dual Inline Package) . Totalmente desaparecido. Sólo mencionar que en los primeros PC’s se conectaba los DIPS en zócalos libres de la placa base. SIMM (Módulo de Memoria Simple, Single Inline Memory Module) . Pequeña placa de circuito impreso que almacena chips de memoria solo por un lado de la placa, y se inserta en un zócalo SIMM en la placa madre. El primer formato tenía 30 contactos. Un formato más largo en centímetros, que usa 72 contactos y puede almacenar hasta 64MB de RAM, se popularizó con los procesadores Pentium. Las memorias en este formato presentaban la restricción de tener que instalarse siempre por pares de la misma capacidad. Es decir, para conseguir 16MB, eran necesarios 2 SIMM’s de 8MB o 4 SIMM’s de 4MB. Esta restricción venía impuesta porque estos módulos permitían almacenar 32 bits por ciclo, y los procesadores Pentium utilizaban un bus externo de 64 bits. DIMM (Módulo de Memoria Dual, Dual Inline Memory Module) . Pequeña placa de circuito impreso que almacena chips de memoria por ambos lados de la placa, y que se inserta en un zócalo DIMM. Existen 2 tipos de módulos DIMM: SDR SDRAM: 168 contactos. DDR SDRAM: 184 pines. RIMM (Rambus Inline Memory Module) . Pequeña placa de circuito impreso que almacena chips de memoria por ambos lados de la placa y disponen de disipadores de calor. Tienen 184 pines. Colocando la Memoria SIMM Para ampliar la memoria SIMM hay que tener en cuenta que es necesario insertar pares de módulos debiendo ser los módulos iguales en velocidad y tamaño. Para poder ampliar memoria de este tipo, es preciso tener, al menos, 2 zócalos libres en la placa. Los pasos a dar son los siguientes: Descargarse de la electricidad estática. Orientar y presentar el módulo de forma correcta sobre el zócalo de la placa. Los contactos de los módulos están numerados del 1 al 30 o 72, dependiendo del tipo de módulo. Solo es posible una orientación del módulo en la placa ya que en la base, el conector nº 1 del zócalo tiene una pequeña rebaba que imposibilita insertar mal el módulo. Para poder insertar el módulo hay que inclinarlo 45º una vez que éste está sobre el zócalo. Posteriormente se levanta el extremo superior sin sacarlo de la ranura hasta que quede perpendicular a la placa. Para extraer la memoria SIMM hay que presionar las lengüetas de los extremos de los zócalos hacia el exterior. Colocando la Memoria DIMM Estos son los módulos de 168 contactos (SDRAM) y 184 contactos (DDR, RAMBUS). No presentan los inconvenientes de los SIMM: Se pueden insertar módulos de diferente capacidad. No es necesario colocar los módulos por pares. Sí es conveniente tener los módulos de la misma velocidad. Los pasos a dar son los siguientes: Localizar los zócalos en la placa sobre los que se insertarán los módulos de memoria. Tener en cuenta que algunas placas soportan los dos tipos de módulos: SDRAM y DDR. Orientar y presentar el módulo de forma correcta sobre el zócalo de la placa. Para ello observar que en la parte de los pines, la placa de memoria tiene unos huecos (2 para el caso de SDRAM y uno para DDR) que determinan la posición exacta. Es conveniente comenzar a insertar módulos de memoria por el primer zócalo del primer banco, aunque no es estrictamente necesario. Para anclar el módulo sobre la ranura, hay que presionar suavemente sobre el módulo una vez que esté correctamente situado. Estará bien colocado cuando se escuche un pequeño clic. Bibliografia Scribd (Ibiza Ales) Gestión de la configuración. Gestión de librerías de programas y de medios magnéticos. Controles de cambios y de versiones. Los lenguajes de control de trabajos. Las técnicas y herramientas de operación automática.Gestión de Librerías de ProgramasCuando hablamos de librerías nos referimos a conjuntos de programas, rutinas o funciones ya preparadas y a disposición de los programadores durante el desarrollo de aplicaciones. Su practicidad reside en que evitan la reescritura de algoritmos usados con frecuencia, ya que éstos pueden ser incluidos en librerías que posteriormente podrán ser llamadas desde los distintos programas a implementar. Gestión de Librerías en CPropiedades de las librerías en C: Una librería es un archivo que agrupa a otros archivos denominados miembros de la librería. La estructura de las librerías hace posible que puedan extraerse sus miembros. Al agregar archivos a una librería, se introducirá en la misma tanto el contenido de aquellos como su información de gestión (fechas, propietarios, grupos, permisos, etc). Librerías Estáticas y Dinámicas Librerías Estáticas También denominadas librerías-objeto, son agrupaciones de archivos objeto (.obj) compilados en un solo archivo de extensión .OBJ o .LIB . Los modelos de las funciones empleadas en estas librerías, junto con algunas constantes predefinidas y macros que facilitan su uso, constituyen los denominados archivos de cabecera, debido a que suelen ser llamados desde las primeras líneas (cabeceras) de los distintos archivos fuente. Las librerías estáticas están constituidas por uno o varios archivos .lib , .obj o .bpi junto con uno o varios archivos de cabecera ( .h ). Al compilar un programa, el linkador agrega al ejecutable los módulos que incluyen a las funciones utilizadas en el programa, pasando aquellos a formar parte del ejecutable. Esta forma de enlazar las librerías con los programas es la que les da el nombre de estáticas. Librerías Dinámicas Las librerías de enlazado dinámico (DLL) son muy utilizadas en la programación para SO Windows; sistemas que incluyen multitud de librerías de este tipo en disposición de ser utilizadas por cualquier aplicación. Aunque las librerías dinámicas se asocian generalmente a la extensión .DLL , también pueden estar definidas con extensiones del tipo .EXE , .BPI , .DRV , .FON , etc. La programación de aplicaciones Windows consiste en la concatenación de llamadas a librerías dinámicas. Manejo de Librerías Para la programación en lenguaje C, el manejo de librerías presenta dos aspectos: La utilización de librerías. La construcción de librerías. La utilización es segura para cualquier programa, ya que, como mínimo, habrá que hacer uso de alguna librería perteneciente a la Librería Estándar. En cuanto a la construcción, también podría darse en cualquier programa, pero dada la gran cantidad de librerías existentes, lo normal es que sólo se necesite crear una librería cuando el programa a desarrollar sea considerablemente extenso. Evidentemente, tanto la utilización como la construcción de librerías serán diferentes dependiendo de si se trata de librerías estáticas o dinámicas. Librerías de Enlace Dinámico en WindowsComo mencionábamos en un epígrafe anterior, las librerías dinámicas son archivos que contienen funciones y/o recursos que pueden ser requeridos por cualquier aplicación Windows. También indicábamos que podían tener tanto la extensión .DLL como extensiones del tipo .EXE (ejecutable), .DRV (controlador de dispositivo), .FON (fuente de Windows), etc. La diferencia entre las librerías cuyo archivo tiene extensión .DLL y las creadas sobre archivos .EXE , .DRV , .FON , etc, es que, mientras que las primeras se cargan porque son solicitadas por los programas al SO, el resto se cargan porque aparecen referenciadas (por el propio Windows o por un determinado programa) en archivos de inicialización de Windows. Ventajas e Inconvenientes del Empleo de DLL’s Ventajas: El contenido de una DLL puede ser usado por cualquier aplicación Windows. La reutilización de las DLL’s implica una reducción en el tamaño de las aplicaciones. Reducción del tiempo de compilación y/o carga de las aplicaciones, debido a la disminución del tamaño de las mismas. Ahorro de espacio en disco. Independencia de las DLL’s respecto de las aplicaciones. Inconvenientes: Tienen que almacenarse en la carpeta del sistema para poder ser utilizadas. El tiempo que tarda la aplicación en acceder al código que necesita de la DLL es mayor del que emplearía si dicho código formara parte de la propia aplicación. Estructura de una DLL de 32 Bits Podríamos dividir la DLL en los siguientes elementos: Archivo de cabecera. Conjunto de declaraciones y/o definiciones (de variables, funciones, procedimientos, etc) a usar por la librería. Punto de entrada y salida de la DLL (DllEntryPoint). Función que se ocupa de la carga y descarga de la DLL en la memoria principal. Funciones de la DLL. Aquellas especificadas e implementadas por el programador de la librería. Creación de una DLL de 32 Bits Para la creación de una DLL podemos usar lenguajes del tipo Visual Basic, Delphi, Visual C++, etc. Con cualquiera de los lenguajes deberemos crear varios archivos, cada uno de los cuales contendrá un tipo de elemento útil para la construcción de la librería. Por ejemplo, si empleásemos Visual C++, deberíamos crear: Un archivo con extensión .c que contendrá el código fuente de las funciones de la librería. Un archivo con extensión .def que contendrá la información necesaria para el linkador. Dos archivos con extensión .h que será los archivos de cabecera del archivo fuente (estos sólo serán necesarios para crear un programa que utilice la DLL, pero no para la creación de la DLL en sí). Compilación y Linkado de la Librería Tras la compilación de los archivos anteriores, el compilador generará un archivo .lib . Después del linkado, se creará un archivo .dll (esta sería la librería en sí). El acceso a esta DLL podrá hacerse mediante dos tipos de llamadas: Llamada Estática Este tipo de llamada va a utilizar el archivo creado por el compilador ( .lib ). Con este método, el enlace entre el programa y los recursos de la DLL tiene lugar durante el linkado del programa. Es decir, será el linkador quien, utilizando los archivos objeto ( .obj ), los archivos librerías ( .lib ) y los archivos de recursos compilados (.res), cree la aplicación Windows ( .exe ). Con este proceso, el código de la librería queda incluido en el ejecutable. Ventajas: La librería se carga junto con el ejecutable (la contiene). El enlace tiene lugar en tiempo de compilación. Las funciones de la librería pueden ser utilizadas como funciones internas de la aplicación. Inconvenientes: La aplicación almacena en su interior el código de la librería, lo que hace que su tamaño sea mayor. La librería tiene que incluirse en cada aplicación que la necesite. El objetivo de la reutilización sólo se cumple en parte. La memoria principal contiene a la librería durante todo el tiempo de ejecución de la aplicación. La librería y la aplicación tienen una dependencia total. Llamada Dinámica La llamada dinámica emplea el archivo creado por el linkador ( .dll ). El enlace dinámico, como su nombre indica, se producirá en tiempo de ejecución; es decir, la librería se cargará en memoria cuando la aplicación la requiera al sistema. Este proceso utilizará las funciones LoadLibrary y FreeLibrary para la carga y descarga, respectivamente, de la dll en la memoria principal. Ventajas: La aplicación no almacena junto con su código a la librería, lo que reduce el tamaño de la aplicación (la librería se almacena en un archivo aparte). Ninguna aplicación que utilice a la librería deberá incluirla en su código. Se utilizan los beneficios de la reutilización en su totalidad. La librería sólo se carga en la memoria principal cuando va a utilizarse. Cuando deja de utilizarse podrá descargarse de la memoria. La librería y la aplicación son independientes. Inconvenientes: Necesidad de solicitar al SO la carga de la librería. El enlace se produce en tiempo de ejecución, hecho que dificulta la manipulación de la librería. Las funciones de la librería deben ser llamadas mediante punteros. Gestión de Medios MagnéticosDiscos MagnéticosPropiedades de los Discos Magnéticos Un disco magnético (rígido o flexible) consiste en un soporte de almacenamiento externo que complementa a la memoria principal (RAM) de una computadora. Sus propiedades más significativas son: Capacidad de almacenamiento masivo de información en un espacio muy reducido, con el consiguiente bajo coste relativo por byte almacenado. El cliente realiza una llamada a un servicio como si fuera local. El memoria no volátil, ya que mantiene la información almacenada aún a falta de suministro eléctrico. Proporciona acceso casi directo al lugar donde se encuentra el bloque de datos a leer o escribir. La información almacenada en un disco se agrupa en archivos o ficheros (files) identificables por su nombre. Actualmente, la mayoría de procesos de E/S de datos utilizan en su origen o destino los discos magnéticos: La inmensa mayoría de las aplicaciones se encuentran almacenadas en disco (en forma de archivos ‘ejecutables’). Cuando van a utilizarse estas aplicaciones, se copian (en parte) en la memoria principal y son ejecutadas desde ésta. Después de procesar los datos que se encuentran en la memoria principal, los resultados de este proceso se almacenarán en disco. Por último, otra característica a indicar sobre los discos magnéticos (los ‘discos duros’ en este caso), es que se pueden utilizar como memoria virtual; es decir, como una extensión de la memoria principal del ordenador. Estructura Física de los Discos Magnéticos Físicamente, los discos magnéticos están fabricados con: mylard en el caso de los discos flexibles y aluminio o cristal cerámico en el caso de los discos rígidos. La estructura física de un disco la forman unas superficies magnéticas denominadas caras , cada una de las cuales se divide en anillos concéntricos que constituyen las pistas , que a su vez agrupan a los sectores (unidades mínimas de almacenamiento cuya capacidad habitual suele ser de 512 bytes de información). El proceso de grabación de los discos se logra, al igual que en un grabador de audio, por la acción de un campo magnético de polaridad reversible (N-S ó S-N), que imanta la pista al actuar sobre ella. Para este proceso, existe una cabeza para cada cara del disco. Los brazos que soportan a las cabezas se mueven juntos; es decir, que si la cabeza de la cara superior está sobre una determinada pista, la de la cara inferior se encontrará situada en la misma pista. La lectura la realizan las mismas cabezas, mediante un proceso inverso al de grabación, a través del cual detectarán los campos magnéticos existentes a lo largo de la pista accedida. En el proceso, tanto de grabación como de lectura, sólo podrá encontrarse activa una única cabeza de las existentes en el medio magnético (dos en los discos flexibles y múltiples en los rígidos). En las propiedades indicadas en el epígrafe anterior se hacía referencia al acceso casi directo de los discos magnéticos. Conociendo ya la estructura física de estos discos podemos indicar que lo de directo se refiere a la forma de acceso a las pistas y lo de casi hace referencia a la forma de acceso a los sectores una vez situados en la pista correspondiente (este último es un acceso secuencial cuyo tiempo es tan reducido que se considera despreciable). Para esta operación de localización de un sector concreto dentro del disco se emplea lo que se conoce como su dirección o CHS (número de cilindro, número de cabeza, y número de sector). Importancia del concepto de cilindro El hecho de que un disco rígido sea en realidad una agrupación de discos (o platos) cada uno de los cuales dispone de dos caras, además de duplicar la capacidad de almacenamiento, permite la lectura o escritura del doble de datos antes de desplazar el cabezal a otra pista, accediendo a una cara y luego a la contraria. De esto surge el concepto de cilindro que no es más que el conjunto de pistas que se sitúan bajo las cabezas de lectura/escritura en un momento determinado (o conjunto de pistas de un disco que tienen el mismo radio). Este concepto también es aplicable a los discos flexibles, aunque, al disponer estos de dos caras únicamente, se suele asociar a los discos rígidos porque en estos el concepto de cilindro es gráficamente más evidente. De lo anterior se deduce que la mejor forma de graba la información sobre los discos magnéticos es cilindro a cilindro acelerando con ello el proceso de escritura/lectura al minimiza los movimientos de los cabezales en búsqueda de las pistas. El número de cilindros de un disco, por tanto, se corresponderá con el número de posiciones en las que pueden situarse los cabezales; enumerándose aquellos desde 0 (el más exterior) en forma creciente hacia el interior, correspondiendo el número mayor al más interno. Posicionamiento, Latencia y Acceso en un Disco Rígido o Flexible El acceso a un sector situado en una determinada cara del disco, pasa por posicionar el cabezal sobre el cilindro donde se encuentra la pista que contiene el sector, y, posteriormente, esperar a que, mediante el giro del disco, el sector deseado se sitúe debajo de la cabeza. En esta operación intervienen dos tiempos: Posicionamiento . Tiempo necesario para que el brazo con la cabeza correspondiente se coloque directamente sobre el cilindro seleccionado (pocos milisegundos). Latencia (demora rotacional) . Tiempo necesario para que el sector a localizar se sitúe bajo la cabeza lectora/escritora (en promedio es el tiempo de media vuelta). El tiempo de acceso resulta pues, la suma de los anteriores, o lo que sería igual, el tiempo que transcurre desde que la controladora envía la orden al cabezal de posicionarse sobre un cilindro, hasta que la cabeza correspondiente accede al sector buscado. Estructura Lógica de los Discos Magnéticos Desde el punto de vista de la estructura lógica de un disco duro, podríamos dividirlo en: Sector de arranque (Master Boot Record) . Es el primer sector del disco (0, 0, 0), y en él se encuentra la tabla de particiones y un pequeño programa de inicialización. Este programa se ejecuta al encender la computadora, y su función es leer la tabla de particiones y ceder el control a la partición primaria activa. Espacio particionado . Zona del disco que contiene las particiones. Una partición es cada una de las divisiones de tamaño fijo de un disco que se asocia a una unidad lógica (C:, D:, etc, en el caso de los SO Windows). Cada partición ocupa un bloque de cilindros contiguos del disco duro, pudiendo establecerse distintos sistemas de archivos (FAT, NTFS, …) para las distintas particiones posibles. Espacio NO particionado. Se trata de la zona de disco que no ha sido particionada y que, por lo tanto, no puede ser utilizada. La tabla de particiones del disco duro puede contener hasta 4 entradas, lo que determina el número máximo de particiones primarias que se pueden crear en el disco. No obstante, este límite de particiones puede superarse empleando una de las entradas para almacenar una partición extendida (tendríamos 3 primarias y 1 extendida). La partición extendida podrá contener tantas unidades lógicas como necesitemos. La principal diferencia entre las particiones primarias y las extendidas es que, mientras que las primeras son arrancables y pueden ser utilizadas para contener a los SO, las extendidas no son arrancables y se utilizan normalmente para almacenar datos. Además, de entre las distintas particiones primarias, habrá que indicar cuál es la activa, es decir, la verdaderamente arrancable. Visto lo anterior, lo primero que hay que hacer con un disco duro antes de su utilización es: Crear las particiones (utilizando herramientas del tipo FDISK). Formatear las particiones creadas . Este proceso consiste en la creación de la estructura que permita el almacenamiento de información utilizando un determinado sistema de archivos. En el sistema de archivos FAT (MS-DOS y sistemas Windows), la estructura lógica de una partición la forman: el sector de arranque, varias copias de la tabla de asignación de archivos, el directorio raíz y el área de datos. La FAT (tabla de asignación de archivos) es el índice del disco. En ella se indican los clusters (unidades de asignación) que utiliza cada archivo, así como los libres y los defectuosos. La estructura lógica de una partición utilizada por un sistema UNIX tradicional está constituida: un bloque de arranque, un superbloque (contiene el número de inodos, el número de bloques, etc), un vector de inodos (similar a la FAT anterior) y los bloques de datos. Herramientas para la Gestión de Discos Magnéticos Considerando como herramientas de gestión las expuestas en el punto anterior (herramientas de particionado y formateo), en este punto vamos a centrarnos en otras herramientas no tan esenciales como aquellas, pero sí bastante comunes en la gestión de discos magnéticos: Comprobador de errores . Su misión es analizar el contenido del disco en búsqueda de incoherencias en el sistema de archivos. Si, por ejemplo, en un sistema FAT existen dos archivos que apuntan al mismo contenido aparecerá un error de vínculos cruzados o si aparecen datos no asociados a ningún archivo se indicará el error de cadenas perdidas. Comprobadores de errores usuales son: chkdsk /f en MS-DOS, ScanDisk en Windows y fsck en UNIX. Desfragmentador de disco . Esta herramienta busca la agrupación física (sobre el disco) de toda la información concerniente a un mismo archivo, con el fin de acelerar la lectura de datos. La fragmentación se produce por la creación, modificación y eliminación de archivos. El sistema de archivos de UNIX no precisa de desfragmentador debido a que su velocidad de trabajo no se degrada con la creación, modificación y eliminación de archivos. Compresor de datos . Se trata de un método que busca maximizar la capacidad de las particiones mediante la compresión de la información que contienen. No obstante, esta metodología ralentiza el funcionamiento general del SO, debido a que deben ejecutarse continuamente algoritmos de compresión/descompresión. Además, tiene como inconveniente la dependencia de la información del programa de compresión, circunstancia que podría provocar problemas de incompatibilidad d futuros en caso de producirse errores. Normalmente, resulta más eficaz la compresión de ficheros de forma independiente (en lugar de particiones completas). Los SO actuales incorporan sus propios métodos de compresión/descompresión (en UNIX: gzip -&gt; para archivos independientes, tar -&gt; para árboles de archivos). Además, existen herramientas ajenas a los SO para realizar estas operaciones (WinZip, WinRAR, IsoBuster, …) Copias de seguridad . La realización de copias de seguridad del contenido del disco en otro medio de almacenamiento, es un método para garantizar la recuperación de datos destruidos por errores humanos, de situaciones imprevistas o de hardware. Sistemas RAID (Redundant Array Of Independent Or Inexpensive Disks) Los sistemas de matriz de discos independientes (baratos) redundantes son utilizados para el control de errores en los discos. Emplean varios discos para evitar (o minimizar) la pérdida de información en caso de que se produzca algún error. La redundancia hace referencia a la información extra que no sería necesaria si no se produjesen errores. La gestión de los sistemas RAID no es accesible por el usuario, pudiendo ser gestionada por hardware (tarjetas RAID) o por software (SO). como suele ocurrir, el método más eficiente (pero más costoso económicamente) es el que utiliza tarjetas hardware, debido a que desocupa a la CPU de las tareas RAID. A continuación se enumeran los niveles RAID más habituales: RAID 0 (disk striping, discos en bandas). En este nivel, la información se distribuye entre todos los discos que forman el conjunto RAID, proporcionando una mayor velocidad en las transferencias debido al trabajo conjunto de todos los discos para acceder a un mismo archivo. No obstante, si falla alguno de los discos perderemos toda la información. La implementación de RAID 0 precisa de 2 discos como mínimo. RAID 1 (disk mirroring, discos en espejo). Basado en el empleo de discos para duplicar la información. Con este método, cada vez que se escriba en un disco, deberá grabarse la información de su disco copia para mantener la coherencia. A diferencia del método anterior, si en éste falla un disco, el sistema podrá continuar funcionando sin detenerse. Es habitual implementar RAID 1 con 2 discos. Este sistema permite una capacidad de almacenamiento igual a la mitad de la capacidad total de los discos de que disponemos. Pueden combinarse RAID 0 y RAID 1 para formar el sistema RAID 10. Con RAID 10, la información se distribuye en bandas por varios discos y cada disco se duplica, lo que requiere un número par de discos (4, 6, 8, …). RAID 2 . Ofrece detección y corrección de errores en los discos mediante la utilización de códigos de Hamming. Este nivel está incluido en la actualidad en los propios discos, por lo que ha dejado de ser un sistema a elegir por el usuario. RAID 3 . Emplea un disco para almacenar la paridad. La información se distribuye a nivel de bits entre los distintos discos. Si un disco falla, la información se reconstruiría mediante la operación O-exclusiva (XOR) de los discos restantes. Son necesarios un mínimo de 3 discos para implementar un RAID 3. Todos los discos funcionan a la vez, lo que hace bajar el rendimiento con sistemas transaccionales (múltiples accesos sobre pequeñas cantidades de datos). RAID 4 . Utiliza un disco para el almacenamiento de la paridad, al igual que el anterior; si embargo, los datos se distribuyen a nivel de bloque (en lugar de a nivel de bits) y se puede acceder a cada disco de forma individual. Este hecho mejora el rendimiento en sistemas transaccionales. RAID 5 . La paridad se almacena entre todos los discos, eliminando el excesivo uso del disco de paridad que hacían los dos niveles anteriores. Este método es el más eficiente, ofreciendo la mayor tasa rendimiento/coste y el menos coste por megabyte de información. Se necesitan al menos 3 discos para su desarrollo; no obstante, el funcionamiento óptimo se alcanza a partir de los 7 discos. Los últimos tres niveles se denominan de discos en bandas con paridad (disk striping with parity), y en ellos, podremos calcular la capacidad máxima de información que pueden almacenar sumando la capacidad de todos los discos y restándoles la capacidad de uno (redundancia). Otros Medios MagnéticosZIP Tienen un tamaño similar a los floppys de 3,5″, lo que los hace fácilmente portables. Sus capacidades habituales son 100 y 250 Mb, aunque actualmente existen de 750 Mb. JAZZ Son discos similares a los anteriores (son compatibles) pero con capacidades de 1 y 2 Gb. Tecnología Magneto-Óptica LS-120 Superdisk La tecnología Láser Servo fue desarrollada en 1996. Se trata de una tecnología mixta (magnética y óptica) compatible con la de los floppys tradicionales; es decir, un lector/grabador de este tipo puede leer y escribir sus propios discos de 120 Mb y los floppys convencionales de 1,44 Mb. Este sistema es producto de una mezcla de tecnologías de los floppys, discos duros y CD-ROM’s. Combina una medio magnético con un método óptico utilizado para el posicionamiento de las cabezas de lectura/escritura, lo que conduce a un aumento considerable de la capacidad del medio respecto de los floppys, y a lograr velocidades de transferencia de hasta 400 kb/s (la mitad de veloces que los ZIP). Discos Magneto Ópticos (MO) Emplean, para la grabación del medio, un láser que calienta la superficie del disco (302º F). Existen dos variantes de funcionamiento de esta tecnología: El calor provoca la oxidación del metal del medio, lo que permite la orientación de su magnetismo mediante un imán (es la técnica más empleada). El calor cambia la estructura del medio, provocando que sea cristalino o amorfo. Existen discos de 5,25″ (650 Mb, 1.3 Gb, 2.6 Gb y 4.6 Gb) y de 3,5″ (128, 230 y 640 Mb). Cintas Magnéticas Ofrecen una gran capacidad de información junto con velocidades de transferencia muy bajas; motivo por el cual son empleadas casi exclusivamente para realizar copias de seguridad. Suponen un coste ínfimo por Mb. Copias de Seguridad (BACKUP)Las copias de seguridad o backups pueden definirse como copias de la información realizadas usando un medio de almacenamiento secundario, cuyo objetivo es salvaguardar la información ante posibles errores humanos, de hardware, etc. Pérdida de información Las pérdidas de datos pueden provenir de las circunstancias más variadas: Fallo del disco duro. Error humano (eliminación no deseada). Interrupción de una aplicación por fallo durante la grabación de la información en el disco. Acción de un virus o troyano. Accidente inevitable en el entorno del sistema informático (incendio, inundación, etc). Frecuencia de Backups La frecuencia de ejecución de backups dependerá tanto de la frecuencia de actualización de la información del sistema como de la información que el administrador del mismo esté dispuesto a perder. En sistemas relativamente importantes los backups son diarios. Medios empleados para las Backups Las copias de seguridad pueden hacerse, entre otros medios, sobre: Una partición dentro del mismo disco duro que contiene la información a proteger (mínima protección). Un disco duro auxiliar, dentro del mismo equipo donde se encuentra el disco duro con la información a proteger. Un disco duro en un equipo distinto al que contiene la información a proteger (backup por red). Un CD-R, CD-RW, DVD-RW, DVD+RW, etc. Una cinta magnética (tape backup). Floppys, ZIP’s, JAZZ’s, etc. (para copias de pequeñas cantidades de información). Tipos de Backups Completa. Copia de toda la información a salvaguardar. Progresiva o Incremental. Copia de la información nueva o modificada desde el último backup completo o progresivo (se necesitaría la última copia completa y todas las copias incrementales para restaurar la información). Diferencial. Copia de la información nueva o modificada desde la última copia completa (la recuperación de los datos precisa de la última copia completa y la última diferencial). Controles de CambioControlar el Proyecto y Eliminar los RetrasosLos cambios son un pilar básico dentro de la vida del desarrollo de software. En la práctica, el trabajo requiere de una administración formal de los cambios. Si contamos con una administración de cambios del software realmente efectiva podremos conseguir que: Los equipos de desarrollo puedan entregar el software dentro del tiempo y presupuesto establecidos y con una calidad predecible. Los líderes de proyecto conozcan en todo momento el estado y avance del desarrollo del software y tengan certeza del mismo dentro del tiempo prefijado. Los desarrolladores utilicen y controlen con orden y seguridad sus colecciones de archivos y componentes diferentes para cada aplicación. Los ‘probadores’ sepan cuándo una nueva construcción de software requiere ser sometida a un paquete de pruebas y las mejoras o correcciones que debe presentar. Las organizaciones de desarrollo exitosas consideran que el control de cambios durante todo el ciclo es la clave para asignar prioridades a las actividades del equipo, así como para controlar las dificultades que surjan durante el desarrollo. Si no implementamos dicho control, el caos de los cambios se apoderará del control del proyecto. La Administración de la Configuración y Control de Cambios (SCM) es la disciplina de la ingeniería de software que agrupa las herramientas y técnicas de uso de las mismas que una compañía emplea para administrar los cambios de los componentes de software. Cuando la SCM se encuentra integrada en otras actividades del desarrollo (requerimientos, análisis y diseño, construcción, pruebas), se denomina Gestión de Cambio Unificada (UCM). Existen guías que describen cómo controlar, dar seguimiento y monitorear los cambios para permitir un desarrollo iterativo exitoso; así como la forma de establecer espacios de trabajo seguros para cada desarrollador, aislándolo de los cambios realizados en otros espacios de trabajo y controlando los cambios de todos los artefactos de software (modelos, código, documentos, etc) Para llevar a cabo estas metodologías, se utilizan herramientas de ‘control de versiones y configuraciones’ y de ‘control de cambios’ que, por un lado, automatizan las metodologías, y por otro unen al equipo de desarrollo para conseguir un trabajo paralelo y coordinado. Estas herramientas permiten a cada desarrollador contar con un espacio de trabajo seguro donde puede realizar los cambios de manera independiente para que una vez probados puedan integrarse con el resto del desarrollo, garantizando de esta forma la calidad, el tiempo de entrega y la satisfacción del cliente con el producto desarrollado. Gestión de CambiosPodemos distinguir dos enfoques diferentes dentro de la gestión de cambios, dependiendo del mayor o menor grado de modificación del producto. Si el cambio a realizar afecta a gran parte de los componentes del producto, podrá plantearse como un nuevo desarrollo, y aplicar un nuevo ciclo de vida desde el principio, aunque aprovechando lo ya desarrollado de la misma forma que se reutilizan los prototipos. Si el cambio afecta a una parte bastante localizada del producto, entonces se puede organizar como simple modificación de elementos. Hay que tener en cuenta que cualquier cambio en el código del producto software siempre implicará una revisión de los elementos de documentación afectados; es decir, cambiar el código de algunos módulos puede requerir, además, modificar los documentos de diseño o incluso, en el caso de mantenimiento perfectivo, modificar el documento de especificación de requisitos. Tomando como referencia la gestión, la realización de cambios se puede controlar mediante dos clases de documentos, que en ocasiones pueden unirse para forma un único informe: Informe de problema: describe una dificultar en la utilización del software que precisa de alguna modificación para subsanarla. Informe de cambio: describe la solución dada a un problema y el cambio realizado en el producto software. El primer documento puede ser originado por los propios usuarios. Este informe se pasa a un grupo de ingeniería para la comprobación y codificación del problema planteado, y posteriormente a un grupo de gestión para decidir la solución a adoptar. Este grupo de gestión da comienzo al informe de cambio, que se pasa de nuevo al grupo de ingeniería para su total desarrollo y ejecución. Comprobación de los Objetivos del Control de CambiosLos objetivos del control de cambios son comprobados al realizar entrevistas con: El Director de Tecnologías de Información. La Administración de la Función de Servicios de Información. La Administración de Desarrollo de Sistemas, Aseguramiento de la Calidad de Control de Cambios, Operaciones y Seguridad. La Administración de Usuarios implicada en el diseño y manejo de aplicaciones de sistemas de información. De las que se obtienen: Procedimientos organizacionales relacionados con la planificación de sistemas de información, el control de cambios, la seguridad y el ciclo de vida de desarrollo de sistemas. Procedimientos de la función de servicios de sistemas de información relacionados con la metodología del ciclo de vida de desarrollo de sistemas, el aseguramiento independiente de la calidad, los estándares de seguridad, la implementación, la distribución, el mantenimiento, la liberación del software, los cambios de emergencia y el control de versiones del sistema. Un plan de desarrollo de aplicaciones. Formato y bitácora de requisitos de control de cambios. Contratos con proveedores relacionados con servicios de desarrollo de aplicaciones. Evaluación de los ControlesLa evaluación de los controles de cambios deberá tener en cuenta si: La bitácora de control de cambios garantiza que cualquiera de los cambios mostrados han sido resueltos. El control de cambios es un procedimiento formal tanto para los grupos de desarrollo como para los usuarios. El usuario está conforme con el resultado de los cambios solicitados, el tiempo de realización de los mismos y los costes. Para una muestra de cambios en la bitácora de control de cambios: la documentación actual refleja con exactitud el ambiente modificado, los cambios hayan sido efectuados tal y como fueron documentados, el cambio implicó modificaciones en los programas y operaciones. El proceso de cambios es controlado respecto de las mejoras en el conocimiento, la efectividad en el tiempo de respuesta y la satisfacción del usuario con el resultado del proceso. El mantenimiento del sistema de Intercambio de Rama Privada (PBX) se incluye en los procedimientos de control de cambios. Evaluación de la SuficienciaLa comprobación de la suficiencia del control se realizará probando que: Para una selección de cambios, la administración ha aprobado los siguientes puntos: petición del cambio, descripción del cambio, acceso al programa fuente, terminación del cambio por parte del programador, solicitud para trasladar el programa fuente al entorno de prueba, finalización de las pruebas de aceptación, solicitud de compilación y traslado al grupo de producción, identificación y aceptación del impacto general y específico, elaboración de un proceso de distribución, y revisando el control de cambios en cuanto a la inclusión de: fecha del cambio solicitado, persona o grupo que lo solicita, petición aprobada de cambios, aprobación del cambio efectuado (servicios de información), aprobación del cambio efectuado (usuarios), fecha de actualización de la documentación, fecha del traslado al grupo de producción, aprobación del cambio por parte del grupo de aseguramiento de la calidad, aceptación por parte del grupo de operaciones. También se tendrán en cuenta, a la hora de evaluar esta suficiencia: Los tipos de análisis de cambios realizados sobre el sistema para la determinación de tendencias. La valoración de la adecuación de las librerías de la función de servicios de información y la identificación de la existencia de niveles de código base para advertir y prevenir la ocurrencia de errores. Si existen procedimientos de E/S (“check in/check out”) para cambios. Si la totalidad de los cambios en la bitácora fueron resueltos con la conformidad de los usuarios y si no se llevaron a cabo cambios que no hayan sido anteriormente especificados en la bitácora. Si los usuarios tienen conocimiento de la necesidad de procedimientos formales de control de cambios. El proceso de reforzamiento del personal garantiza el cumplimiento de cada uno de los procedimientos de control de cambios. Evaluación del Riesgo de los Objetivos de Control NO AlcanzadosEsta evaluación se realizará llevando a cabo mediciones (“benchmarking”) de la administración del control de cambios contra organizaciones similares o estándares internacionales de buenas prácticas reconocidas en la industria correspondiente. Para sistemas seleccionados de la función de servicios de información, se ejecutará: Una verificación para comprobar si la documentación determina el requerimiento o si el cambio del sistema ha sido aprobado y priorizado por parte de la administración de las áreas usuarias afectadas y el proveedor de servicios. La confirmación de la existencia y adecuación de evaluación del impacto en formas de control de cambios. La obtención del conocimiento del cambio mediante una comunicación de la función de servicios de información. La asignación del cambio a los correspondientes recursos de desarrollo. La adecuación de los sistemas y los planes de prueba de los usuarios. La migración formal de prueba a producción a través del grupo de aseguramiento de la calidad. La puesta al día de los manuales de usuario y de operación para mostrar el cambio efectuado. El reparto de la nueva versión a los usuarios correspondientes. Además, la evaluación de este riesgo concluirá determinando, para una selección de cambios de información, que: Sólo se efectuaron cambios que hayan sido aprobados por la función de servicios de información. Todos los cambios han sido tenidos en cuenta. Las librerías actuales (fuente y objeto) muestran los últimos cambios llevados a cabo. Las modificaciones en el procedimiento de control de cambios de: aplicaciones internas y adquiridas, software de sistemas y de aplicación, gestión del control de cambios por parte del proveedor. Bibliografía Scribd (Ibiza Ales) Administración de redes locales. Gestión de usuarios. Gestión de dispositivos. Monitorización y control de tráfico. Gestión SNMP. Gestión de incidencias.IntroducciónLos detalles de administración de una red dependen en gran medida del SO de red utilizado. Se ha optado por desarrollar la mayor parte de los apartados desde la perspectiva de dos SO: Windows 2000 y Red Hat Linux 6.0. La estructura general del presente tema será la siguiente. En primer lugar se realiza la introducción a las funciones del administrador de redes. A continuación se presentan algunos conceptos básicos de TCP/IP necesarios para comprender la mayoría de las herramientas de administración de red. En tercer lugar se presentan algunas de las facetas de administración de redes bajo el prisma de los dos SO elegidos. Por último se hace un breve repaso a las herramientas de monitorización de tráfico y supervisión de red. Funciones del Administrador de la RedLas funciones de un administrador de red son tan amplias y variadas como desconocidas en muchos ámbitos. Frente a la idea preconcebida y bastante generalizada de que un administrador es la persona encargada de dar de alta a los usuarios y administrar el acceso a los recursos más habituales como programas, archivos e impresoras, encontramos una realidad mucho más amplia. Para que una red funcione correctamente y esté preparada para crecer en un entorno de alta demanda de recursos es necesario realizar un importante trabajo que no siempre es conocido. Las funciones de un administrador de red pueden dividirse en varias categorías principales: Administración de usuarios. Mantenimiento del hardware y software. Configuración de nodos y recursos de red. Configuración de servicios y aplicaciones internet. Seguridad de red. Supervisión y optimización. Administración de usuariosDesde el momento en que se crea una cuenta de usuario para su acceso a la red es necesario estar preparado para darle el soporte necesario con el fin de facilitarle el acceso a los recursos del sistema. Algunas de las tareas más comunes relacionadas con la gestión de usuarios son: Creación y borrado de cuentas de usuario. Creación de grupos. Formación de usuarios. Definición de políticas de uso. Configuración de sistema de ayuda para gestionar el soporte a los usuarios. Asignación de espacio de almacenamiento y ficheros en el sistema de archivos compartido. Creación de cuentas de correo electrónico. Mantenimiento del hardware y softwareFundamentalmente este punto cubre la necesidad de establecer y mantener los recursos de infraestructura de la red. En este punto podemos destacar las siguientes tareas: Instalación y configuración del SO. Administración de discos y del sistema de archivos. Administración RAID. Instalación de controladores de dispositivos. Recuperación y diagnóstico de fallos del sistema. Adición y eliminación de hardware del sistema. Configuración de nodos y recursos de redPara que la red funcione como un sistema coherente con entidad propia, y no como una serie de nodos aislados, es necesario llevar a cabo una serie de tareas: Instalación y configuración de tarjetas de red. Conexión de host a concentradores y conmutadores. Conexión de host a routers. Configuración de routers. Configuración de impresión en red y compartición de archivos. Administración de DNS. Integración de diferentes SO de red. Configuración de servicios y aplicaciones internetActualmente son pocos las redes locales que pueden concebirse sin pensar en una interconexión con la red WAN más extendida: las red internet. Por ello, dentro de las funciones de un administrador de red debemos incluir: Configuración de servidores y clientes de correo electrónico. Instalación y configuración de servidores web. Instalación y configuración de servidores de aplicaciones. Instalación y configuración de servidores de BD. Seguridad de redEl impresionante auge de Internet en los últimos años ha provocado un crecimiento exponencial de los usuarios de esta red. Este hecho proporciona una serie de ventajas derivadas de la diversidad de los servicios e información que podemos encontrar en Internet, pero también supone un gran potencial para el uso indebido y la actividad criminal. Por otra parte, es necesario contemplar también el aspecto de seguridad interna, dentro de la propia red local. Para ello, deben considerarse las siguientes actividades: Configuración de firewalls. Acceso remoto a la red. Creación de directivas de seguridad. Supervisión y OptimizaciónEs necesario controlar los problemas derivados de un malfuncionamiento de la red, de un mal uso de la misma o de debilidades en la estrategia de seguridad: Afinar el rendimiento de la red. Supervisión y registro. Gestión de cambios. Auditoria y pruebas. Conceptos básicos de TCP/IP para AdministradoresRedes TCP/IPEl propósito de TCP/IP es proporcionar a los equipos un método para transmitir datos entre sí. TCP/IP proporciona los medios para que las aplicaciones envíen datos entre redes y para que las redes entreguen esos datos a las aplicaciones de otros equipos o host . En TCP/IP normalmente a los host se les asignan nombres que corresponden a sus direcciones IP, para facilitar la tarea de identificación de las máquinas. La transmisión de datos en TCP/IP se realiza con paquetes de datos. Los paquetes son bloques diferenciados de datos que se transmiten de forma independiente de la red. Cada paquete contiene los datos que cada host desea compartir, así como la información necesaria para encaminar el paquete al destino correcto a través de la red. Cada protocolo agrega sus propios datos en el encabezado para establecer la información que le permita mantener una conversación lógica con el protocolo homónimo en el host con el que transfiere información. La siguiente figura muestra un ejemplo simplificado del intercambio de información que se produce entre una estación de trabajo y un host para intercambiar una página HTML a través del protocolo HTTP sobre TCP/IP: Como puede verse, el protocolo TCP proporciona a los dos nodos que se comunican los recursos necesarios para establecer sesiones de diálogo. Una vez establecida dicha sesión, se transfieren paquetes de datos con un protocolo http, situado en un nivel superior de la pila de protocolos. TCP/IP frente al Modelo OSITCP/IP es una familia de protocolos y aplicaciones que realizan funciones diferenciadas que se corresponden con capas específicas del modelo OSI (Open System Interconnection). En este punto es importante establecer un matiz importante: TCP/IP no es una familia de protocolos del modelo OSI. De hecho, el modelo OSI nació de la necesidad de establecer un estándar que garantizara la interconexión de sistemas y equipos de diferentes fabricantes, cuando el estándar de facto que se ha impuesto mayoritariamente en el mercado es TCP/IP. Sin embargo, si existe una clara similitud entre TCP/IP y OSI en cuanto a la separación de responsabilidades a través del establecimiento de capas de protocolos, tal como muestra la siguiente tabla: IP es el protocolo de capa de red más utilizado en todo el mundo. La capa de red tiene dos misiones fundamentales: Proporcionar una dirección única a cada host de la red: los host de cada red concreta tienen direcciones que los identifican unívocamente como miembros de dicha red. Encaminar los paquetes a través de la red, seleccionando el camino más adecuado. TCP es un protocolo orientado a conexión de nivel de transporte que asigna a las aplicaciones y protocolos de nivel superior un número de puerto. Estos números se transmiten junto con los datos a través de la red y son utilizados por el receptor para determinar a qué proceso debe entregar los datos recibidos. TCP/IP no implementa estrictamente los niveles 5, 6 y 7 del modelo OSI como componentes independientes y diferenciados. Las aplicaciones que utilizan TCP/IP normalmente implementan alguno de estos niveles, o todos, dentro de la aplicación según sea necesario. Direccionamiento IPCada dirección IP está formada por un conjunto de 32 bits, organizados en 4 grupos de 8 bits cada uno. A cada grupo se le denomina octeto y está separado de los demás por un punto. Normalmente las direcciones IP se escriben en formato decimal, y tienen el siguiente aspecto: 192.168.15.1 No obstante, en muchas ocasiones resulta útil representar las direcciones IP en formato binario, considerando los bits de orden superior los situados a la izquierda y los de orden inferior los situados a la derecha: 11000000.10101000.00001111.00000001 Cada dirección IP contiene una parte de host y una parte de red. El espacio de direcciones disponible se divide en redes de clase A, clase B y clase C. Cada red contiene diferentes cantidades de redes y host por clase. Esto se consigue reservando para cada clase un número de bits de orden superior diferente para la dirección de red, tal como muestra la siguiente tabla: Este esquema está pensado para asignar a las empresas y organizaciones direcciones IP de acuerdo con el número de host para los que necesitan una dirección. Las direcciones de clase A se asignan únicamente a grandes organizaciones y empresas, mientras que las de clase C se asignan a organizaciones y empresas pequeñas. Las redes 0 y 127 están reservadas para propósitos especiales. Es importante destacar la diferencia entre una dirección IP pública y una dirección IP privada. En el ámbito de la red internet, una IP pública es una dirección de red accesible y reconocible desde cualquier punto de la red. Los organismos encargados de asignar direcciones IP públicas lo hacen siguiente el criterio de clases expuesto en la tabla anterior según las necesidades de cada compañía. Una dirección IP privada es aquella que únicamente es visible desde un ámbito de red local, pero no desde el ámbito de internet. En este sentido, las compañías tienen total libertad para escoger direcciones IP de la clase que les resulte más conveniente, ya que no interfieren con el mundo exterior. Segmentación de RedesA menudo puede resultar conveniente subdividir una red local en varias subredes. Los motivos pueden ser diversos: Si el número de terminales y host es muy elevado. Para reducir el número de colisiones en la red. Para facilitar la administración. Permite crear dominios o subredes lógicas en las que un conjunto relativamente homogéneo de usuarios accede con frecuencia al mismo conjunto de recursos compartidos. Para subdividir el espacio de direcciones de una red IP, se ha provisto a la dirección IP de dos componentes: la dirección IP en sí misma, y la máscara de subred. Dirección IP: 192.168.15.1Máscara de subred: 255.255.255.0 La máscara de subred, en su representación binaria, está formada por una serie de bits a 1 en la parte de orden superior, y una serie de bits a 0 en la parte de orden inferior. Sin dividirla en subredes, una red de clase A tiene una máscara de subred de 8 bits (255.0.0.0); una rede de clase B tiene una máscara de 16 bits (255.255.0.0); y una de clase C tendrá una máscara de subred de 24 bits (255.255.255.0). Es decir, en cualquiera de los casos para una red sin subdivisiones la máscara de red está formada por el número de bits necesarios para determinar la clase de la red más el número de bits disponibles para identificar la red. Para subdividir una red se agregan bits a 1 por la derecha a la parte de red de la máscara de subred. Al hacerlo, es necesario tener en cuenta las siguientes consideraciones: La primera subred (todos los 0) de cualquier red está reservada. Las direcciones primera y última de los host de cualquier subred están reservadas como direcciones de difusión. En el siguiente ejemplo se muestra una red de clase C dividida en 8 subredes: Dirección IP: 192.168.1.1Máscara de subred: 255.255.255.224 Es decir, se han añadido 3 bits a la máscara de red. Esto también suele expresarse de la siguiente forma: 192.168.1.1/27 que indica que la máscara de red tiene en la parte de orden superior 27 bits a 1. El número de subredes en los que se divide la red se calcula como 2 elevado al número de bits añadidos. En este ejemplo, 8 subredes. A esta técnica para subdividir redes también se le conoce con el nombre de subnetting. Sistema DNSCada host dentro de una red local está unívocamente identificado por una dirección IP. No obstante, resultaría bastante complicado memorizar las direcciones IP de todos los host. Por este motivo, se ha creado un sistema que establece una asociación entre la dirección IP y un nombre simbólico que resulta fácilmente memorizable. Además de esto, el establecimiento de mnemónicos para identificar las máquinas permite modificar fácilmente el direccionamiento de forma transparente al usuario: se modifica la dirección IP pero no se cambia el mnemónico. Inicialmente, todos los registros de nombres de host se administraban en un repositorio oficial gestionado por el NIC (Network Information Center). Cada sitio Internet tenía que actualizar su copia local de este repositorio cuando cambiaba. La dificultad de mantener este esquema trajo consigo la creación del servicio DNS (Domain Name Service). El servicio DNS utiliza una BD distribuida de los registros de nombres que se envía automáticamente a los host que lo necesitan. El DNS utiliza una jerarquía de nombres similar a la usada por los nombres de archivos de un sistema UNIX. Hay un dominio raíz en lo más alto de la jerarquía al que sirven un grupo de servidores de dominios denominados servidores raíz. La siguiente figura representa esta situación: Justo debajo del dominio raíz se encuentran los dominios de primer nivel, divididos en dos categorías: territoriales y genéricos. Cada país tiene asociado un dominio territorial representado por un código de dos letras (es, para España; uk, para Reino Unido; etc). Los dominios genéricos son administrados por una empresa llamada Internic . Para solicitar un dominio de segundo nivel es necesario solicitarlo a Internic y abonar una cuota anual. Por otra parte, los dominios territoriales son administrados por organismos nacionales. La siguiente tabla muestra una relación de dominios genéricos: Un servidor local de nombres DNS envía una consulta a un servidor raíz de nombres cuando un cliente envía una consulta de búsqueda directa solicitando una dirección IP desde un dominio para el que el servidor local de nombres DNS no tiene autoridad. WindowsMicrosoft Windows utiliza DNS como método principal para la resolución de nombres, pero sigue siendo compatible con el servicio de nombres Internet de Windows (WINS, Windows Internet Naming Services). WINS es el método de resolución de nombres utilizado en Microsoft Windows NT versión 4.0 y anteriores. WINS también suele ser necesario por compatibilidad para resolución de nombres con máquinas Windows 95 y 98. Es decir, en primer lugar el servidor que necesita obtener la IP busca en el fichero hosts la correspondencia; si no la encuentra busca a través del servicio DNS; como último recurso, busca en el servicio de WINS y si tampoco lo encuentra lanza un broadcast a la red. El servidio Servidor DNS de Windows proporciona la capacidad de utilizar nombres de dominios completo (FQDN, Fully Qualified Domain Names) jerárquicos en lugar de las convenciones de nomenclatura de NetBIOS que admite WINS. Los clientes utilizan el servicio Servidor DNS para la resolución de nombres y la ubicación de servicios, incluida la ubicación de controladores de dominio que proporcionan autenticación de usuarios. Red Hat LinuxEl software DNS más utilizado para sistemas UNIX y LINUX es BIND (Berkeley Internet Net Daemon). Los servidores de nombres se pueden dividir en tres categorías dependiendo de cómo estén configurados: Maestro: servidor a partir del cual se derivan todos los datos relativos a un dominio. Los datos son suministrados directamente por el administrador. Esclavo: al igual que el maestro tiene toda la información sobre un dominio, pero los obtiene del servidor primario. De sólo caché: guarda en la caché los resultados de las consultas, siendo éste el único medio que tiene para obtener los datos. Todos los sistemas UNIX y LINUX tienen una biblioteca denominada resolver . Esta biblioteca está asociada a un archivo de configuración /etc/resolv.conf que permite especificar los servidores DNS a los que se va a consultar. Para instalar un servidor DNS en un sistema Red Hat se deben instalar los paquetes bind y caching-nameserver . La configuración de BIND normalmente se almacena en el archivo /etc/named.conf . En este archivo se especifican los siguientes parámetros: Opciones globales: nombre del directorio de trabajo del servidor; direcciones IP de servidores DNS alternativos a los que se pueden redirigir las peticiones; prevalencia de los servidores alternativos frente al propio host. Instrucciones de zona: las zonas BIND son de cuatro clases: maestras (primario), esclavas (secundario), reducidas y de ayuda (caché). Por otra parte, existen una serie de archivos denominados archivos de zona que almacenan la información de las BD de los dominios. Cada archivo de zona tiene una serie de registro de recursos (RR), compuesto por los siguientes campos: Nombre del host al que se hace referencia. TTL (Time to Live): tiempo, en segundos, que debe almacenarse la información del registro en una caché remota. Clase: IN, para Internet. Tipo de registro (NS, Name Server; A, address; PTR, pointer; etc). Datos relativos al registro. Gestión de usuariosLa atención a los usuarios y el establecimiento de normas que regulen el uso de la red constituyen una de las razones de ser de los administradores de sistemas. Esta atención va mucho más allá del mero hecho de administrar las cuentas de acceso al sistema, o instalar un software en la estación de trabajo del usuario. Para que la interacción de los usuarios y los administradores de red sea óptima es necesario un apoyo por parte de la dirección de la compañía u organización en la aplicación de estas normas, de forma que quede claro cómo y cuando deben notificar los usuarios los problemas del sistema al administrador, cómo solicitar que los usuarios lleven a cabo parte del trabajo y cómo hacer un seguimiento de estos problemas y peticiones. Algunas de las normas a tener en cuenta son las siguientes: El usuario no es ignorante: simplemente carece del nivel de conocimientos técnicos sobre la red que tienen los administradores de sistemas. El usuario debe recibir el soporte que solicita, siempre que lo haga por las vías establecidas. Todos los usuarios deben ser tratados por el mismo patrón, sin favoritismos. Las normas deben ser debidamente documentadas y difundidas. Una buena posibilidad es crear documentos en formatos HTML y publicarlos en un servidor web al que tengan acceso todos los usuarios de la red. Es necesario revisar periódicamente las normas, para mejorarlas e incorporar nuevos aspectos. Las interrupciones de los usuarios deben ser canalizadas convenientemente. Probablemente la mejor opción es una situación de equilibrio entre una atención directa al usuario para incidencias sencillas y una gestión de avisos por correo electrónico para incidencias más complicadas. La formación a los usuarios es uno de los elementos clave para reducir las incidencias. Una forma sencilla de ofrecer la formación básica es elaborar una guía de orientación de sistemas que describa donde encontrar determinados elementos en la red, como solicitar nuevo software y cómo utilizar determinadas herramientas. Es recomendable la utilización de herramientas online para plantear soluciones y obtener respuestas. Estas herramientas se suelen conocer como escritorios de ayuda . Las características de un buen escritorio de ayuda son: Deben ofrecer una interfaz simple a los usuarios. Deben ofrecer un mecanismo que informe a los usuarios sobre el estado de su petición. Deben permitir elaborar automáticamente estadísticas e informes de las incidencias recibidas. Debe tener un sistema para establecer prioridades. Entre los escritorios de ayuda que podemos encontrar se pueden citar WREQ, herramienta de código abierta utilizada habitualmente en sistemas Linux. En sistemas con Windows pueden utilizarse las directivas de grupo ofrecidas por el sistema para reducir los costes de operación de la red. Una directiva de grupo permite establecer plantillas administrativas para configurar las aplicaciones, establecer la apariencia de los escritorios y el comportamiento de los servicios del sistema, manejar opciones para la configuración de los equipos locales y la red, administración central de instalación de software, etc. Una vez realizada esta introducción, vamos a ver en detalle el procedimiento para gestionar usuarios y sus privilegios de acceso a los recursos del sistema en los SO de red que estamos tratando. WindowsIntroducción al Active Directory La administración de usuarios, grupos y privilegios en Windows se realiza a través del Active Directory . Active Directory es el servicio de directorio de una red Windows. Un servicio de directorio es un servicio de red que almacena información sobre los recursos de la red, incluyendo su nombre, descripción, claves de búsqueda, etc. El Active Directory ofrece un medio para organizar y administrar de forma centralizada el acceso a los recursos de la red, haciendo transparente la topología de la red y sus protocolos. Un usuario puede tener acceso a cualquier recurso sin saber cómo está conectado físicamente. El Active Directory organiza la información que almacena en secciones que permiten el almacenamiento de una gran cantidad de objetos. Esta estructura proporciona un método para diseñar una jerarquía de directorios comprensible para los usuarios y los administradores. La siguiente figura muestra los componentes lógicos de la estructura del Active Directory: Dominios : conjunto de equipos definidos por el administrador que comparten una BD de directorio común. En una red Windows, el dominio sirve como límite de seguridad, de forma que el administrador de un dominio tienen los permisos necesarios para llevar a cabo la administración únicamente dentro de ese dominio. Una red de múltiples dominios es útil para organizaciones que utilizan un modelo administrativo descentralizado. Cada dominio es gestionado por un servidor denominado controlador de dominio . Los controladores de dominio mantienen la información completa del directorio asociado a su dominio. Unidades organizativas (OU) : objeto contenedor que se utiliza para organizar objetos dentro de un dominio. Una unidad organizativa puede contener objetos como cuentas de usuarios, grupos, equipos, impresora, etc. Las unidades organizativas permiten delegar el control administrativo sobre los objetos que contienen, asignando a usuarios o grupos de usuarios permisos específicos. Las unidades organizativas suelen formar una estructura jerárquica que representa la propia estructura organizativa de la organización o un modelo administrativo de red. Árboles : disposición jerárquica de dominios que comparten un espacio de nombres contiguo. Cuando se agrega un dominio a un árbol existente, el nombre del dominio secundario (el recién añadido) se combina con el nombre del dominio principal para formar su nombre DNS. Algunas de las razones para tener más de un dominio son: Administración de red descentralizada. Mejor control para la duplicación de los directorios. Necesidad de gestionar una gran cantidad de objetos en la red. Nombres de dominio de Internet diferentes. Requisitos de contraseña diferentes entre organizaciones. Bosques : grupo de árboles que no comparten un espacio de nombres contiguo. Los árboles de un bosque comparten una configuración, un esquema y un catálogo global comunes. El procedimiento para crear la estructura del Active Directory es crear en primer lugar la estructura de dominios, para completar luego la estructura lógica de la red con una jerarquía de unidades organizativas que contengan usuarios, equipos y otros recursos. Por último, se controlará el acceso a los objetos del Active Directory mediante el establecimiento de permisos. Cuentas de Usuario Las cuentas de usuario se emplean para autenticar al usuario y para conceder permisos de acceso a los recursos de red. Una cuenta de usuario se crea dentro de una unidad organizativa, realizando las siguientes operaciones: En el menú Herramientas Administrativas , abrir Usuarios y equipos de Active Directory . Pulsar el botón derecho del ratón en la unidad organizativa deseada, seleccionar Nuevo y pulsar Usuario . Al hacerlo aparecerá el cuadro de diálogo de la siguiente figura (todo depende del sistema Windows utilizado): Cuentas de Equipo Las cuentas de equipo son similares a las cuentas de usuario en el sentido de que pueden utilizarse para autenticar y auditar el equipo, y para conceder permisos a los recursos de red. Los equipos cliente deben tener una cuenta de equipo válida para poder unirse al dominio. Para crear una cuenta de equipo se realizan las siguientes operaciones: En el menú Herramientas Administrativas , abrir Usuarios y equipos de Active Directory . Pulsar el botón derecho del ratón en la unidad organizativa deseada, seleccionar Nuevo y pulsar Equipo . Escribir un nombre de equipo que sea único dentro del bosque. Designar al usuario o grupo de usuarios con permisos para hacer que el equipo se una al dominio. Esto hará que durante el proceso de unión del equipo al dominio el sistema muestra un diálogo solicitando una cuenta válida para realizar la operación. Grupos de Usuarios Los grupos de usuarios son un mecanismo para simplificar la administración de permisos. Asignar una sola vez un permiso a un grupo de usuarios es más sencillo que hacerlo tantas veces como usuarios tenga el grupo de forma aislada. Existen dos tipos de grupos de Active Directory: Grupos de seguridad: se utilizan para conceder o denegar permisos. Grupos de distribución: se utilizan para enviar mensajes de correo electrónico. No pueden utilizarse con fines de seguridad. Los grupos de distribución y seguridad tienen además un atributo de ámbito, que determina quién puede ser miembro del grupo y dónde puede usar ese grupo en la red: Grupos locales de dominio: pueden contener usuarios, grupos globales y grupos universales pertenecientes a cualquier dominio del bosque, así como grupos locales del mismo dominio. Grupos globales: pueden contener cuentas de usuario y grupos globales del dominio en el que existe el grupo. Grupos universales: pueden contener cuentas de usuario, grupos globales y otros grupos universales de cualquier dominio de Windows en el bosque. Para crear un grupo se realizan las siguientes operaciones: En el menú Herramientas Administrativas , abrir Usuarios y equipos de Active Directory . Pulsar el botón derecho del ratón en la unidad organizativa deseada, seleccionar Nuevo y pulsar Grupo . Al hacerlo se obtiene un cuadro de diálogo desde el que se pueden establecer las opciones para el grupo creado y agregar miembros (otros grupos o cuentas de usuario). Red Hat LinuxConsideraciones generales En Linux los usuarios y grupos se utilizan para determinar el propietario y permisos de los archivos y dispositivos, y para establecer permisos de acceso a casi todas las funciones del sistema. El programa Linuxconf permite acceder a la información de usuarios y grupos. Para administrar convenientemente los permisos en Linux hay que evitar en la medida de los posible el uso de cuentas compartidas, por varios motivos: Es importante controlar las acciones de los usuarios para saber quiénes actúan correctamente y quiénes no lo hacen. El uso del mismo identificador de usuario por varias personas hace que esta tarea sea inviable. Asimismo, por motivos de seguridad resulta completamente desaconsejable que un grupo de usuarios comparta la misma contraseña para acceder al sistema. Si se utiliza un inicio de sesión compartido los archivos de configuración no pueden ser específicos para cada usuario. Por lo tanto, todos deberán utilizar la misma estructura de escritorio y los mismos alias de la shell . Esto significa que todos los cambios realizados pueden afectar al entorno de trabajo de los usuarios que comparten la cuenta, y que cualquiera de ellos puede leer un e-mail enviado a la cuenta compartida. Por último, como varias personas pueden acceder a un archivo al mismo tiempo, la última modificación sobrescribirá los cambios que puedan haber realizado otros usuarios. En vez de utilizar cuentas compartidas, resulta más recomendable utilizar grupos para proporcionar a los usuarios el acceso a las mismas partes del sistema de archivos manteniendo distintos identificadores de inicio de sesión, archivos de configuración y directorios principales. Archivos del Sistema Existen una serie de archivos importantes para configurar y modificar los usuarios y grupos del sistema, que se describen a continuación: /etc/passwd : Contiene información sobre los identificadores de los usuarios, los directorios principales y los comandos que se ejecutan cuando se inicia una sesión. /etc/group : Contiene la información sobre los grupos del sistema. /etc/shadow : Archivo utilizado para ocultar las contraseñas del archivo /etc/passwd , cuya lectura es universal. Contiene la cadena de la contraseña codificada e información sobre el vencimiento de las cuentas. /etc/gshadow : Realiza para los grupos la misma función que hace /etc/shadow para los usuarios. Contiene las contraseñas codificadas para los grupos e información de las cuentas. /etc/login.defs : Contiene información predeterminada sobre la creación y mantenimiento de las cuentas. /etc/skel : Contiene archivos de configuración predeterminados que se utilizan al crear cuentas nuevas. En este directorio se guardan los archivos de configuración de la shell, los archivos de configuración del administrador de ventanas o los archivos y directorio que un usuario deba tener en su directorio principal. Configuración de Cuentas de Usuario Para crear nuevos usuarios en Linux es posible seguir varios procedimientos. Se pueden modificar directamente los archivos de sistema expuestos en el punto anterior, pero esto supone una labor bastante tediosa. El comando useradd simplifica estas operaciones, permitiéndonos crear nuevos usuarios en una sola línea. Sin embargo, la forma más cómoda de hacerlo es utilizar una herramienta GUI como Linuxconf. Este programa puede ejecutarse en X-Windows o en línea de comandos. Configuración de Grupos de Usuarios Al igual que ocurre con las cuentas de usuario, en Linux existen diversas formas de configurar los grupos de usuarios. Esta tarea puede realizarse mediante comandos del sistema o utilizando una herramienta GUI como Linuxconf. A continuación se presentan algunos de los comandos y scripts de shell para gestión de grupos: groupadd : añade un grupo al archivo /etc/group groupdel : elimina un grupo del archivo /etc/group groupmod : modifica las entradas en /etc/group grpck : realiza una comprobación de integridad del archivo /etc/group Gestión de privilegios Existen varias alternativas para establecer los permisos de acceso de los usuarios a archivos y directorios. El comando chown permite otorgar a un usuario la propiedad de un archivo o directorio. No obstante, éste no es el mejor método si necesitamos que varios usuarios accedan a un archivo, ya que sólo puede existir un propietario en un momento dado. Otra opción mejor es utilizar el comando chmod para conceder al usuario los permisos necesarios. Si el usuario no es propietario del archivo, será necesario cambiar la configuración de permisos para ‘otros’, de forma que los cambios se apliquen a todos los usuarios que no sean propietarios del archivo. Por último, se puede conceder permisos a los usuarios utilizando la configuración del grupo propietario de un archivo y estableciendo los permisos para grupos. Manipulando los permisos de los grupos en un archivo o dispositivo es posible proporcionar o anular el acceso a ese archivo a nivel de grupo. Otros comandos para configuración de Usuarios y Grupos En este apartado se detallan otros comandos de shell útiles para la configuración de usuarios y grupos de usuarios: newusers : añade usuarios nuevos al sistema en modo de procesamiento por lotes. chpasswd : actualiza el archivo de contraseñas en modo batch . Lee parejas nombre_usuario-contraseña de la entrada estándar y actualiza la contraseña para el usuario dado. usermod : permite modificar o desactivar cuentas de usuario. Afecta a los archivos /etc/passwd y /etc/shadow . userdel : elimina una cuenta de usuario. newgrp : permite cambiar temporalmente el grupo predeterminado a otro grupo del que sea miembro o del que conozca la contraseña. chage : permite configurar el contenido de /etc/shadow . chgrp : permite cambiar el grupo propietario de un archivo. chown : permite cambiar el usuario propietario de un archivo. chsh : cambia la shell de inicio de sesión. gpasswd : permite a los administradores añadir y eliminar usuarios de los grupos y cambiar las contraseñas de los mismos. groups : imprime los grupos de los cuales es miembro un usuario. passwd : cambia las contraseñas. _su_ : permite a un usuario hacerse pasar temporalmente por otro. Administración de Recursos de Impresión y ArchivosWindowsCompartición de impresoras Compartir una impresora de red en el entorno Windows es bastante sencillo. Desde el panel de control del equipo donde está instalada localmente la impresora, seleccionaremos Impresoras y Faxes . Esto nos mostrará una vista con todas las impresoras instaladas. Seleccionamos una de ellas y pulsamos con el botón derecho del ratón, seleccionando a continuación la opción Compartir . Después únicamente queda rellenar las opciones solicitadas por el cuadro de diálogo. Compartición de Archivos Administración de los recursos de archivos: Publicación de recursos de archivo en el Active Directory: permite a los usuarios encontrar los recursos fácilmente en la red. Sistema de archivos Dfs (Distributed file system) : permite la transparencia de las estructuras de red y el sistema de archivos de cara a los usuarios, posibilitando el acceso a los recursos de archivo de forma centralizada. Proporciona un árbol lógico único para los recursos del sistema de archivo que pueden estar en cualquier lugar de la red. Permisos especiales del sistema de archivos NTFS: Proporcionan un mayor grado de control a la hora de conceder acceso a los recursos. Por ejemplo, permite asignar a un usuario permiso para cambiar permisos de archivos o carpetas, o la posibilidad de tomar posesión de archivos o carpetas. Sistema de archivos de cifrado (EFS, Encrypting File System) : Proporciona la tecnología de cifrado de archivos para el almacenamiento de archivos NTFS en disco. Asignación de cuotas de disco en volúmenes NTFS: Permiten asignar espacio de disco a los usuarios en función de los archivos y carpetas que poseen. También se puede supervisar la cantidad de espacio de disco duro que los usuarios han utilizado. Para compartir recursos en una red es necesario compartir carpetas. Esta tarea se lleva a cabo de la siguiente forma: Abrir Administración de equipos desde el menú Herramientas Administrativas . En el árbol de la consola de Administración de equipos , seleccionar Carpetas compartidas bajo Herramientas del Sistema. Bajo Carpetas compartidas , seleccionar Recursos compartidos . Aparecerán en el panel de detalles todas las carpetas compartidas desde el equipo local. Pulsar el botón derecho del ratón en el panel de detalles y hacer clic en Nuevo recurso compartido de archivo . Seguir las instrucciones del Asistente para la creación de carpetas compartidas. Para publicar una carpeta en el Active Directory se procederá de la siguiente manera: Abrir Usuarios y equipos de Active Directory en el menú Herramientas Administrativas . En el árbol de la consola de Usuarios y equipos de Active Directory pulsar con el botón derecho del ratón en el dominio en el que se desee publicar la carpeta compartida y seleccionar Nuevo / Carpeta compartida . Escribir el nombre con el que se desea que aparezca la carpeta compartida en el Active Directory. Especificar la ruta de la carpeta compartida. Red Hat LinuxCompartición de Impresoras Red Hat Linux da soporte a las impresoras de red mediante el demonio lpd, o con el paquete LPRng. Para configurar una impresora en red se realiza la instalación de una impresora local y posteriormente se permite el acceso desde otras máquinas de la red. Para poder instalar una impresora local es necesario que ésta tenga un controlador para GhostScript . GhostScript es un paquete de software que implementa el lenguaje PostScript , y que incluye controladores para muchas impresoras conocidas. Linux almacena la configuración de las impresoras en el archivo /etc/printcap . Puede utilizarse la utilidad PrintTool como herramienta GUI para crear las entradas de este archivo. El acceso a otras máquinas a una impresora local a través de la red se realiza configurando entradas en el archivo /etc/hosts.lpd , en el que se especifican los nombres de las máquinas que pueden acceder al demonio de la impresora en línea en el host local. Por último, es necesario definir la impresora remota en las máquinas a las que se les ha dado acceso. Esto puede hacerse también mediante PrintTool, estableciendo de forma gráfica la entrada correspondiente para la impresora remota en el archivo printcap. Compartición de Archivos El método estándar para compartir archivos en Linus es NFS (Network File Services) . La exportación de sistemas de archivos se lleva a cabo mediante la configuración en el servidor NFS del fichero /etc/exports . Las entradas de este fichero definen qué clientes pueden acceder a los sistemas de archivo compartidos y con qué permisos. Existen numerosas opciones para controlar los permisos de acceso al sistema de archivos, que se muestran en la tabla siguiente: En el cliente se ejecutan una serie de operaciones para acceder a un sistema de archivos compartidos mediante NFS: Asegurarse de que se tiene acceso al recurso. Para ello se ejecutará en la máquina cliente el comando showmount , pasándole como parámetro el nombre del servidor NFS. Modificar el archivo /etc/fstab para incluir información sobre el sistema de archivos que se quiere montar. Crear los puntos de montaje adecuados, y montar el sistema de archivos. Esto se lleva a cabo utilizando el comando mount , que da instrucciones para montar todos los sistemas de archivos descritos en el archivo /etc/fstab . Existen otras alternativas además de NFS para compartir sistemas de archivos en Linux. Entre ellas se encuentran AFS, un sistema de archivos comercial, y Coda, un sistema de archivos gratuito con bastantes similitudes con AFS. Administración de Dispositivos de AlmacenamientoConfiguraciones RAIDLa idea básica del RAID (Redundant Array of Independent Disk) es combinar varios discos relativamente poco costosos en un array de discos para obtener eficiencia, capacidad y fiabilidad que supere a la ofrecida por un disco de alta capacidad. El array de discos aparece en la máquina como un disco lógico simple. Existen dos conceptos básicos necesarios para comprender las distintas configuraciones RAID: Striping Mirroring (Espejo) Striping Constituye la tecnología fundamental del RAID. Es el método para combinar múltiples discos en una unidad de almacenamiento lógica simple. Esta técnica particiona el espacio de almacenamiento de cada disco en franjas o stripes , que pueden ser tan pequeñas como un sector (normalmente 512 bytes), o del orden de megabytes. Estas franjas se intercalan en una secuencia rotativa, de forma que el espacio combinado se compone de franjas alternativas de cada uno de los discos. La mayoría de los SO soporta actualmente operaciones concurrentes de E/S sobre múltiples discos. Sin embargo, para optimizar el rendimiento, la carga de E/S se debe balancear entre todos los discos de forma que cada disco se mantenga ocupado el máximo tiempo posible. En un sistema de discos múltiples sin striping, la carga de E/S nunca está perfectamente balanceada. Algunos discos contendrán ficheros de datos frecuentemente accedidos, y otros discos serán raramente accedidos. Creando los discos del array con stripes lo suficientemente grandes para que un registro entre enteramente en un solo stripe, la mayoría de los registros se pueden distribuir entre todos los disco. Esto hace que los discos del array se mantengan ocupados en situaciones de fuerte carga. Con ello se consigue que los discos trabajen de forma concurrente en operaciones de E/S, maximizando así el número de operaciones simultáneas de E/S que se pueden llevar a cabo en el array. Mirroring (Espejo) La técnica de mirroring se basa en replicar en dos o más discos los datos para garantizar su disponibilidad. Todas las escrituras se deben ejecutar en todos los discos del array en espejo, de forma que mantengan idéntica información. No obstante, con ello también se multiplica la eficiencia en lectura mientras que la eficiencia en escritura permanece invariable respecto a un solo disco ya que se ejecuta en paralelo. RAID 0 Está constituido por un grupo de discos a los que se le aplica la técnica de striping, sin paridad o información de redundancia. Los arrays RAID 0 ofrecen la mejor eficiencia en cuanto a espacio de almacenamiento y eficiencia de acceso. La desventaja que tienen es que si uno de los discos del array falla, falla el array completo. RAID 1 También se le conoce como configuración en espejo. Consiste en una serie de discos (normalmente dos) que almacenan información duplicada, pero que se muestra a la máquina como un disco simple. Aunque la técnica de striping no se usa en un par de discos en espejo, es posible utilizar esta técnica en arrays RAID 1 múltiples, creando un array simple y grande de parejas de discos en espejo. RAID 2 Estos arrays utilizan la técnica de striping en sectores de datos a lo largo de grupos de discos, dedicando algunos discos a almacenar información de redundancia. Puesto que todos los discos contienen actualmente información de redundancia en cada sector, RAID 2 no ofrece ventajas significativas respecto a otras configuraciones RAID. RAID 3 Como en RAID 2, se utiliza la técnica de striping con los sectores de datos en grupos de discos, pero un disco del grupo se dedica a almacenar información de paridad. RAID 3 se apoya en la información de redundancia que tiene cada sector para la detección de errores. Cuando se produce el fallo de un disco, la recuperación de datos conlleva el cálculo XOR de la información almacenada en los discos restantes. Los registros de datos habitualmente se reparten entre todos los discos, lo que optimiza la velocidad de transferencia. Debido a que cada petición de E/S accede a cada disco en el array, RAID 3 sólo puede ejecutar una petición en cada momento. Por ello proporciona la mejor eficiencia para aplicaciones monousuario o monotarea con registros grandes. La configuración RAID 3 requiere discos y controladoras especiales y costosos para mantener los discos sincronizados de forma que se evite la degradación de eficiencia en el acceso a registros cortos. RAID 4 Esta configuración es idéntica a RAID 3 con la excepción de que se utilizan stripes de tamaño largo, de forma que los registros se pueden leer desde cualquier disco individual del array. Esto permite que las operaciones de lectura se puedan solapar. Sin embargo, puesto que todas las operaciones de escritura deben actualizar el disco de paridad, no se pueden solapar. RAID 5 Esta configuración evita los cuellos de botella en escritura que provoca la utilización de un único disco para la paridad en RAID 4. En RAID 5 la información de paridad se distribuye entre todos los discos. Al no haber un disco de paridad dedicado, todos los discos contienen datos y las operaciones de lectura se pueden solapar sobre cualquier disco del array. Las operaciones de escritura accederán habitualmente a un disco de datos y a un disco de paridad. No obstante, ya que los diferentes registros almacenan su paridad en diferentes discos, las operaciones de escritura se pueden solapar con frecuencia. Arrays de Doble Nivel Además de los niveles RAID estándar, se pueden combinar varios arrays RAID en un grupo de arrays simple. Los arrays de doble nivel aportan un equilibrio entre la alta disponibilidad de datos de RAID 1 y RAID 5 y la mayor eficiencia de lectura de RAID 0. Estos arrays se conocen como RAID 0+1 o RAID 10, y RAID 0+5 o RAID 50. WindowsWindows ofrece dos tipos de almacenamiento en disco: almacenamiento básico y almacenamiento dinámico: Almacenamiento básico: Contiene hasta cuatro particiones primarias o tres particiones primarias y una extendida. Este es el sistema de particiones ya utilizado en MS-DOS. Almacenamiento dinámico: Ofrece los siguientes tipos de volúmenes: Volumen simple: contiene el espacio de un único disco. Volumen distribuido: contiene el espacio de dos o más discos (hasta 32). Es equivalente a una configuración RAID 0. Volúmenes con espejo: Son dos copias idénticas de un volumen simple, cada una de ellas en un disco duro independiente. Es equivalente a la configuración RAID 1. Volumen seccionado: combina en un único volumen áreas de espacio libre de 2 a 32 discos duros. Volúmenes RAID 5. Windows permite realizar la administración de discos a través del MMC (Microsoft Management Console) . Con esta herramienta se puede administrar el espacio de almacenamiento, ver las propiedades del disco, ver las propiedades de particiones y volúmenes y actualizar la información de administración de discos. Red Hat LinuxEl soporte para RAID en Red Hat Linux se suministra con el Kernel a través del controlador _md_ . Este controlador se implementa en dos modos: Modo lineal: permite la concatenación de múltiples dispositivos físicos en una única unidad lógica. Sin embargo, no ofrece mejoras en el rendimiento ya que no se accede a los discos en paralelo. La única función del modo lineal es ligar múltiples dispositivos físicos en un único dispositivo lógico, permitiendo sistemas de archivos más grandes. Soporte RAID: en este modo se soportan los niveles RAID 0, 1, 4 y 5. La versión 6.0 de Red Hat incluye la posibilidad de reconstrucción de los arrays dañados, la detección automática de RAID en tiempo de arranque y la posibilidad de añadir y extraer dispositivos físicos de un array en ejecución. Existen una serie de comandos para controlar las configuraciones RAID, que se muestran a continuación: La información sobre configuraciones RAID se almacena en el archivo /etc/raidtab . Cada entrada de este archivo contiene el nivel RAID, el número y nombres de las particiones que constituyen el array e información complementaria. Monitorización y Control de TráficoLa supervisión de la red es una tarea esencial para los administradores de sistemas. Mediante esta actividad, se puede ver la actividad de la red, de manera que los problemas se pueden diagnosticar con rapidez y exactitud. También resulta muy útil para resolver huecos de seguridad y cuellos de botella en el tráfico de red. Normalmente la supervisión de red se basa en la captura y examen de paquetes de datos desde una tarjeta de red. Es decir, los paquetes de datos se inspeccionan antes de que se realice ningún tratamiento de los mismos en la máquina. Esto permite realizar numerosas actividades, como la detección de paquetes corruptos o el diagnóstico de problemas de mala configuración. Por otra parte, con la supervisión de red es posible examinar el tráfico entre los diferentes host de la red. Los cuellos de botella y los conflictos de direcciones IP son algunos de los problemas que se pueden resolver de esta forma. En cuanto a temas de seguridad, la supervisión de red permite la detección de contraseñas fáciles de adivinar, el uso ilegal de ancho de banda por parte de determinados usuarios o ataques de denegación de servicio. Esquema de un sistema de supervisiónLa siguiente figura muestra el esquema de componentes de un sistema de supervisión de red: Punto de acceso a la red: proporciona la conexión física entre el host supervisor y la red supervisada. La ubicación del punto de acceso de red depende mucho de la naturaleza conmutada o no conmutada de la red. En las redes no conmutadas los host comparten un único canal de comunicaciones, por lo que el ancho de banda es compartido entre todos ellos. En una red conmutada cada host tiene su propio canal de comunicación, disponiendo de todo el ancho de banda. También influye en la colocación del punto de acceso la forma en la que esté segmentada la red, siendo una tarea más sencilla en las redes de un solo segmento. Sistema de captura de paquetes: captura el tráfico en bruto procedente del punto de acceso a la red. La mayoría de los sistemas de supervisión de red utilizan un API para realizar el acceso a los paquetes capturados. Motor de análisis: efectúa la decodificación del protocolo de la red y, si es posible, la reconstrucción de la sesión de red. Además agrupa las estadísticas y examina las tendencias de tráfico. Suelen utilizar plantillas de protocolo para separar el encabezamiento de los paquetes de la carga útil de datos. Subsistema de registro: módulo utilizado por el motor de análisis para almacenar los datos conseguidos, incluyendo estadísticas, tendencias y paquetes capturados. El soporte de almacenamiento puede ser variado, desde ficheros de texto a BD. Interfaz de usuario: conjunto de vistas a través de las cuales el usuario puede visualizar de forma coherente la información capturada y analizada. El interfaz de usuario mostrará estadísticas de utilización, listados de host activos, estadísticas de protocolos, etc. Las características de un buen sistema de supervisión de red son las siguientes: Portabilidad del hardware del sistema de supervisión. Compatibilidad con una o más topologías de red. Capaz de supervisar a velocidades cercanas a las del cable. Capaz de almacenar grandes cantidades de datos y estadísticas. Decodificación del tráfico de red en múltiples niveles de la pila de protocolos. Existen básicamente dos tipos de sistema de supervisión de red que se estudiarán en los siguientes apartados: los rastreadores y los analizadores de tráfico. RastreadoresUn rastreador o sniffer captura el contenido real de las sesiones de red y de la transmisión de datagramas. Permiten almacenar información de los protocolos y las actividades del nivel de aplicación. Esta faceta los hace enormemente potentes porque permite detectar problemas derivados de malas configuraciones de aplicaciones o de red, ataques de denegación de servicio, etc. El rastreador funciona capturando los datos de los paquetes de acuerdo a criterios de filtrado específico y extrayendo el contenido de carga útil de los paquetes recibidos. Los rastreadores avanzados dan soporte a una técnica denominada reconstrucción de sesión de red , que permite que las transmisiones de datos que emplean múltiples paquetes simultáneos sean reconstruidas como un único flujo de comunicaciones coherente. Dada su capacidad de analizar el tráfico a nivel de aplicación, los rastreadores son extremadamente útiles para proporcionar una visión profunda de la red, analizando el contenido de las sesiones de red en lugar de sólo la información de cabecera del protocolo. Algunos de los ataques de denegación de servicio más comunes que puede detectar un rastreador son: Avalanchas SYN: consiste en inundar un host de destino con peticiones de conexión TCP no válidas, de forma que la cola de conexiones del servidor atacado se colapse. Ping de la muerte: consiste en enviar un paquete fragmentado mayor de lo normal para provocar el desbordamiento del identificador de longitud de paquetes en su reconstrucción, de forma que acabe tomando un valor negativo. En las máquinas más vulnerables, esto suele provocar el desbordamiento de la pila. Smurf: consiste en el envío de una petición de ping ICMP a la dirección de difusión de una red. Cada host que reciba la petición generará una respuesta, pudiendo provocar una situación de sobrecarga. Los rastreadores también son útiles para detectar problemas habituales de la red, como los siguientes: Direcciones IP duplicadas: dos o más host comparten la misma dirección IP en la red. Errores ARP. Problemas de encaminamiento. Analizadores de TráficoLos analizadores de tráfico son herramientas para el diagnóstico y solución de problemas, y para la mejora del rendimiento. Muestran exactamente lo que está sucediendo en una red de computadoras. Funciona capturando paquetes de datos que viajan por la red, decodificando el protocolo y elaborando estadísticas. Algunas de las utilidades que los analizadores de tráfico tienen son: Detección de cuellos de botella: un cuello de botella es un punto de la ruta entre dos host en el que el ancho de banda disponible es muy limitado. Esto puede repercutir en una degradación grave del rendimiento de la red. Los cuellos de botella pueden estar producidos por múltiples causas: errores de configuración, problemas hardware, servidores saturados… Análisis del tipo de tráfico: permiten catalogar el tráfico que circula por la red mediante el establecimiento de criterios de filtrado. Bibliografía Scribd (Ibiza Ales) Planificación física de un centro de tratamiento de la información. Vulnerabilidades, riesgo y protección. Dimensionamiento de equipos. Factores a considerar. Virtualización de plataforma y de recursos. Virtualización de puestos de trabajo.Vulnerabilidades, Riesgo y ProtecciónLos Centros de tratamiento de la información, más comúnmente conocidos como Centros de Proceso de Datos (CPD), son los recintos que albergan los equipamientos informáticos principales que dan soporte al conjunto de información de la organización. A grandes rasgos se puede considerar que los Centros de Proceso de Datos (CPD) son los depositarios, los “guardianes”, de la información utilizada por todas las áreas de la organización. Esta responsabilidad implica que la información confiada al CPD está convenientemente salvaguardada (funcionalidad y disponibilidad), que se procesa de acuerdo con las instrucciones (fiabilidad e integridad) y que se devuelve intacta a quien la solicite y esté autorizado a obtenerla (accesibilidad y confidencialidad). El objetivo de la seguridad es garantizar la continuidad de la explotación o, lo que es lo mismo, evitar los riesgos potenciales de ataque, robo o daño a los Sistemas de Información de la empresa, tanto accidentales como intencionados, que puedan ocasionar la interrupción total o parcial de las actividades de negocio o bien causar una pérdida a la organización que hubiera podido evitarse. Un aspecto importante a tener en cuenta es el factor coste-beneficio. Por ejemplo, la costosa instalación de sistemas de supresión de incendios puede ser fundamental para proteger un gran ordenador que procese datos corporativos críticos, pero puede no ser justificable para proteger un simple PC. Los riesgos potenciales a los que está sometido un sistema informático se pueden clasificar de acuerdo con su origen en accidentales e intencionados. Entre los primeros podemos destacar: Desastres naturales: vendavales, sismos, rayos, etc. Incendios. Inundaciones. Averías en los equipos. Averías en las instalaciones eléctricas o de suministro. Averías en la climatización. Perturbaciones electromagnéticas. Errores en la introducción, transmisión o utilización de los datos. Errores de explotación. Errores de diseño y desarrollo de las aplicaciones. Entre los intencionados: Fraudes. Robo de elementos del equipo. Robo de información. Modificación de los datos con la finalidad de causar perjuicio a la organización en beneficio propio. Sabotajes y atentados. Huelgas. Abandono de la empresa de personal estratégico. Difusión o salida controlada de información al exterior. Frente a estos riesgos podemos adoptar unas posturas determinadas: Aceptar el riesgo, confiando en su baja probabilidad de incidencia. Transferir el riesgo, contratando seguros, aunque ello no repone la información perdida ni compensa los posibles efectos adversos. Evitar el riesgo con la elaboración y puesta en marcha de un Plan de Seguridad Informática, cuyas medidas de carácter preventivo minimicen la probabilidad de ocurrencia de un siniestro. Seguridad Física y Lógica del CPDLa seguridad física consiste en el conjunto de mecanismos y normas encaminados a proteger las personas, instalaciones, equipos centrales y periféricos y los elementos de comunicaciones contra daños eventuales. Está relacionada con los controles que protegen de los desastres naturales como incendios, inundaciones o terremotos, de los intrusos o vándalos, de los peligros medioambientales y de los accidentes. Los controles de seguridad física regulan además de la sala donde se alberga el equipo del ordenador, la entrada de datos, el entorno (bibliotecas, registros cronológicos, medios magnéticos, áreas de almacenamiento de copias de seguridad y salas de instalaciones de servicios) y todos los detalles o requerimientos tanto arquitectónicos como de preinstalación y mantenimiento de todos los servicios e infraestructuras, incluso la previsión de disponer de una seguridad física integral del entorno. En el pasado, seguridad física significaba mantener un ordenador y su información alejados de cualquier daño físico, rodeando la instalación del ordenador con cierres de seguridad, vallas y guardias (se basaba casi exclusivamente en la seguridad perimetral). El concepto de seguridad física ha cambiado para acomodarse a las realidades del entorno de los ordenadores de hoy en día, un entorno que con frecuencia es la típica oficina repleta de PCs. Por ello se deben adaptar los conceptos de seguridad perimetral y de controles de acceso físico a la situación concreta de cada CPD. La seguridad lógica consiste en el conjunto de operaciones y técnicas orientadas a la protección de la información contra la destrucción, modificación indebida, divulgación no autorizada o retraso en su gestación. Se centra, sobre todo, en los controles de acceso a la información manteniendo una política de password, utilizando encriptación en el almacenamiento de la información y estableciendo unos niveles de acceso apropiados en las comunicaciones. La seguridad física y la lógica deben estar completamente coordinadas, ya que ambas están estrechamente relacionadas y comparten objetivos y presupuestos. Análisis de riesgos y planes de contingenciaDebemos establecer un compromiso entre la necesaria operatividad del sistema frente a los diversos riesgos potenciales, los mecanismos y técnicas que permiten minimizar sus efectos y costes directos e indirectos del empleo de dichas técnicas. Se considera, pues, la seguridad informática como un problema de gestión, en el que se trata de alcanzar unos objetivos determinados mediante la asignación óptima de unos recursos (humanos, técnicos, tiempo, económicos). La seguridad debe ser, por tanto, cuidadosamente presupuestada y planificada determinándose el nivel aceptable de seguridad para la organización y los medios más idóneos para conseguirlo. Del estudio pormenorizado de los riesgos y de la criticidad se determina el nivel aceptable de seguridad y se eligen las medidas a adoptar. Estas medidas se traducen en la Seguridad Preventiva y el Plan de Contingencia. Análisis de Riesgos Deberemos determinar cuantitativa y cualitativamente los riesgos a que esté sometida la organización. Una vez tipificados procederemos a estimar la probabilidad de ocurrencia de cada uno. Esta probabilidad no depende tanto del riesgo en si como de las características concretas de cada CPD. Debido a la dificultad de determinar la probabilidad de ocurrencia de un riesgo se establece una escala con unos niveles subjetivos (evaluación de la ocurrencia de los riesgos): También se puede recurrir a las estadísticas propias de la instalación (caso de existir) o a las editadas por empresas de consultoría o seguros. Análisis de criticidad Se establece un listado priorizado de elementos críticos (aplicaciones, bases de datos, software de base, equipos centrales, periféricos, comunicaciones) según el impacto que su carencia o malfuncionamiento causaría en la operatividad del sistema. Para ello podemos utilizar una escala que marque el tiempo que se podría tolerar un fallo de funcionamiento de cada elemento: 24 horas, 2-3 días, 1 semana, 15 días, más de un mes. También podemos tener en cuenta la diferente criticidad según la época del año, del mes o de la semana. Determinación del nivel aceptable de seguridad Debemos encontrar un equilibrio entre los beneficios de las técnicas a emplear y su coste. Para cada riesgo deberemos cuantificar los daños que puede ocasionar y una estimación de los costes de dichos daños junto con los costes de implantación y mantenimiento de las técnicas apropiadas para evitarlo o reducir su impacto. Elección de medidas a adoptar Consiste en seleccionar las medidas de seguridad que permitan prevenir los daños en lo posible y corregirlos o minimizarlos una vez acaecidos, determinando los recursos necesarios para su implantación. Plan de contingencia Las medidas de corrección se plasman en un plan con unos objetivos concretos: Minimizar las interrupciones en la operación normal. Limitar la extensión de las interrupciones y de los daños. Posibilitar una vuelta rápida y sencilla al servicio. Ofrecer a los empleados unas normas de actuación frente a contingencias. Proveer los medios alternativos de proceso en caso de catástrofe. Para garantizar su validez y que no quede obsoleto con el tiempo, deberá estar en continua revisión. Además el personal debe estar entrenado mediante pruebas simuladas periódicas. En la elaboración del plan debe intervenir la dirección, los técnicos de explotación, los técnicos de desarrollo, el personal de mantenimiento, los usuarios y los proveedores. El plan debe recoger, en forma de planes unitarios, las respuestas a los diferentes problemas que puedan surgir, y se desglosa en: Plan de Emergencia : Guía de actuación “paso a paso” en cada fallo o daño. Determina una serie de acciones inmediatas (parada de equipos, aviso de responsables, activar o desactivar alarmas, uso de extintores u otros elementos auxiliares, llamada a mantenimiento lanzar salvaguardas o listados, etc), una serie de acciones posteriores como salvamento, valoración de daños, elaboración de informes, relanzar procesos, relanzar el SO, recuperar copias de seguridad, saltar procesos, etc, así como una asignación de responsabilidades, tanto para las acciones inmediatas como para las posteriores. Para mayor eficacia se procederá en cadenas secuenciales de actuaciones y a introducir una duplicidad humana para asegurar su realización. Plan de Recuperación : Desarrolla las normas de actuación para reiniciar todas las actividades normales de la organización, bien en el propio CPD, bien en otro centro de respaldo. Si se recupera en el propio centro, se deberán activar los equipos duplicados o auxiliares (si no es automático), se utilizarán los soportes de procesamiento alternativos, se iniciarán las actuaciones de mantenimiento o sustitución de equipos dañados y se utilizarán si es preciso las copias de seguridad. Si se utiliza un centro de respaldo, se deben definir los procedimientos y emplear según la causa que originó el problema, se debe realizar una política de traslados (y vuelta posterior al centro original), se debe recuperar el SO, el software de base y las aplicaciones), se deben relanzar las operaciones (recuperando desde la última salvaguarda en caso de necesidad) y se debe revisar la operación mediante la introducción de pruebas que aseguren el correcto funcionamiento. Plan de Respaldo : Especifica todos los elementos y procedimientos necesarios para operar en el centro de respaldo (si existe) y mantener en el mismo información sobre la configuración del equipo y de las comunicaciones, del SO, software de base, de las aplicaciones, soporte humano y técnico, suministros de documentación y formularios, modo de regenerar el software para su operativa normal, reglas de explotación y operación, política de accesos y confidencialidad, identificación de usuarios, terminales, etc. Medios de Detección y ProtecciónLas áreas controladas deben contar con medios de detección de situaciones anómalas, tales como: Puertas abiertas. Acceso de intrusos. Inundación, humos, control de temperatura, fuegos, etc. Su objetivo es permitir un conocimiento inmediato y preciso del hecho y su localización, por lo que su actuación debe ser absolutamente fiable dentro de unos parámetros previamente establecidos. Ello exige unas revisiones de funcionamiento y un riguroso mantenimiento preventivo cuya periodicidad dependerá del sistema de detección y del tipo de área controlada al que se aplique. La detección de un hecho anómalo requiere la información necesaria para una reacción proporcionada. Dependiendo de la información suministrada por el medio de detección y los parámetros previamente establecidos, antes de llegar a un estado de alarma se puede pasar por un estado de alerta. Así los medios de reacción se van organizando en previsión de su posible actuación. Todos los medios de detección deben integrarse en el Sistema de Gestión de la Seguridad para que los gestione y: Avise de la anomalía y su gravedad. Inicie acciones de corrección automáticas o proponga acciones manuales a realizar por el personal entrenado para ello. Controle las actuaciones (qué, quién, cómo, dónde y cuándo). Este sistema debe estar bajo vigilancia permanente y combinado con los servicios de mantenimiento, para los casos de mal funcionamiento de cualquier medido de detección. Hay que subrayar que los sistemas de detección deben funcionar incluso con el suministro eléctrico de emergencia. En caso de incendio, su extinción puede realizarse con medios manuales o automáticos. Los medios manuales se basan en extintores portátiles, mangueras, etc. Es importante resaltar que: Existen diferentes tipos de fuego (de sólidos, líquidos, gases eléctricos) y hay extintores apropiados para cada tipo. El elemento extintor localizado en un área debe ser el apropiado para el previsible tipo de incendio a declararse en ella. Cualquier medio de extinción puede ser excelente utilizado en un área o más dañino que el propio fuego, si es usado en otra. Nunca debe empelarse un medio de extinción manual basado en agua donde pueda haber fuego eléctrico por peligro de electrocución. No es aconsejable la intervención de personal no entrenado para ello. Siempre que se disponga de tiempo, hay que avisar a la brigada interior de incendios (si la hubiera) o al Servicio de Bomberos. Los medios automáticos se basan en la inundación del área mediante agua, CO² u otros agentes extintores. El más recomendable es el basado en agua, por su bajo coste y su nulo impacto en el entorno. Los sistemas automáticos basados en el agua deben tener un mecanismo de preacción que, en caso de llegar a un estado de alerta o alarma, sustituye el aire de la conducción por agua. La actuación de estos sistemas de extinción debe ser combinada con la previa desconexión del suministro de energía eléctrica del área afectada. Los medios basados en el gas halón (basado en clorofluorocarbonos: CFCs) aunque son efectivos, entrañan peligro para las personas y para el medio ambiente (capa de ozono, efecto invernadero), estando totalmente prohibido por una u otra causa en la mayoría de los países firmantes del Protocolo de Montreal (control de uso de los CFCs). Las áreas controladas deben contar con medios automáticos y manuales de extinción de incendios. Seguridad perimetralSe refiere a las medidas que podemos establecer para evitar un acceso indebido al conjunto del CPD. Han sido ya referenciadas en la seguridad física. El establecer un área segura es importante para el buen funcionamiento del centro, puesto que la información almacenada y los procedimientos que se realizan en el CPD son vitales para la organización. Por ello se deben adoptar todas las medidas cuyo coste esté justificado. Entre ellas: Servicio de seguridad: que no sólo controle los accesos al recinto, sino que también realice inspecciones periódicas de las dependencias, sobre todo de las que no tengan personal en cada momento. Su importancia se hace evidente en horas nocturnas o días festivos. Barreras, puertas de seguridad, ausencia de ventanas. Son medidas que tienden a dificultar el acceso de personal no autorizado. Video vigilancia y alarmas volumétricas: controladas por una centralita en la cabina de seguridad. Control de Acceso FísicoSe basan en medidas de identificación unívoca de las personas que acceden al CPD. El servicio de seguridad debe llevar un registro de las entradas y salidas al centro. Las visitas autorizadas deben llevar obligatoriamente una tarjeta identificativa o etiqueta en lugar visible que indique claramente que es una visita a las áreas a las que puede acceder y el tiempo de validez (suele ser diaria). El personal propio debe portar una tarjeta identificativa con fotografía. Se pueden utilizar colores para identificar las áreas a las que puede acceder. En centros de alta seguridad pueden requerirse medidas auxiliares de identificación: Huellas dactilares. Fondo de ojo (retina). Introducción de códigos de acceso para abrir las puertas. Niveles de seguridad de acceso Las instalaciones de la empresa deben clasificarse en varias áreas o zonas que, dependiendo de su utilización y los bienes contenidos, estarán sometidas a unos u otros controles de acceso. Las instalaciones pueden clasificarse de acuerdo con los criterios y denominaciones siguientes: Áreas Públicas: espacios en los que no hay ningún tipo de restricción de acceso a empleados o personas ajenas a la empresa. Áreas Privadas: espacios reservados habitualmente a los empleados y personas ajenas a la empresa con autorización por motivos de negocio. En ellos puede haber recursos informáticos con un valor bajo. Áreas de Acceso Limitado (AAL): espacios cuyo acceso está reservado a un grupo reducido de empleados y personas ajenas a la empresa autorizadas por un acuerdo escrito. Pueden concentrarse en ellos recursos informáticos que, en su conjunto, tiene un valor medio. Áreas de Acceso Restringido (AAR): espacios cuyo acceso está reservado a un grupo muy reducido de empleados y personas ajenas de la empresa autorizadas por un acuerdo escrito, que tengan necesidad de acceder por razones de negocio. En ellos se encuentran recursos informáticos que, en conjunto, tienen un alto valor o contienen activos de información críticos para las actividades del negocio. A las dos últimas se les denomina Áreas Controladas. Tienen que permanecer cerradas, incluso cuando estén atendidas, y sus accesos controlados. En las áreas controladas, todos los empleados y las personas ajenas a la empresa con autorización para acceder por razones de negocio tienen que llevar permanentemente y en lugar visible un identificador: Los empleados, al menos, con fotografía y nombre. Las restantes personas, al menos el nombre (legible) y distintivo de la función que cumplen (ej: visita, contratado, suministrador, etc). Los identificadores de los empleados con acceso a áreas controladas pueden tener la posibilidad de lectura por banda magnética o por cualquier otro medio, para facilitar el control de accesos y su registro. Todo identificador, especialmente los que permiten el acceso a áreas controladas, es personal y debe ser considerado como una contraseña de acceso físico y no compartirlo con nadie, para evitar verse envuelto en algún incidente de seguridad no deseado. En las áreas controladas tienen que estar prohibido comer, fumar, consumir bebidas alcohólicas y cualquier tipo de drogas. Las dos últimas están consideradas de alto riesgo potencial para la instalación, por lo que adicionalmente debe impedirse la entrada a cualquier área controlada a las personas de quién se sospeche el consumo. Los suministros informáticos que sean peligrosos o combustibles tienen que ser almacenados a una distancia prudencial y no trasladarlos al área donde se encuentran el resto de recursos informáticos hasta el momento de su utilización. De igual forma, hay que retirarlos de la zona inmediatamente después de finalizar su uso. Control de Accesos Los responsables de las áreas controladas deben mantener unos controles de acceso efectivos y proporcionales al valor de los activos a proteger para que puedan cumplir con unos requisitos de auditabilidad mínimos. Los objetivos son: Permitir el acceso únicamente a las personas autorizadas por el responsable del área. Registrar las entradas y/o salidas (quién, por dónde y cuándo). Para facilitar el control de los accesos a estas áreas, es recomendable la existencia de un único punto o puerta de acceso habitual para entrada y salida, sin perjuicio de que existan otras salidas para emergencias que se puedan abrir desde el interior mediante el empuje de una barra. Las entradas en las Áreas de Acceso Limitado (AAL) tiene que efectuarse desde un área interna, nunca desde un área pública. Cada área de acceso limitado debe tener identificado formalmente un responsable o propietario cuyas responsabilidades son: Aprobar y mantener actualizada la relación de personas con autorización de acceso permanente. Las personas que tengan su autorización cancelada, por petición de su dirección o por haber causado baja en la empresa, deben ser eliminadas de la relación de acceso en un tiempo razonable. Aprobar accesos temporales a estas áreas. En este caso la persona autorizada debe saber que la autorización es para una sola vez. Las Áreas de Acceso Restringido (AAR) no deben tener ventanas al exterior y la entrada en las mismas tiene que efectuarse desde un área interna o un Área de acceso limitado, nunca desde un Área pública. Tienen que tener barreras de aislamiento de suelo y techo, incluyendo el falso suelo y el falso techo, o bien detectores volumétricos de intrusos. Cada área de acceso restringido debe tener identificado formalmente un responsable o propietario cuyas responsabilidades son: Aprobar y mantener actualizada la relación de las personas con autorización de acceso permanente, generalmente, porque el trabajo a realizar requiere su presencia dentro del área. La lista de acceso debe ser actualizada siempre que haya cambios que así lo aconsejen y revisada formalmente, al menos, cada seis meses. Las personas que tengan su autorización cancelada por petición de su dirección o por haber causado baja en la empresa tienen que ser eliminados de la lista de acceso inmediatamente. Aprobar los accesos temporales a estas áreas, incluyendo los accesos del personal que, estando destinado en el área, accede fuera de su jornada laboral. En este caso, la persona autorizada debe saber que la autorización es para una sola vez. Las autorizaciones temporales deben contener: Nombre de quien autoriza si no es el propietario. El nombre de la persona autorizada. Razón social (si corresponde) o motivo. Fecha y hora de acceso y firma. Fecha hora de salida y la firma. Tienen que ser guardadas como documentos auditables, al menos un año. El propósito de este registro es tener un archivo histórico de accesos, a utilizar en caso de investigación de incidente de seguridad, pero en ningún caso debe ser una herramienta de control de los empleados. El propietario del área debe revisar, al menos mensualmente, que estos registros de acceso contienen la información descrita. Es preciso revisar y documentar que las salidas de emergencia tengan alarmas y sean audibles y/o visibles, en la propia sala y en el Centro de Control de Seguridad. Esta revisión debe realizarse al menos, anualmente e incluir la verificación de su correcto funcionamiento incluso con alumbrado de emergencia, si hay pérdida de suministro eléctrico. La documentación de revisiones de funcionamiento de alarmas se guarda como documento auditable. Valoración de las áreas Los requisitos de control de acceso físico deben basarse en el valor de los sistemas de información contenidos en cada área controlada y en la importancia de las actividades de negocio suministradas por ellos. El valor de un sistema de información puede obtenerse de acuerdo con los criterios siguientes: Alto valor: sistemas corporativos grandes y medios. Valor medio: pequeños sistemas corporativos y redes de área local (LAN). Bajo valor: pequeños sistemas (PC’s) y terminales. Para cada caso deben considerarse otros aspectos, como el coste y la necesidad de sustitución de los equipos acumulados en un área o el impacto que podría ocasionar a la empresa la carencia prolongada de una actividad y la no disponibilidad de la información que suministra. Sistemas esenciales son aquellos que contengan actividades críticas para el negocio de la empresa. La valoración final se realiza teniendo en cuenta todos los aspectos descritos para definir los requisitos de control de acceso y seleccionar el tipo de área controlada y deberían estar en consonancia con la tabla siguiente: Para determinar la correcta implantación y la efectividad del control de acceso físico, los propietarios de las áreas controladas deben mantener, al menos, la documentación siguiente: La identificación del área, el uso a que se destina, el nivel de información clasificada soportada, el valor de los equipos, la valoración del servicio y los requisitos de control requeridos. La forma de comunicar a los usuarios de los servicios localizados en el área el nivel de información clasificada soportada, las medidas de seguridad adoptadas y los requisitos para su cumplimiento. La valoración final, junto con los aspectos considerados, tiene que ser documentada y guardada por el responsable del área controlada como documentos auditables. Control de PeriféricosMedios de Almacenamiento No debemos olvidar que la información vital para la organización no sólo está en los discos magnéticos de los equipos, sino que también se puede encontrar en otros soportes como papel y dispositivos de almacenamiento como disquetes, cintas y otros soportes magneto-ópticos. Ello obliga a introducir sistemas de limitación de acceso a los mismos y sistemas de destrucción o borrado seguro tras su utilización. Como regla general deben almacenarse, mientras sean útiles, en armarios especiales o zonas restringidas. El custodio (“librarian”) es el responsable de almacenar y controlar los medios de almacenamiento desmontables. El custodio tienen que poder controlar todos los movimientos de los medios de almacenamiento desmontables, incluidos los traslados, a través de una aplicación o producto de uso exclusivo que le facilite la realización del inventario y la reconciliación. En los casos en que la empresa tenga más de un Centro de Proceso de Datos, tiene que haber un custodio por cada uno de los centros. En las LAN y sistemas distribuidos, la información suele ser creada, accedida y almacenada en los discos magnéticos no desmontables de estaciones de trabajo y servidores. Siempre que en este tipo de sistema exista información en medios de almacenamiento desmontables, tiene que ser nombrado un custodio por cada LAN o sistema. En algunas instalaciones, con gran movilidad de personal o temporalidad del mismo, se llega a bloquear o limitar el uso de las disqueteras de los equipos terminales (PC’s), de manera que se dificulte la copia masiva de datos sensibles. También se puede recurrir a la encriptación del almacenamiento para dificultar su acceso mediante herramientas de SO o externas a la aplicación. Todos los medios de almacenamiento bajo el control del custodio deben estar situados en una AAL (área de acceso limitado) o AAR (área de acceso restringido), dependiendo de la ubicación del sistema donde se procesen, y en una zona aislada cerrada a la que pueda acceder exclusivamente el custodio. Los medios de almacenamiento dedicados a salvaguardas, la recuperación del sistema y los servicios soportados, deben estar situados en otra zona aislada del centro. Lo mismo se aplicaría en el centro alternativo, si lo hubiera. Así si hubiera, por ejemplo, un incendio en el centro podríamos disponer de estos medios de almacenamiento y no se quemarían junto con nuestro CPD. Tiene que haber un control para evitar que los medios de almacenamiento desmontables sean montados o accedidos sin autorización. Los movimientos de medios de almacenamiento entre distintos centros, incluidos los dedicados a salvaguarda, tienen que ser registrados y guardados por el custodio de cada centro. Los medios de almacenamiento en tránsito tienen que ser protegidos contra su pérdida, deterioro o uso indebido, desde que el custodio del centro origen los ponen en manos del transportista hasta que son recibidos por el custodio del centro de destino. Durante el traslado, el soporte de almacenamiento y la información contenida deben asegurarse teniendo en cuenta: Protección física: para que no sean robados, sustituidos o dañados. Protección lógica: para que no sean leídos, copiados o modificados. Para salvaguardar la confidencialidad, integridad y disponibilidad de la información transportada, tienen que usarse medios de transporte fiables, propios o de empresas solventes responsables. Para la información sensible o con el más alto nivel de clasificación tienen que utilizarse contenedores cerrados y que sólo puedan ser abiertos por los custodios de los centros origen y destino. En casos excepcionales, habrá que fraccionar el envío en más de una entrega y realizarlo por rutas diferentes. Inventario y Reconciliación El custodio de medios de almacenamiento es responsable de: Implantar el procedimiento de control de inventario. Realizar, al menos anualmente, el inventario de medios de almacenamiento. Efectuar la correspondiente reconciliación, en caso de haber discrepancias. El proceso de inventario y reconciliación debe: Contemplar todos los medios de almacenamiento removibles incluidos los volúmenes manejados por robots y los que estén sin grabar. Ser realizado por personas, al menos una, directamente relacionadas con la responsabilidad de medios de almacenamiento y con la participación del custodio. Iniciarse partiendo de las cifras finales del anterior inventario y completarse incluyendo nuevos volúmenes y los recibidos de otros centros, eliminando los volúmenes retirados o destruidos y los enviados a otros centros y obteniendo la cifra final que será utilizada por el próximo inventario. Las discrepancias deben ser documentadas e iniciar el proceso de reconciliación. Incluir, en la documentación relativa al proceso, cualquier informe de discrepancias o incidentes. Esta documentación tiene que ser firmada por el custodio y por su línea de dirección o el propietario de la librería de medios de almacenamiento. La documentación de soporte de los últimos inventarios y reconciliaciones tiene que ser guardada como documentos auditables. Impresoras y Listados Según su ubicación, se consideran dos tipos de impresoras: Las locales del sistema, situadas en las áreas de acceso limitado/restringido. Las impresoras remotas que no están situadas en las áreas dedicadas a sistemas. Se deben aplicar los siguientes criterios: La responsabilidad de especificar las reglas de utilización de cada impresora y de cada sistema de almacenamiento magnético es del propietario o responsable del mismo. El control de las salidas impresas es responsabilidad del usuario final que las envía a las impresoras. El control de los soportes de almacenamiento es responsabilidad del que los genera y del que los utiliza. El propietario del sistema o servicio no es responsable de que el propietario o el usuario de las impresoras remotas cumplan con los procedimientos de seguridad descritos. Las salidas impresas de información clasificada tienen que ser protegidas contra accesos no autorizados. Las impresoras remotas situadas en áreas internas (no situadas en AAL o AAR) tiene que tener alguno de los controles siguientes: Tener designado un responsable de dirigir las salidas impresas al usuario final que las envió. Estar directamente atendida por el usuario final. Recoger los listados personalmente e inmediatamente después de terminar la impresión. Tener la posibilidad de borrado de listados pendientes de impresión. Las impresoras locales o remotas situadas en AAL o AAR no requieren ningún control adicional para imprimir información clasificada. No puede haber impresoras remotas situadas en áreas públicas. Sistema Integral de Gestión de la SeguridadSe contemplará y analizará la seguridad física independientemente de los sistemas de gestión y control implementados en el CPD. Se instalará un sistema informatizado para la gestión y el control integral de todas las alarmas procedentes del equipamiento informático, de las infraestructuras y de las instalaciones específicas de seguridad del CPD. Dicho sistema, recibirá las señales de alarma, dispondrá de la gestión de las mismas y de la posibilidad de realizar desde el mismo la modificación de ciertos parámetros u operaciones de parada, arranque o maniobra del equipamiento de las salas de informática o del recinto del CPD: Red de incendios (Sala del CPD, áreas de servicios y despachos, zonas del SAI y del grupo electrógeno). Alarmas en general. Arranque, paro o maniobra del entorno industrial del CPD. Control de accesos y movimientos. Control de ahorro de energía. Control en el bloque de multicasilleros de reparto. Control de la expedición de la producción. Control de los stocks de almacenes. Estado de las baterías de los SAIs y control de los grupos electrógenos. Control de climatización, sobrepresión y renovación ambiental. Rede de sondas ambientales en falso suelo, techo y sala de ordenador. Red de detección de humedad. InstalacionesEntornoLos edificios o instalaciones de los CPDs requieren unas características adicionales de protección física que deben ser consideradas antes de seleccionar su ubicación, teniendo en cuenta aspectos tales como la posibilidad de daños por fuego, inundación, explosión, disturbios civiles, cercanía de instalaciones peligrosas (depósitos de combustible, aeropuertos, acuartelamientos, etc) o cualquier otra forma de desastre natural o provocado. Se deberán analizar de forma integral las características dominantes de los distintos entornos, evaluando las ventajas y los riesgos potenciales que pudieran afectar al buen funcionamiento del CPD y planteando las respuestas adecuadas en relación con: Entorno Natural Tendremos en cuenta: Climatología: tormentas, precipitaciones de agua y nieve, temperaturas extremas, huracanes, ventisca y vientos dominantes. Geotecnia: mecánica de los suelos (corrimientos de tierra, hundimientos, estructura físico-química, humedad, existencia de minerales magnéticos, sismicidad, etc) Hidrología: proximidad de ríos, proximidad del mar, embalses cercanos y posibles avenidas, etc. Entorno Artificial Podemos considerar: Acceso a medios de emergencia: bomberos, policía, servicios sanitarios, etc. Centrales de gas o depósitos de gas, centrales nucleares. Redes de telecomunicaciones. Suministro eléctrico, redes de suministro accesibles. Plantas petroquímicas, fábricas de cemento, betunes, derivados del vidrio, etc. Conducciones o depósitos de líquidos (agua potable, aguas residuales, combustibles, etc). Contaminación atmosférica: polvo, vapores corrosivos o tóxicos, etc. Perturbaciones locales: ruidos, vibraciones, radiaciones parásitas (radares, balizas de navegación, emisoras de radio y televisión, torres de telecomunicaciones, líneas de alta tensión cercanas, grandes transformadores o motores, centrales eléctricas, repetidores, centrales nucleares, aeropuertos). Entorno Urbanístico Tenemos entre otras: Dotaciones urbanas: metro, autobuses, intercambiadores ferroviarios, autopistas, aeropuertos, puertos marítimos, hospitales, universidades, supermercados, etc. Zonas urbanas: parcelas abiertas, edificaciones colindantes, zonas de oficinas y negocios, parques empresariales, recintos feriales. Ambiente de trabajo y salud laboral (microclima de trabajo, contaminación ambiental, sobrecargas físicas y psíquicas influyentes, etc). Actualmente se ha acuñado el término “AMENITIES” para abarcar todos los servicios complementarios que no son estrictamente necesarios para el desempeño de la actividad de proceso de datos, pero que pueden ofrecerse en el conjunto de la oferta inmobiliaria sobre todo en los parques empresariales o zonas singulares. Entre ellos están: Áreas de descanso, ocio y servicios terciarios. Guardería. Aparcamiento. Clubes, gimnasios e instalaciones deportivas. Cajeros automáticos. Restaurantes y cafeterías. Hoteles. Centros comerciales. Características de ConstrucciónUna vez seleccionada la ubicación física del edificio que albergará el CPD, habrá que analizar las características especificas de las instalaciones, haciendo hincapié en algunos aspectos: Deben estar diseñadas de forma que no se faciliten indicaciones de su propósito ni se pueda identificar la localización de los recursos informáticos. Incluir zonas destinadas a carga y descarga de suministros y su inspección de seguridad. Cumplir, en los elementos constructivos internos (puertas, paredes, suelos, etc), el máximo nivel de protección exigido por la Norma Básica de Edificación (NBE/CPI-91). Disponer de canalizaciones protegidas de cableado de comunicaciones y de electricidad, para evitar ataques (sabotajes, fuego, roedores, insectos), intercepción o perturbaciones por fuentes de emisión próximas (radio, electricidad magnetismo, calor). HabitabilidadLa mayoría de construcciones de edificios públicos, de oficinas o de negocios no empezaron a cubrir las necesidades de preinstalaciones e instalaciones informáticas hasta bien entrados los años ochenta. Actualmente, el diseño arquitéctónico de un CPD debe estar lo más cercano posible a la arquitectura inteligente. Este hecho ha dado cabida a la domótica. La domótica comprende todos aquellos desarrollos tecnológicos enfocados al diseño de soluciones rentables que pueda tener el inmueble en el marco de la propia génesis del proyecto arquitectónico. Es la automatización del edificio más la disponibilidad de los recursos de las telecomunicaciones y la ofimática. Los requerimientos de habitabilidad tienen en cuenta la arquitectura informática del momento. Prevén no sólo el crecimiento del equipamiento informático, sino también el cambio total a otro entorno informático y mantienen rentable las infraestructuras y las dotaciones inteligentes o servicios avanzados del inmueble: Habitabilidad en horizontal, es el edificio informático óptimo, el de pocas plantas. Habitabilidad en torre, las torres pierden en diafanidad, dificultan la extensión horizontal de la sala de ordenadores y complican la evacuación de emergencia, etc. En las edificaciones informáticas se contemplarán y analizarán al menos los siguientes aspectos, aportando las soluciones más adecuadas: Diseño y ergonomía del inmueble: tienen en cuenta las exigencias de explotación, producción y los requerimientos físicos de la arquitectura informática residente. Serán estas exigencias funcionales las que determinarán el diseño exterior e interior así como la estructura del edificio para las salas de informática. Flexibilidad: informa sobre la capacidad del edificio para satisfacer las necesidades futuras, entre las que destaca la posibilidad de modificar distribuciones tanto de la arquitectura informática residente o de nueva implantación como de sus preinstalaciones y las áreas del personal de producción o explotación. Integración de servicios: permite saber cuándo un edificio entra dentro del concepto de “arquitectura inteligente”, que al menos debe cumplir: Automatización de la actividad de mantenimiento. Automatización de los servicios comunes del edificio. Mejora de la calidad de vida en el trabajo. Ofimática. Planificación del espacio. Telecomunicaciones. Requerimientos de las Edificaciones e InstalacionesSe aplicarán las normas generales de obligado cumplimiento: Norma Básica de la Edificación. Normas tecnológicas de la Edificación. Ordenanzas Municipales. Reglamentos electrotécnicos. Verificación de los Productos y Suministros Industriales en el marco de la construcción. Normas de Preinstalación de las Firmas Informáticas. Además de la aplicación de las normas generales, el estudio para la elección del edificio deberá comprender todo lo que compete a la arquitectura tradicional y muy especialmente a: La estructura y sobrecargas de uso. Las fachadas del inmueble. Accesos a los almacenes. Instalaciones para las salas de informática. Muelles de carga y descarga, elevadores, montacargas, etc. Acceso al edificio de mercancías pesadas (montacargas industrial). Acceso a la Sala de Informática (siempre puertas doble hoja). Existencias de salidas de Seguridad al CPD. Falso suelo y techo tecnológicos. Protección contra las infiltraciones de agua y humedad. Suministros de energía eléctrica y agua. Iluminación de día y de emergencia. Resistencia al Fuego en minutos de la estructura, forjados y muros de carga. Muros cortafuegos. Puertas contra incendios. Situación de las puertas de acceso y evacuación. Túneles de seguridad y escaleras de emergencia. Particiones interiores o mamparas dobles. Que no crucen las salas de informática conducciones de aguas tanto pluviales como de desagües excepto las propias de la climatización. Tratamientos referentes a resistencias eléctricas, acústicas y mecánicas. Protección contra la energía eléctrica de reacción: Toma de tierra del edificio, pararrayos. Características de las InfraestructurasInstalaciones Eléctricas Los cuadros de mandos se instalarán en lugares fácilmente accesibles, con espacio holgado (previendo las posibles ampliaciones), correcta y claramente etiquetados y por supuesto con el más estricto rigor en materia de calidad de aparatos y montaje (deberán cumplir con las normas habituales de protección y seccionamiento). Se evitarán las perturbaciones electromagnéticas, aislando adecuadamente aquellas máquinas generadoras de campos inductivos y armónicos. Se evitará la electricidad estática empleando los revestimientos más adecuados, instalando las tomas de tierra convenientes y manteniendo la humedad en el rango adecuado (al menos del 55%). Los recursos informáticos son sensibles a las variaciones de tensión y de frecuencia de la corriente eléctrica. Los requerimientos básicos para el suministro de energía eléctrica son dos: Calidad y Continuidad. Relacionado con la Calidad se puede destacar que: Las variaciones de frecuencia deben corregirse con equipos estabilizadores que la mantengan dentro de los rangos establecidos por los fabricantes de los recursos informáticos a alimentar, aunque algunos recursos informáticos de nueva tecnología los llevan incluidos. Las variaciones de tensión deben ser manejadas por un Sistema de Alimentación Ininterrumpida (SAI en inglés UPS), de modo que se puedan prevenir los efectos de posibles microcortes. En relación con la continuidad del suministro eléctrico debe tenerse en cuenta que las caidas de tensión pueden ser manejadas por un SAI (UPS), pero sólo por tiempo limitado, ya que el desgaste de sus acumuladores es muy rápido y su recarga muy lenta para utilizarlo en cortes sucesivos y nunca como única alternativa. Las soluciones habituales se basan en una de las siguientes o en la combinación de varias de ellas: Conexión conmutada a dos compañías suministradoras. Conexión conmutada a dos estaciones transformadoras de la misma compañía pero situadas en rutas de suministro diferentes. Capacidad de transformación de corriente asegurada mediante equipos redundantes. Equipos electrógenos de combustión. Siempre que el volumen de las instalaciones informáticas así lo aconseje, el suministro eléctrico y las tomas de tierra deben ser independientes de las generales del edificio y a suficiente distancia de ellas, correctamente aisladas y rigurosamente mantenidas. Recinto de Protección Combinada Son recintos de protección combinada aquellas compartimentaciones dentro de los CPD capaces de garantizar una custodia segura de los soportes magnéticos de respaldo ante los agentes más peligrosos que puedan atacarlos. Estarán dotados al menos con: Apantallamiento electromagnético y jaulas de Faraday. Protección contra reacciones químicas que produzcan HCL y gases de combustión corrosivos dentro de la cámara. Protección contra intrusión y el robo (puerta de seguridad). Protección contra el vandalismo y explosiones. Protección contra incendios y sus efectos derivados (humos y vapores). Protección contra las inundaciones interiores del CPD. Protección contra el impulso electromagnético nuclear (NEMP). Sellado de las instalaciones y de la cámara contra altas frecuencias e incendio. Instalaciones de Agua Se evitará, en lo posible, las canalizaciones de agua en la sala de ordenadores (sobre todo por falso techo, falso suelo o visibles). En todo caso se preverán los mecanismos de detección de fugas y la instalación de válvulas que puedan cerrar las conducciones afectadas. Los detectores de agua se basarán en sensores puntuales o de banda que cubran áreas completas. El cableado debe estar impermeabilizado cuando discurra por zonas con riesgo de humedad o inundación. Si no es posible separar los conductos de agua del resto de instalaciones, se preverá dotar al techo o solera del forjado, por donde discurran las tuberías, de la inclinación oportuna para evacuar el agua hacia los puntos de drenaje establecidos, evitando su acumulación. Si existen en el edificio o adosados a él depósitos de agua u otro tipo de líquido, se asegurará la estanqueidad de los mismos y se instalarán de forma que su rotura no afecte a los servicios esenciales ni por supuesto a las personas. En el caso de salas de informática situadas en sótanos se reforzará la estanqueidad de paredes, pisos, techos, puertas y ventanas. Se preverá la instalación de bombas automáticas para evacuar eventuales inundaciones, que deben alimentarse con un sistema eléctrico independiente del resto de la sala para permitir su funcionamiento independiente. Si las CPU precisan agua fría para la refrigeración, se preverá la red de tuberías con sus válvulas de corte y retención, sondas detectoras y sistema auxiliar de emergencia desde el contador del canal con filtrado del líquido. Medidas de las Instalaciones contra Incendios El fuego causa el mayor número de accidentes en los CPDs. Por ello es imprescindible controlar puntos zonales y además realizar un estudio en función de los agentes extintores (tener en cuenta la prohibición del uso del HALÖN, Protocolo de Montreal sobre CFCs). Se procederá a estudiar como medidas: El acceso de los bomberos a cualquier zona del edificio previendo las tomas de agua a presión convenientes. La resistencia al fuego de los materiales de construcción, carpintería, revestimiento, etc. Se evitarán aquellos materiales que generen productos tóxicos o gran cantidad de humo al ser sometidos al fuego (NBE-CPI-91). También hay que evitar que se acumulen listados de control y otros papeles en el CPD. El mecanismo más adecuado para cortar la alimentación eléctrica en caso de incendio. Los mecanismos idóneos para evitar que los conductores de refrigeración y ventilación actúen como chimeneas y contribuyan a propagar el incendio, parándose automáticamente el aire acondicionado en caso de incendio. La compartimentación del edificio, aislando aquellas zonas que contengan materiales fácilmente combustibles, que se limitarán al máximo. Tabicados de hormigón con mamparas y puertas ignífugas. La Instalación de puertas contra fuegos dotadas de los mecanismos que aseguren su cierre de forma automática. La prohibición de fumar, colocando carteles claramente visibles, en las zonas de mayor riesgo. El mobiliario, fabricado con materiales resistentes al fuego. Los contenedores de papel, materiales plásticos, etc, deberán tener una tapa metálica, que permanecerá cerrada de forma automática. La construcción de recintos de protección combinada o la disposición de armarios ignífugos. La instalación de un sistema de alarmas cruzadas y centralizadas en el Sistema Integral de Gestión de la Seguridad, para la detección o extinción de incendios en el CPD. La mayoría de los armarios que se utilizan en las salas de informática no son ignífugos sino refractarios o simples cajas fuertes. No corresponden al grado de vulnerabilidad exigido en la CEE. Acondicionamiento de Aire Con la evolución tecnológica ya existen en el mercado recursos informáticos que reducen (prácticamente eliminan) los tradicionales requerimientos de aire acondicionado. Sin embargo, debido al parque existente en España y a su antigüedad media, se deben tener en cuenta las siguientes consideraciones: Para mantener el ambiente con la temperatura y la humedad adecuadas, especialmente los de las grandes instalaciones, hay que disipar el calor que generan a través del aire acondicionado. La suficiente potencia y redundancia de estos equipos permitirá que trabajen desahogadamente y que las operaciones de mantenimiento sean sencillas y frecuentes. Un elemento fundamental del sistema acondicionador de aire es el mecanismo de corte automático tras producirse una detección de incendio. Planes de Emergencia en Instalaciones y EvacuaciónTiene que haber implantado, de acuerdo con las leyes y reglamentos en vigor (especialmente con la norma NBE/CPI-91), un Plan de Emergencia y Evacuación de las instalaciones de la empresa. Este plan sólo afecta a la protección de las personas que trabajan o se hallan circunstancialmente en las instalaciones de la empresa y por tanto también afecta al CPD. No tiene relación directa con el plan de seguridad del CPD o de emergencia, aunque deben estar completamente coordinados. Los objetivos de este Plan deben ser: Conocer los edificios y sus instalaciones, las áreas de posibles riesgos y los medios de protección disponibles. Evitar, o al menos minimizar, las causas de las emergencias. Garantizar la fiabilidad de los medios de protección. Informar de las medidas de protección a todos los ocupantes de las instalaciones. Disponer de personal organizado y adiestrado para las situaciones de emergencia. Hacer cumplir la vigente normativa de seguridad. Preparar la posible intervención de recursos externos (Policía, Bomberos, Ambulancias, etc). El plan de evacuación debe ser estudiado y comprobado. Se colocarán señalizaciones hacia las salidas de emergencia en todas las salas y pasillos de forma que sean fácilmente visibles desde cualquier ubicación y situación (elementos luminosos con baterías propias, elementos fosforescentes). Se instalarán alarmas acústicas y luminosas para alertar de las emergencias. Se realizarán periódicamente simulaciones de evacuación. Dimensionamiento de EquiposEvaluación del Rendimiento de un Sistema InformáticoSe define evaluación del rendimiento de un sistema informático como la medida de cómo un software determinado está utilizando el hardware con una determinada carga del sistema. Por ejemplo, para un computador se entiende por carga del sistema a una determinada combinación de programas. La mayor dificultad que tiene la evaluación de las prestaciones de un sistema informático se atribuye al hecho de que la carga real de un sistema informático cambia continuamente, lo que impide poder repetir la medida a no ser que se trabaje en un entorno controlado de carga. Todas las actividades que forman parte del estudio del comportamiento de un sistema se denominan actividades de evaluación de sus prestaciones. La necesidad de evaluar las prestaciones de un sistema informático ha surgido como una consecuencia natural del aumento de la potencia y de la complejidad de los sistemas. Esta evaluación no es una tarea sencilla, ya que ha de tener en cuenta muchos y variados aspectos del hardware, del software y de las aplicaciones que se han de llevar a cabo en el sistema informático. La evaluación de un sistema informático sirve para: Comprobar que el funcionamiento del sistema es el correcto. Detectar y eliminar los denominados “cuellos de botella”. Influir en las decisiones de diseño, implantación, compra y modificación de los sistemas informáticos, es decir, en todas las etapas de su ciclo de vida. Comparar un cierto número de diseños alternativos del sistema (diseñador de sistemas). Analizar el sistema más adecuado para ejecutar un determinado número de aplicaciones (administrador de sistemas). Planificar la capacidad, es decir, predecir el comportamiento del sistema con nuevas cargas. Por lo tanto, es necesario evaluar un sistema informático cuando se quiere: Diseñar una máquina. Diseñar un sistema informático. Seleccionar y configurar un sistema informático. Planificar la capacidad de un sistema informático. Sintonizar o ajustar un sistema informático. Caracterizar y predecir la carga. El comportamiento de un sistema es muy dependiente de la carga aplicada al mismo. Debido al crecimiento vegetativo de la carga de un sistema informático se produce una disminución de las prestaciones del mismo. Para evitar esta disminución es necesario ajustar o cambiar algunos de los parámetros del SO. En ciertos casos, si el sistema no se puede cambiar, hay que intentar mejorar el comportamiento mediante la modificación de la carga (programas).7 Sistemas de ReferenciaSe distinguen tres tipos de sistemas de referencia o tipos de funcionamiento de un sistema informático a la hora del estudio de las prestaciones y su evaluación: Sistema por lotes (batch) : Básicamente consiste en que el computador ejecuta una serie de programas que previamente el responsable del sistema deja almacenados en memoria. Es dicha persona quién decide los trabajos que deben estar en ejecución en cada instante. Por lo tanto, la planificación interna del SO está ayudada por la externa humana. Estos trabajos realizan ciclos de uso de la CPU y de los discos de forma continua hasta que finalizan. Algunos índices de las prestaciones de estos sistemas son los siguientes: Tiempo de respuesta (Turnaround time) , es el tiempo que transcurre desde que se lanza la ejecución de un trabajo hasta que se termina. Diseño y evaluación de configuraciones. Productividad medida en trabajos por unidad de tiempo. Sistema transaccional : es aquél en que un conjunto de terminales remotos conectados al sistema interaccionan con un conjunto de programas. Cada interacción constituye una transacción . Ejemplos: el sistema informático de un banco o el de reserva de billetes o el que recibe medidas de un satélite. La planificación interna del SO debe gestionar las peticiones recibidas sin intervención humana. Un entorno de este tipo queda definido por el flujo de transacciones que le llega, siendo su índice de prestaciones característico el tiempo de respuesta tr = tra + ten + tr , donde: tra es el tiempo de reacción , que se define como el tiempo que transcurre desde que la transacción llega al sistema hasta que comienza la ejecución. ten es el tiempo de ejecución , que se define como el tiempo que transcurre desde que el sistema comienza la ejecución de la transacción hasta que termina. tro es el tiempo de retorno , que se define como el tiempo que transcurre desde que finaliza la ejecución hasta que, eventualmente, se completa la respuesta hacia el usuario. Sistema interactivo o por demanda : Un sistema interactivo es aquél en que los usuarios acceden a él desde terminales remotos teniendo acceso a la totalidad del SO. En estos sistemas, un usuario da una orden al terminal que pasa a procesarse por el conjunto CPU+discos y, transcurrido un cierto tiempo, produce una respuesta en el terminal. En estos sistemas no existe planificación humana que ayude a la planificación del SO. Además, queda definido por los siguientes índices: Número de usuarios que tiene conectados. Tiempo de reflexión de los usuarios, que es el tiempo que transcurre desde que el usuario recibe la respuesta y envía otra nueva orden. Los índices de prestaciones característicos son: el tiempo de respuesta y la productividad , medida esta última en peticiones por unidad de tiempo. Técnicas de Evaluación de un Sistema InformáticoSe denominan té cnicas de evaluación a los métodos y herramientas que permiten obtener los índices de prestaciones de un sistema que está ejecutando una carga dada con unos valores determinados de parámetros del sistema. Se distinguen tres tipos de técnicas: Monitorización . Los monitores son unas herramientas de medición que permiten seguir el comportamiento de los principales elementos de un sistema informático, cuando éste se haya sometido a una carga de trabajo determinada. Estas herramientas hacen un seguimiento de lo que sucede en el sistema, que es lo que se denomina monitorización . Modelado . Es la herramienta que hay que utilizar cuando se trata de evaluar el comportamiento de un sistema en el que hay algún elemento que no está instalado. El modelado se puede realizar de dos formas: Métodos analíticos que proporcionan las teorías de colas. Se basan en la resolución de las ecuaciones matemáticas que representan el equilibrio existente en los eventos que se producen el sistema, mediante algoritmos aproximados. Su principal inconveniente es la limitación para tratar determinadas estructuras de colas que existen en los sistemas informáticos. Simulación . Consiste en la construcción de un programa que reproduce el comportamiento temporal del sistema, basándose en sus estados y sus transiciones. Los resultados se obtienen por extracción de estadísticas del comportamiento simulado del sistema. Requieren de más tiempo de cálculo y esfuerzo de puesta a punto que los métodos analíticos. La principal dificultad del modelado reside en la obtención de datos lo suficientemente precisos para ejecutar el modelo y obtener resultados con un grado de aproximación adecuado. Benchmarking . Se trata de un método bastante frecuente de comparar sistemas informáticos frente a la carga característica de una instalación concreta. La comparación se realiza básicamente a partir del tiempo de ejecución. Las principales dificultades que plantea este método están relacionadas con la utilización de una carga que sea lo suficientemente reducida para ser manejable y lo suficientemente extensa para ser representativa. Monitores Un monitor es una herramienta utilizada para observar la actividad de un sistema informático mientras es utilizado por los usuarios y para cuantificar los resultados de dicha observación. En general, los monitores observan el comportamiento del sistema, recogen datos estadísticos de la ejecución de los programas, analizan los datos recogidos y presentan los resultados. Se define monitorización como el seguimiento de la actividad realizada por un sistema informático. Se ha de tener en cuenta que en informática, puesto que no es posible repetir las mismas condiciones de carga en los mismos instantes, el resultado de una medición será distinto unas veces de otras, es decir, no se da la repetibilidad de la medida. La información aportada por el monitor puede ser útil para: El usuario y el administrador, ya que les permite conocer toda una serie de características del sistema (capacidad, posibilidad de ampliación, planificación, etc.) El propio sistema, para la realización de la adaptación dinámica de la carga. La calidad de un monitor viene determinada por las siguientes características: Sobrecarga o interferencia . La energía del sistema consumida por el instrumento de medida debe ser tan poca como sea posible, de forma que la perturbación introducida por el instrumento no altere los resultados de la observación. Los monitores hardware presentan este peligro en sus puntos de conexión y para evitarlo utilizan sondas electrónicas de muy alta impedancia. Por otro lado, los monitores software aumentan la carga del sistema y alteran por consiguiente su comportamiento, por lo que se debe tratar de minimizar al máximo este efecto. Precisión . Es el error que puede afectar al valor de los datos recogidos. Estos errores son debidos a diferentes causas: la interferencia del propio monitor, una incorrecta instalación o utilización, el número de dígitos para representar la medición, etc. Resolución . Es la capacidad de la herramienta de separar dos acontecimientos consecutivos en el tiempo. También se define como la máxima frecuencia a la que se pueden detectar y registrar correctamente los datos. Ámbito o dominio de medida . Hace referencia al tipo de acontecimientos que puede detectar, es decir, a las características que puede observar y medir. Anchura de entrada . Es el número máximo de bits que el monitor puede extraer en paralelo y procesar cuando se produce un acontecimiento. Capacidad de reducción de datos . Es la capacidad que puede tener el monitor de analizar, procesar y empaquetar datos durante la monitorización para un mejor tratamiento y compresión de los mismos y para reducir el espacio necesario para almacenar los resultados. Compatibilidad . El hardware y el software de monitorización debe ser fácilmente adaptable a cualquier entorno y requerimiento de la aplicación. Coste (adquisición, instalación, mantenimiento, formación y operación). Facilidad de instalación y de utilización . Los monitores se pueden clasificar atendiendo a tres aspectos: Forma de implantación. Mecanismo de activación. Forma de mostrar los resultados. Según su implantación se clasifican en: Monitores software : Son programas o ampliaciones del SO que acceden al estado del sistema, informando al usuario sobre dicho estado. Son los más adecuados para monitorizar los SO, las redes y las BD, así como las aplicaciones que las utilizan. Cada activación del monitor implica la ejecución de varias instrucciones por parte de la CPU del sistema que está analizando, lo que puede provocar una gran sobrecarga en el sistema si la causa de la activación se produce con gran frecuencia. Monitores hardware : Son dispositivos para medir las prestaciones de sistemas informáticos y se conectan al hardware del sistema que se va a monitorizar por medio de sondas electrónicas, que son elementos capaces de detectar eventos de tipo eléctrico. Un monitor hardware podrá reconocer todos aquellos acontecimientos que se reflejen en puntos fijos del sistema. Su principal característica es que son externos al sistema que van a medir, lo que implica: No utilizan recursos del sistema que van a monitorizar. No producen interferencias con éste. Son muy rápidos. Sus principales desventajas son: Son más difíciles de instalar. Existen magnitudes a las que no se puede acceder. Requieren para su operación y análisis de resultados de personal especializado. Pueden interactuar a nivel eléctrico con el sistema que se va a monitorizar, provocando perturbaciones que resulten en un funcionamiento anómalo del sistema monitorizado. Monitores híbridos : Son una combinación de las dos técnicas anteriores, intentando combinar las ventajas de una y otra. Según su mecanismo de activación se clasifican en: Monitores de eventos o acontecimientos . Son aquellos que se activan por la aparición de ciertos eventos. Si el evento se da con frecuencia, la sobrecarga que se produce es elevada. Monitores de muestreo . Son aquellos que se activan a intervalos de tiempo fijos o aleatorios mediante interrupciones de reloj. La frecuencia de muestreo viene determinada por la frecuencia del estado que se desea analizar y por la resolución que se desee conseguir. Según su forma de mostrar los resultados se clasifican en: Monitores de tiempo real , que constan de un módulo analizador que procesa los datos a medida que los recibe. Monitores batch , que primero recogen la totalidad de la información para posteriormente analizarla. Técnicas Analíticas: Teoría de Colas La teoría de colas permite determinar el tiempo que un trabajo pasa esperando en las distintas colas de un sistema. Una red de colas es un conjunto de estaciones de servicio y de clientes. Las estaciones de servicio representan los recursos del sistemas y los clientes usualmente representan a los usuarios. Una estación de servicio consta de un servidor más una cola de espera asociada. Se establece por tanto la siguiente relación entre los modelos y los sistemas reales: Servidor (modelo) &lt;=&gt; Recurso del sistema (hardware)Cola (modelo) &lt;=&gt; Cola (software) asociada al recurso Se denomina resolución o evaluación analítica a la obtención de los índices de prestaciones del sistema a partir de un modelo de colas. Básicamente, consisten en la resolución de un conjunto de ecuaciones que se deducen a partir del modelo y de sus parámetros. El objetivo del es llegar a establecer relaciones entre las variables que caracterizan la carga y las que miden el comportamiento. El término operacional implica que el sistema es directamente medible. Por tanto, una suposición o hipótesis operacional será aquella que puede ser comprobada o verificada mediante la medida. Esquema de una estación de servicio: Una petición tendrá que esperar en la cola hasta que el servidor quede libre. Una vez que un cliente recibe el servicio abandona la estación. El conjunto (Estación de servicio + clientes) constituye la versión más simple de un modelo de red de colas. Básicamente este modelo tendrá dos parámetros: La intensidad de carga o tasa de llegada de los clientes, que se mide en peticiones/segundo. La demanda de servicio, que es el tiempo medio de servicio de un cliente y se mide en segundos. Los clientes en una estación de cola compiten por el servidor. Por ello, el tiempo de residencia estará compuesto por un posible tiempo de espera y un tiempo de servicio. Las variables operacionales básicas son las que se pueden medir directamente sobre el sistema durante un intervalo de observación finito: T (seg), intervalo de observación o de medida del sistema. A (peticiones o clientes), número de peticiones llegadas o clientes durante el intervalo T. C (peticiones o clientes), número de peticiones completadas o servidas durante el intervalo T. B (seg), tiempo durante el cual el recurso observado (la estación de servicio) ha estado ocupado. Benchmarks En general, se puede decir que los benchmarks son programas utilizados para medir el rendimiento de un sistema informático o de alguna de sus partes. La finalidad de sus estudios puede ser muy variada: comparación de sistemas, su sintonización, la planificación de su capacidad, la comparación de compiladores en la generación de código eficiente, el diseño de sistemas o procesadores, etc. Como programa benchmark se puede usar prácticamente cualquier programa ejecutable, escrito en cualquier lenguaje de programación, incluso en lenguaje máquina. se denomina benchmarking al proceso de comparar dos o más sistemas mediante la obtención de medidas. Para conseguir un buen paquete de programas benchmark se deben seguir una serie de pasos, siendo los principales: Determinar los objetivos de utilización de benchmark. Escoger los mejores programas de benchmark según los objetivos determinados en el paso anterior. Por ejemplo, si se desea estudiar el rendimiento de E/S de un sistema, se elegirán programas con un consumo importante de E/S y no programas con consumo intensivo de CPU. Se deben comprobar los aspectos del sistema bajo estudio que influyen en el rendimiento, como pueden ser el SO, los compiladores y su nivel de optimización, el uso de memorias caché, etc. Además se debe comprobar que los programas, versiones y datos usados como benchmark sean los mismos en todas las pruebas. Finalmente, obtenidos los resultados y entendiendo perfectamente qué hace cada programa benchmark, se debe intentar estudiar la causa de las diferencias obtenidas en los distintos sistemas evaludados. La palabra benchmark se puede definir de dos formas: Definición 1: Los benchmarks son una forma de evaluar las prestaciones de un sistema informático, bien en su conjunto o de alguna de sus partes. Además, si el benchmark está estandarizado, se puede utilizar para comparar diferentes sistemas. Definición 2: Los benchmarks se pueden definir como conjuntos de programas completos escritos en lenguaje de alto nivel y que se consideran representativos de la carga real. Una vez dadas las diferentes definiciones de benchmark, conviene conocer las principales aplicaciones de este tipo de programas: En la comparación del rendimiento de diferentes sistemas informáticos con vistas a la adquisición de equipos. Se debe remarcar que estas comparaciones serán tanto más importantes cuanto mayores y más complejos sean el sistema y las aplicaciones que debe soportar. De esta forma será más importante en máquinas de tipo UNIX (multitarea y multiusuario) trabajando sobre arquitecturas diversas que en máquinas MSDOS (monopuesto y monotarea) trabajando sobre la base de una misma familia de chips. Con vistas a la consecución de este objetivo, interesa que los benchmarks estén formados por programas extraídos de la carga real, programas estándar o una mezcla de ambos y servirán para comparar el rendimiento de sistemas informáticos. En la sintonización de sistemas , es decir, cuando se quiere mejorar el rendimiento de un sistema informático que ya está en funcionamiento. En este sentido, interesa que los benchmark permitan detectar qué partes del sistema deben mejorarse o, una vez introducidas las modificaciones, comprobar que efectivamente se ha aumentado el rendimiento del sistema. Suelen ser programas extraídos de la carga real. En la planificación de la capacidad de un sistema informático, es decir, conocer la capacidad que le queda disponible en previsión de posibles ampliaciones. Interesa que los benchmarks usados lleven el rendimiento del sistema hasta el límite, para así poder prever las carencias que presentará el sistema en el futuro. Para este fin, se utilizan programas artificiales que disponen de parámetros regulables que permiten modificar la cantidad de consumo de recursos en el sistema. En la comparación de compiladores desde el punto de vista de la generación de código. Los programas elegidos para la comparación de compiladores pueden ser estándar o extraídos de la carga real. Además conviene indicar sobre qué arquitectura y SO se realizan las pruebas. En el diseño de sistemas informáticos o de procesadores . En este caso, se parte de un sistema general inicial y, mediante simulaciones, tomando como entrada los benchmarks elegidos, se obtiene unos resultados a partir de los cuales se intentará ir mejorando paulatinamente el diseño del sistema. En el diseño se debe tener en cuenta el tipo de compilador que se emplea, ya que deben ser tan independientes como sea posible de la arquitectura. También se pueden estudiar las interrelaciones existentes entre las distintas arquitecturas, sus implantaciones, los lenguajes de programación y los algoritmos que ejecutan. Cuellos de BotellaUn cuello de botella es una limitación de las prestaciones del sistema provocada por diversas causas: un componente hardware, un componente software o la organización del sistema. Un cuello de botella produce una ralentización considerable del tráfico de los procesos en un área del sistema. Así, cuando la demanda de servicio de un determinado componente excede en frecuencia e intensidad a la capacidad de servicio de ese componente, se dan las condiciones para la aparición de un cuello de botella. El término cuello de botella sólo es apropiado cuando el problema en las prestaciones puede ser atribuido a uno o dos recursos del sistema, ya que en un sistema donde todos o casi todos los componentes están sobrecargados no se pueden encontrar cuellos de botella concretos y se habla de un sistema sobrecargado o saturado . En ocasiones, la eliminación de un cuello de botella hace que aparezca otro distinto, diciendo en ese caso que un cuello de botella esconde otro. Se deben eliminar todos los cuellos de botella hasta conseguir que el sistema se encuentre equilibrado ( balanced ). Por otra parte, los cuellos de botella no están directamente ligados a una configuración dada sino que son función, en gran medida, de la carga. Además como la carga en los sistemas suele variar con el tiempo, pueden aparecer cuellos de botella temporales , que son aquellos que aparecen por un espacio de medida relativamente corto respecto a la sesión de medida. Por ello para su detección se suele utilizar un método de interpretación de las medidas en tiempo real (on line). Existen diversas aproximaciones para la detección de los cuellos de botella. Conceptualmente todas ellas son bastante similares, aunque están basada en técnicas diferentes (simulación, modelos analíticos, medidas sobre el sistema, …). El método más común es el basado en la interpretación a posteriori ( off-line ) de las medidas realizadas sobre el sistema. El procedimiento que sigue cualquiera de estos métodos es el siguiente: Debido a síntomas de ineficiencia o algunos de los estudios de evaluación que se realizan al sistema se sospecha de la existencia de un cuello de botella. Se debe formular una hipótesis acerca de la localización y causa de dicho cuello de botella. Se procede a la validación de la hipótesis acerca de la causa del cuello de botella. Para ello se recurre a los datos medidos sobre el sistema o, si éstos son insuficientes, a la recogida de más datos y a su análisis. Una vez confirmada la hipótesis se plantea el problema de eliminar el cuello de botella o de reducir sus efectos al menos. Para ello hay que determinar el método más eficaz. En general se pueden distinguir dos tipos de modificaciones del sistema para la eliminación de cuellos de botella: Terapias de reposición (upgradding) , hacen referencia a modificaciones del hardware como añadir, reemplazar o incluso eliminar uno o más componentes hardware. Terapias de sintonización (tunning) , se trata de modificaciones que no alteran la configuración pero de alguna manera tienen efecto sobre la organización del sistema como por ejemplo, cambiar ficheros de un disco a otro, cambiar un disco de canal, etc. Este tipo de terapias son, en general, más económicas y menos radicales que las terapias de reposición. Se procede a la modificación del sistema de acuerdo con el método seleccionado. SintonizaciónPara mejorar las prestaciones y la eficiencia de un sistema informático es necesario realizar un estudio de evaluación de las prestaciones de dicho sistema. Las operaciones que hay que llevar a cabo en un método de mejora de prestaciones se pueden agrupar en las siguientes fases: La definición de los objetivos del estudio es una fase fundamental, ya que en función de los mismos se determinará el método que se utilizará para analizar las prestaciones del sistema, la cantidad de recursos que es preciso emplear y la forma de justificar la inversión necesaria. Inicialmente los objetivos deben ser modestos y estar basados en el análisis de los datos proporcionados por las rutinas de contabilidad, que proporcionan los datos de partida del estudio y pueden revelar la existencia de problemas no detectados. Estos problemas se pueden agrupar en las siguientes clases: Análisis de los dispositivos de hardware. Eficiencia de los programas. Problemas de la carga. Localización de los cuellos de botella. Cada una de estas clases o áreas de estudio requerirá unas herramientas y unas técnicas especificas. Por otra parte, también es conveniente fijar el ámbito de estudio, es decir, si se va a tratar un problema concreto o un problema global. Ejemplos de objetivos de estudio a nivel global del sistema son: Verificar la posibilidad de evitar, o de al menos, posponer por algún tiempo, la adquisición de nuevo hardware (memoria, periféricos, CPU, …). Reducir el overhead del sistema y en general todas las actividades que no sean productivas. Reducir la carga actual. La consecución del segundo y tercer objetivo aumentará la capacidad residual del sistema y resultará de interés si lo que se pretende es encontrar espacio para nuevas aplicaciones sin tener que expandir la configuración del sistema. Por otro lado, el primero y especialmente el segundo de los objetivos suelen requerir la localización de posibles cuellos de botella del sistema. Ejemplos de objetivos específicos son: Reducir el tiempo medio de respuesta en un porcentaje dado. Determinar si la tasa de paginación es tan elevada que pueda ser necesario algún tipo de intervención (ej: la ampliación de la memoria principal). Determinar la mejor distribución de los archivos en los discos conectados a los diversos canales. Determinar la relación existente entre las utilizaciones de memoria y de CPU y el número de usuarios conectados. Equilibrar y optimizar la actividad de los canales. En ocasiones para lograr un determinado objetivo es necesario analizar y resolver previamente otros problemas. Factores a considerarPara evaluar el comportamiento de un sistema es necesario disponer de una serie de medidas cuantitativas o parámetros que: Caracterizan el comportamiento tanto del hardware como del software del computador. Hacen referencia a cómo el usuario (visión externa) y el responsable del sistema (visión interna) ven su comportamiento. Estas magnitudes o parámetros están relacionadas con tres tipos de medidas correspondientes a: Consumo de tiempos. Utilización de recursos o dispositivos. Trabajo realizado por el sistema o componentes del mismo. Variables externas o perceptibles por el usuario Productividad (Throughput) , es la cantidad de trabajo útil ejecutado por unidad de tiempo (u.t.) en un entorno de carga determinado. Normalmente se mide en (trabajos/hora) o en (transacciones/segundo). Capacidad , es la máxima cantidad de trabajo útil que se puede realizar por u.t. en un entorno de carga determinado. Tiempo de respuesta , es el tiempo transcurrido entre la entrega de un trabajo o una transacción al sistema y la recepción del resultado o la respuesta. Variables Internas o del Sistema Factor de utilización de un componente , es el porcentaje de tiempo durante el cual un componente del sistema informático (CPU, dispositivo de E/S, canal, etc.) ha sido realmente usado. Solapamiento de componentes , es el porcentaje de tiempo durante el cual dos o más componentes del sistema han sido utilizados simultáneamente. Overhead , es el porcentaje de tiempo que los distintos dispositivos del sistema (CPU, disco, memoria, etc) han sido utilizados en tareas del sistema no directamente imputables a ninguno de los trabajos en curso. Factor de carga de multiprogramación , es la relación entre el tiempo de respuesta de un trabajo en un determinado entorno de multiprogramación y su tiempo de respuesta en monoprogramación. Factor de ganancia o multiprogramación , es la relación entre el tiempo total necesario para ejecutar un conjunto de programas secuencialmente en monoprogramación y en multiprogramación. Frecuencia de fallo de página , es el número de fallos de página que se producen por unidad de tiempo en un sistema de memoria virtual paginada. Frecuencia de swapping , es el número de programas expulsados de memoria por unidad de tiempo a causa de falta de espacio o con el fin de permitir su reorganización para recuperar espacio en ella o para disminuir la paginación. Otras Magnitudes Relativas al comportamiento Fiabilidad , es una función del tiempo definida como la probabilidad que el sistema trabaje correctamente a lo largo de un intervalo de tiempo dado. Se mide por la probabilidad de fallos por unidad de tiempo o por el tiempo medio entre fallos. Disponibilidad , es una función del tiempo definida como la probabilidad de que el sistema esté trabajando correctamente y por lo tanto se encuentre disponible para realizar sus funciones en el instante considerado _t_ . Seguridad , es la probabilidad que el sistema esté realizando correctamente sus funciones o parado de forma tal que no perturbe el funcionamiento de otros sistemas ni comprometa la seguridad de las personas relacionadas con él. Rendimiento , es una función del tiempo definida como la probabilidad que las prestaciones del sistema estén por encima de un cierto nivel en un instante determinado. Mantenibilidad , es la probabilidad que un sistema averiado pueda ser reparado y devuelto al estado operacional dentro de un periodo de tiempo determinado. Magnitudes que caracterizan la cargaSe denomina carga de prueba la carga usada en el estudio de las prestaciones de un sistema informático. Se distinguen dos tipos de carga: Carga real , se observa en un sistema durante su funcionamiento normal. Su principal inconveniente es que no permite repeticiones para eliminar los errores de medición, por lo que es difícilmente utilizable como carga de prueba. Carga sintética , está constituida por un conjunto de programas extraídos o no de la carga real del sistema informático que la reproduce de forma compacta. Puede utilizarse repetidamente y puede modificarse sin afectar a la operatividad del sistema. En muchos sistemas la evaluación se suele realizar en un sistema distinto pero equivalente al real, es decir, creándose dos sistemas paralelos. Magnitudes que caracterizan cada componente de la carga Tiempo de CPU de trabajo , es el tiempo total de CPU necesario para ejecutar un trabajo (programa, transacción, etc) en un sistema determinado. Es una función directa del número de instrucciones que se ejecutan para realizar ese trabajo, del volumen de datos procesados y de la velocidad del procesador. Número de operaciones de E/S por trabajo , es el número total de operaciones de E/S que requiere la ejecución de un trabajo. Caracte rísticas de las operaciones de E/S por trabajo , hacen referencia al soporte (cinta, disco, etc) y, en el caso de discos, a la posición que ocupa el archivo sobre el que se efectúan. Prioridad , es la que el usuario asigna a cada uno de los trabajos que procesa el sistema. Memoria , es la que requiere ocupar, para su ejecución, un trabajo determinado. Puede ser constante o variable. Localidad de las referencias , es el tiempo en que todas las referencias hechas por un trabajo permanecen dentro de una página (segmento) o conjunto de páginas (segmentos). Magnitudes que caracterizan el conjunto de carga Tiempo entre llegadas , es el tiempo entre dos requerimientos sucesivos para un servicio (ejecución de un trabajo o transacción) del sistema. Frecuencia de llegada , es el número medio de llegadas de nuevas peticiones de ejecución que se producen por unidad de tiempo. Es la inversa del tiempo entre llegadas. Distribución de trabajos , define la proporción existente entre las ejecuciones de los distintos trabajos que constituyen la carga. Magnitudes que caracterizan las cargas convencionales Tiempo de reflexión del usuario , es el tiempo que el usuario de un terminal de un sistema interactivo necesita para generar una nueva petición al sistema (tiempo de leer la respuesta previa, de pensar en la nueva acción que se vaya a formar y de teclearla. Número de usuarios simultáneos , es el número de usuarios interactivos que trabajan simultáneamente sobre el sistema en un instante dado. Intensidad del usuario , es la relación entre el tiempo de respuesta de una petición y el tiempo de reflexión del usuario. Magnitudes para controlar el comportamientoAlgunas de las modificaciones que se pueden introducir en un sistema para mejorar su comportamiento son: Ajuste de los parámetros del SO. Tamaño del quantum , es la cantidad de tiempo de uso ininterrumpido de la CPU que un sistema de tiempo compartido asigna a los diferentes trabajos. Si el guantum es demasiado grande se favorece a los trabajos con mucho uso de la CPU, mientras que si es demasiado pequeño se puede introducir un overhead importante debido a los continuos cambios de contexto de un programa a otro cada vez que se agota el quantum. Prioridad interna , es el nivel inicial de prioridad interna que recibe un programa en función de la prioridad externa asignada. Factor de multiprogramación , es el número máximo de trabajos que están simultáneamente en memoria principal y, por lo tanto, que tienen opción a utilizar la CPU y los demás recursos activos del sistema. Cuanto mayor sea este valor tanto mejor será el aprovechamiento de todos los recursos del sistema, aunque también aumentará el overhead. Tamaño de la partición de memoria , es la cantidad fija de memoria principal asignada a una cola de trabajos. Máxima frecuencia de fallo de página , es el valor de la frecuencia de fallo de página por encima del cual se produce un excesivo overhead. A partir de este valor de frecuencia se efectúa la suspensión o swapping de alguno de los trabajos en curso. Número máximo de usuarios simultáneos. Modificación de las políticas de gestión del SO , como por ejemplo cambiar las prioridades de los diferentes tipos de tareas. Equilibrado de la distribución de cargas , se pretenden utilizar de la forma más uniforme posible todos los dispositivos del sistema informático. Cuando el uso de los mismos está desequilibrado se deben disponer los cambios necesarios para lograr el equilibrio deseado. Este tipo de corrección acostumbra, en muchos casos, a proporcionar mejoras espectaculares en el comportamiento del sistema. Sustitución o ampliación de los componentes del sistema , cuando los métodos anteriores no funcionan se debe modificar la configuración del sistema, bien sea sustituyendo determinados elementos por otros de mayor capacidad o rapidez, o bien sea por aumento del número de dispositivos que constituyen la configuración del sistema. Es importante darse cuenta de que la ampliación de la configuración debe hacerse de tal forma que se despeje el posible cuello de botella que se pueda haber detectado, ya que de lo contrario el comportamiento conjunto del sistema ampliado no variará de forma significativa. Modificación de los programas , de tal forma que su ejecución promedio requiera de menos recursos. Esto se puede conseguir bien mediante recodificación de los caminos del programa recorridos con mayor asiduidad, o bien por un montaje que agrupe en la misma página o segmento aquellos módulos del programa que deben coexistir en memoria para la ejecución del programa, etc. Hay que destacar que este método provoca la modificación de la carga del sistema y normalmente se considera la carga como un dato del problema que no se puede modificar. Bibliografía Scribd (Ibiza Ales) Redes conmutadas y de difusión. Conmutación de circuitos y de paquetes. Integración voz-datos. Protocolos de encaminamiento. Ethernet conmutada. MPLS. Calidad de servicios (QOS).ConmutaciónLa conmutación surge en las redes de larga distancia para reducir el número de enlaces, simplificar el mantenimiento y ahorrar costes. La conexión de N elementos de una red todos con todos, con enlaces punto a punto requeriría N(N-1)/2 enlaces, que lo hace totalmente inviable. La transmisión a larga distancia se realiza a través de una red de nodos de conmutación intermedios. Los conmutadores forman un mosaico de NxM puntos de conmutación que conectan todas las líneas. Estos nodos de conmutación no están interesados por el contenido de la información, simplemente los encaminan desde la fuente hacia el destino mediante su conmutación de nodo en nodo. En una red de conmutación suele haber más de un camino entre cada par de estaciones. Surge así la necesidad de determinar cual es la ruta de encaminamiento óptima. Existen dos tecnologías de conmutación diferentes: conmutación de circuitos y conmutación de paquetes. Conmutación de CircuitosEl funcionamiento de las redes de conmutación de circuitos se basa en los dos principios siguientes: Se establece un circuito para la comunicación entre dos usuarios que piden el intercambio de información. El circuito se asigna durante todo el tiempo que dure la comunicación. En la conmutación de circuitos quien establece la llamada determina el destino enviando un mensaje especial a la red con la dirección del destinatario de la llamada. Se establece entonces una comunicación directa entre las dos estaciones mediante la conmutación adecuada de todos los nodos intermedios. La red asigna recursos para que tenga lugar la comunicación y se enviará el mensaje cuando quien ha efectuado la llamada tenga conocimiento de que ésta ha sido establecida. Las redes de conmutación de circuitos pueden ser analógicas, como la Red Telefónica Pública Conmutada, o digitales como la Red Digital de Servicios Integrados. Aunque la conmutación de circuitos puede utilizarse para la transmisión de datos, resulta ineficiente para este propósito en términos de los recursos de línea puesto que la línea se mantiene ocupada durante toda la sesión, incluso cuando no hay información circulando por ella. Además la necesidad del establecimiento de un enlace de conexión antes de enviar los datos, puede generar un retraso de tiempo significativo respecto al tiempo de transferencia de los datos. La conmutación de circuitos se caracteriza por: Manejar un ancho de banda fijo. La información sigue una ruta preestablecida que no puede modificarse. Si existiera un bloqueo, no se podría enviar la información por otro camino. Los nodos intermedios no almacenan información. Si existiera un bloqueo, la información se perdería. No proporciona control de errores. Es muy rápida, tanto como permita el ancho de banda, y segura. Las redes de conmutación de circuitos trabajan casi exclusivamente con elementos de tipo síncrono, planteándose entonces el problema de sincronización a través de toda la red. Nos encontramos con tres tipos de sincronización de los centros de conmutación: Sincronización por valor medio de fase: el reloj de cada nodo de la red se ajusta con el valor obtenido calculando la media de los valores de los relojes de los nodos vecinos y del propio nodo. Sincronización por director-subordinado: se ajustan los relojes de los distintos nodos con el valor de un reloj maestro o director. Sincronización de forma anárquica: los relojes de los distintos nodos son de gran precisión y cada uno de ellos actúa de forma independiente. Conmutación de PaquetesLa conmutación de paquetes es la técnica utilizada más comúnmente para la comunicación de datos. Los mensajes que se quieren comunicar se dividen en varias partes denominados paquetes. Cada paquete se transmite de forma individual a través de la red y pueden incluso seguir rutas diferentes. En el destino se reensamblan los paquetes en el mensaje original. Al igual que en la conmutación de circuitos las lineas se conectan a centros de conmutación, pero en este caso los paquetes se transmiten al conmutador y son almacenados en una cola en espera de su envío (store an forward) y cuando hay un tiempo muerto en la comunicación entre la fuente y el destino, las líneas pueden ser utilizadas para otras comunicaciones. En esta técnica se pone un límite al tamño de los paquetes, que depende, entre otros factores, de: El número de enlaces que se pueden conectar al centro de conmutación. La ocupación de la línea de transmisión. El tiempo de respuesta requerido. La conmutación de paquetes se caracteriza por: Permite realizar varias transmisiones simultáneamente. Existe un retardo mínimo entre usuarios. El conmutador de paquetes proporciona normalmente tratamiento de errores y conversiones de código entre los distintos terminales y ordenadores. Se permite trabajar a distintas cadencias de línea. El ancho de banda no es rígido para toda la comunicación. Cada paquete lleva una cabecera que contiene la información sobre el camino que debe seguir el paquete. Permite la multidifusión. Un mismo mensaje se puede enviar a varios usuarios. Se pueden establecer niveles de prioridad en los mensajes. Existe otra técnica de conmutación, la conmutación de mensajes, que se basa en los mismos principios que la conmutación de paquetes, pero en la que los mensajes no son fragmentados en paquetes sino que se envían como un bloque completo. Comparada con la conmutación de paquetes presenta dos desventajas importantes: el espacio necesario para el almacenamiento de mensajes puede llegar a ser muy grande y lo mismo sucede con los retrasos de tiempo, lo que la hace inapropiada para el trabajo en tiempo real. Existen dos esquemas en la conmutación de paquetes: Datagramas : cada paquete se trata como una entidad independiente y es encaminado individualmente a través de la red. La cabecera de cada paquete contiene información completa acerca de su destino. Circuito virtual : se utiliza una fase inicial para establecer una ruta (un circuito virtual) entre los nodos intermedios para todos los paquetes transmitidos durante la sesión de comunicación entre dos nodos finales. En el método datagrama los paquetes no siguen una ruta preestablecida, los nodos intermedios examinan la cabecera y deciden a que nodo enviar el paquete para que éste alcance su destino. La entrega no está garantizada y los paquetes pueden llegar al destino en un orden distinto al que fueron enviados, así que deben ser ordenados en el destino para formar el mensaje original. La principal implementación de red de conmutación de paquetes en modo datagrama es Internet con el protocolo IP. En el método circuito virtual en cada nodo intermedio se registra una entrada en una tabla indicando la ruta que ha sido establecida para la conexión, de esta forma los paquetes pueden tener cabeceras más cortas al contener sólo un identificador del circuito virtual y no información completa sobre su destino. Los paquetes llegan al destino en el orden correcto y hay una cierta garantía de que llegan libres de errores. Las formas más comunes de redes de conmutación de paquetes de circuito virtual son X.25, Frame Relay y ATM. EncaminamientoComo ya hemos comentado, cuando hay mas de un camino entre cada par de estaciones hay que determinar cual es la ruta de encaminamiento óptima para el intercambio de información. Los protocolos de encaminamiento son aquellos protocolos que proporcionan técnicas para encaminar la información y que además proporcionan técnicas o mecanismos para compartir la información de encaminamiento. Los distintos protocolos de encaminamiento, que actúan en la capa de red del modelo de referencia OSI, utilizan diferentes algoritmos de encaminamiento y diferentes métricas para evaluar qué camino es el mejor para transportar el paquete, pero entre sus objetivos de diseño todos tienen una o más de las características siguientes: Optimalidad: se refiere a la capacidad del algoritmo de encaminamiento empleado de seleccionar la mejor ruta, que va a depender de las métricas utilizadas y de como se ponderen al realizar los cálculos. Simplicidad y baja sobrecarga: deberían ofrecer su funcionalidad de una forma eficiente con mínima utilización de información de sobrecarga. Ser robustos y estables: se espera que actúen de una forma correcta cuando se enfrenten a situaciones poco usuales o imprevistas, tales como fallos del hardware, implementaciones incorrectas o condiciones de sobrecarga de tráfico. Convergencia rápida: la convergencia es el proceso de acuerdo sobre las rutas óptimas por parte de todos los routers. Flexibilidad: deberían ser capaces de adaptarse de una manera rápida y apropiada a las condiciones cambiantes de la red. Las métricas se pueden calcular atendiendo a diferentes factores, entre ellos: Ancho de banda: se refiere a la capacidad de transporte de datos de un enlace. Número de saltos: el número de routers que se atraviesan en el camino entre el origen y el destino. Carga: la cantidad de datos que pasa por una interfaz determinada. Fiabilidad: la tasa de error de un enlace determinado. Retardo: lo que tarda el paquete desde el origen al destino. Coste de la comunicación: término genérico que engloba los gastos de operación de los enlaces. Antes de ver la clasificación de los métodos de encaminamiento conviene detallar la estructura general de un nodo de conmutación. LOCAL : información sobre el entorno local del nodo (memoria disponible, enlaces locales, etc.). PDU (Protocol Data Unit): unidad básica de información. R-PDU (Routing PDU): información de control entre nodos. Son paquetes de control enviados por otros nodos con información sobre la red, no contienen datos. FIB (Fordward Information Base): es la tabla de encaminamiento que es consultada para el reenvío de los paquetes (representados en la figura por la PDU). RIB (Routing Information Base): es la tabla que almacena las distancias o costes a los nodos. Es la base de información de encaminamiento que se utiliza para formar la FIB. La información de la RIB se consigue mediante la recepción de R-PDUs procedentes de otros nodos vecinos y por la interacción con el entorno local de cada nodo. Los métodos de encaminamiento se pueden clasificar, atendiendo al lugar dónde se decide el encaminamiento, en: Encaminamiento fijado en origen (Source Routing): la ruta que debe seguir el paquete se fija en el sistema terminal origen y son estos los que contienen las tablas de encaminamiento. Cada paquete lleva un campo que especifica su ruta y los nodos de conmutación simplemente reenvían los paquetes por esas rutas ya especificadas. Salto a Salto (Hop by Hop): cada nodo, sabiendo donde está el destino, conoce sólo el siguiente salto a realizar. En función de la adaptabilidad a los cambios se clasifican en: No adaptativos (estáticos) : las tablas de encaminamiento de los nodos se configuran manualmente y permanecen fijas hasta que se vuelve a actuar sobre ellas. Una variante de los algoritmos estáticos son los Q-estáticos que poseen una cierta adaptabilidad: en lugar de dar una sola ruta fija, se proporcionan además varias alternativas para el caso en que falle la principal. Adaptativos (dinámicos) : Se tienen tres tipos de encaminamiento dinámico: Adaptativo Centralizado : todos los nodos se consideran iguales excepto uno, el nodo central. Este nodo cuenta con información de todos los nodos y centraliza el control. Cada nodo envía al nodo central las R-PDUs con información de control. Es el nodo central el encargado de formar la tabla de enrutamiento de cada nodo. Adaptativo Aislado : en cada nodo sólo se cuenta con información local, pero el control es distribuido: Cada vez que un nodo recibe un paquete que no es para él lo reenvía por todos los enlaces excepto por el que llegó. Los principales métodos de encaminamiento adaptativo aislado son los algoritmos de inundación y de estado de enlaces . Adaptativo Distribuido : son los más utilizados. Todos los nodos envían y reciben información de control a sus vecinos y calculan su tabla de encaminamiento en función de su RIB. El control de encaminamiento es distribuido. La adaptación a los cambios es óptima (siempre que estos cambios sean notificados). Entre los métodos de encaminamiento adaptativo distribuido se encuentran los algoritmos de vector de distancias . En los algoritmos de inundación los nodos no intercambian información de control. Cuando llega un paquete a un nodo lo que hace es conmutarlo por todos los puertos de salida sin mirar ninguna tabla de enrutamiento. Tienen el problema potencial de que si la topología de la red tiene bucles el paquete puede estar dando vueltas de manera indefinida. Una solución pasa por limitar la vida del paquete en la red, incluyendo un campo en el paquete que contenga el número de saltos que puede dar. Cada vez que se conmuta el paquete se decrementa en una unidad el valor de dicho campo y cuando llegue a cero en lugar de conmutarlo se descarta. En los algoritmos de estado del enlace cada nodo difunde a todos los nodos de la red sus distancias con los nodos vecinos, es decir, cada nodo comunica su información local a todos los nodos. En los algoritmos de vector de distancias cada nodo informa a todos sus nodos vecinos de las distancias o costes conocidos por él mediante los vectores de distancias, que son vectores de longitud variable que contienen un par (nodo, distancia de nodo) por cada nodo conocido por el nodo en cuestión. Estos vectores se envían periódicamente y cada vez que varían las distancias. Con todos los vectores recibidos cada nodo monta su tabla de enrutamiento. X.25El escenario de las redes de comunicación en los primeros años 70 estaba caracterizado por la existencia de muchas redes públicas de datos, muy diferentes internamente, en manos de organizaciones gubernamentales y compañías privadas. Con el crecimiento de la interconexión de redes se hizo patente la necesidad de un protocolo de red común capaz de proporcionar la conectividad entre estas redes públicas de datos. En 1976, la CCITT (ITU-T desde 1993) recomendó X.25 como el protocolo común. Este protocolo ha sido revisado posteriormente en varias ocasiones. X.25 es un conjunto de protocolos que define una recomendación internacional tanto para el intercambio de datos como para el control de la información entre un dispositivo de usuario, el Equipo Terminal de Datos (ETD), y un nodo de red, el Equipo de Terminación del Circuito de Datos (ETCD), de una red de conmutación de paquetes (PSN, Packet Switched Network). La velocidad típica de una red X.25 está entre 9,6-64 Kbps. Para el establecimiento de la comunicación entre dispositivos en diferentes emplazamientos, una organización puede construir su propia PSN privada o puede abonarse al servicio de una PSN pública. Las facilidades ofrecidas al abonado y la estructura de tarifas dependerán del proveedor del servicio. X.25 utiliza un servicio orientado a la conexión. Un nodo extremo indica a la red que desea iniciar una conversación con otro nodo extremo. La red envía la petición al destinatario, que puede aceptarla o rechazarla. Si la acepta, se establece la conexión. Según la terminología usada en X.25, los dispositivos de red se clasifican en tres categorías: Equipo Terminal de Datos (ETD) : también conocido por DTE (Data Terminal Equipment). Son los equipos finales que se comunican a través de la red X.25. Generalmente son terminales, PCs o hosts localizados en el local del abonado del servicio. Equipo Terminal del Circuito de Datos (ETCD) : también conocido por DCE (Data Communication Equipment). Es un dispositivo en el punto de entrada a la red. Son DCEs por ejemplo los módems o los conmutadores de paquetes. Cada ETD debe estar asociado a un ETCD. Equipo de conmutación de datos (ECD) : también conocido por DSE (Data Switching Equipment). Es un nodo de conmutación en la red de conmutación de paquetes. Transfiere los datos de un DTE a otro DTE. Cada sistema en una red X.25 tiene una dirección que lo identifica y que es proporcionada por el proveedor. Para asegurar la unicidad de las direcciones, la norma X.121 define un esquema de numeración internacional. Esta dirección se llama dirección de usuario de red (NUA, Network User Address) y está compuesta de dos partes: Código de identificación de la red de datos, DNIC . Consta de 4 dígitos divididos en: Código del país, DCC . Formado por tres dígitos, el primero identifica una zona geográfica a escala mundial y los dos siguientes un país específico. Código de la red de datos pública : un único dígito que identifica una PDN específica. NTN, Número de terminal nacional : son 10 dígitos asignados por el proveedor y que no tienen una regla determinada para su formación. Circuitos Virtuales. Canales LógicosLa capacidad de transferencia de datos de la línea X.25 puede estar compartida entre un número de sesiones diferentes. El número de sesiones está en función del tipo de suscripción y de las capacidades software y hardware del DTE. Cada sesión constituye lo que se llama un circuito virtual . Un circuito virtual es un camino lógico bidireccional entre los sistemas local y remoto que puede tener su ruta conmutada en la red, es decir, físicamente la conexión puede pasar a través de cualquier número de nodos intermedios, tales como DCEs o DSEs. En X.25 existen dos tipos de circuitos virtuales: circuitos virtuales conmutados (SVC, Switched Virtual Circuit) y circuitos virtuales permanentes (PVC, Permanent Virtual Circuit). Los circuitos virtuales conmutados son los más habituales, también se les denomina llamadas virtuales . Este tipo de circuitos son conexiones temporales utilizadas para transferencias de datos esporádicas. Requieren, cada vez que dos dispositivos DTE necesitan comunicarse, que se establezca, se mantenga y se termine una conexión. Los circuitos virtuales permanentes son conexiones establecidas de forma permanente utilizadas para transferencias de datos frecuentes y no necesitan que se establezca o termine la conexión porque esta está siempre activa. Para permitir circuitos virtuales simultáneos, ya sean permanentes o conmutados, se utilizan los llamados canales lógicos . Los canales lógicos son los caminos de comunicación entre un DTE y su DCE asociado. El proveedor asigna números de canales lógicos específicos, y cada número debe hacer corresponder el DTE con su DCE. El rango de números de canales lógicos válidos está entre 0 y 4095. Cuando se utilizan SVCs no es necesario conocer el número de canal lógico en uso, el software X.25 asigna el número de canal lógico durante la fase de establecimiento de la comunicación, eligiendo el número dentro de la gama establecida mediante acuerdo con el proveedor en el momento del abono. Para PVCs si es necesario conocer el número, los PVCs están configurados de manera permanente. Hay tres tipos de canales lógicos para SVC: Unidireccionales Entrantes : el DTE sólo puede recibir llamadas en ese canal. Unidireccionales Salientes : el DTE sólo puede iniciar llamadas en ese canal. Bidireccionales : el DTE puede iniciar y recibir llamadas en ese canal. Si se utiliza más de un tipo de canal lógico, los números deben asignarse de acuerdo a la siguiente jerarquía de menor a mayor: PVCs. SVCs unidireccionales entrantes. SVCs bidireccionales. SVCs unidireccionales salientes. Niveles en X.25X.25 tiene tres niveles que se corresponden con las tres primeras capas de la arquitectura de siete niveles del modelo de referencia OSI de ISO. Estos niveles son: Nivel físico: describe la interfaz con el medio físico. Es similar a la capa física del modelo de referencia OSI. Nivel de enlace: es el responsable de la comunicación fiable entre el DTE y el DCE. Es similar a la capa de enlace del modelo de referencia OSI. Nivel de paquete: describe el protocolo de transferencia de datos en la red de paquetes conmutados. Se corresponde con la capa de red del modelo de referencia OSI. Nivel Físico El nivel físico, como en cualquier otra pila de protocolos, especifica las características mecánicas, eléctricas, funcionales y de procedimiento que son necesarias para activar, mantener y terminar una conexión física entre un DTE y un DCE. Se implementa como un controlador de corrientes y realiza las funciones siguientes: Activa y desactiva los circuitos físicos mediante el uso de señales eléctricas. Mantiene las características de línea de la interfaz seleccionada. Indica fallos en las tramas de entrada (por ejemplo trama con longitud incorrecta). La recomendación X.25 no especifica en sí misma como debe funcionar el nivel físico sino que referencia otras recomendaciones de la ITU-T, especificando cuales de ellas se pueden utilizar. Las recomendaciones especificadas son: X.21, X.21bis, X.31 y las interfaces de la serie V como V.24. La recomendación X.21 es una interfaz de señalización digital que opera sobre 8 circuitos de intercambio. Sus características funcionales están definidas en la recomendación X.24 y sus características eléctricas en la recomendación X.27. Los 8 circuitos de intercambio son: X.21bis define la interfaz analógica para permitir el acceso a la red digital de circuitos conmutados utilizando un circuito analógico. Proporciona procedimientos para el envío y recepción de información direccionada, lo que permite al DTE establecer circuitos conmutados con otros DTE que tengan acceso a la red. Nivel de Enlace En el nivel de enlace se especifica el procedimiento de acceso al enlace para el intercambio de datos a través del enlace físico. El nivel de enlace garantiza una transferencia fiable de los datos entre el DTE y el DCE, transmitiendo los datos como una secuencia de tramas. Esto significa que además de proporcionar los mecanismos para la transmisión debe de proporcionar medios para informar de si los datos han alcanzado el destino correctamente, y si no es así retransmitirlo. Las funciones realizadas en el nivel de enlace incluyen: Transferencia de datos de manera eficiente y ajustada en tiempo. Sincronismo de enlace para asegurar que el receptor está en fase con el transmisor. Detección y recuperación de errores de transmisión. Identificación de errores de procedimiento e informe a las capas superiores para su recuperación. Hay varios protocolos que se pueden utilizar en el nivel de enlace: LAPB (Link Access Protocol, Balanced) : es el utilizado generalmente. Se deriva del protocolo HDCL y además de todas las características de éste permite formar una conexión de enlace lógico. LAP (Link Access Protocol) : es una versión temprana de LAPB y hoy día prácticamente no se utiliza. LLC (Logical Link Protocol) : es un protocolo IEEE 802 para Redes de Área Local que permite transmitir paquetes X.25 a través de un canal RAL. En LAPB se describen los procedimientos para el intercambio de información entre un DTE y un DCE. Este intercambio de información puede ser a través de un único circuito físico o de varios. El funcionamiento por múltiples circuitos físicos, que es opcional, se denomina multienlace y es necesario si se quiere evitar que las averías del circuito interrumpan el funcionamiento de la capa de paquete. LAPB maneja tres tipos de tramas: Tramas de Información (I) : contienen la información real que se quiere transmitir. Tramas Supervisoras (S) : se utilizan para realizar funciones de control de supervisión del enlace de datos, tales como el acuse de recibo, la petición de retransmisión y la petición de una supresión temporal de la transmisión de tramas I. Tramas no Numeradas (U) : se utilizan para proporcionar funciones adicionales de control del enlace de datos. Cada trama, excepto las tramas U, llevan una numeración secuencial, pudiendo ser esta numeración en módulo 8, módulo 128 o módulo 32768. Los números secuenciales adoptan cíclicamente todos los valores de la gama, entre 0 y el módulo menos 1. Se tienen así tres modos de funcionamiento de X.25: Funcionamiento básico: en módulo 8. Funcionamiento ampliado: en módulo 128. Funcionamiento superampliado: en módulo 32768. Ejemplos de tramas S son: RECEIVE READY (RR): trama de reconocimiento (ACK) indicando el número de la siguiente trama de información esperada. RECIVE NOT READY (RNR): trama que indica al transmisor que detenga la transmisión debido a problemas temporales. REJECT (REJ): es una trama de reconocimiento negativo (NACK) que indica la petición del inicio de un proceso de recuperación por la pérdida de tramas de información. SELECTIVE REJECT (SREJ): es como REJ pero además permite la recuperación de errores de secuencia de tramas. En el modo de funcionamiento básico no se pueden utilizar tramas SREJ, son obligatorias las tramas REJ. En el modo de funcionamiento superampliado es al contrario. En el modo ampliado se elige en el momento del abono el tipo de trama que se va a utilizar, REJ o SREJ. Ejemplos de tramas U son: DISCONNECT (DISC): permiten anunciar a la máquina una desconexión. SET NORMAL RESPONSE TIME (SNRT): permite anunciar a la máquina que ha vuelto. UNNUMBERED ACKNOWLEDGEMENT (UA): se utiliza para el acuse de recibo y para la aceptación de instrucciones de fijación de modo. FRAME REJECT (FRMR): se utiliza para indicar que se ha recibido una trama de semántica imposible. La estructura general de una trama LAPB es la siguiente: Campo de bandera (8 bits) : indica el comienzo y el final de cada trama. Está formado por la secuencia 01111110. Campo de dirección (8 bits) : contiene la dirección del DTE o DCE, identifica al receptor previsto en una trama de instrucción o al transmisor en una trama de respuesta. Campo de control (8/16/64 bits) : contiene los números de secuencia o las instrucciones y respuestas para controlar el flujo de datos entre el DTE y el DCE. En el funcionamiento en módulo 8 siempre tiene una longitud de 8 bits. En el funcionamiento ampliado y superampliado tiene una longitud de 16 y 64 bits respectivamente para tramas que contengan números secuenciales; para tramas que no contengan números secuenciales su longitud es siempre de 8 bits. Campo de información (longitud variable) : no siempre existe, pero si existe sigue al campo de control y precede al campo de verificación de trama (FCS). Cuando se transmite del DCE al DTE, si el número de bits de información que ha de insertarse en el campo de información no es múltiplo de 8, el DCE rellenará este campo de información con ceros de modo que los octetos del campo de información queden alineados. Cuando se transmita del DTE al DCE, el DTE transmitirá únicamente información alineada por octetos. Campo de secuencia de verificación de trama (FCS) (16 bits) : sirve para la comprobación de errores de transmisión. Es una variante del CRC. Nivel de Paquete El nivel de paquete gobierna la comunicación extremo a extremo entre los diferentes DTEs. Crea unidades de datos de red, denominados paquetes, que contienen información de control y datos de usuario. Se tienen entonces dos tipos de paquetes: paquetes de control y paquetes de datos. Cada paquete que deba transferirse a través de la interfaz DTE/DCE estará contenido en el campo de información de la capa de enlace de datos. La capa de paquete proporciona procedimientos para manejar los siguientes servicios: Circuito Virtual Conmutado (SVC) : como ya hemos visto es una asociación temporal entre dos DTEs. Este servicio se inicia por un DTE que envía una señal de petición de llamada (CALL REQUEST) a la red y garantiza una entrega ordenada de paquetes entre dos DTEs en cualquier dirección. Circuito Virtual Permanente (PVC) : es una asociación permanente entre dos DTEs y que por tanto no requiere una acción de establecimiento o liberación de llamada. Selección rápida : es un servicio que permite a los paquetes que establecen el SVC llevar también datos. Establecimiento y liberación de llamada : son servicios requeridos por el SVC. Control de flujo y manejo de errores de cada canal lógico . En la tabla siguiente se muestran los distintos tipos de paquetes y su utilización en los servicios SVC y PVC. Todos los tipos de paquetes contienen un encabezamiento que contiene los siguientes campos: Identificador de protocolo (1 byte) : para funcionamiento en módulo 8 y 128 no aparece en ningún tipo de dato. Para funcionamiento en módulo 32768 aparece en el primer byte de cada paquete de datos. Identificador general de formato (4 bits) : es un campo codificado que indica el formato general del resto del encabezamiento. El primer bit se utiliza para el bit calificador en paquetes de datos; para el bit de dirección en los paquetes de establecimiento y liberación de la comunicación; y se pone a 0 en el resto de paquetes. El siguiente bit se utiliza para el procedimiento de establecimiento de la comunicación y de confirmación de entrega en paquetes de datos; se pone a 0 para los demás paquetes. Los dos siguientes bits indican el modo de funcionamiento: 01 para módulo 8, 10 para módulo 128 y 11 para 32768. Identificador de canal lógico (12 bits) : está formado por el número de grupo de canales lógicos (4 bits) y el número de canal lógico (8 bits). Como en número 0 se reserva para futuros usos, los 12 bits dan un máximo de 4095 posibles números. Identificador del tipo de paquete (8 bits) : es una codificación dependiente del tipo de paquete y modo de funcionamiento. El formato del resto del paquete depende del tipo. Establecimiento y Liberación de la ComunicaciónCuando un DTE A quiere establecer una comunicación con un DTE B, A indica una petición de llamada transfiriendo un paquete PETICIÓN DE LLAMADA (CALL REQUEST) a su DCE por la interfaz DTE/DCE. El canal lógico seleccionado por el DTE está en estado “DTE en espera” . El paquete incluye la dirección del DTE llamado y puede incluir también la dirección del DTE llamante. Cuando el paquete llega al DCE asociado a B, el DCE indica que hay una llamada entrante transfiriendo por la interfaz DTE/DCE, un paquete de LLAMADA ENTRANTE (INCOMING CALL). Esto hace pasar al canal lógico al estado “DCE en espera” . El paquete incluye la dirección del DTE llamante. B indicará su aceptación de la llamada transfiriendo un paquete de LLAMADA ACEPTADA (CALL ACEPTED) que especifique el mismo canal lógico que el paquete LLAMADA ENTRANTE. Esto hace pasar al canal lógico especificado al estado “transferencia de datos” . Cuando el paquete llega al DCE asociado a A transfiere por la interfaz DTE/DCE un paquete de COMUNICACIÓN ESTABLECIDA (CALL CONNECTED). La recepción de este paquete en A con el mismo canal lógico que el establecido en el paquete PETICIÓN DE LLAMADA, le indica que la llamada a sido aceptada por el DTE llamado B y el canal lógico pasa al estado “transferencia de datos” . Cuando el DTE A recibe el paquete de COMUNICACIÓN ESTABLECIDA se establece el Circuito Virtual. El DTE determina el número de canal lógico para la petición de llamadas y el DCE para las llamadas entrantes. Cuando un DTE y un DCE transfieren simultáneamente un paquete PETICIÓN DE LLAMADA y un paquete de LLAMADA ENTRANTE con el mismo número de canal lógico se produce una colisión de llamadas . En este caso en la norma X.25 determina que se dar curso a la petición de llamada y se cancela la llamada entrante. En cualquier momento cualquiera de los DTEs puede indicar la liberación de la llamada transfiriendo por la interfaz DTE/DCE un paquete de PETICIÓN DE LIBERACIÓN (CLEAR REQUEST). El canal lógico está en este caso en estado “petición de liberación por el DTE” . El DCE del otro extremo indica la liberación transfiriendo un paquete de INDICACIÓN DE LIBERACIÓN (CLEAR INDICATION). El canal lógico está entonces en el estado de “indicación de liberación por el DCE” . El DTE responde transfiriendo un paquete de CONFIRMACIÓN DE LIBERACIÓN POR EL DTE, quedando el canal lógico en estado “preparado” . Cuando un DTE y un DCE transfieren simultáneamente un paquete PETICIÓN DE LIBERACIÓN y un paquete de INDICACIÓN DE LIBERACIÓN con el mismo número de canal lógico se produce una colisión de liberaciones . En este caso el DCE considera completada la liberación. Frame RelayFrame Relay (FR) es una tecnología WAN de alto rendimiento que opera en los niveles 1 y 2 (nivel físico y nivel de enlace de datos) del modelo de referencia OSI. Diseñado originalmente para operar con las interfaces de RDSI, en la actualidad se usa también sobre otras interfaces de red. FR es un ejemplo de tecnología de conmutación de paquetes que se considera como una evolución de X.25 y un paso de transición hacia ATM. Técnicamente es una tecnología diseñada para transmitir y distribuir datos a alta velocidad en unidades de longitud variable denominadas tramas. FR consigue rendimientos muy superiores a los de X.25 al eliminar la mayoría de los controles de errores, con lo que se disminuye el trasiego de información, pero esto hace que se necesiten líneas que garanticen un mínimo de fiabilidad. En FR se soportan velocidades de transmisión de hasta 45 Mbps aunque las implementaciones típicas no pasan de 1.5/2 Mbps. Se adapta muy bien a la interconexión de LANs y al tráfico en ráfagas que éstas presentan. Utiliza técnicas de multiplexación estadística que permiten que la conexión virtual que tiene tráfico en un momento dado utilice parte del ancho de banda que no está siendo utilizado en las otras conexiones con las que comparte el mismo enlace físico. La multiplexación estadística proporciona a la red ancho de banda bajo demanda: la red es capaz de obtener el ancho de banda que necesita cuando lo necesita sin tener que reservarlo por adelantado y mantenerlo sin usar hasta que sea requerido. FR nació como un estándar de facto en 1990 resultado del acuerdo entre un grupo de fabricantes de equipos de telecomunicaciones. Surgió como una solución transitoria para cubrir necesidades del mercado no satisfechas hasta ese momento, pero ha logrado una gran aceptación y en la actualidad juega un papel importante en la interconexión de redes. FR se desarrolló básicamente por las siguientes motivaciones: Demanda de mayores velocidades: esta necesidad venía provocada por el aumento creciente del tráfico de datos a ráfagas, la proliferación de las LAN, la arquitectura cliente/servidor y la integración LAN-WAN. Disponibilidad de mejores medios de transmisión: protocolos como X.25 o SNA se desarrollaron con una cierta complejidad para introducir procedimientos de corrección de errores, uno de los factores principales en la limitación del ancho de banda. Con la aparición de las líneas digitales, sobre todo la fibra óptica, se puede eliminar la mayor parte de estos procedimientos al ser medios con una tasa muy baja de errores. Mayor capacidad de procesamiento de los equipos conectados a la red: equipos como PCs o estaciones de trabajo demandan mayores velocidades a la vez que tienen capacidades de procesamiento que facilitan la gestión de tramas. Además existen ya familias de protocolos como TCP/IP capaces de gestionar el control de errores y secuenciación de tramas. Existen tres estándares FR: el estándar del FR Forum (asociación de fabricantes entre los que están Cisco, DEC y Nortel), el estándar ANSI y el estándar de la ITU-T. Estas tres fuentes de normas no son siempre coincidentes, lo que no ocurría en X.25. Para la transmisión de datos entre estaciones finales se utiliza el protocolo Q.992, que es una versión mejorada del protocolo LAP-D utilizado en RDSI. En FR sólo se utilizan las funciones básicas de Q.922: Delimitación, alineamiento y transparencia de tramas por medio de las banderas típicas de la familia de protocolos HDCL. Alineamiento de los límites de la trama. Detección de errores de transmisión mediante un campo FCS incluido en la trama. Multiplexación y desmultiplexación de tramas mediante el campo de direcciones incluido en la trama. Control de congestión. Circuitos Virtuales e Interfaces Frame RelayFrame Relay es un sistema orientado a conexión en el sentido de que antes de establecer la comunicación entre dos o más puntos se requiere previamente haber definido un camino o ruta por la cual tenga lugar la comunicación, es decir, existe una conexión definida entre cada par de dispositivos y estas conexiones están asociadas con un identificador de conexión. De manera similar a X.25 este servicio se implementa por medio de un circuito virtual Frame Relay. Los circuitos virtuales ofrecen una trayectoria de comunicación bidirecciónal de un dispositivo DTE a otro y se identifican de manera única por medio del Identificador de Conexiones de Enlace de Datos (DLCI, Data-Link Connection Identifier ). Además se pueden multiplexar muchos circuitos virtuales en un único circuito físico. Los DLCI generalmente son asignados por el proveedor del servicio y su valor tiene significación local, es decir, los DLCI son únicos en la LAN pero no necesariamente en la WAN Frame Relay. Al igual que en X.25 se tienen circuitos virtuales permanentes y conmutados. En FR lo más común son los PVC, que una vez programados permanecen en funcionamiento (se les use o no) hasta que se les desconecta. Tanto para los PVCs como para los SVCs se distinguen dos interfaces: UNI (User-to-Network Interface) : interfaz de usuario red. La interfaz UNI se establece entre el dispositivo de acceso a la red del usuario y un conmutador de la red. NNI (Network-to-Network Interface) : interfaz red a red. La interfaz NNI se establece entre dos conmutadores que pueden ser de la misma o de diferentes redes Frame Relay. En Frame Relay Forum (FRF) recomienda las siguientes interfaces físicas en la UNI: ANSI T1.403: interfaz a 1.5 Mbps. UIT-T V35: interfaz full-duplex a 56 o 64 Mbps. UIT-T G.703: interfaz a 2 Mbps. UIT-T X.21: interfaz síncrona. Una sesión de comunicación a través de un SVC consta de los siguientes estados operacionales: Establecimiento de llamada : se establece el circuito virtual entre dos dispositivos DTE. Transferencia de datos : los datos se transmiten entre los dispositivos DTE a través del circuito virtual. Desocupado (Idle) : la conexión entre los dispositivos DTE permanece activa pero no hay transferencia de datos. Si un SVC permanece durante un determinado periodo de tiempo en estado desocupado, la llamada puede darse por terminada. Finalización de llamada : se da por finalizado el circuito virtual entre los dispositivos DTE. En los PVC, al ser conexiones establecidas de forma permanente, no se requiere ni el establecimiento de llamada ni la finalización. Operan siempre en los estados transferencia de datos o desocupado. Control de Congestión y Control de TráficoFrame Relay, más que un control de flujo explícito por cada circuito virtual, implementa sencillos mecanismos de notificación de congestión. En general Frame Relay se implementa sobre medios de transmisión de red fiables para no sacrificar la integridad de los datos, el control de flujo se puede realizar por medio de los protocolos de las capas superiores. En FR se tienen dos mecanismos de notificación de congestión: FECN (Fordward-Explicit Congestion Notification) y BECN (Backward-Explicit Congestion Notification). Tanto FECN como BECN se implementan mediante los bits del subcampo control de congestión del campo direcciones de la trama FR. Este campo de control de congestión está formado por tres bits: el bit FECN, el bit BECN y el bit DE (Discard Eligibility). El mecanismo FECN se inicia en el momento en que un DTE envía tramas a a la red. Si la red está saturada, los DCE fijan el valor del bit FECN a 1. Cuando las tramas llegan al DTE destino, el bit FECN activado indica que en su trayectoria del origen al destino hubo problemas de congestión. El DTE puede enviar esta información a los protocolos de las capas superiores para su procesamiento. En el mecanismo BECN los DCEs establecen el valor del bit BECN a 1 en aquellas tramas que viajan en sentido opuesto a las tramas con bit FECN igual a 1. Esto permite al DTE receptor saber que una trayectoria específica en la red está saturada. Posteriormente el dispositivo DTE envía información a los protocolos de las capas superiores para su procesamiento. Dependiendo de la implementación, la indicación de congestión puede ser ignorada o puede que se inicien los procedimientos de control de flujo. Los DTEs pueden fijar el valor del bit DE de una trama a 1 para indicar que esta trama tiene una importancia menor respecto a otras tramas. En caso de congestión de la red DCE descartaran las tramas con el bit DE a 1 antes de descartar aquellas que no la tienen. Con esto se disminuye la probabilidad de que se eliminen datos críticos durante el blindaje de saturación. El control de tráfico en Frame Relay se basa en la especificación de varios parámetros de usuario: CIR (Committed Information Rate): es el caudal medio garantizado que la red se compromete a dar en una conexión. Bc (Committed Burst Size): máxima cantidad de bits que la red se compromete a enviar, en condiciones normales, en un intervalo de tiempo definido Tc . Este periodo Tc es acordado entre la red y el usuario y es por tanto conocido por ambos. EIR (Excess Information Rate): especifica un caudal adicional que la red no debería sobrepasar nunca ya que las tramas recibidas por encima de este valor serán descartadas directamente por el conmutador. Be (Excess Burst Size): máxima cantidad de bits que se le permite al usuario sobrepasar Bc durante el periodo Tc. Cada PVC y SVC tiene asignado un valor CIR que indica la capacidad de transmisión que ha sido acordada para el servicio. La relación entre estos parámetros es CIR = Bc/Tc y EIR = Be/Tc . Aunque la red no puede impedir que un usuario exceda el valor CIR sí incluye funciones de aviso de situaciones problemáticas y de recomendación de reducción de la velocidad de transmisión. Si la velocidad de transmisión es inferior al CIR todo el tráfico es cursado con garantías. Si la velocidad supera este valor las tramas se marcan como descartables activando el bit DE y serán transportadas por la red con una política “best effort”, llegarán al destino si durante el camino no se encuentran con congestiones importantes. Si se sobrepasa el valor EIR, todas las tramas recibidas por encima de ese valor serán descartadas. En la siguiente figura se esquematiza este mecanismo de control de tráfico. Formato de Trama Frame RelayEl formato de una trama FR es el siguiente: Bandera (8 bits): delimitan el comienzo y el final de cada trama. Su valor es siempre el mismo 01111110 (7E en hexadecimal). Dirección (16 bits): contiene la información siguiente: DLCI (10 bits): como ya se ha comentado identifica de manera única al circuito virtual. No todos los valores de DLCIs pueden ser asignados a conexiones virtuales para datos de usuarios, sino los valores del 16 al 1007. Otros DLCIs están reservados para señalización (como lo son los valores 0: reservado para la señalización de control de llamada y 1023: reservado para la Interfaz de Gestión Local) y otros valores están reservados para definiciones futuras como los valores del 1 al 15 y del 108 al 1022. EA (2 bits): bit de dirección extendida. Se utiliza para indicar el último byte del campo de dirección mediante un valor 1. Aunque todas las implementaciones actuales de Frame Relay utilizan un DLCI de dos bytes, esta característica permite que en el futuro se utilicen DLCIs más largos. El octavo bit de cada byte del campo de direcciones se utiliza para indicar el EA. C/R (1 bit): este bit comando/respueta está sin definir. Control de congestión (3 bits): consta de los bits FECN, BECN y DE. Datos (longitud variable): contiene información encapsulada de las capas superiores. Contiene los datos de usuario o carga útil, que tiene una longitud variable. Pudiendo llegar hasta los 8.188 bytes. FCS (16 bits): el campo de secuencia de verificación de trama sirve para la comprobación de errores de transmisión. LMI, Interfaz de Administración LocalLMI (Local Management Interface) es una interfaz que fue definida por los primeros fabricantes de sistemas FR (Cisco Systems, StrataCom y DEC entre otros)para ofrecer un conjunto de extensiones a la especificación FR básica. LMI ha servido de modelo para los estándares de señalización de Frame Relay de otros organismos de estandarización. Las principales extensiones de LMI son direccionamiento global, mensajes de status de circuito virtual y multidifusión. El direccionamiento global LMI da a los valores DLCI un significado global más que local. Los convierten en direcciones DTE únicas en la WAN Frame Relay. Esto agrega funcionalidad y facilita la administración de las redes Frame Relay al permitir, por ejemplo, que las interfaces de red individuales y los nodos terminales conectados a ellos se pueden identificar por medio de técnicas estándares de descubrimiento y resolución de direcciones. Además, toda la red Frame Relay aparece como una típica LAN. Los mensajes de status de circuitos virtuales LMI permiten la sincronización entre los DTEs y DCEs y la comunicación de su estado. Los mensajes de status se utilizan para informar de manera periódica del estado de los PVCs con lo que se previene el envío de datos a agujeros negros (esto es, a través de PVCs inexistentes). La multidifusión LMI permite que se asignen grupos de multidifusión. Con la multidifusión se ahorra ancho de banda pues permite que los mensajes sobre la resolución de direcciones y de actualizaciones de encaminamiento sean enviados solamente a grupos específicos de routers. ATMLa tecnología de Modo de Transferencia Asincrónica (ATM), está basada en los estudios del Grupo de Estudio XVIII de la ITU-T para el desarrollo de la Red Digital de Servicios Integrados de Banda Ancha (B-ISDN) para la transferencia de voz, vídeo y datos a altas velocidades a través de la red pública. En 1991 se crea el Forum ATM (fundado por Cisco Systems, NET/ADAPTIVE, Nortern Telecom y Sprint) que ha jugado un papel importante en el desarrollo de la tecnología ATM. En la actualidad con ATM es posible transferir voz, vídeo y datos a través de redes privadas y a través de redes públicas. ATM continúa evolucionando con varios grupos estándares que tratan de finalizar las especificaciones que permitan la interoperabilidad entre los equipos de diferentes vendedores en las industrias de redes públicas y privadas. ATM es una tecnología de conmutación de paquetes orientada a conexión que como tal crea circuitos virtuales entre los sistemas que desean intercambiar información. Estos circuitos se denominan canales virtuales (VC, Virtual Channel) en el estándar. Los nodos terminales de las redes ATM se denominan hosts y los nodos intermedios de encaminamiento se denominan conmutadores (de forma análoga a X.25 o Frame Relay). Los conmutadores ATM son siempre equipos de comunicaciones especializados y de elevadas prestaciones, nunca ordenadores de propósito general. En ATM existen tanto PVCs como SVCs. Los PVCs se configuran de manera estática en los conmutadores. Un PVC está establecido siempre que estén operativos los conmutadores por los que pasa y los enlaces que los unen, es decir siempre que la red está operativa. Los SVCs se crean y destruyen dinámicamente, según se necesita. El protocolo utilizado para establecer SVCs en ATM es el Q.2931, y está basado en el Q.931 utilizado en la señalización de RDSI. Una red (o subred) ATM está formada por un conjunto de conmutadores unidos entre sí por líneas punto a punto de alta velocidad, normalmente SONET/SDH de 155,52 Mbps, aunque también existen interfaces de velocidades inferiores. La interfaz que conecta los hosts con la subred es la UNI (User-Network Interface), y la que comunica los conmutadores entre sí es la NNI (Network-Network Interface). En ATM se agrupan los VCs entre dos nodos terminales en los denominados caminos o trayectos virtuales (VP, Virtual Path). Tanto los VCs como los VPs se numeran para su identificación. Para establecer una comunicación entre dos nodos es preciso especificar el número de VP y de VC. Cuando un nodo desea establecer un VC con otro ha de enviar un mensaje solicitando la conexión por el VC reservado para señalización, que por convenio es el VP 0 VC 5. Los estándares de ATM han definido paquetes de tamaño fijo llamados celdas con una longitud de 53 bytes . Una celda ATM consta de dos partes: la carga útil (payload) de 48 bytes que transporta la información generada por el emisor o transmisor, y la cabecera de 5 bytes que contiene la información necesaria para la transferencia de la celda. Formato de Cabecera de la Celda ATMEn los estándares ATM se definen dos formatos de cabecera: el formato de la cabecera de la UNI y el formato de la cabecera de la NNI. La recomendación I.361 de la ITU-T es la base de estas definiciones, con clarificaciones más amplias dadas en las especificaciones ANSI T1.627 y el Forum ATM UNI o B-ICI. El formato de la cabecera UNI es el siguiente: GFC, control de flujo genérico (4 bits) : este campo generalmente no se utiliza y se le asigna un valor por defecto, pero puede ser utilizado para proporcionar estaciones locales como por ejemplo la identificación de estaciones que comparten una única interfaz ATM. VPI, identificador de camino virtual (8 bits) : identifica el camino virtual por el que debe circular una celda. VCI, identificador de canal virtual (16 bits) : identifica el canal virtual por el que debe circular la celda dentro del VP especificado por el VPI. El VCI junto con el VCI identifica el próximo destino de la celda que pasa a través de una serie de conmutadores ATM en su trayecto hasta el destino final. PTI, tipo de carga (3 bits) : el primer bit indica si la celda contiene datos de usuario o datos de control. Si la celda contiene datos de usuario, el segundo bit indica si se detecta o no congestión; y el tercer bit indica si la celda es la última en una serie de celdas que representan a una única trama AAL5. CLP, prioridad de pérdida de celda (1 bit) : indica si la celda debería o no descartarse en el caso de que se encuentre en congestión extrema cuando se mueve a través de la red. HEC, control de error de cabecera (8 bits) : es un CRC que suministra la información de verificación de error de la cabecera. La cabecera NNI se diferencia de la cabecera UNI en que no existe el campo GFC y el campo VPI ocupa 12 bits. Categorías de ServicioPara poder satisfacer una amplia gama de necesidades se han definido en ATM las llamadas categorías de servicio. Cada una de ellas da al usuario un nivel de garantía diferente respecto a la disponibilidad de los recursos de red solicitados. Se han definido cuatro categorías de servicio: CBR (Constant Bit Rate), VBR (Variable Bit Rate), ABR (Available Bit Rate) y UBR (Unespecified Bit Rate). La categoría de servicio CBR garantiza una capacidad determinada y constante, que está continuamente disponible durante el tiempo de vida de la conexión, con independencia de la utilización que hagan de la red otros usuarios. Este servicio es el más sencillo de implementar y el más seguro de todos, ya que la red reserva la capacidad solicitada en todo el trayecto de forma estática. No se realiza ningún tipo de control de congestión, ya que se supone que ésta no puede ocurrir. El servicio CBR es equivalente a una línea dedicada punto a punto. La categoría de servicio VBR está pensada para un tráfico a ráfagas. El usuario especifica un caudal medio pero puede utilizar ocasionalmente caudales superiores en función de sus necesidades y del estado de la red. Esto da mayor flexibilidad al permitir ajustar el caudal a las necesidades medias. En algunos servicios VBR el tráfico excedente sale marcado con el bit CLP. Desde el punto de vista de la red, VBR tiene una complejidad superior a CBR. VBR tiene dos modalidades definidas por el Forum ATM: RT-VBR (Real-Time Variable Bit Rate), con requerimientos de bajo retardo y jitter para aplicaciones en tiempo real (videoconferencia, vídeo bajo demanda, etc.); y NRT-VBR (Nor-REAL-TIME Variable Bit Rate) para aplicaciones en las que el control del retardo no es tan importante, como por ejemplo la transferencia de ficheros. La categoría ABR , también pensada para tráfico a ráfagas, es de todas las categorías de servicio que ofrece ATM la que más se parece a Frame Relay. Permite establecer un ancho de banda mínimo garantizado y fijar un valor máximo orientativo. ABR es la única categoría de servicio ATM en la que se prevé que la red suministre control de flujo al emisor para que reduzca el ritmo de transmisión en caso de congestión, lo que hace de ABR apropiada para tráfico de datos pero menos apropiada para aplicaciones isócronas. Debido a su funcionalidad ABR es la categoría de servicio más compleja de implementar. La categoría de servicio UBR puede considerarse la de más baja calidad. No existe ningún tipo de garantía en cuanto al retardo o ancho de banda, y tampoco se informa al emisor en caso de congestión. UBR utiliza la capacidad sobrante en la red de las demás categorías de servicio. Puede utilizarse para enviar tráfico IP cuando el costo sea el factor principal y la calidad de servicio no sea importante. El ATM Forum ha definido una variante del servicio UBR denominada UBR+. Añade al servicio UBR la posibilidad de especificar una capacidad mínima requerida, lo que la hace similar a ABR pero sin el control de congestión. Calidad de Servicio y Descriptores de TráficoUna de las grandes virtudes de ATM es la posibilidad de establecer una Calidad de Servicio (QoS, Quality of Service) garantizada. En las redes ATM se pueden establecer una larga serie de parámetros que definen los niveles mínimos de calidad que el operador debe ofrecer al usuario para cada una de las categorías de servicio mencionadas en el punto anterior. Estos parámetros se pueden clasificar en dos grupos: parámetros de tráfico y parámetros de QoS. No todos los parámetros tienen sentido en todas las categorías de tráfico. Los parámetros de tráfico son los siguientes: MCR (Minimum Cell Rate): velocidad mínima que se considera aceptable para establecer el circuito ATM. PCR (Peak Cell Rate) y CDVT (Cell Delay Variation Tolerance): máximo caudal que permite el VC y tolerancia (pequeña) respecto a este caudal. SCR (Sustainable Cell Rate) y BT (Burst Tolerance): caudal medio máximo permitido y tolerancia a ráfagas (grande) respecto a este caudal. Los parámetros de Calidad de Servicio son los siguientes: MCTD (Maximum Cell Transfer Delay): es el retardo máximo permitido, es decir, el tiempo máximo que puede tardar la red en transmitir una celda de un extremo a otro del circuito. Peak-to-Peak CDV (Peak To Peak Cell Delay Variation): es el jitter o fluctuación máxima que se podrá producir en el retardo de las celdas. CLR (Cell Loss Ratio): es el porcentaje máximo aceptable de celdas que la red puede descartar debido a congestión. Cuando una celda es entregada en el destino con un retardo superior a MCTD se considera una celda perdida. No todos los parámetros son aplicables a todas las categorías de servicio. Por ejemplo en un servicio CBR se especifica PCR, pero no SCR ni MCR. En un servicio UBR no se especifica ningún parámetro. En la siguiente table se muestran los parámetros que se especifican normalmente en cada categoría de servicio: Modelo de Capas de ATMEl modelo de capas de ATM está formado por tres capas: capa física, capa ATM y capa de adaptación ATM. Capa Física La Capa Física controla la transmisión y recepción de bits en el medio físico y mantiene el rastro de los límites de las celdas y de los paquetes de celdas dentro del tipo de trama apropiado al medio físico utilizado. Está dividida en dos partes: subcapa dependiente del medio físico y subcapa de convergencia de transmisión. La Subcapa Dependiente del Medio Físico (PMD) es responsable de enviar y recibir un flujo constante de bits, junto con información de temporización con el fin de sincronizar la transmisión y la recepción. La Subcapa de Convergencia de Transmisión (TC) es la responsable de: Delimitación de Celdas: mantiene los límites de las celdas ATM. Generación y verificación del HEC: genera y chequea el código de control de error de cabecera para garantizar datos válidos. Desacoplamiento de velocidad de la celda: inserta o suprime celdas ATM no asignadas para adaptar a la velocidad válida de celdas ATM la capacidad de carga útil del sistema de transmisión. Adaptación a la trama de transmisión: empaqueta celdas ATM en tramas aceptables para su implementación en un medio físico en particular. Generación y recuperación de tramas de transmisión: genera y mantiene la estructura apropiada de la trama de la capa física. Capa ATM La Capa ATM es responsable del establecimiento de las conexiones y del paso de las celdas a través de la red ATM, para lo que utiliza la información contenida en la cabecera de cada celda ATM. Aquí es donde aparecen los conceptos de VCs y VPs vistos anteriormente. Entre las funciones de esta capa se incluyen: Control Genérico de Flujo. Generación/Extracción del encabezado de la celda. Enrutamiento de las celdas basado en los VPI/VCI de la celda. Detección de errores basado en el campo HEC. Multiplexación y Demultiplexación de celdas. Capa de Adaptación ATM (AAL) Dentro del modeloATM la capa que se ocupa de la comunicación host-host, y que por tanto podemos considerar de transporte, es la denominada Capa de Adaptación ATM (AAL, ATM Adaptation Layer). La ITU-T define la capa AAL en la recomendación I.363. Esta recomendación ha sido fruto de diversos compromisos y reajustes sobre la marcha. Dado que el objetivo de la capa AAL es adaptar diversos tipos de tráfico para su transporte sobre redes ATM, la ITU-T empezó estudiando y clasificando las clases de tráfico que podían tener cierto interés. Desde el punto de vista de la ITU-T los parámetros relevantes para esa clasificación eran tres: tiempo real o no tiempo real (tráfico isócrono o asíncrono); caudal de tráfico constante o variable; y servicio orientado a conexión o no orientado a conexión. Considerando entonces cuatro clases de tráfico: Clase A : en tiempo real con caudal de tráfico constante y servicio orientado a la conexión. Clase B : en tiempo real con caudal de tráfico variable y servicio orientado a la conexión. Clase C : no en tiempo real con caudal de tráfico variable y servicio orientado a la conexión. Clase D : no entiempo real con caudal de tráfico variable y servicio no orientado a la conexión. Para las cuatro clases descritas se definieron inicialmente cuatro protocolos denominados de AAL1 a AAL4 para las cuatro clases descritas. Posteriormente se observó que los requerimientos de los protocolos AAL3 y AAL4 eran similares y fueron agrupados en un protocolo conjunto AAL3/4. Las empresas fabricantes de equipos informáticos (conmutadores y adaptadores ATM), que se incorporaron tarde al proceso de estandarización de los protocolos AAL decidieron crear un nuevo protocolo que denominaron AAL5, para transportar la misma clase de datos que AAL3/4 pero de forma más eficiente. Se tienen entonces 5 tipos de protocolos AAL: AAL1 : soporta tráfico de clase A normalmente con una categoría de servicio CBR. El AAL1 es apropiado para soportar tráfico de voz y tráfico de vídeo no comprimido. Garantiza un mínimo retardo, un jitter pequeño y un reducido overhead de proceso y de información de control. AAL2 : es el estándar de protocolo utilizado para soportar el tráfico de clase B, generalmente con una categoría de servicio rt-VBR. AAL3/4 : soporta tráfico de clases C y D y puede utilizar cualquiera de las categorías de servicio, aunque la más utilizada es VBR. Es adecuado para tráfico de datos sensibles a pérdidas de celdas, pero no a retardos. AAL5 : soporta el transporte del tráfico de clase C y al igual que la clase anterior utiliza generalmente VBR, aunque se puede usar cualquiera de las categorías de servicio. La capa AAL está compuesta de dos subcapas: Subcapa de Segmentación y Reensamblado (SAR, Segmentation And Reassembly) : es la subcapa inferior y se ocupa, como indica su nombre, de crear en el emisor las celdas a partir de los datos recibidos de la subcapa superior, y de reconstruir en el receptor los datos originales a partir de las celdas recibidas. Subcapa de Convergencia (CS, Convergence Sublayer) : es la subcapa superiro y actúa de interfaz entre la capa AAL y la aplicación. Esta subcapa a su vez se subdivide en otras dos subcapas: Subcapa de Convergencia de Parte Común (CPCS, Common Part Convergence Sublayer) : constituye la parte baja de la CS. No depende de la aplicación, de ahí su nombre, pero sí depende del tipo de protocolo AAL utilizado. Subcapa de Convergencia Específica del Servicio (SSCS, Service Specific Convergence Sublayer) : representa la parte alta de la CS y es específica de la aplicación. Esta subcapa puede no estar presente, puede ser nula y hasta ahora está definida para Frame Relay y para SMDS. No se requiere para IP, pues IP es soportado directamente por laCPCS. Integración de Voz y DatosEl concepto básico para la integración de voz y datos es relativamente simple: se trata de transforma la voz en paquetes de información que puedan transmitirse por una red de conmutación de paquetes. A lo largo de los últimos años el avance tecnológico ha creado un entorno que posibilita la transmisión de voz sobre este tipo de redes. Entre los factores que han posibilitado este desarrollo se encuentran: Técnicas avanzadas de digitalización de voz. Protocolos de transmisión en tiempo real. Nuevos mecanismos de control y priorización del tráfico. Nuevos estándares que permiten la calidad de servicio sobre redes de paquetes. Uniendo a lo anterior el espectacular desarrollo de Internet, junto al ahorro que el uso de este tipo de tecnologías trae consigo, se ha llegado a la situación de considerar la integración de voz y datos un tema estratégico para muchas Organizaciones. Las tecnologías para el transporte de voz en forma de paquetes se enfrentan a una serie de retos: Retardo. Para proporcionar una calidad de servicio aceptable el retardo inducido en la red debe ser minimizado. El retardo deteriora la calidad de la voz y provoca interrupciones en una conversación normal extremo a extremo. Supresión de silencios. En el flujo de una conversación normal existen pausas y periodos de silencio. Esta característica se puede utilizar para el ahorro de ancho de banda mediante la parada de la transmisión de paquetes en estos periodos de silencio. Señalización. La señalización se refiere al uso eficiente de los recursos para la transferencia de información de control. Estos tres aspectos se ven afectados a su vez por las características de la red de transporte subyacente que puede ser FR, IP o ATM. Cuando la red de transporte es una red FR se habla de VoFR (Voice over Frame Relay), si es una red IP se habla de VoIP (Voice over IP) y si es una red ATM de VoATM (Voice over ATM). VoFRVoFR representa una tecnología relativamente estable para la integración de voz y datos. El Forum FR ha publicado dos especificaciones para permitir la interoperatividad entre productos VoFR de diferentes fabricantes: FRF.11 (VoFR): especifica los tipos de codificación y los formatos de trama para el transporte de tráfico de fax y de voz sobre redes FR. FRF.12 (Frame Relay Fragmentation): especifica los medios para la fragmentación y el reensamblado de grandes tramas de datos deforma que se minimice el retardo que se produciría al tener que encolar estas tramas en las tramas VoFR que son más pequeñas. VoIPVoIP es una tecnología, más reciente que VoFR, para la integración de voz en redes IP. El objetivo de VoIP es asegurar la interoperabilidad entre equipos de diferentes fabricantes. Se especifican aspectos tales como la supresión de silencios, la codificación de la voz y el direccionamiento, a la vez que se establecen nuevos elementos para permitir la conectividad con la infraestructura telefónica tradicional. Estos elementos se refieren básicamente a los servicios de directorio y a la transmisión de señalización por tonos multifrecuencia (DTMF). Está basado en la familia de estándares H.323 de la ITU-T que ya cubría la mayor parte de las necesidades para la integración de voz y gracias a otros protocolos de comunicación, como el RSVP (Resource ReSerVation Protocol), es posible reservar cierto ancho de banda dentro de la red que garantice la calidad de la comunicación. Dentro de esta familia, el protocolo de señalización H.225 cubre las especificaciones de registro, autorización y status de entidades H.323 con el Gatekeeper H.323. Para la negociación del establecimiento de llamada y de los parámetros de la codificación y sesión de voz se tienen los protocolos Q.931 y H.245 respectivamente. H.245 especifica el protocolo RTPC (Real-Time Control Protocol) para el canal de control de señalización de llamada y finalmente se tiene el protocolo RTP (Real-Time Transport Protocol) con la especificación de los canales bidireccionales entre las entidades llamantes y las entidades llamadas. VoATMVoATM es la opción más reciente para el transporte de voz sobre redes de datos. Permite a los conmutadores ATM transportar tráfico de voz sobre una red ATM. Su principal ventaja es el amplio soporte que proporciona para QoS. Cuando se envía tráfico de voz sobre ATM, éste es encapsulado en paquetes AAL1 o AAL2, aunque el servicio AAL1 CBR es menos eficiente para VoATM que el servicio AAL2 VBR. Bibliografía Scribd (Ibiza Ales) La seguridad en redes. Seguridad perimetral. Control de accesos. Técnicas criptográficas y protocolos seguros. Mecanismos de firma digital. Redes privadas virtuales. Seguridad en el puesto del usuario.Control de AccesosIntroducción¿Qué nos sugiere el título principal de este apartado?, “Control de accesos” , un análisis de estas tres palabras nos lleva a hacernos las siguientes preguntas: ¿Acceder a qué y por parte de quién ? ¿ Qué significa controlar el acceso? ¿ Por qué es preciso controlar el acceso a algo? Responder a la primera pregunta resulta sencillo, en el entorno de la seguridad en redes y en la informática en general, s e accede a información , a datos. También parece obvio que quién accede a estos datos, en última instancia, será una persona, aunque en pasos intermedios del proceso los datos puedan haber sido obtenidos por mediación de máquinas, que hayan tenido accesos unas a otras; pero en última instancia será una persona la que interprete los datos. Una máquina puede tomar decisiones en función de unos datos, pero siempre habrá sido programada por una persona. El conocimiento de las cosas supone poder y control sobre ellas, históricamente el tener datos o conocimientos no poseídos por rivales ha supuesto la victoria. Es decir, los datos son un bien preciado y también privado. En definitiva, el poseedor de información trascendental debe controlar el acceso que otros individuos pueden tener a estos datos, pues su publicidad no deseada podría ser peligrosa para él. Con este razonamiento se responde a la segunda y tercera preguntas planteadas. Controlar el acceso significa evitar que nuestros datos sean conocidos, modificados o borrados por intrusos, normalmente con malas intenciones. Tener el control de la privacidad de los datos porque de no hacerlo podría suponer un grave riesgo en múltiples facetas de nuestra vida. Siempre hay un riesgo de amenaza que atenta contra nuestra información. Cualquier dato privado puede estar amenazado por individuos que podrían hacer uso fraudulento y perjudicial para el propietario. Este riesgo se multiplica por un factor enorme si además estos datos van a estar en circulación, con esto estamos indicando que son datos que necesitan ser conocidos por distintas personas en distintos lugares. Si ya existe un riesgo en la privacidad y seguridad de los datos estando a “buen recaudo” por su dueño, mucho mayor será ese riesgo si los datos han de transmitirse de una fuente a un destino. La transmisión de datos implica un alto riesgo para la integridad y privacidad de los datos transmitidos. En nuestro días, el advenimiento de Internet, la red de redes, supone el intercambio de datos de forma inimaginable hace tan sólo unas décadas, el riesgo de amenazas y ataques ante expuesto hace que haya que tomarse muy en serio el control de acceso a la información y todo lo que ello implica. Un esquema básico para centrar las ideas antes expuestas y comenzar afrontando su estudio lo muestra la siguiente figura: Amenazas y ataques a la posesión y la transmisión de información. Por tanto, el control de acceso que vamos a estudiar en este apartado trata sobre “proteger” el acceso a los datos privados o esenciales de una entidad, ya sea una persona o una empresa. Protegerlos de cualquier tipo de ataque y/o de amenaza que pueda poner en manos no deseadas la información que se desea proteger. Seguridad en la RedEl concepto de seguridad en la información es más amplio que la simple protección de los datos. Para proporcionar una seguridad real se han de tener en cuenta múltiples factores internos y externos. En primer lugar habría que caracterizar el sistema que va a albergar la información para poder identificar las amenazas, y en este sentido se podrá realizar una división, muy general, entre: Sistemas aislados : Son los que no están conectados a ningún tipo de red. Sistemas interconectados : Hoy casi cualquier ordenador pertenece a alguna red, enviando y recogiendo información del exterior. Esto hace que las redes de ordenadores sean cada día más complejas y más peligrosas. Durante las primeras décadas de su existencia, las redes de ordenadores fueron usadas principalmente por investigadores universitarios para el envío de correo electrónico, y por empleados corporativos para compartir impresoras. En estas condiciones, la seguridad no recibió mucha atención. Pero ahora, cuando millones de ciudadanos comunes usan redes para sus transacciones bancarias, compras y declaraciones de impuestos, la seguridad de las redes es un problema potencial de grandes proporciones. Amenaza o ataque a una red. La mayoría de los problemas de seguridad en una red son causados intencionalmente por gente maliciosa que intenta ganar algo o hacerle daño a alguien. Algunos tipos más comunes de intrusos son: Los problemas de seguridad de las redes pueden dividirse en términos generales en cinco áreas interrelacionadas: Confidencialidad : La confidencialización, o secreto, tiene que ver con mantener la información fuera de las manos de usuarios no autorizados. Esto es lo que normalmente viene a la mente al pensar en la seguridad de las redes. Validación de Identificación : La validación de identificación determina con quién se está hablando antes de revelar información delicada o hacer un trato de negocios. Debemos tener seguridad en la identificación del remitente y destinatario, con lo cual aseguramos que “en el otro lado” está el usuario deseado y reconocido. No Repudio : El no repudio se encarga de las firmas digitales (analogía informática de las firmas del mundo real), con las que se pretende solventar el problema planteado por preguntas como la siguiente: ¿cómo comprobar que su cliente realmente solicitó (firmó) una orden electrónica por 2 millones de unidades del producto “X” a un precio de 5 euros cada una si más tarde el cliente alega que el precio estipulado fueron 4 euros? Integridad : Característica que previene contra la modificación o destrucción no autorizadas de los datos. También supone responder a la pregunta: ¿cómo puede asegurarse de que un mensaje recibido realmente fue el enviado, y no algo que un adversario malicioso modificó en el camino o preparó por su propia cuenta? Disponibilidad : Los elementos de un sistema deben estar disponibles para las entidades autorizadas. Puede ocurrir que haya elementos del sistema no disponibles por algún tipo de reparación o mejora técnica ya prevista por los propietarios de tal sistema. Lo que no debe tolerarse es que el sistema deje de estar en funcionamiento debido al ataque de un intruso malintencionado, es decir, que se pierda la disponibilidad como consecuencia de una seguridad pobre. Control de Acceso a RedesUno de los problemas esenciales de la seguridad cuando se involucra la conexión en red es la extensión, posiblemente incontrolada, de nuestro perímetro de seguridad. Sin embargo, esta extensión se produce en dos ámbitos bien distintos: uno controlable, pues afecta a dispositivos que se encuentran bajo nuestro dominio y responsabilidad,y otro incontrolado, pues corresponde al “mundo exterior. Echemos un vistazo a las medidas de control físico que debemos aplicar dentro de nuestra organización. No conviene olvidar que frecuentemente los ataques más fáciles y más frecuentes se producen desde dentro. Es decir, lo primero que hay que considerar es la propia red local (LAN). Una red de área local (LAN – L ocal A rea N etwork-) normalmente conecta equipos físicamente cercanos (por ejemplo, en el mismo edificio o grupo de edificios) que pertenecen a la misma organización, de quien depende su instalación, administración y mantenimiento. Las redes locales tienen como características genéricas relevantes su extendida utilización, su flexibilidad de uso, su facilidad para ampliar la conectividad y su inherente descentralización. Representan la primera capa en el esquema de conectividad de una organización. Pasemos ahora a analizar los principales elementos físicos a considerar en una red de área local. Elementos de Conexión Las LANs multiplican los problemas de seguridad física de una instalación informática. Por un lado, los accidentes, como por ejemplo, los cortes de corriente, pueden tener un efecto mucho más devastador que cuando afectan a un equipo aislado. Por otro lado, las redes incorporan nuevos elementos susceptibles de sufrir ataques a la seguridad física: los cables de una instalación pueden ser saboteados, pinchados o derivados y las tomas de datos pueden además ser reconectadas con fines ilícitos. Para hacer frente a estas eventualidades se recomienda reforzar todos los mecanismos de seguridad física. Los servidores de red deben estar especialmente protegidos, sobre todo en el aspecto eléctrico: tomas de tierra seguras, protección antiestática, e instalación de dispositivos de suministro ininterrumpido de corriente. Además, el cableado de la instalación debe estar siempre documentado, procurando que esté a la vista o que sea fácilmente registrable, así como que sea de apariencia homogénea y ordenada. Es imprescindible su inspección metódica y periódica. En algunos casos debe contemplarse la instalación de cableado redundante para proporcionar rutas alternativas en caso de problemas en las líneas de datos. El uso de cables blindados es recomendable si se desea evitar la colocación de vampiros, y cuando se trate de fibra óptica es importante tener en cuenta los repetidores, en los que la señal se torna eléctrica, y por tanto más vulnerable. Si se utiliza radiofrecuencia o infrarrojos para la transmisión, es imprescindible el cifrado de la señal. Los armarios de conexiones y concentradores deben disponer de cerradura, ésta debe usarse sistemáticamente (cosa que se verifica menos a menudo) y debe ser sólida (cosa aún más infrecuente). Ubicación y Uso de los Ordenadores Los servidores de red tienen el peligro de volverse “invisibles”, en el sentido de que, una vez funcionando, no requieren mucha atención. Su acceso debe estar restringido al máximo, o en todo caso debe deshabilitarse su disquetera, para evitar contaminación por virus. En el otro extremo están los equipos y rosetas de conexión poco utilizados, que pueden convertirse en puntos de acceso ilícito privilegiados. Ello no se debe solamente a que carezcan de vigilancia, sino también a que tienden a ser olvidados en las actualizaciones de los planes de seguridad. En este sentido una buena solución es instalar mecanismos de gestión de inventario, que se ocupen de rastrear e informar para que el administrador de seguridad pueda saber con precisión qué equipos están conectados en cada momento, dónde están y qué configuración tienen. Por último debemos citar el problema de las terminales desatendidas, que pueden provocar graves incidentes de seguridad si son aprovechadas por personal desaprensivo. Aparte del trabajo de concienciación de los usuarios, suele dar buen resultado instalar facilidades que despiden automáticamente la conexión a la red en caso de producirse un período de inactividad preestablecido. La Interconexión de RedesCon toda la importancia que tiene el no descuidar los aspectos citados en el capítulo anterior, sin duda el mayor desafío a la seguridad se produce cuando se conectan varias redes para formar una unidad superior. La organización de estas redes de redes es absolutamente distribuida, ya que conectan entre sí equipos de usuarios y organizaciones de todos tipo. Su soporte físico puede ser extraordinariamente variopinto: líneas telefónicas (cable, microondas o fibra óptica), transmisiones vía satélite, cables de TV, etc. Y, lo peor de todo, los problemas de seguridad en estos casos son la suma de los de sus componentes. Y, como suele decirse, un sistema es tan seguro como el más inseguro de sus componentes. IntrusionesEl análisis de los problemas de seguridad asociados a las redes de ordenadores se puede desdoblar en dos aspectos: Por un lado, la red puede ser vista como un punto de acceso adicional desde el cual nuestros bienes informáticos pueden ser atacados, dañados, sustraídos, etc. Es decir, susceptibles de algún tipo de intrusión por parte de individuos normalmente malintencionados. Por otro lado, la propia comunicación es un bien en sí mismo y la necesidad de su protección se añade a las que teníamos previamente. En el primer caso nos preocupa preservar los servicios de seguridad tradicionales (confidencialidad, integridad, disponibilidad, …) y contemplar las amenazas derivadas de la conexión en red y las posibles formas de ataque a nuestros datos. En definitiva, nos preocupará especialmente el problema de la intrusión y, por consiguiente, daremos relevancia al problema de la autenticación remota. En el segundo, aunque los ataques a la comunicación también se describen en función de los servicios de seguridad amenazados, suele ser frecuente hacer una clasificación algo distinta de las mismas. Si un ordenador (emisor) envía información de cualquier clase y por cualquier medio a otro (receptor). Formas Genéricas de AmenazaEn el mundo “normal”, fuera de la informática, las personas validan la identificación e otras personas al reconocer sus caras, voces, letra, etc. Las pruebas de firmas se manejan mediante firmas en papel, sellos, etc. Generalmente puede detectarse la alteración de documentos con el auxilio de expertos en escritura, papel y tinta. Ninguna de estas opciones está disponible electrónicamente. Es obvio que se requieren otras soluciones. En general, no se puede proteger un sistema pensando únicamente en un tipo de amenaza, las amenazas reales suelen ser combinaciones de varios tipos específicos. En términos globales, las formas de amenazas a la seguridad de un sistema informático lo podemos caracterizar teniendo en cuenta como esta información es almacenada, suministrada o transmitida por el sistema. Ya sabemos que, en general, hay un flujo de un almacén fuente de información a un almacén destino. El flujo ideal de información sería el que no tuviese ninguna amenaza, tal como indica la siguiente figura: Flujo ideal de información. El flujo ideal de información raramente sucede. Lo habitual es que la trasmisión esté supeditada a algún tipo de amenaza o riesgo potencial. Teniendo en cuenta esto, podemos señalar cuatro categorías de forma de amenazas. Interrupción Se produce interrupción cuando una tercera parte impide que la comunicación se establezca, evitando que los datos del emisor lleguen al receptor. Se puede realizar con conocimiento de los agentes de la comunicación o sin él, aunque este segundo supuesto es más difícil. En esencia es cuando un elemento del sistema es destruido o se hace inservible. Es una amenaza a la disponibilidad . Ejemplos son la destrucción de algún elemento hardware (discos, líneas de comunicación, etc.) o a la desactivación del sistema de gestión de ficheros. Interrupción. Intercepción Se produce intercepción (vulgarmente hablando, una escucha ) cuando una tercera parte no autorizada accede al contenido de la comunicación mientras esta se está produciendo. Normalmente la escucha se realiza sin necesidad de dejar huella alguna en la comunicación, por lo que ni el emisor y ni el receptor tienen por qué apercibirse de que se ha producido. Se trata de una amenaza contra la confidencialidad de los datos transmitidos. La parte no autorizada puede ser una persona, un programa o un ordenador. Ejemplos son la copia ilícita de programas y la visualización de ficheros que han de permanecer ocultos. Intercepción. Modificación Se produce modificación , también llamada manipulación , cuando una tercera parte no autorizada accede al contenido de la comunicación y lo modifica de forma que los datos que llegan al receptor difieren en algo de los enviados originalmente por el emisor. Si la manipulación está bien hecha también resulta transparente a los agentes de la comunicación, aunque a medida que transcurre el tiempo van aumentando sus posibilidades de ser descubierta. Se trata de una amenaza contra la integridad de los datos transmitidos. Una parte no autorizada no sólo obtiene acceso sino que puede modificar un elemento relacionado con la seguridad. Ejemplos son la alteración del contenido de un fichero y modificar un programa para que funcione de forma diferente. Modificación. Suplantación Se produce suplantación , también llamada impostura o fabricación , cuando una tercera parte no autorizada introduce mensajes propios en la comunicación para hacer creer al receptor que proceden del emisor. Como en el caso anterior, el propósito de la suplantación suele ser mantener el engaño durante un lapso de tiempo suficiente para realizar algún tipo de acción maligna. Se trata de una amenaza contra la integridad de los datos transmitidos. Una parte no autorizada inserta nuevos elementos en el sistema. Por ejemplo, la adición de registros a un fichero y la inclusión de mensajes espurios en una red. Suplantación. Tipos de AtaquesEn principio, consideramos que una vez que se cumple la amenaza ya estamos hablando de un ataque, aunque pensando únicamente en la seguridad, ambos conceptos pueden tomarse como equivalentes. Pueden considerarse como la misma cosa, pero una en potencia (amenaza) y otra en acto (ataque). Vamos a analizar varios tipos de ataques bajo distintas consideraciones. En ocasiones algunas de estas distinciones se solapan, pero lo importante es tener una idea del amplio repertorio de ataques que existen. Una distinción básica de los ataques es: Ataques Pasivos : Las agresiones pasivas son del tipo de las escuchas, o monitorizaciones. La meta del oponente es obtener información que está siendo transmitida. Existen dos tipos: Divulgación del contenido de un mensaje : Una conversación telefónica, un correo electrónico o un archivo transferido, puede contener información confidencial; por lo tanto sería deseable prevenir que el oponente se entere del contenido de estas transmisiones. Análisis de Tráfico : El agresor podría determinar la localización y la identidad de los computadores que se están comunicando y observar la frecuencia y la longitud de los mensajes intercambiados. Esta información puede ser útil para extraer la naturaleza de la comunicación que se está realizando. Los ataques pasivos son muy difíciles de detectar ya que no implican la alteración de la información. Sin embargo, es factible prevenir el éxito de estas agresiones. Así, el énfasis para tratar estas agresiones es la prevención antes que la detección. Ataques Activos : Suponen alguna modificación del flujo de datos o la creación de flujos falsos. Se subdividen en cuatro categorías: Enmascaramiento : Tiene lugar cuando una entidad pretende ser otra entidad. Por ejemplo, se puede captar una secuencia de autenticación y reemplazarla por otra secuencia de autenticación válida, así se habilita a otra entidad autorizada con pocos privilegios a obtener privilegios extras suplantando a la entidad que los tiene. Repetición : Supone la captura pasiva de unidades de datos y su retransmisión posterior para producir un efecto no autorizado. Modificación de mensajes : Alguna porción de un mensaje legítimo se altera para producir un efecto no deseado. Por ejemplo, un mensaje con significado “Permitir a X leer el archivo confidencial cuentas” se modifica para tener el significado “Permitir a Y leer el archivo confidencial cuentas”. Denegación de un servicio : Previene o inhibe el uso o gestión normal de una comunicación. Por ejemplo, una entidad puede suprimir todos los mensajes dirigidos a un destino particular. Otro ejemplo es la perturbación sobre una red completa, deshabilitándola o sobrecargándola con mensajes de forma que se degrade su rendimiento. Otra distinción de los tipos de ataque es: Ataques Accidentales : No son premeditados y en ellos podemos incluir los posibles fallos del hardware y software de nuestra instalación. Ataques Intencionados : Por medio de algo o de alguien se produce un ataque a nuestra información para fines distintos de los que fueron creados. También tenemos: Ataques Indiscriminados : Suelen ser los más frecuentes, y también los menos dañinos. Precisamente por su carácter general, existen programas específicos que nos protegen de ellos, como los antivirus. Ataques a Medida : Menos comunes que los anteriores, y más peligrosos, usualmente ataques que generalmente llevan a cabo los hackers. En estos casos las víctimas son casi siempre grandes corporaciones, y muchas veces la información ni siquiera es destruida o comprometida, puesto que los hackers sólo persiguen enfrentarse al reto que supone para ellos entrar en un sistema grande. Desde el punto de vista del acceso a la información , podemos distinguir dos grandes grupos de ataques: Ataques al Almacenamiento de la información. Ataques a la Transmisión de la información. Los ataques al almacenamiento se refieren al acceso no permitido a los lugares donde se guarda la información. Un ejemplo, fuera del mundo informático, sería el robo de una caja fuerte, es decir, un acceso fraudulento al contenido. En el mundo de las redes informáticas estas “cajas fuertes” son los sistemas de almacenamiento de datos de las computadoras, principalmente de servidores de información. En cuanto al almacenamiento de la información, hoy en día ésta se almacena en unidades de memoria masiva, principalmente en soporte magnético o soporte óptico. Se pueden atentar contra estos soportes de información de dos formas, a saber: Ataque Interno : Obtener el propio soporte de la información o poder acceder directamente a él, de forma que se pueda conseguir obtener la información que contiene, o modificarla o borrarla. Por ejemplo, sustraer un disco duro y leer su información o tener acceso a la computadora que contiene el disco. Ataque Externo : Acceder al soporte de información pero sin acceder físicamente a él. Normalmente por intromisión en el medio de transmisión de los datos del soporte fuente al soporte destino (cable eléctrico, onda electromagnética, fibra óptica, etc). Pero no es esta la única forma de ataque externo a un sistema, pueden existir otras maneras tal como veremos más adelante. Los ataques a la transmisión , también llamados ataques a las líneas de transmisiones, hace mención al peligro que existe de que unos datos que se transmiten por algún medio sean leídos, cambiados, eliminados, o cualesquiera otra acción por parte de terceros no autorizados a tener acceso a esos datos durante la mencionada transmisión. Se pueden distinguir los siguientes tipos de amenazas a las líneas de transmisiones: Ataques Pasivos : Tratan de monitorizar transmisiones de una organización. Son ataques relacionados con la intercepción y que afectan a la confidencialidad . Puesto que estas amenazas son difíciles de detectar, los esfuerzos deben encaminarse hacia su prevención más que a su detección y solución. Ataques Activos : Implican alguna modificación del flujo de datos, o la creación de un flujo de datos falso. Podemos subdividirlas en tres categorías: Modificación del flujo de información : Para producir un efecto no autorizado; afecta a la integridad. Denegación de servicio : Inhibiendo el uso normal de las facilidades de comunicación; afecta a la disponibilidad. Por ejemplo: La supresión de mensajes dirigidos a ciertos destinos, el trastorno del servicio, la deshabilitación de una red o sobrecargándola con mensajes, etc. Enmascaramiento : Cuando una entidad pretende ser otra; afecta a la integridad. Normalmente, un ataque de este tipo incluye alguno de los anteriores. Desde otro ángulo, una vez que se ha conseguido acceder a la información, podemos distinguir otros dos tipos de ataques que suponen una amenaza, estos son: Ataques al Hardware . Ataques vía Software . Los ataques al hardware , cuyo objetivo es la destrucción de datos por medio del acceso físico a éstos, puede ser: Destrucción del soporte físico que contiene los datos. Borrado directo de los datos que contienen el soporte, sin la destrucción de este. Por ejemplo, los datos en soportes magnéticos pueden ser borrados o alterados mediante la aplicación de un campo magnético externo que no sea el del propio cabezal de escritura y/o lectura. Otra forma típica es el borrado directo por medio de órdenes al SO, del tipo format , delete , etc. Los ataques vía software tienen su origen en programas que explotan las debilidades de los sistemas. Estos programas se dividen en dos grupos: aquellos que necesitan un programa anfitrión y aquellos que son independientes. Los primeros son trozos de programas que no pueden existir de forma autónoma, mientras que los segundos son programas completos que pueden ser planificados y ejecutados por el SO. También hay que distinguir entre programas que no se replican y los que lo hacen. Estos últimos son programas o trozos de programas que cuando se ejecutan pueden generar una o más copias de ellos mismos, que serán posteriormente activadas, y donde se pueden distinguir los siguientes tipos de ataques de origen software: Bomba Lógica : Es código incrustado en un programa que comprueba si ciertas condiciones se cumplen, en cuyo caso ejecuta alguna acción no autorizada. Estas condiciones pueden ser la existencia de ciertos ficheros, una fecha particular, la ejecución de una aplicación concreta, etc. Una vez que la bomba explota, puede alterar o eliminar datos, parar el sistema, etc. Puerta Falsa (Trapdoor) : Es un punto de entrada secreto en un programa, de forma que alguien que conozca la existencia de dicha puerta puede obtener permisos de acceso sin tener que pasar por los mecanismos normales de autentificación. La puerta falsa es un código que reconoce alguna secuencia de entrada especial o se dispara si es ejecutado por cierto usuario o por la ocurrencia de una secuencia determinada de sucesos. Caballo de Troya (Trojan Horse) : Es una rutina oculta en un programa de utilidad. Cuando el programa se ejecuta, se ejecuta la rutina y ésta realiza acciones no autorizadas y perniciosas. Estos programas permiten realizar de forma indirecta acciones que no puede realizar de forma directa. Por ejemplo, un programa caballo de Troya puede ser un editor que cuando es ejecutado modifica los permisos de los ficheros que edita de forma que éstos puedan ser accedidos por cualquier usuario. Virus : Es código introducido en un programa que puede infectar otros programas mediante la copia de sí mismo en dichos programas. Además de propagarse, un virus realiza alguna función no permitida. Bacteria : Programa que consume recursos del sistema replicándose, pero no daña ningún fichero. Se suele reproducir exponencialmente, por lo que puede acaparar recursos como CPU, memoria y disco. Gusano (Worm) : Es un software que usa las redes de computadores para pasar de unos sistemas a otros. Una vez que llega a un sistema, el gusano se puede comportar como un virus o una bacteria, puede implantar programas caballo de Troya, o puede realizar acciones no autorizadas. Para replicarse, los gusanos emplean algunos programas que proporcionan servicios de red, como correo electrónico, ejecución remota de programas y conexión a sistemas remotos. Señuelos : Son programas diseñados para hacer caer en una trampa a los usuarios. Señuelos muy comunes consisten en instalar, o de alguna forma hacer que la víctima instale (mediante algún tipo de engaño), un programa que registre las teclas presionadas para después analizar la información en busca de contraseñas, y posteriormente transmitir esta información a otro lugar de la red. Muchos de estos ataques o amenazas descritas son difíciles de prevenir, por tanto se deben dedicar esfuerzos sobre todo a la prevención, y obviamente a su detección y la recuperación de los trastornos o retardos que puedan causar. Esto también puede tener un efecto disuasorio, que refuerza la propia prevención. Amenazas GlobalesSupóngase una red de ordenadores locales, únicamente se comunican entre sí por medio de conexiones directas por cable eléctrico y están totalmente aislados, en términos de comunicación eléctrica, con cualquier computadora o sistema exterior. En suma, estamos convencidos de que no hay posible transmisión fuera de la red local. Por otra parte también vamos a suponer que nadie no autorizado puede tener acceso a los ordenadores, de forma que en apariencia la amenaza física no existe. ¿Están pues los datos contenidos en nuestra red local seguros?. La respuesta es NO. Esto es debido a que las computadoras actuales funcionan con circuitos eléctricos, y cualquier circuito eléctrico en funcionamiento emite radiación electromagnética. Esta radiación es una “huella” que revela la historia de funcionamiento del circuito. Además, si no se hace nada por “apantallarla” esta radiación puede alcanzar centenares de metros de distancia. Con lo que la seguridad física se hace muy difícil, resulta casi imposible controlar un espacio físico de centenares de metros cuadrados (por ejemplo, un edificio de oficinas con múltiples plantas y sus alrededores) de forma permanente en el tiempo. Existen dispositivos capaces de “rastrear” estas ondas y poder interpretarlas, obteniendo, con un proceso complicado pero factible, los datos que contienen los ordenadores que emiten la radiación. Durante muchos años muchos piratas informáticos se han dedicado a rondar edificios de importantes empresas con dispositivos de rastreo en busca de un precio botín (datos referentes a contraseñas, número secretos de cuenta y cualquier otro tipo de información privada o confidencial). No queda claro si la forma de amenaza descrita anteriormente es puramente lógica o puramente física, probablemente tenga un poco de ambas. Por una parte, aunque no se accede directamente a los ordenadores sí que hay que estar cerca para poder acceder a los datos. Por otro lado, aunque no estamos accediendo al medio formal de comunicación entre los ordenadores (por el cable de conexión directa), sí que estamos accediendo a un medio de transmisión no formal (residual) fruto de un efecto físico inevitable. Las dos formas básicas de evitar esta amenaza físico-lógica son: Apantallamiento Electromagnético : Aunque no se puede evitar la radiación electromagnética de los circuitos eléctricos, si se pueden crear pantallas para reducir la emisión de radiación, tal que ésta apenas alcance unos metros, con lo cual la seguridad física puede garantizarse. Materiales como el hierro o el plomo son los más utilizados para generar estos “apantallamientos”. Normalmente los ordenadores con información crítica son mantenidos en armarios de plomo y los cables de comunicación entre ellos utilizan algún tipo de cubierta que apantalla la radiación y elimina interferencias. Utilizar dispositivos de detección de rastreo : Al igual que existen sofisticados equipos de rastreo de señales, también existen dispositivos muy avanzados dedicados a localizar posibles rastreadores. Computación Óptica : En los últimos tiempos se está investigando mucho en computación sin circuitería eléctrica, de forma que todos los cálculos se hacen mediante procedimientos ópticos, que no producen la radiación espuria propia de los circuitos eléctricos. Las formas de amenazas y ataques rara vez se corresponde de forma pura a alguna de las expuestas hasta ahora, normalmente la amenaza suele ser una combinación de varios tipos, una amenaza de índole general o global. En definitiva, los ataques pueden venir de múltiples lados, en ocasiones de muchos. Jamás se puede decir que la seguridad es de un cien por cien, ni siquiera apagando todos los ordenadores y no haciendo transmisiones tenemos seguridad total, pues podemos ser víctimas de ataques de almacenamiento de tipo interno; siempre puede producirse un robo de algún soporte de información crítica, y no siempre este robo es realizado por extraños o terceros, también puede ser hecho por personas conocidas. No sería la primera vez que un empleado ha robado datos de su empresa para proporcionárselos a otra empresa rival (el conocido espionaje industrial). Como ya hemos comentado, muchas veces el ataque viene “desde dentro”. Valoración de las AmenazasDependiendo del contexto y la situación, el hecho de que cada una de las amenazas o ataques descritos anteriormente puede ser más o menos peligroso para los emisores y destinatarios de la información. Existen situaciones en las que lo importante es que llegue una información fidedigna, aunque sea interceptada por terceros. En otras circunstancias es preferible que no llegue ninguna información a que llegue información falsa. Lo que si se tiene claro es que hay que procurar que ninguna de las amenazas antes descritas llegue a concretarse. Desde el punto de vista conceptual el medio carece de importancia. Pues el hecho en sí, por ejemplo, de interrumpir un dato es independiente del medio. Si que dependerá del medio el cómo interrumpir, interceptar, modificar o suplantar los datos. A título de ejemplo, los métodos técnicos para interceptar información transmitida a través de un cable eléctrico no son los mismos que para interceptar una emisión por ondas electromagnéticas (radio, televisión, microondas, …). Otro factor a tener en cuenta cuando se valora la importancia de una amenaza es la relación entre la posibilidad de que esa amenaza se dé y el coste que supondría tener una protección muy elevada, rara vez puede ser absoluta, frente a esa amenaza. ¿Hasta qué punto merece la pena gastarse muchísimo dinero para conseguir una seguridad cercana al cien por cien sobre unos datos cuyo contenido no son de vital importancia?. Se pueden definir medidas de la valoración de un conjunto de amenazas con cocientes parecidos al siguiente: Puede variarse el factor anterior añadiendo algún tipo de “peso” o “sumando” a considerar, pero esencialmente la valoración responde a la idea de probabilidad de amenaza frente al coste de la protección. No tener en cuenta esto puede llevar a que el coste de la protección sea superior al valor de la propia información que se desea proteger, cosa totalmente irrazonable. Control de Acceso a la InformaciónEn esta sección daremos respuesta a esta pregunta: ¿Cómo protegerse de los ataques contra nuestros datos?. Veremos cuales son los principios de diseño de una buena protección para mantener nuestros datos y transmisiones seguras. Principios de Diseño para la Seguridad Algunos principios fundamentales de diseño relacionados con la seguridad, válidos para cualquier tipo de ataque o amenaza a la que haya que enfrentarse, son: Diseño abierto : La seguridad de un sistema no debe de depender de la suposición de que su diseño es secreto. Asumir que un intruso no lo conoce es engañar a los diseñadores. Negación en caso de duda : Por defecto, se deben negar los accesos no autorizados. Verificación completa : Toda operación debe ser contrastada con la información de control de acceso. Principio del menor privilegio : Todos los procesos deberían tener los mínimos privilegios necesarios para realizar sus tareas. Economía : El mecanismo de protección debe ser simple, uniforme e integrado hasta las capas más bajas del sistema. Aceptabilidad : El sistema elegido debe ser psicológicamente aceptable por los nuevos usuarios, ya que en caso contrario éstos no lo usarán. Protección Física La protección física , también llamada protección o seguridad interna, intenta mantener los datos privados sólo accesibles a los usuarios autorizados por medio de la seguridad física, es decir, por medio de elementos como: Personal de seguridad . Sala de acceso restringido por identificación personal . Reconocimiento de alguna propiedad física , como por ejemplo: Huella dactilar . Reconocimiento facial . Análisis de retina . Control de escritura . Reconocimiento de voz . Análisis de ADN . Tarjeta Identificadora . Supervisión por personal autorizado . Recintos vallados . En general, la práctica de la seguridad interna se basa en gran medida en la utilización de políticas de contraseñas y control de acceso a los contenedores de información. Protección Lógica Puede considerarse como protección lógica toda aquella que no es física, es el tipo de protección que no se fundamenta en restricciones impuestas por algo físico, como ocurren en la protección física de la información. Estas técnicas y mecanismos de seguridad constituyen la lógica que implanta un servicio de seguridad particular y responden a cómo implantar los servicios de seguridad. Los principales mecanismos de protección lógica son: Autenticación del Usuario Es una protección básica en cualquier sistema de información. Muchos esquemas de protección se basan en que el sistema conoce la identidad de todos los usuarios. El problema de identificar a los usuarios cuando éstos se conectan se denomina autentificación. La mayoría de los métodos de autentificación se basan en identificar algo que el usuario tiene o conoce. El mecanismo más común de autentificación consiste en que todo usuario ha de introducir una contraseña, solicitada por el programa de conexión cuando el usuario introduce su nombre. El inconveniente es que las contraseñas pueden ser averiguadas si el usuario utiliza su nombre, dirección, o datos similares como contraseña. Existen sistemas que procuran evitar esto asignándole a cada usuario un libro con una secuencia de contraseñas, de forma que cada vez que se conecta tiene que introducir la palabra de paso siguiente. El problema está en qué hacer si se pierde el libro de contraseñas. Otra forma obvia de averiguar una contraseña consiste en probar todas las combinaciones de letras, números y símbolos de puntuación hasta adivinar la contraseña. En instalaciones en las que la seguridad es prioritaria, estas medidas se pueden complementar con las protecciones de tipo físico vistas anteriormente, como son las restricciones de acceso a la habitación en la que se encuentran los terminales, asignar a cada usuario un terminal concreto, una tarjeta de identificación, establecer un horario concreto de trabajo, etc. Cortafuegos (Firewalls) Podríamos definir un cortafuegos como aquel sistema de red expresamente encargado de separar redes de comunicación de datos, efectuando un control del tráfico existente entre ellas. Este control consiste, en última instancia, en permitir o denegar el paso de la comunicación de una red a otra. En definitiva, son sistemas que controlan el tráfico dentro de las redes utilizando programas de seguridad situados en un servidor u ordenador independiente. Se diseñan para restringir el acceso a las redes de las organizaciones, especialmente desde el exterior. Analizando dónde se originan los paquetes, los dejan pasar o no. Los cortafuegos pueden tener distintas formas: filtrador de paquetes, cortafuegos a nivel de circuitos y a nivel de aplicación. El concepto que subyace detrás de un sistema cortafuegos es el de Seguridad Perimetral Centralizada, es decir, la creación de perímetros de separación implantados mediante puntos donde se centraliza el control de las comunicaciones. El caso más básico involucra a dos redes, una red a proteger (normalmente una red corporativa) y una red externa (normalmente Internet). Software de protección Aplicaciones utilizadas para proteger de los ataques vía software (virus, bacterias, troyanos, …) o en caso de que se haya producido ya el ataque, reparar el sistema y recuperar las pérdidas en lo posible. Los más conocidos de estas aplicaciones de protección son los antivirus. Criptografía Consiste en modificar los datos almacenados o el mensaje transmitido de forma que no sea entendible para un intruso que no conozca el sistema de alteración usado. A este proceso se le denomina cifrado . La filosofía subyaciente es que si no se puede evitar tener accesos a los datos, por lo menos se puede evitar que se entienda su significado, pero que solamente sea ininteligible para los intrusos, sin embargo, puede ser entendible para la persona a quien queremos enviar el mensaje. Realmente la criptografía es una rama de otra ciencia más amplia que es la criptología, que contempla tanto el cifrado de datos como el descifrado (operación inversa al cifrado). Firma digital Se basa en técnicas criptográficas, y cumple las mismas funciones que la firma manual: el receptor debe ser capaz de validar la firma del emisor, no debe ser falsificable y el emisor de un mensaje no debe poder repudiarlo posteriormente. Relleno del tráfico Se basa en introducir tráfico espurio junto con los datos válidos para que no se pueda conocer si se está enviando información o qué cantidad de datos útiles se están enviando. Etiquetas de seguridad Permiten que los mensajes sean clasificados para facilitar un correcto control de acceso y la separación de datos según clases de seguridad. Funciones de dispersión seguras (Hash ) Son funciones matemáticas sin inversa, que aplicadas a un elemento o dato que se transfiere impiden que este sea descifrado. También sirven para verificar la correcta recepción de los mensajes. Terceras Partes de Confianza (TTP) Son entidades cuyos informes se consideran fiables por todos los elementos del dominio de seguridad. Pueden tener registros y firmas digitales y emitir certificados dentro del sistema. Técnicas CriptográficasLa Criptología (del griego kryptos (oculto) y logos (estudio), estudio de lo oculto, lo escondido) es la ciencia que trata los problemas teóricos relacionados con la seguridad en el intercambio de mensajes en clave entre un emisor y un receptor a través de un canal de comunicaciones. Como hemos indicado en un apartado anterior, la idea es “ si no podemos evitar que capturen nuestro mensaje, por lo menos intentaremos que sea ininteligible para cualquier posible intruso, pero no para el destinatario legal “. Uso de la criptografía. Esta ciencia está dividida en dos grandes ramas: Criptografía : Se ocupa del cifrado de mensajes. Criptoanálisis : Es la parte contraria a la criptografía. Trata de descifrar los mensajes en clave y determinar la forma (el algoritmo) bajo el cual se ha obtenido el mensaje en clave. Es decir, conviene distinguir entre la palabra Criptografía , que sólo hace referencia al uso de códigos y la palabra Criptoanálisis , que engloba a las técnicas que se usan para romper dichos códigos. En cualquier caso ambas disciplinas están íntimamente ligadas; no olvidemos que cuando se diseña un sistema para cifrar información, hay que tener muy presente su posible criptoanálisis, para evitar sorpresas desagradables. Finalmente, como ya hemos comentado, el término Criptología se emplea habitualmente para agrupan tanto la Criptografía como el Criptoanálisis. CriptografíaLos sistemas criptográficos están teniendo un gran auge últimamente ante el miedo de que una transmisión en Internet pueda ser interceptada y algún desaprensivo pueda enterarse de alguna información que no debería. Y no estamos hablando de un correo electrónico en el que pensamos quedar con unos amigos, nos referimos a, por ejemplo, una transacción comercial o una información sobre temas empresariales. Desde la Antigüedad todas las civilizaciones han desarrollado sistemas de criptografía para que las comunicaciones no fueran públicas. Incluso hoy en día muchas personas utilizan lenguajes específicos para que solamente los iniciados en ellos puedan comprender la conversación como, por ejemplo, las jergas utilizadas en ambientes delictivos. Hay muchos sistemas para “camuflar” lo que escribimos. Quizá el más fácil sea la transposición o sustitución del texto. Consiste en cambiar cada letra del texto por otra distinta. Por ejemplo, si escribimos “boujwjsvt”, solamente las personas que supieran que hemos puesto la letra siguiente del alfabeto para escribir la palabra “antivirus” podrían entender la palabra. Otro sistema sencillo es el de elegir una frase fácil que recordar por el usuario y eliminar alguna parte también fácil de recordar, por ejemplo, si escribo “qtrlsvcls”, puede observarse que son las consonantes correspondientes a la frase “ q ui t a r l a s v o c a l e s “. Evidentemente los sistemas criptográficos actuales van mucho más allá de un sistema como el de transposición, o semejantes; fáciles de descubrir en unos cuantos intentos. Incluso si en lugar de trasponer un determinado número de espacios elegimos aleatoriamente las letras a sustituir, también bastaría con un ordenador que tuviera un simple corrector ortográfico para, en unos cuantos intentos, descubrir el significado de un mensaje. Los sistemas criptográficos de hoy en día se basan en la utilización de matemática avanzada, sobre todo a ciertas cualidades de los números, cuyo estudio pertenece a una rama avanzada de la matemática denominada teoría de números. Un tipo de números cuyas características son muy aprovechadas en la criptografía son los números primos. Una de a las tareas que más tiempo ocupa a los grandes sistemas de ordenadores es el cálculo de número primos cada vez mayores. El objetivo sería poder obtener un número que sirva para cifrar mensajes y que luego sea muy complicado descifrarlos. CriptosistemaSe define un criptosistema como una quíntupla ( M , C , K , E , D ), donde: M : Representa el conjunto de todos los mensajes sin cifrar (lo que se denomina texto claro, o plaintext) que pueden ser enviados. C : Representa el conjunto de todos los posibles mensajes cifrados, o criptogramas. K : Representa el conjunto de claves que se pueden emplear en el criptosistema. E : Es el conjunto de transformaciones de cifrado o familia de funciones que se aplica a cada elemento de M para obtener un elemento de C. Existe una transformación diferentes Ek para cada valor posible de la clave k. D : Es el conjunto de transformaciones de descifrado, análogo a E. Veamos un ejemplo. Retomemos el ejemplo de la transposición visto anteriormente, pero en este caso vamos a expresarlo con nuestra nueva terminología. Ejemplo de algoritmo de transposición. Vamos a suponer una transposición cíclica, es decir, la letra siguiente a la “z” es la “a” y la letra anterior a la “a” es la “z”. También suponemos, para mayor sencillez, que sólo usamos las 27 letras minúsculas del abecedario “ abcdefghijklmnñopqrstuvwxyz ” y palabras de 8 letras. La palabra antivirus es una más de nuestro conjunto M , puesto que es la palabra sin cifrar; y la “palabra” (por llamarla de alguna forma) boujwjsvt pertenece al conjunto C . En realidad, ambas palabras pertenecen a ambos conjuntos, pues ambas son susceptibles de ser cifradas y de ser descifradas, aunque no parezca tener mucho sentido cifrar una “palabra” como boujwjsvt. Es fácil ver que en nuestro caso M = C . Respecto al número de palabras posibles en M o en C, como tenemos 27 letras, en palabras de 8 letras donde el orden importa y puede darse la repetición, lo que tenemos (recordando la combinatoria) es una variación con repetición de 27 elementos tomados de 8 en 8. Así pues: Como se ha dicho, E es el conjunto de posibles transformaciones a aplicar a nuestro mensaje, en nuestro caso se trata de sustituir o transponer cada letra por la siguiente, pero podría ser, por ejemplo, no sustituir por la siguiente sino por la siguiente de la siguiente (la 2ª siguiente), y así sucesivamente, (3ª siguiente), etc. Así pues, considerando para el cifrado desplazamientos fijos hacia adelante, hacia la siguiente letra en el abecedario, tendremos: E \\= Transformaciones con desplazamiento fijo hacia delante sobre palabras de ocho letras en minúsculas (abecedario de 27 letras). Obsérvese que, al considerar un ciclo de 27 letras, la transposición (28ª siguiente) es igual a (1ª siguiente). Decimos que es fijo porque consideraremos que se aplica el mismo desplazamiento a todas las letras de la palabra, no vamos a considerar desplazamiento del estilo: la primera letra por la siguiente, la segunda por la 2ª siguiente, la tercera por la 3ª siguiente, etc. Ya tenemos el algoritmo, pero si nos dan un mensaje cifrado (perteneciente a C ) y nos piden descifrarlo, si no sabemos la cantidad de desplazamientos, tendríamos que probar con los 26 desplazamientos posibles y aún así no sabríamos con qué palabra de las 26 resultantes quedarnos. Es decir, se necesita saber el desplazamiento. Este desplazamiento es la denominada clave de cifrado del algoritmo. En nuestro ejemplo existen 26 posibles desplazamientos hacia delante, pues el desplazamiento 27 equivale al desplazamiento 0 (sin desplazamiento), el desplazamiento 28 equivale al desplazamiento 1, y así sucesivamente. Por tanto:K = {1, 2, 3, …, 26}En nuestro ejemplo es fácil ver que k=1. Esto lo indicaremos de lasiguiente forma:E1 = Cifrado por transposición con desplazamiento fijo hacia delante con k=1De análoga forma, tendremos que:D = Transformaciones con desplazamiento fijo hacia atrás sobre palabras de 8 letras en minúscula (abecedario de 27 letras).D1 = Descifrado por transposición con desplazamiento fijo hacia atrás con k=1. Y esto es todo. Vemos pues que únicamente hemos dado una notación matemáticaa lo que ya teníamos en mente. Sin embargo, esta definición de criptosistemay la notación definida nos va a venir muy bien para analizar ciertaspropiedades de la criptografía. Por ejemplo, en nuestro caso tenemos:E1(antivirus) = boujwjsvtD1(boujwjsvt)=antivirus Veamos ahora cómo expresar con esta notación una propiedad de todo criptosistema. Todo criptosistema ha de cumplir la siguiente condición : Es decir, que si tenemos un mensaje m , lo ciframos empleando la clave k y luego lo desciframos empleando la misma clave, obtenemos de nuevo el mensaje original m . En nuestro ejemplo: Vemos claramente como la ventaja de la notación utilizada es que, definiendo el criptosistema con la quíntupla (M,C,K,E,D) podemos expresar con ecuaciones, tan sencillas y cortas como la anterior, propiedades que nos llevaría mucho explicarlas de palabra. Y no sólo eso, sino que con esta notación podremos realizar operaciones algebraicas con las ecuaciones obteniendo resultados nuevos. Veamos ahora cuales son los dos tipos fundamentales de criptosistemas que existen: Criptosistemas Simétricos o de Clave Privada . Son aquellos que emplean la misma clave k tanto para cifrar como para descifrar. Presentan el inconveniente de que para ser empleados en comunicaciones la clave k debe estart anto en el emisor como en el receptor, lo cual nos lleva al grave problema de cómo transmitir la clave de forma segura; puesto que en muchos casos el emisor y receptor están a distancia y no es posible una comunicación personal entre ellos para intercambiar la clave de forma privada. Criptosistemas asimétricos o de llave pública, que emplean una doble clave (kp, kP) . kp se conoce como Clave Privada y kP se conoce como Clave Pública . Una de ellas sirve para la transformación E de cifrado y la otra para la transformación D de descifrado. En muchos casos son intercambiables, esto es, si empleamos una para cifrar la otra sirva para descifrar y viceversa. Estos criptosistemas deben cumplir además que el conocimiento de la clave pública kP no permita calcula la clave privada kp. Ofrecen un abanico superior de posibilidades, pudiendo emplearse para establecer comunicaciones seguras por canales inseguros -puesto que únicamente viaja por el canal la clave pública, que sólo sirve para cifrar-, o para llevar a cabo autentificaciones. En la práctica se emplea una combinación de estos dos tipos de criptosistemas, puesto que los segundos presentan el inconveniente de ser computacionalmente mucho más costosos que los primeros. Cuando utilizamos el término “computacional” nos referimos al tiempo que tarda un ordenador en calcular un algoritmo. Se dice que un algoritmo es computacionalmente intratable (con los ordenadores y tecnologías de hoy en día) cuando su resolución llevaría una cantidad de tiempo desorbitada, por ejemplo, cientos de años. En el “mundo real” se codifican los mensajes (largos) mediante algoritmos simétricos, que suelen ser muy eficientes, y luego se hace uso de la criptografía asimétrica para codifica las claves simétricas (cortas). En la inmensa mayoría de los casos los conjuntos M y C definidos anteriormente son iguales (como hemos visto en nuestro ejemplo). Esto quiere decir que tanto los texto claros como los textos cifrados se representan empleando el mismo alfabeto -por ejemplo, cuando se usa el algoritmo DES, ambos con cadenas de 64 bits-. Por esta razón puede darse la posibilidad de que exista algún _k_ perteneciente a K tal que Ek(m) = m, lo cual sería catastrófico para nuestros propósitos, puesto que el empleo de esas claves dejaría todos nuestros mensajes sin codificar. (En nuestro ejemplo anterior, esto pasaría si se utilizase k = 0, 27, 45, etc. De ahí que hallamos definido K = {1, 2, … 26}). También puede darse el caso de que ciertas claves concretas generen textos cifrados de poca calidad. Una posibilidad bastante común en ciertos algoritmos es que algunas claves tengan la siguiente propiedad: Ek(Ek(m)) = m, lo cual quiere decir que basta con volver a codificar el criptograma para recuperar el texto claro original. Estas circunstancias podrían llegar a simplificar enormemente un intento de violar nuestro sistema, por lo que también habría que evitarlas a toda costa. La existencia de claves con estas características, como es natural, dependen en gran medida de las peculiaridades de cada algoritmo en concreto, y en muchos casos también de los parámetros escogidos a la hora de aplicarlo. Llamaremos en general a las claves que no codifican correctamente los mensajes claves débiles (weak keys). Normalmente en un buen criptosistema la cantidad de claves débiles es cero o pequeña en comparación con el número total de claves. Pero conviene conocer esta circunstancia para evitar en lo posible sus consecuencias. Algoritmos SimétricosDesde muy antiguo han existido algoritmos de cifrado, algunos se remontan incluso, como el algoritmo de César, a la Roma Imperial. Todos estos algoritmos son algoritmos de clave privada o simétricos. Hasta la llegada de las computadoras, una de las restricciones principales del cifrado había sido la capacidad de la persona encargada del codificado para realizar las transformaciones necesarias, frecuentemente en un campo de batalla (la historia de la criptografía está muy unida al ejército) con poco equipo. Una restricción adicional ha sido la dificultad de cambiar rápidamente de un método de cifrado a otro, puesto que esto significa el reentrenamiento de una gran cantidad de gente. Sin embargo, el peligro de que un empleado fuera capturado por el enemigo ha hecho indispensable la capacidad de cambiar el método de cifrado al instante, de ser necesario. De estos requisitos en conflicto se deriva el modelo de la figura siguiente: El modelo de cifrado tradicional. Hoy en día, cualquier ordenador doméstico podría descifrarlos rápidamente, pero que fueron empleados con éxito hasta principios del siglo XX. Conviene detenerse someramente en su estudio pues mantienen un interés teórico que nos van a permitir explotar algunas de sus propiedades para entender mejor los algoritmos modernos. Veamos los más conocidos de estos algoritmos simétricos clásicos. Cifrados Monoalfabéticos. Se engloban en este apartado todos los algoritmos criptográficos que, sin desordenar los símbolos dentro del mensaje, establecen una correspondencia única para todos ellos en todo el texto. Es decir, si al símbolo A le corresponde el símbolo D, esta correspondencia se mantiene a lo largo de todo el mensaje. Los ejemplos más típicos de cifrados monoalfabéticos son: Algoritmo de César : El algoritmo de César, llamado así porque es el que empleaba Julio César para enviar mensajes secretos, es uno de los algoritmos criptográficos más simples. Consisten en sumar 3 al número de orden de cada letra. De esta forma a la A le corresponde al D, a la B la E, y así sucesivamente. Si asignamos a cada letra un número (A = 0, B = 1, …), y consideramos un alfabeto de 27 letras anterior, la transformación criptográfica sería: C = (M + 3) mod 26 ; siendo A mod B = Resto de la división A/B. Obviamente, para descifrar basta con restar 3 al número de orden de las letras del criptograma. Al ser siempre la clave fija, en este caso puede decirse que este algoritmo no tiene clave. Obsérvese la forma de expresar el algoritmo de transposición o desplazamiento mediante la operación (mod). Nuestro ejemplo anterior es equivalente a este algoritmo pero con C = (M+1) mod 26 (obviando el hecho de que habíamos fijado el tamaño de las palabras en ocho caracteres, por cuestiones de comodidad, cosa que evidentemente no ocurre en el algoritmo de César). Sustitución Afín : La sustitución afín es el caso más general del algoritmo de César. Su transformación sería: E(a,b)(M) = (aM + b) mod N . Siendo a y b dos números enteros menores que el cardinal N del alfabeto, y cumpliendo que mcd(a,N) = 1. La calve de cifrado k viene entonces dada por el par (a, b). El algoritmo de César sería pues una transformación afín con k = (1, 3). Nuestro ejemplo anterior es una transformación afín con k=(1,1). Cifrado Monoalfabético General. Es el caso más general de cifrado monoalfabético. Se considera cualquier algoritmo que realice una asociación biyectiva (1 a 1) de las letras del alfabeto consigo mismo. Normalmente la clave se proporciona a través de una tabla que indica la correspondencia aleatoria entre las diferentes letras, salvo el caso de la existencia de una ecuación matemática que pueda servir para establecer la relación biyectiva, como ocurre en la sustitución afín. La sustitución ahora es arbitraria, siendo la clave k precisamente la tabla de sustitución de un símbolo por otro. Cifrado Polialfabético . En los cifrados polialfabéticos la sustitución aplicada a cada carácter varía en función de la posición que ocupe éste dentro del texto claro. En realidad corresponde a la aplicación cíclica de n cifrados monoalfabéticos. El ejemplo más típico de cifrado polialfabético es el Cifrado de Vigenere , que debe su nombre a Blaise de Vigenere, su creador, y que data del siglo XVI. La clave está constituida por una secuencia de símbolos K = {k0, k1, …, kd-1} , y que emplea la siguiente función de cifrado: Ek(mi) = mi + k(i mod d) (mod n) siendo _mi_ el i-ésimo símbolo de texto claro y _n_ el cardinal del alfabeto de entrada. Algoritmo DES El algoritmo DES es el algoritmo simétrico más extendido muldialmente. Se basa en el algoritmo LUCIFER, desarrollado por IBM a principios de los setenta, y adoptado como estándar por el Gobierno de los EE.UU. para comunicaciones no clasificadas en 1976. El algoritmo DES fue diseñado por la NSA ( A gencia N acional de S eguridad de los EE.UU.) para ser implementado por hardware, creyendo que los detalles iban a ser mantenidos en secreto, pero la Oficina Nacional de Estandarización publicó su especificación con suficiente detalle como para que cualquiera pudiera implementarlo por software. No fue casualidad que el siguiente algoritmo adoptado (Skipjack) fuera mantenido en secreto. A mediados de 1998, se demostró que un ataque por la fuerza bruta a DES era viable, debido a la escasa longitud que emplea en su clave. No obstante, el algoritmo aún no ha demostrado ninguna debilidad grave desde el punto de vista teórico, por lo que su estudio sigue siendo plenamente interesante. El algoritmo DES se basa en las denominadas S-Cajas. Una S-Caja de m*n bits es una tabla de sustitución que toma como entrada cadenas de _m_ bits y da como salida cadenas de _n_ bits. (A) S-Caja individual. (B) Combinación de cuatro S-Cajas. DES emplea ocho S-Cajas de 6*4 bits. La utilización de las S-Cajas es sencilla: se divide el bloque original en trozos de m bits y cada uno de ellos se sustituye por otro de n bits, haciendo uso de la S-Caja correspondiente. Normalmente, cuanto más grandes sean las S-Cajas, más resistente sería el algoritmo resultante, aunque la elección de los valores de salida para que den lugar a un buen algoritmo no es en absoluto trivial. Las características básicas del algoritmo DES son: Codifica secuencialmente bloques de 64 bits . Emplea claves de 56 bits . Se usa el mismo algoritmo tanto para cifrar como para descifrar . Es decir, el mensaje original se trocea en bloques o submensajes de 64 bits (8 bytes) de tamaño, que son las unidades que procesa el DES, y luego se unen a la salida para formar el mensaje cifrado. Gráficamente: m1, m2, … mn: Bloques de 64 bits que conforman el mensaje cifrado.c1, c2, …, cn: Bloques cifrados resultado de aplicar DES a los bloques anteriores. Funcionamiento global del algoritmo DES. Funcionamiento específico del algoritmo DES. El texto plano o nativo debe tener una longitud de 64 bits y la clave 56 bits; los textos nativos más grandes se procesan en bloques de 64 bits. La parte izquierda de la figura muestra que el procesamiento del texto nativo se realiza en tres partes: Los 64 bits de texto nativo se transforman por medio de una permutación inicial que reordena a los bits para producir la entrada permutada. Luego sigue una fase de 16 iteraciones de la misma función. La salida de la última iteración consta de 64 bits que son función de la clave y del texto nativo. A continuación se intercambia la mitad derecha con la mitad izquierda para producir la salida previa. La salida previa se permuta con la inversa de la función de permutación inicial para producir los 64 bits de texto cifrado. La parte derecha de la figura muestra como se usan los 56 bits de la clave. Inicialmente se transforma la clave por una función de permutación. Luego se produce una subclave ki para cada una de las 16 iteraciones por medio de un desplazamiento circular y una permutación. La función de permutación es la misma para las 16 iteraciones; pero se produce una subclave distinta para cada una debido al desplazamiento circular de los bits de la clave. El algoritmo DES presenta algunas claves débiles. En general, todos aquellos valores de la llave que conducen a una secuencia inadecuada deki serán poco recomendables. Distinguiremos entre claves débiles, que son aquellas que generan un conjunto de dieciséis valores iguales de ki -y que cumplen Ek(Ek(M)) = M-, y claves semidébiles, que generan dos valores diferentes de ki, cada uno de los cuales aparece ocho veces. En cualquier caso, el número de llaves de este tipo es tan pequeño en comparación con el número total de posibles claves, que no debe suponer un motivo de preocupación. Variantes del Algoritmo DES El algoritmo DES padece de un problema que no radica en su diseño, sino en que emplea una clave demasiado corta (56 bits), lo cual hace que con el avance actual de las computadoras los ataques por la fuerza bruta comiencen a ser opciones realistas. Mucha gente se resiste a abandonar este algoritmo, precisamente porque ha sido capaz de sobrevivir durante veinte años sin mostrar ninguna debilidad en su diseño, y prefieren proponer variantes que, de un lado evitarían el riesgo de tener que confiar en algoritmos nuevos, y de otro permitirían aprovechar gran parte de las implementaciones por hardware existentes de DES. De ahí que se hayan desarrollado múltiples variantes del DES que alivien el problema de la clave demasiado corta pero que permitan aprovechar la gran cantidad de implantación hardware que existe. DES Múltiple : Consiste en aplicar varias veces el algoritmo DES con diferentes claves al mensaje original. El más común de todos ellos es el Triple-DES, cuya longitud de clave es de 112 bits. DES con Subclaves Independientes : Consiste en emplear subclaves diferentes para cada una de las 16 rondas de DES. Puesto que estas subclaves son de 48 bits, la clave resultante tendría 768 bits en total. Sin pretender entrar en detalles, puede demostrarse empleando criptoanálisis diferencial que esta variante podría ser rota con 261 textos claros escogidos, por lo que en la práctica no presenta un avance sustancial sobre DES estándar. DES Generalizado : Esta variante emplea n trozos de 32 bits en cada ronda en lugar de dos, aumentando tanto la longitud de la clave como el tamaño de mensaje que se puede codificar, manteniendo sin embargo el orden de complejidad del algoritmo. No sólo se gana poco en seguridad, sino que en muchos casos se pierde. Algoritmo IDEA El algoritmo IDEA ( I nternational D ata E ncryption A lgorithm) es más joven que DES, pues data de 1992. Para muchos constituye el mejor y más seguro algoritmo simétrico disponible en la actualidad. Sus características son: Trabaja con bloques de 64 bits de longitud. Emplea una clave de 128 bits . Como en el caso de DES, se usa el mismo algoritmo tanto para cifrar como para descifrar . IDEA es un algoritmo seguro, y hasta ahora resistente a multitud de ataques, entre ellos el criptoanálisis diferencial. No presenta claves débiles, y su longitud de clave hace imposible en la práctica un ataque por la fuerza bruta. Como todos los algoritmos simétricos de cifrado por bloques, IDEA se basa en los conceptos de confusión difusión, haciendo uso de las siguientes operaciones elementales (todas ellas fáciles de implantar): XOR. Suma módulo 216. Producto módulo 216 + 1. Como idea general, diremos que el algoritmo IDEA consta de ocho rondas. Dividiendo el bloque X a codificar, de 64 bits, en cuatro partes X1, X2, X3 y X4 de 16 bits. Para la interpretación entera de dichos registros se emplea el criterio big endian , lo cual significa que el primer byte es el más significativo. Las primeras ocho subclaves se calculan dividiendo la clave de entrada en bloques de 16 bits. Las siguientes ocho se calculan rotando la clave de entrada 25 bits a la izquierda y volviendo a dividirla, y así sucesivamente. Algoritmo Rijndael (AES) En octubre de 2000 el NIST ( N ational I nstitute for S tandards and T echnology) anunciaba oficialmente la adopción del algorimto Rijndael como nuevo Estándar Avanzado de Cifrado (AES) para su empleo en aplicaciones critográficas no militares, culminando así un proceso demás de tres años, encaminado a proporcionar a la comunidad internacional un nuevo algoritmo de cifrado potente, eficiente, y fácil de implementar. La palabra Rijndael -en adelante, emplearemos la denominación AES- es un acrónimo formado por los nombres de sus dos autores, los belgas Joan Daemen y Vincent Rijmen . Su interés radica en que todo el proceso de selección, revisión y estudio tanto de este algoritmo como de los restantes candidatos, se ha efectuado de forma pública y abierta, por lo que, prácticamente por primera vez, toda la comunidad criptográfica mundial ha participado en su análisis,lo cual convierte a Rijndael en un algoritmo digno de la confianza de todos. AES es un sistema de cifrado por bloques, diseñado para manejar longitudes de clave y de bloque variables, ambas comprendidas entre los 128 y los 256 bits. Realiza varias de sus operaciones internas a nivel de byte, interpretando éstos como elementos de un cuerpo de Galois GF(28) (un cuerpo de Galois -Matemático Francés del siglo XVIII- es un tipo de estructura de números perteneciente a la teoría de Grupos). El resto de operaciones se efectúan en términos de registros de 32 bits. Sin embargo, en algunos casos, una secuencia de 32 bits se toma como un polinomio de grado inferior a 4, cuyos coeficientes son a su vez polinomios en GF(28). Si bien, este algoritmo soporta diferentes tamaños de bloque y clave, en el estándar adoptado por el Gobierno Estadounidense en noviembre de 2001 (FIPS PUB 197), se especifica: Longitud fija de bloque de 128 bits . Longitud de clave a escoger entre 128, 192 y 256 bits . Algoritmos AsimétricosHistóricamente el problema de distribución de claves siempre ha sido la parte débil de la mayoría de los criptosistemas. Sin importar lo robusto que sea un criptosistema, si un intruso puede robar la clave, el sistema no vale nada. Dado que todos los criptólogos siempre daban por hecho que la clave de cifrado y la clave de descifrado eran la misma (o que se podía derivar fácilmente una de la otra) y que la clave tenía que distribuirse a todos los usuarios del sistema, parecía haber un problema inherente: las claves se tenían que proteger contra robo, pero también se tenían que distribuir, por lo que no podían simplemente guardarse en una caja fuerte. En 1976, dos investigadores de Stanford, Diffie y Hellman (1976), propusieron una clase nueva de criptosistema, en el que: Las claves de cifrado y descifrado son diferentes. La clave de descifrado no puede derivarse de la clave de cifrado, y viceversa. El algoritmo de cifrado (con clave), E, y el algoritmo de descifrado (con clave), D, tenían que cumplir 3 requisitos: D(E(m)) = m. Es extraordinariamente difícil deducir D de E. E no puede descifrarse mediante un ataque de texto normal seleccionado. El primer requisito dice que, si aplicamos _D_ a un mensaje cifrado, E(m) , obtenemos nuevamente el mensaje de texto normal original, _m_ . El segundo requisito no requiere explicación. El tercer requisito es necesario porque, como veremos en un momento, los intrusos pueden experimentar a placer con el algoritmo. En estas condiciones, no hay razón para que una clave de cifrado no pueda hacerse pública. El método funciona como sigue. Una persona, llamémosla Juan, que quiera recibir mensajes secretos, primero diseña dos algoritmos, _Ea_ y _Da_ , que cumplan los requisitos anteriores. El algoritmo de cifrado y la clave, Ea, se hacen públicos, de ahí el nombre de criptografía de clave pública (para contrastar con la criptografía tradicional de clave secreta). Esto podría hacerse poniéndolos en un archivo accesible a cualquiera que quiera leerlo. Alicia publica el algoritmo de descifrado, pero mantiene secreta la clave de descifrado. Por tanto, _Ea_ , es pública, pero _Da_ es secreta. Ahora veamos si podemos resolver el problema de establecer un canal seguro entre Juan y Alicia, que nunca han tenido contacto previo. Se supone que tanto la clave de cifrado de Juan, _Ea_ , como la clave de cifrado de Alicia, _Eb_ , están en un archivo de lectura pública. (Básicamente, se espera que todos los usuarios de la red publiquen sus claves de cifrado tan pronto como se vuelven usuarios de la red). Idea básica de los conceptos de clave pública y privada. Llegado a este punto, Juan toma su primer mensaje, _m_ , calcula Eb(m) y lo envía a Alicia. Alicia entonces lo descifra aplicando su clave secreta _Db_ [es decir, calcula Db(Eb(m)=m) ]. Nadie más puede leer el mensaje cifrado, Eb(m) , porque se supone que el sistema de cirado es robusto y porque es demasiado difícil derivar _Db_ de la _Eb_ públicamente conocida. Alicia y Juan ahora se pueden comunicar con seguridad. Es útil una nota sobre terminología. La criptografía de clave pública requiere que cada usuario tenga dos claves: una clave pública, usada por todo el mundo para cifrar mensajes a enviar a ese usuario, y una clave privada, que necesita el usuario para descifrar los mensajes. Consistentemente nos referiremos a estas claves como claves públicas y privadas , respectivamente, y las distinguiremos delas claves secretas usadas tanto para cifrado como descifrado en la criptografía convencional de clave simétrica. Estos algoritmos de llave pública, o algoritmos asimétricos, han demostrado su interés para ser empleados en redes de comunicación inseguras (Internet). Hasta la fecha han aparecido multitud de algoritmos asimétricos, la mayoría de los cuales son inseguros; otros son poco prácticos, bien sea porque el criptograma es considerablemente mayor que el mensaje original, bien sea porque la longitud de la clave es enorme. Se basan en general en plantear al atacante problemas matemáticos difíciles de resolver. La única dificultad estriba en que necesitamos encontrar algoritmos que realmente satisfagan los tres requisitos indicados anteriormente. Debido a las ventajas potenciales de la criptografía de clave pública, muchos investigadores están trabajando día y noche, y ya se han publicado algunos algoritmos. En la práctica muy pocos algoritmos son realmente útiles. El más popular por su sencillez es RSA, que ha sobrevivido a multitud de ataques, si bien necesita una longitud de clave considerable. Otros algoritmos son los de ElGamal y Rabin. Funcionamiento genérico de la criptografía asimétrica. Expresado de manera formal, tal como muestra la figura, el proceso de transmisión de información empleando algoritmos asimétricos es el siguiente: Paso 1 : _A_ tiene el mensaje _m_ y quiere enviárselo a _B_ . Paso 2 : _B_ envía a _A_ su clave pública, _KP_ . Paso 3 : _A_ codifica el mensaje _m_ y envía a _B_ el criptograma EKP(m) . Paso 4 : _B_ decodifica el criptograma empleando la clave privada _Kp_ . Los algoritmos asimétricos emplean generalmente longitudes de clave mucho mayores que los simétricos. Por ejemplo, mientras que para algoritmos simétricos se considera segura una clave de 128 bits, para algoritmos asimétricos -si exceptuamos aquellos basados en curvas elípticas- se recomiendan claves de al menos 1024 bits. Además, la complejidad de cálculo que comportan estos últimos lo hacen considerablemente más lentos que los algoritmos de cifrado simétricos. En la práctica los métodos asimétricos se emplean únicamente para codificar la clave simétrica de cada mensaje o transacción particular. A modo de resumen de lo dicho anteriormente, es importante recordar que: Los algoritmos asimétricos poseen 2 claves diferentes: Kp (Clave privada) y KP (Clave pública) . Se emplea una de ellas para codificar , mientras que la otra se usa para decodificar . Dependiendo de la aplicación que demos al algoritmo, l a clave pública será la de cifrado o viceversa . Para que estos criptosistemas sean seguros ha de cumplirse que a partir de una de las claves resulte extremadamente difícil calcular la otra . Algoritmo RSA El algoritmo RSA, cuyo nombre deriva de las iniciales de sus tres descubridores (Rivest, Shamir, Adleman), se basa en ciertos principios de la teoría de los números. Hay cuatro pasos previos para encontrar un algoritmo RSA, estos son: Paso 1 : Seleccionar dos números primos grandes, _p_ y _q_ (generalmente mayores que 10 elevado a 100). Paso 2 : Calcular [n = p x q] y [z = (p – l) x (q – 1)] . Paso 3 : Seleccionar un número primo con respecto a _z_ , llamándolo _d_ . Paso 4 : Encontrar _e_ tal que e x d = 1 mod z . Con estos parámetros calculados por adelantado, estamos listos para comenzar el cifrado. Dividimos el texto normal (considerado como una cadena de bits) en bloques, para que cada mensaje de texto normal, _P_ , caiga en el intervalo 0 ≤ P &lt; n . Esto puede hacerse agrupando el texto normal en bloques de _k_ bits, donde _k_ es el entero más grande para el que 2k &lt; n es verdad. Para cifrar un mensaje, _m_ , calculamos C = m elevado a e (mod n) . Para descifrar _C_ , calculamos m = C elevado a d (mod n) . Puede demostrarse que, para todos los _m_ del intervalo especificado, las funciones de cifrado y descifrado son inversas. Para ejecutar el cifrado, se necesitan _e_ y _n_ . Para llevar a cabo el descifrado, se requieren _d_ y _n_ . Por tanto, la clave pública consiste en el par (e, n) , y la clave privada consiste en (d, n) . La seguridad del método se basa en la dificultad para factorizar números grandes. Si el criptoanalista pudiera factorizar _n_ (conocido públicamente), podría encontrar _p_ y _q_ y, a partir de éstos, _z_ . Equipado con el conocimiento de _z_ y de _e_ , puede encontrar _d_ usando el algoritmo de Euclides. Afortunadamente, los matemáticos han estado tratando de factorizar números grandes durante los últimos 300 años, y las pruebas acumuladas sugieren que se trata de _n_ problema excesivamente difícil. De acuerdo con Rivest y colegas, suponiendo el uso del mejor algoritmo conocido y de una computadora con un tiempo de instrucción de 1 microsegundo, la factorización de un número de 200 dígitos requiere 4.000 millones de años de tiempo de cómputo (la edad de la tierra se estima en 4.500 millones de años); la factorización de un número de 500 dígitos requeriría la astronómica cifra de 10e25 años. Aun si las computadores continúan aumentando su velocidad en un orden de magnitud cada década, pasarán siglos antes de que sea factible la factorización de un número de 500 dígitos, y para entonces nuestros descendientes simplemente pueden escoger un _p_ y un _q_ todavía más grandes. La aplicación más inmediata de los algoritmos asimétricos, obviamente, es el cifrado de la información sin tener que transmitir la clave de decodificación, lo cual permite su uso en canales inseguros. Algoritmo DIFFIE-HELLMAN El algoritmo Diffie-Hellman es un algoritmo asimétrico basado, como su nombre indica, en el problema matemático de Diffie-Hellman, que se emplea fundamentalmente para acordar una clave común entre dos interlocutores, a través de un canal de comunicación inseguro. La ventaja de este sistema es que no son necesarias llaves públicas en el sentido estricto, sino una información compartida por los dos comunicantes. Algoritmo el ElGAMAL El algoritmo ElGAMAL fue ideado en un principio para producir firmas digitales, pero después se extendió también para codificar mensajes. Se basa en el problema de los logaritmos discretos, que está íntimamente relacionado con el de la factorización, y en el de Diffie-Hellman. Para generar un par de llaves, se escoge un número primo _n_ y dos números aleatorios _p_ y _x_ menores que _n_ . Se calcula entonces: y = p elevado x (mod n) La llave pública es (p, y, n) , mientras que la llave privada es x . Escogiendo _n_ primo, garantizamos que sea cual sea el valor de _p_ , el conjunto {p, p2, p3, …} es una permutación del conjunto {1, 2, …, n-1} . Nótese que esto no es necesario para que el algoritmo funcione, por lo que podemos emplear realmente un _n_ no primo, siempre que el conjunto generado por las potencias de _p_ sea lo suficientemente grande. Algoritmo de RABIN El sistema de llave asimétrica de RABIN se basa en el problema de calcular raíces cuadradas módulo con un número compuesto. Este problema se ha demostrado que es equivalente al de la factorización de dicho número. En primer lugar escogemos dos números primos, _p_ y _q_ , ambos congruentes con 3 módulo 4 (los dos últimos bits a 1). Estos primos son la clave privada. La clave pública es su producto, n = pq . Para codificar un mensaje _m_ , simplemente se calcula: c = me2 (mod n) . Algoritmo DSA El algoritmo DSA ( D igital S ignature A lgorithm) es una parte del estándar de firma digital DSS ( D igital S ignature S tandard). Este algoritmo, propuesto por el NIST, data de 1991, es una variante del método asimétrico de ElGamal. Dispersión de ClavesLa potencia de cualquier sistema de cifrado se apoya en una técnica de distribución de claves. En concreto, de nada serviría el sistema asimétrico de claves si las propias claves privadas no tuviesen una distribución segura. La distribución de claves se puede efectuar de varias formas. Para dos partes A y B: Opción 1 : _A_ puede seleccionar una clave y entregársela físicamente a _B_ . Opción 2 : Una tercera parte selecciona la clave y la entrega físicamente a _A_ y a _B_ . Opción 3 : Si _A_ y _B_ han utilizado previamente y recientemente una clave, una de las partes podría transmitir la nueva clave a la otra cifrada utilizando la clave previa. Opción 4 : Si _A_ y _B_ tienen cada uno una conexión cifrada a una tercera parte _C_ , _C_ podría entregar una clave a través de los enlaces cifrados a _A_ y a _B_ . La opción 1 y 2 son razonables para cifrado de enlace ya que cada dispositivo de cifrado va a intercambiar datos con su pareja en el otro extremo del enlace. La opción 3 es válida tanto para cifrado de enlace como para cifrado extremo a extremo; pero si un agresor llegara a conseguir una clave, todas las claves siguientes serán reveladas. La opción 4 es la preferible para proporcionar claves de extremo a extremo. Se identifican dos clases de claves: Clave de sesión: Cuando dos sistemas finales desean comunicarse, establecen una conexión lógica. Durante la duración de dicha conexión, todos los datos de usuario se cifran con una clave de sesión de un solo uso. Terminada la conexión, la clave se destruye. Clave permanente: Es la clave usada entre entidades con el objetivo de distribución de claves de sesión. CriptoanálisisA la hora de atacar un texto cifrado, existen dos formas de hacerlo: Criptoanálisis : El criptoanálisis, concepto ya presentado anteriormente, se basa en la naturaleza del algoritmo más algún conocimiento de las características generales del texto nativo o incluso de algunos pares texto nativo-texto cifrado. Este tipo de ataque explota las debilidades del algoritmo (si es que las tiene) o sus puntos menos fuertes para intentar deducir un texto nativo o deducir la clave que se está utilizando. Fuerza bruta : Consiste en probar cada clave posible en un trozo de texto cifrado hasta que se obtenga una traducción inteligible del texto nativo. En este apartado nos centraremos únicamente en el criptoanálisis, pues es obvio que el ataque por fuerza bruta requiere poca consideración. Basta con programar un ordenador,y tiempo (la mayoría de las veces mucho) para llegar a dar con la clave, o claves de cifrado. Es obvio que cuanta mayor potencia tenga el ordenador, más rápido se producirá el descifrado. Hoy en día se utilizan redes de ordenadores trabajando juntos para probar la fortaleza de los algoritmos de cifrado sometidos a ataques por la fuerza. Aún así, ni con los ordenadores más potentes de hoy en día se conseguiría descifrar en un tiempo razonable ciertos algoritmos de cifrado modernos. En cuanto al criptoanálisis, éste se comenzó a estudiar seriamente con la aparición de DES. Mucha gente desconfiaba del algoritmo propuesto por la NSA. Se dice que existen estructuras extrañas, que muchos consideran sencillamente puertas traseras colocadas por la Agencia para facilitar la descodificación de los mensajes. Nadie ha podido aún demostrar ni desmentir este punto. El interés por buscar posibles debilidades en él ha llevado a desarrollar técnicas que posteriormente han tenido éxito con otros algoritmos. Ni que decir tiene que estos métodos no han conseguido doblegar a DES, pero sí representan mecanismos significativamente más eficientes que la fuerza bruta para criptoanalizar un mensaje. Los dos métodos que vamos a comentar seguidamente parten de que disponemos de grandes cantidades de pares (texto claro-texto cifrado) obtenidos con la clave que queremos descubrir. Criptoanálisis Diferencial. Descubierto por Biham y Shamir en 1990, permite efectuar un ataque a DES, con texto claro escogido, que resulta más eficiente que la fuerza bruta. Se basa en el estudio de los pares de criptogramas que surgen cuando se codifican dos textos claros con diferencias particulares, analizando la evolución de dichas diferencias a lo largo de las rondas de DES. Para llevar a cabo un criptoanálisis diferencial se toman dos mensajes cualesquiera (incluso aleatorios) idénticos salvo en un número concreto de bits. Usando las diferencias entre los textos cifrados, se asignan probabilidades a las diferentes claves de cifrado. Conforme tenemos más y más pares, una de las claves aparece como la más probable. Esa será la clave buscada. Criptoanálisis Lineal . El criptoanálisis lineal, descubierto por Mitsuru Matsui, basa su funcionamiento en tomar algunos bits del texto claro y efectuar una operación XOR entre ellos, tomar algunos del texto cifrado y hacerles lo mismo,y finalmente hacer un XOR de los dos resultados anteriores, obteniendo un único bit. Efectuando esa operación a una gran cantidad de pares de texto claro y criptograma diferentes podemos ver si se obtienen más ceros o más unos. Si el algoritmo criptográfico en cuestión es vulnerable a este tipo de ataque, existirían combinaciones de bits que, bien escogidas, den lugar a un sesgo significativo en la medida anteriormente definida, es decir, que el número de ceros (o unos) es apreciablemente superior. Esta propiedad nos va a permitir poder asignar mayor probabilidad a unas claves sobre otras y de esta forma descubrir la clave que buscamos. Mecanismos de Firma DigitalMétodos de AutenticaciónLa segunda gran utilidad de los algoritmos asimétricos es la autenticación de mensajes, con ayuda de unas funciones llamadas “funciones resumen” que nos permitirán una firma digital , también denominada signatura , a partir de un mensaje. A dicha firma hay que exigirle que cumpla: Ser mucho más pequeña que el mensaje original. Que sea muy difícil encontrar otro mensaje que dé lugar a la misma . Veamos el siguiente ejemplo ilustrado en la siguiente figura. Supóngase que _A_ recibe un mensaje _m_ de _B_ y quiere comprobar su autenticidad. Para ello _B_ genera un resumen del mensaje r(m) y lo codifica empleando la clave de cifrado, que en este caso será privada. Proceso de autentificación con algoritmo asimétrico. La autentificación de información empleando algoritmos asimétricos tal como se indica en la figura precedente es: Paso 1 : _A_ , que posee la clave pública _KP_ de _B_ , recibe el mensaje _m_ y quiere autentificarlo. Paso 2 : _B_ genera el resumen de _m_ y envía a _A_ el criptograma asociado EKp(r(m)) . Paso 3 : _A_ genera por su cuenta r’(m) y decodifica el criptograma recibido usando la clave _KP_ . Paso 4 : _A_ compara r(m) y r’(m) para comprobar la autenticidad del mensaje _m_ . La clave de descifrado se habrá hecho pública previamente, y debe estar en poder de _A_ . _B_ envía entonces a _A_ el criptograma correspondiente a r(m) . _A_ puede ahora generar su propia r’(m) y compararla con el valor r(m) obtenido del criptograma enviado por _B_ . Si coinciden, el mensaje será auténtico, puesto que el único que posee la clave para codificar es precisamente _B_ . Nótese que en este caso la clave que se emplea para cifrar es la clave privada, justo al revés que para la simple codificación de mensajes. En muchos de los algoritmos asimétricos ambas claves sirven tanto para cifrar como para descifrar, de manera que si empleamos una para codificar, la otra permitirá decodificar y viceversa. Esto ocurre con el algoritmo RSA, en el que un único par de claves es suficiente para codificar y autentificar. Funciones de Dispersión Segura (HASH)Acabamos de ver que la criptografía asimétrica permite autentificar información. Asimismo también que la autentificación debe hacerse empleando una función resumen y no codificando el mensaje completo. En esta sección vamos a estudiar dichas funciones resumen, también conocidas como Funciones de Dispersión Segura ; que nos van a permitir crear firmas digitales estudiadas en el capítulo anterior. Estas funciones también llamadas muy comúnmente funciones Hash , son, en esencia, funciones matemáticas sin inversa , que aplicadas a un elemento o dato que se transfiere impiden que este sea descifrado. Se utilizan para comprobar la integridad de los datos según un mecanismo por el cual se cifra una cadena comprimida de los datos a transferir mediante una función Hash; este mensaje se envía al receptor junto con los datos ordinarios; el receptor repite la compresión y el cifrado posterior de los datos mediante la aplicación de la función Hash y compara el resultado obtenido con el que le llega, para verificar que los datos no han sido modificados. Para que sea segura, la función de dispersión segura o Hash r(m) , siendo _m_ el mensaje, debe cumplir: r(m) es de longitud fija , independientemente de la longitud de _m_ . Dado _m_ , es fácil calcular r(m) . Dado r(m) , es computacionalmente intratable recuperar m . Dado _m_ , es computacionalmente intratable obtener un m’ tal que r(m) = r(m’) . Gráficamente: Características fundamentales de una función Hash. Existen dos tipos de funciones resumen, a saber: Funciones MDC ( M odification D etection C odes) Como sabemos,un mensaje _m_ puede ser autentificado codificando con la llave privada _Kp_ el resultado de aplicarle una función resúmen, EKp(r(m)) . Esa información adicional (que denominaremos firma o signatura del mensaje _m_ ) sólo puede ser generada por el poseedor de la clave privada _Kp_ . Cualquiera que tenga la llave pública correspondiente estará en condiciones de decodificar y verificar la firma. En general, la funciones resumen se basan en la idea de funciones de compresión, que dan como resultado bloques de longitud _n_ a partir de bloques de longitud mayor _m_ . Estas funciones se encadenan de forma iterativa, haciendo que la entrada en el paso _i_ sea función del i-ésimo bloque del mensaje y de la salida del paso i-1 (tal como mostramos en la siguiente figura). Estructura iterativa de una función resumen. En general, se suele incluir en alguno de los bloques del mensaje _m_ -al principio o al final-, información sobre la longitud total del mensaje. De esta forma se reducen las probabilidades de que dos mensajes con diferentes longitudes den el mismo valor en su resumen. Conviene no confundir las funciones resumen con las funciones de relleno. Estas son funciones que generan texto cifrado continuamente, incluso en ausencia de texto nativo. Cuando hay disponible texto nativo, este se cifra y se transmite. En ausencia de texto nativo, los datos aleatorios se cifran y se transmiten. Esto hace imposible que un agresor distinga entre flujo de datos verdaderos y ruido, resultando imposible deducir la cantidad de tráfico. Funciones MAC ( M essage A uthentication C odes) Frente a las MDC vistas, realmente existe otra clase de funciones resumen, llamada genéricamente MAC ( M essage A uthentication C odes). Los MAC se caracterizan fundamentalmente por el empleo de una clave secreta para poder calcular la integridad del mensaje. Puesto que dicha clave sólo es conocida por el emisor y el receptor, el efecto conseguido es que el receptor puede, mediante el cálculo de dicha función, comprobar tanto la integridad como la procedencia del mensaje. Algoritmos de Generación de Firma DigitalEn este apartado vamos a estudiar dos algoritmos de generación de firmas muy utilizados: MD5 y SHA-1 . Algoritmo MD5 El algoritmo MD5 es el resultado de una serie de mejoras sobre el algoritmo MD4, diseñado por Ron Rivest. Las características básicas de este algoritmo son: Procesa los mensajes de entrada en bloques de 512 bits. Produce una salida de 128 bits. Siendo _m_ un mensaje de _b_ bits de longitud, en primer lugar se alarga _m_ hasta que su longitud sea exactamente 64 bits inferior a un múltiplo de 512. El alargamiento se lleva a cabo añadiendo un 1 seguido de tantos ceros como sea necesario. En segundo lugar, se añaden 64 bits con el valor de _b_ , empezando por el byte menos significativo. De esta forma tenemos el mensaje como un número entero de bloques de 512 bits, y además le hemos añadido información sobre la longitud. Algoritmo SHA-1 El algoritmo SHA-1 fue desarrollado por la NSA, para se incluido en el estándar DSS ( D igital S ignature S tandard). Al contrario que los algoritmos de cifrado propuestos por esta organización, SHA-1 se considera seguro y libre de puertas traseras, ya que el hecho de que el algoritmo sea realmente seguro favorece a los propios intereses de la NSA. Produce firmas de 160 bits, a partir de bloques de 512 bits del mensaje original. El algoritmo es similar a MD5, con la diferencia de que usa la ordenación big endian . Se inicializa de igual manera, es decir, añadiendo al final del mensaje un uno seguido de tantos ceros como sea necesario hasta completar 448 bits en el último bloque, para luego yuxtaponer la longitud en bits del propio mensaje -en este caso, el primer byte de la secuencia será el más significativo-. A diferencia de MD5, SHA-1 emplea cinco registros de 32 bits en lugar de cuatro. CortafuegosLa tecnología de Cortafuegos , o Firewalls (muros de fuego) es relativamente nueva y se ha potenciado al comprobar que una red abierta como es Internet ha incorporado un nuevo tipo de usuario no corporativo, y por tanto más difícil de controlar por las medidas y reglas implantadas en los propios “host” (potentes ordenadores corporativos que suelen tener las grandes empresas). Estos cortafuegos fueron diseñados para impedir a los Hackers o intrusos que utilizan Internet el acceso a redes internas de las empresas. Algunos cortafuegos incluso controlan la información que se se mueve por dichas redes. Se define como Tecnología de Cortafuegos al sistema que controla todo el tráfico hacia o desde Internet utilizando software de seguridad o programas desarrollados para este fin, que están ubicados en un servidor u ordenador independiente . Control de acceso a Internet por Cortafuegos (Firewall). Este sistema comprueba que cada paquete de datos se encamine a donde debe, desde la red Internet a nuestra red privada y viceversa, al mismo tiempo que contiene la política de seguridad especificada por el Administrador del Sistema. Pueden ayudar asimismo a prevenir la entrada de virus encapsulados en los paquetes transmitidos con destino a la red privada. Se utiliza la expresión cortafuegos para designar pasarelas u otras estructuras más complejas, existentes entre la red propia e Internet, con la finalidad de restringir y filtrar el flujo de información entre ambas. Para prevenir o permitir el tráfico de red, comprueba el host , la red y la puerta desde la cual el paquete es originado o destinado. Para conectar directamente ordenadores de un sistema corporativo en red Internet, existe una aplicación que reside en el servidor para permitir un buen acceso a los servicios Internet facilitando al mismo tiempo la mayor seguridad que sea posible. Este servidor comprueba que: El host desde el cual se origina la conexión . El host al cual la conexión es solicitada . Los comandos que se producen en la conexión. Todo ello puede facilitar al Administrador del Sistema la prevención de todas las conexiones desde host especificados o de redes en Internet. Desde esta puerta de entrada, el sistema puede también prevenirse de aquellos usuarios que a través de comandos añaden un factor de riesgo a nuestra seguridad. Se trata de prevenir, por ejemplo, la exportación de información contenida en el cortafuegos o en los servidores al exterior. Mediante aplicaciones residentes en el servidor o cortafuegos se puede: Definir qué usuarios tienen palabra clave de acceso autorizada. Configurar las palabras clave de acceso que deben ser aceptadas por los diferentes hosts configurados en nuestra red privada. Controlar las cuentas de aplicación autorizadas. Evitar que la intrusión pueda cambiar la configuración de la aplicación residente. Controlar los accesos entre la red privada y el servidor como punto de entrada. Llevar un registro de todas las incidencias que se produzcan. Barreras de protección de los datos frente a intrusos en Internet. Los esquemas que se describen a continuación o cualquier combinación de ellas responden a distintas formas de implantar un cortafuegos. Cortafuegos Filtrador de PaqueteEl cortafuegos filtrador de paquetes se basa en un dispositivo denominado encaminador, o más comúnmente conocido por su nombre en inglés, router, que puede filtrar los paquetes de datos, tanto los que salen como los entrantes a la red de la empresa, con destino u origen en Internet. Es el sistema más sencillo de establecer conexión con Internet. También se pueden configurar los protocolos de filtrado para que permitan la entrada en la red sólo de determinados tipos de transmisiones o de aquellas que tengan su origen en emisiones redeterminados. Esquema de un cortafuego por filtrado de paquetes. El problema surge cuando además de controlar esa puerta, tiene que filtrar los encaminamientos a todos o algunos de los hosts y a los distintos tipos de acceso. Por ejemplo, una red interna sólo puede recibir correo electrónico, otra no ser accesible desde Internet, una tercera solo puede facilitar información al exterior, etc. Aunque no pueden facilitar un alto nivel de seguridad, es una implantación bastante sencilla y sólida. Es mucho más asequible tanto en cuanto a coste, como en relación a la experiencia tecnológica necesaria con respecto a otros sistemas. Este sistema se debe apoyar o complementar con un mecanismo de seguridad propio de la aplicación que vaya a tratar la información, con el fin de impedir que lleve otro contenido que no sea el solicitado. Cortafuegos a Nivel de CircuitoEn este caso las medidas de seguridad se establecen a nivel de circuitos, un dispositivo interpuesto que transmite las comunicaciones entre las redes internas y externas. Suele ser un host provisto de dos interfaces operando a modo de pasarela y realiza las tareas de filtrado de paquetes, que en el apartado anterior realizaba el router, pero en este caso pueden añadirse más funciones de seguridad como las de autenticación mediante el uso de contraseñas (passwords) o palabras clave asignadas previamente a los usuarios. De esta forma cualquiera que intente acceder a la red interna mediante la técnica de generar dinámicamente “passwords” aleatorios, se encontrará con un impedimento adicional a las medidas adoptadas, haciendo el acceso mucho más difícil. Esquema de un cortafuegos a nivel de circuito. Cortafuegos a Nivel de AplicaciónLa forma de protección más completa, además de la más conocida y experimentada, es la utilización de cortafuegos a nivel de aplicación. Consiste en crear una subred, que constituya una zona de separación entre las redes internas e Internet. Por ejemplo mediante un router o encaminador, pero también se puede colocar un cortafuegos de acceso a la red interna. Tiene que haber un segundo dispositivo, casi siempre un host (denominado bastión), que se situará delante de la red interna. Los usuarios, tanto de entrada como de salida acceden a este host mediante una operación Telnet (conexión remota) para trabajar con una determinada aplicación ubicada en el mismo. Este Host gestiona las tareas de autentificación de usuarios, limitación de tráfico de entrada y salida, realiza un seguimiento de todas las actividades manteniendo un registro de incidencias. Este tipo de cortafuegos debe incorporar código escrito, especialmente para especificar todas y cada una de las aplicaciones para las cuales existe autorización de acceso. La ventaja de este sistema es que el usuario externo nunca tiene acceso a las redes internas de la empresa, por lo tanto nunca podrá realizar ningún tipo de intrusión. El inconveniente es que supone una fuerte inversión en tiempo y dinero para proporcionar un servicio cuyo grado de utilización puede no llegar a ser rentable. Esquema de un cortafuegos a nivel de aplicación. Aplicaciones de los CortafuegosLas siguientes aplicaciones no son más que diversas formas de usar el concepto de cortafuegos y los tipos de cortafuegos básicos vistos en el apartado anterior. Algunos ejemplos de estos usos son: Aplicación al correo electrónico en uso corporativo. Se puede crear un grupo cerrado que interconecte todos los ordenadores situados en puntos geográficos distintos incluidos en la red Internet, pero que pueden conectarse entre sí para aplicaciones de correo electrónico, transferencia de ficheros y poder conectarse usuarios de otro ordenador incluido en este Grupo, controlados por un sistema de seguridad corporativo. Cada uno de ellos, a su vez, puede conectarse con otros sistemas informáticos bien por Internet, o por otros medios de comunicación. Para evitar intrusiones a través de correo electrónico, se puede colocar un servidor o punto único de entrada que controle los accesos y salidas del grupo cerrado. Se le puede dar funciones de sólo entrada de correo electrónico pudiendo actuar como cortafuegos, y a cada ordenador del grupo darle funciones de sólo salida. Así limitaremos los puntos de acceso a uno solo y con un mayor control en todos los sistemas. Servidor como punto de entrada única a la red interna. Este tipo de entrada proviene de una conexión directa entre el usuario de Internet y la red interna. El filtro de acceso a nuestras aplicaciones controla aquellos usuarios que pueden acceder al interior desde el exterior y viceversa. Sin embargo una aplicación corriendo en este servidor puede establecer conexiones desde el exterior hacia cualquier punto de la red interna. Un problema se nos presenta al ser superada esta barrera, entonces nuestra red está totalmente desprotegida. Servidor como punto de entrada única a las aplicaciones. Como complemento del servidor único como punto de entrada única, se pueden colocar diferentes servidores exclusivos para cada aplicación. Nos garantiza que filtrarán todos los accesos, dejando sólo los que corresponden a la aplicación permitida. Deben estar colocados entre Internet y el servidor único y pueden contener la lista de usuarios que tienen permitido el acceso al host, y/o aplicaciones. Servidor como punto de entrada único al correo electrónico. Una puerta de entrada puede ser la colocación de otro servidor localizado en la red interna. Distribuye los mensajes del correo electrónico entre los diferentes ordenadores que están en la red privada. Contiene a su vez la lista de usuarios que pueden recibir mensajes y los que pueden enviar mensajes al exterior. Se puede usar también para el tráfico interior de la red. Servidor sólo para facilitar la información. Cuando nuestro propósito sea permitir únicamente la lectura de información, deberemos adecuar el control de acceso de usuarios a la función de lectura únicamente. En grandes instalaciones, el ideal sería disponer de otro ordenador único que contenga la información solamente, de esta manera evitaremos cualquier posibilidad de que se pueda obtener otra información distinta a nuestro propósito debido a tener cortados los accesos a cualquier otro sistema. Redes Privadas Virtuales (VPN)El término VPN ( V irtual P rivated N etwork) se ha asociado tradicionalmente a los servicios de conectividad remota de datos ofrecidos por las operadoras de telefonía mediante líneas dedicadas, aunque en la actualidad se refiere al uso de los acceso vía Internet para realizar comunicaciones remotas con las mismas características de seguridad que las líneas dedicadas, con un coste económico mucho menor. Es decir, se aprovecha el bajo coste del acceso a Internet, al que se añaden técnicas de encriptación fuerte para conseguir seguridad y se simulan las clásicas conexiones punto a punto. De esta forma, un usuario o una sede remota que se conecta a través de Internet a su organización y establece un túnel VPN puede estar funcionando como si estuviera dentro de la misma a todos los efectos de conectividad. El factor crucial que convierte a un VPN en una red privada virtual es lo que se denomina un túnel, que no indica una ruta fija y delimita entre ambos extremos que se comunican vía Internet, sino que se refiere a que únicamente ambos extremos son capaces de ver lo que se mueve por el túnel, convenientemente encriptado y protegido del resto de Internet. La tecnología del túnel encripta y encapsula los protocolos de red que utilizan los extremos sobre el protocolo IP , lo que le permite funcionar como si se tratara de un enlace dedicado convencional, de modo transparente para el usuario. El protocolo estándar para el soporte de túneles, encriptación y autenticación en las VPN es IPSec (IPSecurity), que se diseñó para proteger el tráfico de red y que permite gestionar el control de acceso, la integridad de las conexiones, la autenticación del origen de los datos y la confidencialidad del flujo de datos. Este protocolo permite dos modos operacionales. En el modo transporte se utiliza para proteger conexiones individuales de usuarios remotos y las comunicaciones se cifran entre un ordenador remoto (el cliente VPN) y el servidor de VPN, mientras que en el modo túnel se encriptan las comunicaciones entre dos dispositivos de tipo encaminador (o un encaminador y el servidor de VPN), con lo que se protegen todas las comunicaciones de todas las máquinas situadas detrás de cada encaminador. A pesar de que IPSec es el protocolo más utilizado para VPNs y ser el único que está pensado para soportar desarrollos futuros de VPNs, existen otros muy utilizados, fundamentalmente por haber sido desarrollados por Microsoft como soluciones temporales mientras se estandarizaba totalmente IPSec, se trata de: L2TP ( L ayer 2 T unneling P rotocol) PPTP ( P oint-to- P oint T unneling P rotocol) En el caso de L2TP, se necesita IPSec para proporcionar las funciones de encriptación, mientras que PPTP tiene capacidades propietarias de encriptación y autenticación, aunque se le considera ya obsoleto. Bibliografia Scribd (Ibiza Ales) Redes inalámbricas. Protocolos. Características funcionales y técnicas. Sistemas de expansión del espectro. Sistemas de acceso. Modos de operación. Seguridad. Normativa reguladora.IntroducciónEstos últimos años la comunicación inalámbrica ha evolucionado notablemente, el uso de teléfonos móviles ya es masivo y a éstos cada vez se la han ido agregando nuevas funcionalidades, sin embargo esto es sólo el inicio de la comunicación inalámbrica, ya que éstas continúan apostando por convertir al aire en el mejor medio de transporte de datos. La apuesta anterior se basa en las tecnologías de comunicación inalámbrica WI-FI y Bluetooth, las cuales han permitido que muchas personas utilicen tiempo que antes era de ocio pudiendo contar con Internet en situaciones tan comunes como la espera en el aeropuerto, el tiempo de viaje, entre otras ventajas. Estas tecnologías aparentemente pueden parecer similares, sin embargo hay grandes diferencias entre ellas, lo que hace que sean complementarias. Las Redes inalámbricas facilitan la operación en lugares donde la computadora no puede permanecer en un solo lugar, como en almacenes o en oficinas que se encuentren en varios pisos. Pero la realidad es que esta tecnología está todavía en pañales y se deben resolver varios obstáculos técnicos y de regulación antes de que las redes inalámbricas sean utilizadas de una manera general en los sistemas de cómputo de la actualidad. Hasta ahora solo se ha percibido los aspectos positivos de estas tecnologías, sin embargo no hay que olvidar los aspectos más débiles de ésta. El mayor problema al que se enfrenta hoy este tipo de comunicación es la seguridad, y aunque se ha trabajado en este aspecto aún falta para que sea tanto o más confiable que el cable. Otro aspecto que no hay que dejar de lado es el coste que significa contar con éstas. Con respecto a estos problemas, es de esperar que en unos años más se llegue a una solución para así confirmar que el aire es el mejor medio de transportes de datos. No se espera que las redes inalámbricas lleguen a reemplazar a las redes cableadas. Estas ofrecen velocidades de transmisión mayores que las logradas con la tecnología inalámbrica. Sin embargo se pueden mezclar las redes cableadas y las inalámbricas, y de esta manera generar una “Red Híbrida” y poder resolver los últimos metros hacia la estación. Se puede considerar que el sistema cableado sea la parte principal y la inalámbrica le proporcione movilidad adicional al equipo, pudiendo el operador desplazarse con facilidad dentro de un almacén o una oficina. La disponibilidad de conexiones inalámbricas y redes LAN inalámbricas puede ampliar la libertad de los usuarios de la red a la hora de resolver varios problemas asociados a las redes con cableado fijo y, en algunos casos, incluso reducir los gastos de implementación de las redes. Sin embargo, a pesar de esta libertad, las redes LAN inalámbricas traen consigo un nuevo conjunto de desafíos. El amplio interés del sector para que exista interoperabilidad y compatibilidad entre los SO ha permitido resolver algunas de las cuestiones relacionadas con la implementación de las redes LAN inalámbricas. Con todo, las redes LAN inalámbricas exponen nuevos retos en lo que respecta a la seguridad, la movilidad y la configuración. Redes InalámbricasSituación ActualEn los últimos años el crecimiento en la demanda de soluciones inalámbricas para la empresa, y últimamente también para el hogar, ha crecido de una manera espectacular. Las distintas tecnologías inalámbricas permiten dar una cobertura casi en cualquier rincón del planeta. Cientos de millones de personas en todo el mundo se comunican e intercambian información todos los días usando una u otra tecnología inalámbrica, permitiendo el envío de datos y con una movilidad sin precedentes. Las tecnologías más usadas y conocidas hoy día son las de los teléfonos móviles, sistemas de navegación, “buscas”, servicios de mensajes, y un largo etcétera. Pero el gran exponente hoy de la revolución digital y de como los bits forman parte de nuestro día a día, proviene del intercambio de datos digitales. Estos datos no sólo se quedan limitados al ámbito de las computadoras, sino también en una gran cantidad de aplicaciones, pasando desde los grandes sistemas de tratamiento de datos empresariales y científicos hasta las más pequeñas utilidades personales destinadas a mejorar nuestro día a día. Ya no estamos atados a redes cableadas sino que podemos acceder y compartir datos llevándoles con nosotros, dondequiera que vayamos. Desde el principio de los años 70 hemos tenido una red, la red Ethernet, que ha supuesto una estandarización a la hora de transmitir datos, con un gran éxito en todo el mundo. Aunque no es el único, si es el que más ha influenciado el uso habitual de las redes de área local (LAN). Nos hemos acostumbrado a tener redes de computadores de bajo coste, altas velocidades y una relativamente fácil instalación, más que aptas para la mayoría de las aplicaciones. Pero el hecho de tener una infraestructura nos limita a ésas líneas preinstaladas, y los costes ante fallos, mantenimiento y reestructuración se disparan. La flexibilidad es muy baja e incluso no resulta factible la instalación de estas líneas en algunos edificios, antiguos de valor histórico o peligrosos con asbestos u otros materiales. Todo esto puede ser solucionado con las nuevas tecnologías inalámbricas, puesto que ya no es necesaria la instalación de cables. Las redes de área local inalámbricas utilizan el aire como medio de transporte, y su característica principal es complementar, y en algunos casos, reemplazar las redes de área local alámbricas, generalmente basadas en los estándares Ethernet. Pero no todo lo que reluce es oro. Las WLANs tienen sus propios problemas, característicos del medio de transmisión, que dificultan en gran medida la transmisión de datos. Se trata de un medio de transmisión difícil. Al tratarse de bandas libres no tienen la protección de una banda con licencia donde se sabe de antemano que es la única o una de las pocas tecnologías transmisoras en ese medio, teniendo así cierta seguridad. Pero no, las frecuencias empleadas para la transmisión son las ISM, bandas de propósito general que ahora se usan para WLANs. Estas bandas son las de 900MHz, 2.4Ghz y 5 Ghz. En WLANs se usan las dos últimas, siendo la banda de 2.4Ghz la más utilizada y por tanto el medio más contaminado. Por otra parte la banda de 5Ghz se usa en tecnologías más recientes, como puede ser el IEEE802.11a o el Hyperlan/2, pero tiene otros problemas, las licencias. En éstos medios, la calidad no se puede asegurar, con dispositivos móviles la cobertura no siempre está asegurada, se tiene una alta tasa de errores de bit, el problema de los nodos ocultos/expuestos, etc, por lo que la calidad no se puede asegurar ni en tiempo ni en espacio. ¿Cómo Trabajan las WLAN?Se utilizan ondas de radio o infrarrojos para llevar la información de un punto a otro sin necesidad de un medio físico. Las ondas de radio son normalmente referidas como portadoras de radio ya que éstas únicamente realizan la función de llevar la energía a un receptor remoto. Los datos a transmitir se superponen a la portadora de radio y de este modo pueden ser extraídos exactamente en el receptor final. Esto es llamado modulación de la portadora por la información que está siendo transmitida. De este modo la señal ocupa más ancho de banda que una sola frecuencia. Varias portadoras pueden existir en igual tiempo y espacio sin interferir entre ellas, sin las ondas son transmitidas a distintas frecuencias de radio. Para extraer los datos el receptor se sitúa en una determinada frecuencia ignorando el resto. En una configuración de LAN sin cable los puntos de acceso (transceiver) conectan la red cableada de un lugar fijo mediante cableado normalizado. El punto de acceso recibe la información, la almacena y transmite entre la WLAN y la LAN cableada. Un único punto de acceso puede soporta un pequeño grupo de usuarios y puede funcionar en un rango de al menos treinta metros y hasta varios cientos. El punto de acceso (o la antena conectada al punto de acceso) es normalmente colocado en alto pero podría colocarse en cualquier lugar en que se obtenga la cobertura de radio deseada. El usuario final accede a la red WLAN a través de adaptadores. Estos proporcionan una interfaz entre el SO de red del cliente (NOS: Network Operating System) y las ondas, vía una antena. La naturaleza de la conexión sin cable es transparente al sistema del cliente. Configuraciones de las WLANBásicamente, las WLAN están compuestas por dos elementos: Punto de acceso (Access Point – AP por sus siglas en inglés): este elemento es la estación base que generalmente tiene conectividad con el mundo alámbrico (red Ethernet, por ejemplo). El AP crea un torno a sí un área de cobertura donde los usuarios o dispositivos clientes se pueden conectar. El AP cuenta con una o dos antenas y con una o varias puertas Ethernet. Dispositivos clientes: estos elementos son PCs, PDAs u otros que cuentan con tarjeta de red inalámbrica. Estas tarjetas existen en diferentes tipos tales como: PCCard, PCI, Compact Flash, etc. Las redes pueden ser simples o complejas. La más básica se da entre dos ordenadores equipados con tarjetas adaptadoras para WLAN, de modo que pueden poner en funcionamiento una red independiente siempre que estén dentro del área que cubre cada uno. Esto es llamado red de igual a igual (peer to peer). Cada cliente tendría únicamente acceso a los recursos de otro cliente pero no a un servidor central. Este tipo de redes no requiere administración o preconfiguración. Uno de los últimos componentes a considerar en el equipo de una WLAN es la antena direccional. Por ejemplo: se quiere una LAN sin cable a otro edificio a 1Km de distancia. Una solución puede ser instalar una antena en cada edificio con línea de visión directa. La antena del primer edificio está conectada a la red cableada mediante un punto de acceso. Igualmente en el segundo edificio se conecta un punto de acceso, lo cuál permite una conexión sin cable en esta aplicación. Estandarización y CompatibilidadHoy en día existen varias tecnologías y estándares para las comunicaciones de redes de área local inalámbricas. Estos estándares, definen una red formada por un medio compartido y transmisión encriptada de la información. IEEE 802.11 es un estándar para redes inalámbricas definido por la organización Institute of Electrical and Electronics Engineers (IEEE), instituto de investigación y desarrollo, de gran reconocimiento y prestigio, cuyos miembros pertenecen a decenas de países entre profesores y profesionales de las nuevas tecnologías. El estándar IEEE 802.11 es un estándar en continua evolución, debido a que existen cantidad de grupos de investigación, trabajando en paralelo para mejorar el estándar, a partir de las especificaciones originales. 802.11b : Este estándar especifica transmisiones en la banda de frecuencias de los 2.4GHz, con velocidades de hasta 11 Mbps. Es sin lugar a dudas la tecnología más popular y la más económica. 802.11a : Este estándar, posterior al 802.11b, especifica transmisiones en la banda de los 5GHz (una banda menos ruidosa que la de los 2.4GHz) y con una velocidad de hasta 54 Mbps. Posee una menor cobertura que 802.11b. 802.11g : Especifica transmisiones de hasta 54 Mbps en la banda de los 2.4GHz y asegura compatibilidad con dispositivos 802.11b. La alianza WI-FI (Wireless Fidelity) es una organización sin fines de lucro formada en 1999 para certificar la interoperabilidad de los productos 802.11 y para promocionarlos con un estándar global de WLAN en todos los segmentos. Se trata de una especificación en continua evolución con posibilidad de adaptarse a nuevos requerimientos y demandas de usuario en un futuro. IEEE 802.11. TecnologíaEl estándar permite el uso de varios medios y técnicas para establecer conexiones. El estándar original permite usar infrarrojos y espectro expandido, tanto en salto en frecuencias como secuencia directa, con la ventaja de usar una capa de acceso al medio (MAC) común. Ello da mucha flexibilidad a los desarrolladores e investigadores, que pueden olvidarse de ciertos aspectos ya que no existe dependencia directa entre ellos. Los estándares de IEEE 802.11 son de libre distribución y cualquier persona puede ir a la página Web del IEEE y descargarlos. Estos estándares sólo definen especificaciones para las capas físicas y de acceso al medio y para nada tratan modos o tecnologías a usar para la implementación final. Esto debe permitir y facilitar la interoperabilidad entre fabricantes de dispositivos IEEE 802.11 y para asegurarse de ello se ha creado una alianza denominada WECA para crear y definir procedimientos para conseguir certificados de interoperabilidad y de cumplir las especificaciones, todo dentro de un estándar llamado WiFi (Wireless Fidelity). El nombre además es un indicativo del enfoque doméstico y muy enfilado hacia el usuario final. El bloque constructivo básico de una red inalámbrica 802.11 es el denominado conjunto de servicio básico (BSS, Basic Service Set), que es un área geográfica en la que las estaciones inalámbricas se pueden comunicar. El tipo más sencillo de BSS consiste en dos o más equipos que han entrado dentro de las áreas de transmisión respectivas. Este proceso por el que los dispositivos entran en un BSS se denomina asociación . Capa Física (PHY)La capa física en cualquier red define la modulación y características de señalización para la transmisión de datos en ese medio. Para poder transmitir para redes inalámbricas en bandas sin licencia se necesitan usar técnicas de espectro expandido, definidas en los requerimientos de casi todos los países. En el estándar IEEE 802.11 se definen tres medios de nivel físico. Uno usa señales de infrarrojos y los otros dos utilizan señales de radio frecuencia (RF). Los medios de RF 802.11 funcionan en la banda de 2.4Ghz, con un ancho de banda de 83Mhz entre 2.400 y 2.483GHz. Las definiciones para la transmisión por radiofrecuencia en los estándares son espectro expandido por salto en frecuencias (FHSS) y espectro expandido por secuencia directa (DSSS). Ambos están definidos para trabajar en la banda de 2.4Ghz, y DSSS además tienen una variante en la banda de los 5Ghz, que consigue mayores velocidades de transmisión. Infrarrojos Las comunicaciones por infrarrojos utilizan frecuencias entre 850 y 950 nanometros, justo por debajo del espectro de la luz visible. La implementación IEEE 802.11 de infrarrojos, a diferencia de la mayoría de los medios infrarrojos, no requiere comunicación de visión directa, puede funcionar mediante señales reflejadas. Sin embargo, debido a su limitado alcance comparado con los medios de RF y a que sólo puede funcionar adecuadamente en un ambiente interior cuando las superficies proporcionan una buena reflexión de las señales, es raro que se implemente en las redes inalámbricas. Además impone más restricciones en la ubicación física del dispositivo inalámbrico que FHSS o DSSS. FHSS Salto de frecuencias se refiere a un sistema que periódicamente cambia las frecuencias en las que transmite. Se utiliza la banda entera lo que contribuye a aumentar la seguridad frente a escuchas a la vez que ayuda a suprimir el ruido o las interferencias. FHSS tiene 22 patrones de saltos predefinidos usando 79 canales de 1Mhz a un mínimo de 2.5 saltos por segundo, y para resolver los problemas de sincronización, para que tanto transmisor como receptor salten a la vez, se definen paquetes de sincronización. La velocidad de los cambios de frecuencia es independiente de la velocidad de bit de la transmisión de datos. Si la velocidad del salto de frecuencia es menor que la velocidad de bit de la señal, la tecnología se denomina sistema de salto lento , y si es mayor se denomina sistema de salto rápido . Para la modulación FHSS usa FSK gaussiano de 2 ó 4 niveles. Las velocidades típicas conseguidas son de 1 y 2 Mbps para FHSS. DSSS El sistema de radio usando DSSS trabaja en un canal fijo y preconfigurado, lo que le permite obtener mayores tasas de transferencia, pero con la desventaja de ser más sensible a interferencia y a señales procedentes de otros dispositivos usando la misma frecuencia. Es posible tener tres puntos de acceso con tres canales diferentes, sin solapar en un mismo emplazamiento, si tener en cuenta ningún tipo de planificación. Aunque para más de tres puntos de acceso sí es necesaria cierta planificación, para poder mantener las velocidades, puesto que el solape de celdas y frecuencias tendrá un deterioro sobre el rendimiento. Las modulaciones usadas para DSSS son BPSK y DQPSK para el estándar original. Para 11b, que permite conseguir 11Mbps, se utiliza CCK. Además se ha definido una variante de IEEE 802.11, incorporada recientemente a la especificación que permite conseguir 54 Mbps en la banda de 5Ghz, con un ancho de banda de hasta 300MHz y usando una modulación OFDM. Tramas de la Capa Física En lugar de tener un esquema de señalización relativamente simple como en Ethernet y Token Ring que utilizan Manchester y Manchester diferencial respectivamente, los medios que funcionan en 802.11 tienen su propio formato de tramas, que encapsulan las tramas generadas en el nivel de enlace de datos. La trama de FHSS contiene los siguientes campos: Preámbulo (10 bytes): contiene 80 bits de 1 y 0 alternos utilizados por el receptor para detectar la señal y sincronizar los tiempos. Delimitador de comienzo de trama (SFD) (2 bytes): indica el comienzo de la trama. Longitud (12 bits): indica el tamaño del campo de datos. Señalización (4 bits): contiene un bit para indicar si se está utilizando la velocidad de 1 o 2 Mbps. Los otros 3 bits se reservan para uso futuro. Sólo el campo de datos se puede transmitir a 2 Mbps. CRC (2 bytes): contiene un valor de comprobación de redundancia cíclica. Datos (de 0 a 4.095 bytes): contiene la trama del nivel de enlace de datos que se transmite. La trama DSSS contiene los siguientes campos: Preámbulo (16 bytes): contiene 128 bits que el sistema receptor utiliza para ajustarse a la señal entrante. Delimitador de comienzo de trama (SFD) (2 bytes): indica el comienzo de la trama. Señal (1 byte): especifica la velocidad de transmisión. Servicio (1 byte): contiene el valor hexadecimal 00 que indica que el sistema cumple con el estándar 802.11. Longitud (2 bytes): indica el tamaño del campo de datos. CRC (2 bytes): contiene un valor de comprobación de redundancia cíclica. Datos (variable): contiene la trama del nivel de enlace de datos que se transmite. La trama de infrarrojos contiene los siguientes campos: Sincronización (SYNC) (entre 57 y 73 ranuras): utilizadas por el sistema receptor para sincronizar el tiempo y opcionalmente para estimar la relación señal/ruido. Delimitador de comienzo de trama (SFD) (2 ranuras): indica el comienzo de la trama. Velocidad de datos (3 ranuras): especifica la velocidad de transmisión. Ajuste del nivel de DC (CDLA) (32 ranuras): utilizado por el receptor para estabilizar el nivel DC después de transmitir los campos precedentes. Longitud (12 bits): indica el tamaño del campo de datos. CRC (2 bytes): contiene un valor de comprobación de redundancia cíclica. Datos (de 0 a 2.500 bytes): contiene la trama del nivel de enlace de datos que se transmite . La Capa de Acceso al Medio (MAC)La especificación de la capa MAC del IEEE 80.11 tiene muchas similitudes con el estándar de Ethernet cableado (IEEE 802.3). El protocolo del 802.11 es un esquema de protocolo conocido como detección de portadora, acceso múltiple, evitando colisiones (CSMA/CA). Este protocolo evita las colisiones, en vez de detectarlas como el algoritmo 802.3. Es extremadamente difícil detectar colisiones en una red de transmisión de radiofrecuencias y de ahí de que se trate de evitar las colisiones. La capa MAC opera junto con la capa física muestreando la energía del medio transmisor de datos. El protocolo CSMA/CA permite opciones para que se pueda minimizar las colisiones usando tramas de transmisión RTS/CTS (Request-to-send/Clear-to-send), datos y reconocimientos de una manera secuencial. En estas tramas se suelen incorporar datos de duración de los envíos con el objetivo de asegurar que esos envíos no van a ser interrumpidos: los demás nodos saben que deben estar callados durante ese intervalo de tiempo. Todo ello además se asegura y confirma con tramas de reconocimiento (ACK). Pero un problema común a cualquier WLAN es el problema de los nodos ocultos. Esto puede llegar a reducir las prestaciones en un 40% en una WLAN con alta carga. Ocurre cuando un nodo no puede escuchar transmisiones de un nodo y trata de transmitir a un nodo que si puede escucharlas, allí se puede generar muchas colisiones. Algunas mejoras se han incorporado para evitar el problema con el uso de RTS/CTS de una manera inteligente. Además se utiliza tiempos entre tramas para evitar colisiones, ello a parte de evitar colisiones, permite además cierto uso de clases de calidad o por lo menos de preferencia de un tráfico sobre otro, utilizando funciones de coordinación puntual y permitir el acceso al medio de tráfico prioritario antes que a los demás. Tramas del Nivel MAC El estándar 802.11 define tres tipos básicos de tramas en este nivel: Tramas de datos: se usan para transmitir datos de los niveles superiores entre estaciones. Tramas de administración: se usan para el intercambio de información para realizar funciones de red como la autenticación y la asociación. Tramas de control: se usan para regular el acceso al medio y para reconocimiento de las tramas de datos transmitidas. Una trama MAC genérica contiene los siguientes campos: Control de la trama (2 bytes): contiene 11 subcampos que habilitan las distintas funciones del protocolo: Versión de protocolo (2 bits): especifica la versión del estándar que se está utilizando. Tipo (2 bits): indica si la trama es de administración (00), control (01) o datos (00). Subtipo (4 bits): identifica la función específica de la trama. A DS (1 bit): si vale 1 indica que la trama se transmite al sistema de distribución a través de un punto de acceso. De DS (1 bit): si vale 1 indica que la trama se ha recibido de un sistema de distribución. Más fragmentos (1 bit): si vale 1 indica que el paquete contiene un fragmento de una trama y que hay más fragmentos para su transmisión. Reintento (1 bit): un valor de 1 indica que la trama se está retransmitiendo debido a una falta de recepción de un reconocimiento. Administración de energía (1 bit): si vale 0 indica que la estación está funcionando en modo activo; si vale 1 en modo ahorro de energía. Más datos (1 bit): si vale 1 indica que el AP tiene más paquetes almacenados para la estación y en espera de transmisión. WEP (1 bit): si vale 1 indica que el cuerpo de la trama se ha cifrado utilizando WEP. Orden (1 bit): si vale 1 indica que la trama de datos se está transmitiendo utilizando la clase de servicio estrictamente ordenado. Duración/AID (2 bytes): en las tramas de control de sondeo de energía contiene la identidad de asociación (AID) de la estación transmisora. En el resto de tramas contiene el tiempo (en microsegundos) necesarios para transmitir una trama más el intervalo entre tramas. Dirección 1 (6 bytes): contiene una dirección que identifica al receptor de la trama, utilizando uno de los 5 tipos de direcciones definidas en la capa MAC 802.11, dependiendo de los valores de los subcampos A DS y De DS. Dirección 2 (6 bytes): contiene una dirección utilizando uno de los 5 tipos de direcciones utilizadas en el subnivel MAC, dependiendo de los valores de los subcampos A DS y De DS. Dirección 3 (6 bytes): contiene una dirección utilizando uno de los 5 tipos de direcciones utilizadas en el subnivel MAC, dependiendo de los valores de los subcampos A DS y De DS. Control de secuencia (2 bytes): contiene dos subcampos: Número de fragmento (4 bits): contiene un valor que identifica un fragmento particular en una secuencia. Número de secuencia (12 bits): contiene un valor que identifica los fragmentos de la secuencia que componen el conjunto de datos. Dirección 4 (6 bytes): contiene una dirección utilizando uno de los 5 tipos de direcciones utilizadas en el subnivel MAC, dependiendo de los valores de los subcampos A DS y De DS. Cuerpo de la trama (0 a 2.312 bytes): contiene la información que se está transmitiendo a la estación receptora. Secuencia de verificación de trama (4 bytes): contiene un valor CRC. Los cinto tipos de dirección del subnivel MAC son: Dirección origen (SA): una dirección MAC individual que identifica al sistema que generó la información que va en el cuerpo de la trama. Dirección destino (DA): una dirección MAC individual o de grupo que identifica al receptor final de una unidad de datos de servicio. Dirección del emisor (TA): una dirección MAC individual que identifica al sistema que transmitió la información que va en el cuerpo de la trama en el medio inalámbrico actual (un AP). Dirección del receptor (RA): una dirección MAC individual o de grupo que identifica al receptor inmediato de la información en el cuerpo de la trama en el medio inalámbrico actual (un AP). ID del conjunto de servicio básico (BSSID): en una red ad hoc el BDDID es un valor generado aleatoriamente durante la creación del BSS; en una red con infraestructura es la dirección MAC de la estación que funciona como AP del BSS. Tipos de Redes InalámbricasBluetoothEs una tecnología inalámbrica de corto alcance diseñada para reemplazar los cables entre dispositivos. Se ha convertido en la solución inalámbrica ideal para conectar teléfonos móviles con portátiles para su conexión a Internet, o para que otros organizadores de mano, como PDAs puedan conectarse al PC para coordinar sus contactos, e incluso para poder imprimir desde un ordenador de forma inalámbrica. Las características intrínsecas de las tecnologías con bluetooth permiten establecer conexiones seguras, con capacidad de encriptación del canal, autenticación de la red y otros parámetros de seguridad como la localización y dispositivo del usuario. Historia de Bluetooth La historia de Bluetooth comienza en el año 1994, cuando la compañía Ericsson comenzó un estudio para investigar si era factible o no una comunicación vía radio, que fuera barata y que no consumiera tanto, con la única finalidad de conectar celulares y así eliminar cables.Fue así como se conoció un enlace de radio de poco alcance, el que podía ser usado en distintos equipos celulares, e incluso en otro tipo de dispositivos, ya que el chip utilizado era de bajo coste. Este enlace se llamó MC link. En 1997, Ericsson despertó la curiosidad y el interés en otras empresas fabricantes de celulares y equipos portátiles, y fue así que llegado el año de 1998 se creó un grupo llamado SIG, formado por cinco compañías: Ericsson, Nokia, IBM, Toshiba e Intel. De esta forma, el grupo estaba compuesto de dos líderes en telefonía celular, dos en la fabricación de computadores y uno en la fabricación de chips. Así, se podía crear una tecnología que fuese compatible con los distintos equipos fabricados por esas empresas. Comenzado el trabajo, se pensó primero en utilizarlo con productos que usaran los ejecutivos que viajan frecuentemente, y entre los primeros dispositivos en que se podría ocupar dicha tecnología debían estar los celulares, notebooks o computadores portátiles, PDAs y auriculares. Teniendo esto como base, se definieron como objetivos del sistema: Operación en todo el mundo. Poco consumo de energía por parte del emisor de radio (porque los equipos móviles utilizan baterías). Poder transmitir voz y datos. La banda de frecuencia debería ser libre. Este último escollo se saltó utilizando la banda ISM, una banda médico-científica internacional, de 2,54Ghz, que varía en rangos de 2400MHz y 2500MHz y quee sutilizada en todo el mundo. En el mercado existen varias versiones de Bluetooth. Las primeras son BT 1.0 y BT 1.1. La diferencia entre ambas radica en que en la primera sólo se puede conectar un dispositivo con otro al mismo tiempo, mientras que en la segunda se puede conectar un dispositivo con siete simultáneamente, claro que todos deben ser compatibles entre sí. Se puede tener todos los dispositivos que se quiera conectados a través de esta vía, pero sólo se puede establecer comunicación con siete de ellos. Además de estas dos, también existen la BT 1.2 y la BT 2.0, las que han sido desarrolladas por ingenieros de Ericsson Technology Licensing y estudiadas por el SIG. La diferencia principal entre ambas es que la BT 1.2 es para media velocidad, con una velocidad de transferencia de 2 o 3 Mbits por segundo, mientras que la BT 2.0 es para alta velocidad, con una velocidad de transferencia de 4, 8 o 12 Mbits por segundo. ¿Cómo Funciona Bluetooth? La frecuencia de radio en la que trabaja está en el rango de 2.4 y 2.48 Ghz, con amplio espectro y saltos de frecuencia, con posibilidad de transmitir en full duplex con un máximo de 1600 saltos/seg. Los saltos de frecuencia se realizan entre un total de 79 frecuencias con intervalos de 1Mhz, lo cual permite brindar seguridad y robustez. La frecuencia en la cual trabaja le permite atravesar paredes y maletines, por lo cual es ideal tanto para el trabajo móvil, como en oficinas. La potencia de salida para transmitir a una distancia máxima de 10m es de 0dBM (1 mW), mientras que la versión de largo alcance, hasta 100m, transmite entre -30 y 20dBM (100 mW). Para lograr alcanzar el objetivo de bajo consumo y bajo coste, se ideó una solución en un sólo chip utilizando circuitos CMOS. De esta manera, se logró crear una solución de 9x9mm y que consume aproximadamente 97% menos energía que un teléfono móvil común. Cada uno de los cuatro canales de voz en la especificación Bluetooth puede soportar una tasa de transferencia de 64 Kb/s en cada sentido, la cual es suficientemente adecuada para la transmisión de voz. Un canal de datos asíncrono puede transmitir 721 Kb/s en una dirección y 56 Kb/s en la dirección opuesta, sin embargo, para una conexión asíncrona es posible soportar 432,6 Kb/s en ambas direcciones si el enlace es simétrico. Para relacionarse e intercambiar información los dispositivos Bluetooth ofrecen distintos servicios, llamados técnicamente Perfiles, entre estos perfiles se encuentran el Acceso a Redes Locales (LAN), Acceso Telefónico, Fax, Transferencia de Archivos, Sincronización, Intercomunicador o Telefonía inalámbrica, entre otros. De esta manera cuando dos dispositivos se comunican por primera vez intercambian esta información para conocer sus posibilidades de intercomunicación. Las compañías más destacadas en el desarrollo de esta tecnología Bluetooth han sido Ericsson y Nokia. La primera versión del estándar Bluetooth se lanzó en mayo de 1998. Esta tecnología inalámbrica tiene una velocidad de transferencia de datos de 1Mbps y cuenta con un alcance máximo de 100 metros. Sin embargo, lo más utilizado son los 10 metros, ya que el consumo eléctrico aumenta rápidamente con una mayor potencia de recepción o transmisión. Ventajas Menor Radiación. El SIG de Bluetooth recomienda potencias bajas para sus dispositivos móviles, esto es 1 a 10mW, limitando los 100mW para los puntos de acceso, lo cual puede ser comparado con 100mW que utilizan las tarjetas WiFi o los 125 mW de los teléfonos móviles digitales o los 600 mW de los teléfonos móviles analógicos. Bajo Consumo Energético. Bluetooth ha sido concebida como una tecnología de bajo consumo, lo cual permite ser utilizada en equipos móviles como Palms e iPAQ. Junto con sus bajos niveles de consumo se incorpora un administrador de energía el cual disminuye la potencia de transmisión si el dispositivo está cerca del punto de acceso o si está en modo de espera. Alta Movilidad. Producto del bajo consumo energético y el concepto de latencia incorporado por la especificación Bluetooth constituye la mejor alternativa al otorgar la mayor autonomía a los distintos dispositivos que lo utilizan, constituyendo la mejor opción frente a otras tecnologías inalámbricas. WI-FILas redes de área local inalámbricas pueden operar en tres modalidades: Ad-Hoc : En esta modalidad, no existen puntos de acceso, por lo que los dispositivos clientes se comunican entre ellos directamente, y por lo tanto, no existe conectividad con el mundo alámbrico. Infraestructura : En este modo de operación, el más común en el ambiente corporativo, uno o varios APs generan una red inalámbrica que permite que los dispositivos clientes tengan acceso a los recursos de la empres. Bridge : Esta es una aplicación especial que consiste en utilizar dos APs para implementar un enlace inalámbrico entre dos sitios o redes. Generalmente, se utilizan APs que operan en la interperie con antenas especiales para grandes distancias. ¿Cómo Funciona? Se llega a las dependencias del cliente con un acceso banda ancha, ADSL, donde se instala un gateway WI-FI (inalámbrico con norma 802.11b. Los computadores del cliente deben estar provistos con tarjetas de red inalámbricas para que puedan comunicarse con el gateway WIFI. Todos los equipos para comunicarse entre sí deben pasar por el Gateway. Este gateway posee puertas de conexión alámbricas (Ethernet tradicional RJ-45) y puertas virtuales inalámbricas. A las puertas alámbricas es posible conectar equipos que tengan puertas Ethernet. Mientras que a las puertas inalámbricas lo equipos a conectar deben poseer tarjetas WI-FI norma 802.11b o 802.11g. Ventajas Acceso a Internet Banda Ancha. Acceso simultáneo de varios computadores a Internet. Permite montar una RED LAN sin necesidad de cables, permitiendo llegar a zonas difíciles de cablear. Permite movilidad del usuario dentro del recinto con WI-FI. De rápido y fácil montaje. Restricciones Los computadores del cliente deben estar dentro de una misma dirección y casa o departamento. El cliente debe poseer tarjeta WI-FI instalada en cada uno de los computadores a conectar. La señal WI-FI NO atraviesa paredes de hormigón. El alcance máximo de la seña es aproximadamente de 50 mts en planta libre. Requiere que la señal tenga buena recepción entre gateway y PC de cliente. Diferencias entre WI-FI y Bluetooth SeguridadCuando un medio de red nuevo se introduce en un nuevo entorno siempre surgen nuevos retos. Esto es cierto también en el caso de las redes LAN inalámbricas. Algunos retos surgen de las diferencias entre las redes LAN con cable y las redes LAN inalámbricas. Por ejemplo, existe una medida de seguridad inherente en las redes con cable, ya que la red de cables contiene los datos. Las redes inalámbricas presentan nuevos desafíos, debido a que los datos viajan por el aire, por ondas de radio. Otros retos se deben a las posibilidades únicas de las redes inalámbricas. Con la libertad de movimiento que se obtiene al eliminar las ataduras (cables), los usuarios pueden desplazarse de sala en sala, de edificio en edificio, de ciudad en ciudad, etc, con las expectativas de una conectividad ininterrumpida en todo momento. Las redes siempre han tenido retos, pero éstos aumentan cuando se agrega complejidad, tal como sucede con las redes inalámbricas. Por ejemplo, a medida que la configuración de red continúa simplificándose, las redes inalámbricas incorporan características (en ocasiones para resolver otros retos) y métrica que se agrega a los parámetros de configuración. Retos de SeguridadUna red con cable está dotada de una seguridad inherente en cuanto a que un posible ladrón de datos debe obtener acceso a la red a través de una conexión por cable, lo que normalmente significa el acceso físico a la red de cables. Sobre este acceso físico se pueden superponer otros mecanismos de seguridad. Cuando la red ya no se sustenta con cables, la libertad que obtienen los usuarios también se hace extensiva al posible ladrón de datos. Ahora, la red puede estar disponible en vestíbulos, salas de espera inseguras, e incluso fuera del edificio. En un entorno doméstico, la red podría extenderse hasta los hogares vecinos si el dispositivo de red no adopta o no utiliza correctamente los mecanismos de seguridad. Desde sus comienzos, 802.11 ha proporcionado algunos mecanismos de seguridad básicos para impedir que esta libertad mejorada sea una posible amenaza. Por ejemplo, los puntos de acceso (o conjunto de puntos de acceso) 802.11 se pueden configurar con un identificador del conjunto de servicios (SSID). La tarjeta NIC también debe conocer este SSID para asociarlo al AP y así proceder a la transmisión y recepción de datos en la red. Esta seguridad, si se llegase a considerar como tal, es muy débil debido a estas razones: Todas las tarjetas NIC y todos los AP conocen perfectamente el SSID. El SSID se envía por ondas de manera transparente (incluso es señalizado por el AP). La tarjeta NIC o el controlador pueden controlar localmente si se permite la asociación en caso de que el SSID no se conozca. No se proporciona ningún tipo de cifrado a través de este esquema. Aunque este esquema puede plantear otros problemas, esto es suficiente para detener al intruso más despreocupado. Las especificaciones 802.11 proporcionan seguridad adicional mediante el algoritmo WEP (Wired Equivalent Privacy). WEP proporciona a 802.11 servicios de autenticación y cifrado. El algoritmo WEP define el uso de una clave secreta de 40 bits para la autenticación y el cifrado, y muchas implementaciones de IEEE 802.11 también permiten claves secretas de 104 bits. Este algoritmo proporciona la mayor parte de la protección contra la escucha y atributos de seguridad física que son comparables a una red con cable. Una limitación importante de este mecanismo de seguridad es que el estándar no define un protocolo de administración de claves para la distribución de las mismas. Esto supone que las claves secretas compartidas se entregan a la estación inalámbrica IEEE 802.11 a través de un canal seguro independiente del IEEE 802.11. El reto aumenta cuando están implicadas un gran número de estaciones, como es el caso de un campus corporativo. Para proporcionar un mecanismo mejor para el control de acceso y la seguridad, es necesario incluir un protocolo de administración de claves en la especificación. Para hacer frente a este problema se creó específicamente el estándar 802.1x. Retos para los Usuarios MóvilesCuando un usuario o una estación se desplaza de un punto de acceso a oto punto de acceso, se debe mantener una asociación entre la tarjeta NIC y un punto de acceso para poder mantener la conectividad de la red. Esto puede plantear un problema especialmente complicado si la red es grande y el usuario debe cruzar límites de subredes o dominios de control administrativo. Si el usuario cruza un límite de subred, la dirección IP asignada originalmente a la estación puede dejar de ser adecuada para la nueva subred. Si la transición supone cruzar dominios administrativos, es posible que la estación ya no tenga permiso de acceso a la red en el nuevo dominio basándose en sus credenciales. Más allá del simple desplazamiento dentro de un campus corporativo, otros escenarios de usuarios móviles son muy reales. Los aeropuertos y restaurantes agregan conectividad inalámbrica con Internet y las redes inalámbricas se convierten en soluciones de red populares para el hogar. Ahora es más probable que el usuario pueda abandonar la oficina para reunirse con alguien de otra compañía que también disponga de una red inalámbrica compatible. De camino a esta reunión, el usuario necesita recuperar archivos desde la oficina principal y podrá encontrarse en una estación de tren, un restaurante o un aeropuerto con acceso inalámbrico. Para este usuario sería de mucha utilidad poder autenticarse y utilizar esta conexión para obtener acceso a la red de la empresa. Cuando el usuario llegue a su destino, puede que no tenga permiso de acceso a la red local de la empresa que va a visitar. Sin embargo, sería fortuito que el usuario pudiera obtener acceso a Internet en este entorno extraño. Entonces, dicho acceso podría utilizarse para crear una conexión de red privada virtual con la red de su empresa. Después, el usuario podría irse a casa y desear conectarse a la red doméstica para descargar o imprimir archivos para trabajar esa tarde. Ahora, el usuario se ha desplazado a una nueva red inalámbrica, que posiblemente incluso puede ser de la modalidad ad hoc . Para este ejemplo, la movilidad es una situación que debe pensarse muy detenidamente. La configuración puede ser un problema para el usuario móvil, ya que las distintas configuraciones de red pueden suponer un reto si la estación inalámbrica del usuario no tiene capacidad para configurarse automáticamente. Retos de ConfiguraciónAhora que tenemos una conexión de red inalámbrica y la complejidad ha aumentado, posiblemente hay muchas más configuraciones que realizar. Por ejemplo, podría ser necesario configurar el SSID de la red a la que se va a realizar la conexión. O bien, podría ser necesario configurar un conjunto de claves WEP de seguridad; posiblemente, varios conjuntos de claves si es necesario conectarse a varias redes. Podría ser necesario tener una configuración para el trabajo, donde la red funciona en modo de infraestructura, y otra configuración para el domicilio, donde funciona en modo ad hoc . Entonces, sería necesario elegir qué configuración se va a utilizar en función del lugar donde nos encontremos. Mecanismos de SeguridadControl de Acceso: Codificación y Autenticación Por sí solo, el modo de modulación no garantiza una red local inalámbrica segura. Se necesitan varios requisitos para obtener una seguridad real: Asignación de clave SSID: Cada usuario (cliente o punto de acceso) de una red WLAN recibe su propia identificación SSID que ha sido asignada por el administrador de red al configurar la red inalámbrica. Direcciones MAC: El fabricante asigna una única dirección MAC global a cada adaptador de una red WLAN. La dirección se debería introducir en las listas de acceso para el punto de acceso. Todos los demás adaptadores de red se rechazan automáticamente. Autenticación: Cada estación debe probar que está autorizada para conectarse a la red WLAN correspondiente. Por este motivo, los productos para WLAN actuales utilizan el algoritmo WEP. Codificación WEP: El estándar 802.11 implementa WEP como su tecnología de codificación. La versión más segura a 128 bits se debería utilizar (como hace Intel) para disfrutar de una mayor seguridad. Utilice la tecnología de red privada virtual (VPN): Las redes privadas virtuales (VPN) llevan ya bastante tiempo operativas y están consideradas como muy seguras. Se obtendrá una red local inalámbrica segura si se sabe sacar provecho de esta tecnología para redes inalámbricas. Un escenario típico para muchos usuarios es una pequeña oficina (agencias, etc) que se reparte en varias habitaciones de una misma planta. Cifrado WEP Resulta evidente a todas luces que las comunicaciones inalámbricas ofrecen un punto de vulnerabilidad en la transmisión de datos, puesto que las emisiones difícilmente pueden acotarse a la zona de cobertura, sino que habitualmente suelen alcanzar puntos fuera del área de transmisión deseada. Para paliar que otros receptores ajenos a la red corporativa y a los intereses de la empresa puedan hacer un uso indebido de la información que viaje por el aire se ha adoptado un sofisticado mecanismo de control de acceso al medio (DSSS), lo cual evita en gran medida las escuchas indiscretas. No obstante, este sistema no es suficiente, por lo que opcionalmente se puede realizar un proceso de cifrado de los datos que se transmiten por la red inalámbrica. A la hora de proteger la información que viaja por el espacio mediante sistemas de cifrado se puede hacer uso de las técnicas WEP-40 y WEP-128. Estos dos sistemas son funciones opcionales de la especificación IEEE 802.11 que proporcionan una confidencialidad de datos equivalente a la de una LAN cableada sin cifrar. Es decir, el sistema WP hace que el enlace LAN inalámbrico en una red sea tan seguro como el enlace con cable. Como se especifica en el estándar, WEP (Wired Equivalent Privacy) utiliza el algoritmo RC4 con una clave de 40 bits para WEP-40 o una clave de 128 bits para WEP-128. Cuando la función WEP está activada, cada estación (cliente o punto de acceso) se le asigna una clave común. Esta clave desordena los datos y se mezcla entre la información antes de ser transmitida, de tal modo que si una estación recibe un paquete que no está mezclado con la clave correcta, la estación descartará el paquete. A la hora de la verdad, la instalación de esta función es opcional, aunque sumamente sencilla de poner en marcha. Simplemente habrá que seleccionar el tipo de encriptación WEP que se desea implementar, 40 o 128 bits, y a continuación elegir la clave que se utilizará. Obviamente, si la función WEP está activada en uno o más puntos de acceso, todos los dispositivos inalámbricos de la red deberán tener el mismo código WEP, que se establece fácilmente mediante las utilidades de software suministradas. No obstante, existe la posibilidad de establecer una comunicación con células combinadas. Una célula combinada es una red de radio en la que algunos dispositivos utilizan WEP y otros no. Esta opción es posible mediante la simple activación del parámetro “Allow Association To Mixed Cells”. OSA (Open System Authentication) Es otro mecanismo de autenticación definido por el estándar 802.11 para autentificar todas las peticiones que recibe. El principal problema que tiene es que no realiza ninguna comprobación de la estación cliente. Además las tramas de gestión son enviadas sin encriptar, aún activando WEP. Por lo tanto es un mecanismo poco fiable. ACL (Access Control List) Este mecanismo de seguridad es soportado por la mayoría de los productos comerciales. Utiliza, como mecanismo de autenticación, la dirección MAC de cada estación cliente, permitiendo el acceso a aquellas MAC que consten en la Lista de Control de Acceso. CNAC (Closes Network Access Control) Este mecanismo pretende controlar el acceso a la red inalámbrica y permitirlo solamente a aquellas estaciones cliente que conozcan el nombre de la red (SSID) actuando este como contraseña. WPA WPA (Wi-Fi Protected Access) es un nuevo protocolo para reemplazar al desacreditado WEP. Incrementa el nivel de protección de datos y el control de acceso para las WLAN existentes y las futuras. WPA está diseñado para correr en el hardware existente como una actualización de software. Se deriva de la próxima versión del estándar IEEE 892.11, el 802.11i, y desde luego será compatible con él. WPA utiliza el protocolo temporal de intercambio de claves, TKIP, una tecnología de cifrado por claves más segura que la RC4 de WEP. Además proporciona autenticación de usuario, que no estaba casi presente en WEP. Para fortalecer la autenticación de usuario WAP implementa los protocolos 802.1x y EAP (Extensible Authentication Protocol). En un entorno empresarial se utiliza un servidor central de autenticación, como por ejemplo RADIUS, para autenticar a cada usuario antes de que se una a la red y además se utiliza la “autenticación mutua”, de forma que el usuario no se una accidentalmente a una red maliciosa que pudiera robarle sus credenciales de red. En entornos domésticos o de pequeña oficina donde no hay servidores centrales de autenticación o EAP, WAP corre en un modo especial denominado PSK (Pre-Shared Key). En este modo el usuario introduce claves manualmente en los puntos de acceso o pasarelas domésticas inalámbricas y en cada PC que está en la red inalámbrica. Desde este punto WPA toma automáticamente el control: por un lado se permite que se unan a la red sólo dispositivos con la misma password y por otro lado arranca automáticamente el proceso de encriptación TKIP. BeneficiosEntre los beneficios principales de las redes de área local inalámbricas se encuentran: Alternativa o extensión de una solución de cableado: Existen múltiples casos en que es más económico implementar una solución inalámbrica, en reemplazo a un cableado o como extensión de una red alámbrica: segmento residencial, edificios sin cableado estructurado, oficinas arrendadas, campamentos, plantas libres de gran tamaño, etc. Aumentar la productividad de los empleados: Una implementación de WLAN permite que los empleados puedan tener acceso a la información en cualquier lugar. Esto permite que su tiempo sea más eficiente, y cuenten en todo momento con la información necesaria para la toma efectiva de decisiones. En general, cuando se realiza un análisis de retorno de inversión de una solución de WLAN, este aumento de productividad, por sí solo, justifica la implementación de una WLAN. Reutilización de infraestructura: En algunas industrias es necesario reconfigurar la distribución física de una red de datos, o incluso, cambiar de lugar las instalaciones frecuentemente, en estos casos, las WLAN proveen una solución eficiente en coste que permiten la reutilización de la inversión. Información en tiempo real: Una solución de rede de área local inalámbrica permite utilizar dispositivos de captura de datos o computadores portátiles para ingresar información en tiempo real. Glosario de Términos Ad-Hoc : Es un sistema de red inalámbrica (802.11) que permite que los clientes que están situados en un rango determinado puedan conectarse entre ellos sin necesidad de la presencia de un Punto de Acceso. En este tipo de red, cualquier cliente puede hacer las veces de punto de acceso proporcionando a los demás acceso a Internet o cualquier otro servicio. También se le denomina Peer to Peer, o IBSS (referido al estándar 802.11). El otro sistema de red inalámbrica que emplea Puntos de Acceso se le denomina Infraestructura. Bluetooth : Tecnología y protocolo de conexión entre dispositivos inalámbricos que integran un chip específico para comunicarse en la banda de frecuencias 2,402-2,480 GHz con un alcance máximo de 10 metros y tasas de transmisión de datos de hasta 721 Kbps (en la segunda generación de Bluetooth). Cada dispositivo posee una dirección única de 48 bits que lo identifica de manera inequívoca, siguiendo el estándar IEEE 802. Bridge (Puente) : Dispositivo que pasa todos los mensajes de una red a otra sin distinguir a cual red pertenece el destino del mensaje. Cobertura : Área geográfica próxima a un nodo o estación base que recibe suficiente señal para mantener una conexión. Depende de diversos factores como tipo de antena, ubicación, topografía del terreno, potencia de la señal, etc. Enlace Punto a Punto : Enlace en el que las comunicaciones están dirigidas entre dos puntos de conexión concretos. En Redes Wireless suelen ser las conexiones que enlazan dos nodos para ampliar el alcance de la red, o para conectar dos redes remotas. Se suelen emplear antenas directivas de gran ganancia (dependiendo de la distancia que separe los nodos). Hot Spot : Lugar donde existe un punto de acceso en una WLAN que ofrece servicio de banda ancha a usuarios móviles. IEEE 802.x : Conjunto de especificaciones de las redes LAN dictadas por el IEEE (the Institute of Electrical and Electronic Engineers). Existe un comité 802.11 trabajando en una normativa para redes inalámbricas de 1 y 2 Mbps. La norma tendrá una única capa MAC para las siguientes tecnologías: Frequency Hopping Spread Spectrum (FHSS), Direct Sequence Spread Spectrum (DSSS) e infrarrojos. Infraestructura de red : Red inalámbrica centrada en un punto de acceso. En este entorno los puntos de acceso no solo proporcionan comunicación con la red cableada sino que también median el tráfico de red en la vecindad inmediata. PAN (Red de Área Personal) : Sistema de red conectado directamente a la piel. La transmisión de datos se realiza por contacto físico. También se le llama así a la red de área personal, un conjunto de dispositivos que normalmente son de uso personal, ej: micrófono, teclado, impresora conectados con un computador. Punto de acceso : Dispositivo que permite comunicar a varios clientes wireless entre ellos e incluso con otras redes inalámbricas o de cable. Los puntos de acceso hacen las funciones de Bridges y algunos incluso de routers (encaminadores). Router (Encaminador) : Dispositivo hardware (o software) para redes informáticas dotado de capacidad para conmutación y con la principal finalidad de proporcionar un encaminamiento de paquetes IP. SSID : El primer paso para poder autenticar un cliente en una red wireless es el conocimiento del SSID (Servide Set IDentifier). Para obtener acceso al sistema es necesario conocer el SSID. VPN (Red Privada Virtual) : Configuración lógica de una serie de componentes hardware, que permite la utilización de redes públicas para establecer canales de comunicaciones privados a los que sólo pueden acceder usuarios autorizados. WPAN (Red Inalámbrica de Área Personal) : Redes locales-personales que utilizan tecnología Bluetooth. WECA (Alianza para la Compatibilidad de Ethernet Inalámbrica) : Alianza de fabricantes formada para mantener la compatibilidad entre dispositivos wireless. La WECA creó el estándar de dispositivos inalámbricos Wi-Fi, que cumplen la norma IEEE 802.11b. Wi-Fi (Wireless Fidelity) : Sinónimo del estándar IEEE 802.11b, protocolo de transmisión inalámbrica que logra alcanzar desde 2 Mbps hasta un máximo teórico de 11 Mbps. Este estándar fue creado por un grupo de fabricantes de dispositivos inalámbricos para mantener la compatibilidad entre sus productos. Permite crear redes de ordenadores sin que exista un cable de por medio, usando para ello ondas de radio. Wireless (Inalámbrico) : Es un sistema de comunicación que utiliza ondas de radiofrecuencia, ultrasonido o rayos infrarrojos (IR) para intercambiar datos entre dispositivos. Cada vez se está popularizando más el uso de este sistema para transferencia de datos entre cámaras digitales, PDAs, calculadoras, etc. con el ordenador. En Internet, este término es utilizado para indicar que la transmisión de información se efectúa prescindiendo de cables. Es el caso de los celulares con sistema WAP, o las conexiones a Internet. WLAN (Red de Área Local Inalámbrica) : Una WLAN es un tipo de red de área local (LAN) que utiliza ondas de radio de alta frecuencia en lugar de cable para comunicar y transmitir datos entre los clientes de red y los dispositivos. Es un sistema de comunicación de datos flexible implementado como una extensión, o como una alternativa para una LAN conectada. Al igual que una LAN, la red permite que los usuarios de esa ubicación compartan archivos, impresoras y otros servicios.La mayoría de las redes WLAN utilizan tecnología de espectro distribuido. Su ancho de banda es limitado (generalmente inferior a 11 Mbps) y los usuarios comparten el ancho de banda con otros dispositivos del espectro; no obstante, los usuarios pueden operar dispositivos de espectro distribuido sin autorización de la FCC (Comisión Federal de Comunicaciones). Bibliografía Scribd (Ibiza Ales) IP móvil y PLC (Power Line Comunications). Características técnicas. Modos de operación. Seguridad. Normativa reguladora. Ventajas e inconvenientes. Televisión digital. Servicios de televisión (IPTV y OTT). Radiodifusión sonora Digital.IP MóvilIntroducciónEn los últimos años se han ido produciendo numerosos avances en el campo de las tecnologías de comunicación. Dos de los más relevantes son, sin duda, el rápido desarrollo de la informática portátil y la importante implantación de los sistemas de comunicación móviles. La conjunción de ambos factores permite a los usuarios acceder a una red en cualquier momento y en cualquier lugar, aún cuando se encuentren en movimiento. Sin embargo, los actuales protocolos de internet working (TCP/IP, IPX o Apple Talk) presentan serias complicaciones a la hora de tratar con nodos que disponen de un cierto grado de movilidad entre redes. La mayoría de las versiones del protocolo IP (Internet Protocol) asumen de manera implícita que el punto al cual el nodo se conecta a la red es fijo. Por otra parte, la dirección IP del nodo identifica al mismo de manera unívoca enn la red a la que se encuentra conectado. Por consiguiente, cualquier paquete destinado a ese nodo es encaminado en función de la información contenida en la parte de su dirección IP que identifica la red en que está conectado. Esto implica que un nodo móvil que se desplaza de una red a otra y que mantiene su dirección IP no será localizable en su nueva situación ya que los paquetes dirigidos hacia este nodo serán encaminados a su antiguo punto de conexión a la red. El protocolo IP Móvil constituye una mejora del protocolo IP citado anteriormente. Mobile IP permite a un nodo circula libremente a través de Internet siendo éste siempre accesible mediante una única dirección IP. Arquitectura IP Móvil La Internet Engineering Task Force (IETF) propone una arquitectura Mobile IP que funciona, a grandes rasgos, bajo el siguiente concepto: un agente local, denominado Home Agent (HA) y un agente externo, también denominado Foreign Agent (FA) colaboran para permitir que el nodo móvil o Mobile Host (MH) pueda moverse conservando su dirección IP inicial. La Capa de Red InternetEn la capa de red, Internet puede verse como un conjunto de subredes, o sistemas autónomos (AS, Autonomous System) interconectados. No hay una estructura real, pero existen varios backbone principales. Estos se construyen a partir de líneas de alto ancho de banda y enrutadores rápidos. Conectadas a los backbone hay redes regionales (de nivel medio), y conectadas a estas redes regionales están las LAS de muchas universidades, compañías y proveedores de servicio Internet. El pegamento que mantiene unida a Internet es el protocolo de capa de red, IP ( Internet Protocol o Protocolo de Internet ). A diferencia de la mayoría de los protocolos de capa de red anteriores, éste se diseñó desde el principio con la interconexión de redes en mente. Una buena manera de visualizar la capa de red es la siguiente. Su trabajo es proporcionar un medio de mejor esfuerzo para el transporte de datagramas del origen al destino, sin importar si estas máquinas están en la misma red, o si hay otras redes entre ellas. La comunicación en Internet funciona como sigue. La capa de transporte toma corrientes de datos y las divide en datagramas. En teoría, los datagramas pueden ser de hasta 64 Kbytes cada uno, pero en la práctica por lo genera son de unos 1500 bytes. Cada datagrama se transmite a través de Internet, posiblemente fragmentándose en unidades más pequeñas en el camino. Cuando todas las piezas llegan finalmente a la máquina de destino, son reensambladas por la capa de red, dejando el datagrama original. Este datagrama entonces es entregado a la capa de transporte, que lo introduce en la corriente de entrada del proceso receptor. Objetivos de IP MóvilIP Móvil (Mobile IP) es un concepto muy amplio, se puede aplicar a 3 formas distintas de movilidad: Ordenadores portátiles : que se transportan y conectan desde lugares remotos a Internet. Un buen ejemplo sería el ordenador de un hombre de negocios que a lo largo del día podría conectarse a Internet desde casa, la oficina, el centro de conferencias, Internet café, … Se podría usar una configuración dinámica para conseguir direcciones temporales en una red remota, sin embargo, esto no es más que una solución parcial. Por poner un ejemplo, las conexiones TCP se identifican por un par de dirección IP y puerto, por lo que usando direcciones temporales implica que este tipo de conexiones no sobrevivirían. Ordenadores móviles : este tipo de ordenadores pueden mantener la conexión mientras se mueven de forma transparente al usuario análogamente a como lo hace un teléfono móvil. En cada celda hay una estación conectada a Internet que es capaz de intercambiar paquetes entre el canal de radio de la celda e Internet. Los ordenadores estarán escuchando constantemente las señales que proceden de las estaciones y escogerán aquella cuya señal sea más clara. El objetivo de la tecnología de IP móvil es permitir la transición de celda a celda (llamado ‘roaming’ o tránsito, vagabundeo…) mientras se mantiene a la unidad móvil conectada a Internet manteniendo la misma dirección IP, para que así las conexiones TCP puedan mantenerse. La tecnología utilizada a nivel físico va desde transmisión por infrarrojo hasta las ondas de radio de amplio espectro, cada una con sus ventajas e inconvenientes. Redes móviles : se trata de una red que en su totalidad es móvil. Por ejemplo, un portaaviones tiene su propia red interna y además necesita conectividad al exterior. Una de las primeras aplicaciones no militares de este tipo de redes ha sido en la fórmula uno con la conectividad de los coches (que tienen en la actualidad gran cantidad de mini computadores) con los boxes. Se puede añadir una nueva dimensión en la movilidad de IP, añadiendo hosts móviles a estas redes móviles. Cuando el grupo de trabajo de IP Móvil del IETF empezó a trabajar, una des sus primeras tareas fue delimitar los requerimientos del protocolo: Un nodo móvil debe ser capaz de comunicarse con otros nodos después de haber cambiado su punto de conexión a Internet, sin cambiar su dirección IP. Un nodo móvil debe ser capaz de comunicarse con otros nodos que no implementan las funciones de movilidad. De hecho el resto de routers y hosts que no implementan IP Móvil no tienen por qué mejorar su versión del protocolo para poder comunicarse con hosts móviles. Es impensable necesitar cambiar todos los hosts y routers de Internet para adaptarlos a IP Móvil. Todos los mensajes usados para actualizar la información de host móvil deben ser autentificados como medida de protección contra ataques remotos. Estos son los objetivos fundamentales, hay algunos otros secundarios: Posibilidad Multicast . Presenta problemas nuevos a la ya mayor complejidad e Multicast (envío de un mensaje de un usuario a un grupo determinado de usuarios). El hecho de que los usuarios de un servicio Multicast puedan moverse hace que el árbol de expansión de la difusión multicast tenga que ser recalculado constantemente. Privacidad de información de localización. Se pretende esconder la localización de los hosts a otros hosts , para que nadie pueda ‘rastrear’ las celdas por las que un host ha estado moviéndose. Minimizar el número de mensajes administrativos que se envían por dos razones: en primer lugar las conexiones inalámbricas son de bajo ancho de banda y muy propensas a errores; en segundo lugar los nodos móviles están alimentados por baterías y minimizar el consumo de energía es importante. TerminologíaA lo largo del documento usaremos frecuentemente estos términos: Nodo: un host o router . Nodo móvil: un host o router que cambia su ‘punto de anclaje ‘ de una red o subred a otra. Cuando cambia de lugar debe poder mantener la comunicación y su dirección IP. Agente doméstico ( home agent ): un router en la red doméstica del nodo móvil que envía datagramas al nodo móvil cuando está fuera de la red doméstica a través de un túnel. Además mantiene información sobre la localización del nodo móvil. Agente ajeno o exterior ( foreign agent ): un router en la red que está visitando el nodo móvil que da servicios de routing al nodo móvil mientras está registrado. Recibe la información del túnel que ha enviado el agente doméstico y la entrega al nodo móvil. Sirve también como router por defecto para los nodos móviles registrados en su red. Anuncio de Agente ( Agent advertisment ): un mensaje de anuncio construido añadiendo una extensión especial a un mensaje de anuncio. Care-of-address (COA): el nodo móvil recibe una dirección IP fija en su red local, esta es permanente. Cuando el nodo está fuera de su red local se le asigna una dirección de ‘de reenvío’ o care-of address (en inglés care of se usa cuando se envía un correo o fax a una dirección pero va dirigido a otra persona, se podría traducir como ‘ a la atención de …’). Esta care-of address (COA) se asocia con el nodo móvil y representa su punto de anclaje a la Internet actual. El nodo móvil usa su dirección fija como dirección de origen en todos sus datagramas a excepción de algunos mensajes de administración de movilidad. Fundamentos de IP MóvilHoy en día, millones de personas tienen ordenadores portátiles, y generalmente quieren leer su correo electrónico y acceder a sus sistemas de archivos normales desde cualquier lugar del mundo. Estos hosts móviles generan una nueva complicación: para enrutar un paquete a un host móvil, la red primero tiene que encontrarlo. El tema de la incorporación de host móviles en una red es muy nuevo, pero en esta sección plantearemos algunos de os problemas relacionados y sugeriremos una posible solución. Se dice que son estacionarios los usuarios que nunca se mueven; se conectan a la red mediante hilos de cobre o fibra óptica. En contraste distinguimos otros dos tipos de usuarios. Los usuarios migratorios básicamente son los usuarios estacionarios que se mueven de un lugar fijo a otro de tiempo en tiempo, pero que usan la red sólo cuando están conectados físicamente a ella. Los usuarios errantes usan su ordenador en movimiento, y quieren mantener sus conexiones mientras se mueven. Usaremos el término usuarios móviles para indicar las dos últimas categorías. Se supone que todos los usuarios tienen una localidad base que nunca cambia. Los usuarios también tienen una dirección de base permanente, que puede servir para determinar su localidad base, de manera análoga a como el número +034-93-3383694 indica España (código de país +034) y Barcelona (93). La meta de enrutamiento en los sistemas con usuario móviles es posibilitar el envío de paquetes a usuarios móviles usando su dirección base, y hacer que los paquetes lleguen eficientemente a ellos en cualquier lugar en el que puedan estar. Lo difícil, por supuesto, es encontrarlos. El mundo se divide (geográficamente) en unidades pequeñas. Llamémoslas áreas, siendo un área típicamente una LAN o una célula inalámbrica. Cada área tiene uno o más agentes externos, que llevan el registro de todos los usuarios que visitan el área. Además, cada área tiene un agente de base, que lleva el registro de todos los usuarios móviles cuya base está en el área, pero que actualmente están visitando otra área. Al entrar un usuario nuevo en un área, ya sea al conectarse a ella (por ejemplo, conectándose a la LAN), o simplemente al entrar en la célula, su ordenador debe registrarse con el agente externo de ese lugar. El procedimiento de registro, funciona de esta manera: Periódicamente, cada agente externo difunde un paquete que anuncia su existencia y dirección. Un host móvil recién llegado puede esperar uno de estos mensajes, pero si no llega ninguno, el host móvil puede difundir un paquete que diga: “¿hay agentes externos por ahí?”. El host móvil se registra con el agente externo, dando su dirección base, su dirección actual de capa de enlace de datos y cierta información de seguridad. El agente externo se pone en contacto con el agente de base del host móvil y le dice: “uno de tus hosts está por aquí”. El mensaje del agente externo al agente de base contiene la dirección de red del agente externo, así como la información de seguridad, para convencer al agente de base de que el hosts móvil en realidad está ahí. El agente de base examina la información de seguridad, que contiene una marca de tiempo, para comprobar que fue generada en los últimos segundos. Si está conforme, indica al agente externo que proceda. Cuando el agente recibe el reconocimiento del agente de base, hace una entrada en sus tablas e informa al host móvil que ahora está registrado. Movilidad IPMuchos usuarios de Internet tienen ordenadores portátiles y quieren mantenerse conectados a Internet en sus desplazamientos. Desafortunadamente, el sistema de direccionamiento IP hace que el trabajo lejos de casa sea más fácil de plantear que de hacer. El verdadero problema es el esquema de direccionamiento. Cada dirección IP contiene tres campos: la clase, el número de red y el número de host. Por ejemplo, considere la máquina con dirección IP 160.80.40.20. El 160.80 da la clase (B) y el número de red; el 40.20 es el número de host. Los enrutadores de todo el mundo tienen tablas de enrutamiento que indican la línea a usar para llegar a la red 160.80. Cuando llega un paquete con una dirección IP de destino de forma 160.80.xxx.yyy, sale por esa línea. Si de pronto la máquina con esa dirección se lleva a algún lugar lejano, los paquetes para ella se seguirán enviando a su LAN (o enrutador) base. El dueño ya no podrá recibir correo electrónico, etc. Darle a la máquina una nueva dirección IP correspondiente a su nueva ubicación no es muy atractivo, pues habría que informar a una gran cantidad de gente, programas y BD sobre el cambio. Otro enfoque es hacer que los enrutadores usen direcciones IP completas para el enrutamiento, en lugar de sólo la clase y la red. Sin embargo, esta estrategia requeriría que cada enrutador tuviera tablas de millones de entradas, con un coste astronómico para Internet. Cuando la gente comenzó a exigir la posibilidad de tener hosts móviles, el IETF (Internet Engineering Task Force) estableció un grupo de trabajo para encontrar una solución. El grupo de trabajo pronto formuló varias metas deseables en cualquier solución. Las principales fueron: Todo host móvil debe ser capaz de usar su dirección IP base en cualquier lugar. No se permiten cambios al software de los hosts fijos. No se permiten cambios al software del enrutador ni a sus tablas. La mayoría de los paquetes para los hosts móviles no deben desviarse en el camino. No debe incurrirse en carga extra cuando un host móvil está en su base. En síntesis, la solución escogida consiste en que cada instalación que quiera permitir la movilidad de sus usuarios debe crear un agente de base. Cada instalación que permita visitantes tienen que crear un agente externo. Al aparecer un host móvil en una instalación externa, se pone en contacto con el host externo y se registra. El host externo entonces se comunica con el agente de base del usuario y le da una dirección de encargado (care-off address), normalmente la misma dirección IP del agente externo. Al llegar un paquete a la LAN base del usuario, llega por un enrutador conectado a la LAN. El enrutador entonces trata de localizar al host de la manera normal, difundiendo un paquete ARP que pregunta por ejemplo: “¿cuál es la dirección Ethernet de 160.80.40.20?”. El agente de base responde a esta solicitud dando su propia dirección Ethernet. En enrutador entonces envía los paquetes para 160.80.40.20 al agente de base. Éste a su vez, los envía a través de un túnel a la dirección de encargado, encapsulándolos en el campo de carga útil de un paquete IP dirigido al agente externo. El agente externo entonces los desencapsula y los entrega a la dirección de enlace de datos del host móvil. Además, el agente de base entrega la dirección de encargado al transmisor, para que los paquetes futuros puedan enviarse en túnel directamente al agente externo. Esta solución cumple con todos los requisitos indicados antes. En el momento de moverse el host móvil, el enrutador probablemente tiene en caché sus direcciones Ethernet (que pronto dejarán de ser válidas). Para reemplazar esa dirección de Ethernet por la del agente de base, se usa un truco llamado ARP gratuito (gratuitious ARP). Éste es un mensaje especial al enrutador, no solicitado, que causa que reemplace una entrada específica de caché, en este caso la del host móvil a punto de desconectarse. Al regresar el host móvil, se usa el mismo mecanismo para actualizar la caché del enrutador. Nada en el diseño impide que un host móvil sea su propio agente externo, pero este enfoque sólo funciona si el host móvil (en su capacidad como agente externo) está conectado lógicamente a Internet en su instalación actual. También, debe poder adquirir una dirección IP de encargado (temporal) para usarla. Esa dirección IP debe pertenecer a la LAN a la que está conectado actualmente. La solución IETF (Internet Engineering Task Force) para hosts móviles resuelve otros problemas no mencionados hasta ahora. Por ejemplo, ¿cómo localizar a los agentes?. La solución es que cada agente difunda periódicamente sus direcciones y el tipo de servicios que está dispuesto a proporcionar (por ejemplo, base, externo o ambos). Al llegar un host móvil a algún lado, simplemente puede esperar estas difusiones, llamadas anuncios (advertisements). Como alternativa, puede difundir un paquete anunciando su llegada y esperar que el agente externo local responda. Otro problema que tenía que resolverse es lo que había que hacer con los hosts móviles que salen sin decir adiós. La solución es hacer que el registro sea válido sólo durante un intervalo de tiempo fijo; si no se renueva periódicamente, termina su temporización, y así el agente externo puede limpiar sus tablas. Un tema más es el de la seguridad. Cuando un agente de base recibe un mensaje solicitándole que redirija todos los paquetes de Pepe a alguna dirección IP, no debe acceder a menos que esté convencido de que Pepe es el origen de la solicitud, y no alguien que está tratando de hacerse pasar por él. Se usan protocolos específicos cifrados de verificación de autenticidad para este fin. Alternativas a IP Móvil para Dotación de Movilidad a las Estaciones IPPara dotar de movilidad a un nodo de la red, aparecen diferentes alternativas a IP Móvil que son estudiadas con más detalle para ver la viabilidad de su implementación en Internet. Así se concluirá que IP Móvil es la solución adecuada para proporcionar movilidad IP. Algunas de las soluciones que apuntamos son las siguientes: Establecimiento de rutas específicas para terminales con movilidad. Cambio de la dirección IP de los terminales. Soluciones basadas en realizar cambios a nivel de la capa de enlace. Rutas específicas para Nodos con Movilidad La utilización de rutas específicas para los nodos a los que se les quiere dotar de movilidad implica la reconfiguración de las tablas de encaminamiento de los dispositivos de interconexión de red (routers) para permitir contactar con el nodo móvil en su nueva ubicación. Esta solución es extremadamente costosa debido al gran incremento de tráfico que se generaría en la red para soportar la movilidad de los terminales. Para ello sería necesario actualizar las tablas de encaminamiento de cómo mínimo todos los routers entre el enlace local y el nuevo punto de enlace. Si se tiene en cuenta el número de nodos móviles en la red y la velocidad con que éstos cambian de ubicación, estas actualizaciones podrían llegar a colapsar la red. Por lo tanto es importante minimizar el número de routers a actualizar y esto a su vez limitará las posibilidades de encaminamientos alternativos propias del protocolo IP. Cambio en la Dirección IP Otra posible solución consiste en asignar al nodo móvil una nueva dirección IP acorde con su nuevo punto de conexión a la red. Esta solución no es en absoluto recomendable ya que requiere que la entrada del nodo móvil cambia de dirección IP. Si esta operación no se realiza de forma instantánea, cualquier consulta de la dirección IP del nodo móvil puede ser errónea. Por otra parte, y dada la velocidad a la cual el nodo móvil puede cambiar de dirección IP, se hace necesario un mecanismo para verificar la actualidad de la dirección IP devuelta por el servidor de nombres de dominio (DNS). El resultado es un gran número de consultas y actualizaciones que generan, al igual que en el caso anterior, un alto nivel de tráfico inyectado a la red. Finalmente a nivel local un cambio de dirección IP, provoca generalmente el cierre inmediato de todas las aplicaciones abiertas asociadas a la antigua dirección IP. Soluciones a Nivel de la Capa de Enlace Existen dos grandes soluciones a nivel de la capa de enlace que pretenden permitir la movilidad de los nodos. La primera de ellas se basa en el Cellular Digital Packet Data (células de paquetes de datos digitales CDPD), que se trata de un estándar diseñado para transmitir paquetes IP a través de canales de radio no utilizados por el servicio de voz en el sistema de telefonía celular norteamericano. El CDPD asigna a cada nodo móvil una dirección IP fija dentro de su área de cobertura. La segunda solución se basa en el estándar IEEE 802.11, realizado por el Institute of Electrical and Electronics Engineers (IEEE) para la comunicación de redes de área local inalámbricas. Ambas soluciones presentan dos grandes inconvenientes. Por un lado, las soluciones a nivel de la capa de enlace proporcionan movilidad para un solo tipo de medio físico. Por lo tanto, para N tipos de medios físicos diferentes, se requieren N soluciones de movilidad diferentes. Por otro lado, estas soluciones proporcionan una movilidad más o menos limitada geográficamente, lo cual entra en directa contradicción con el afán expansionista de Internet. Como presentaremos en la siguiente sección, el protocolo IP Móvil es el único capaz de proporcionar movilidad en cualquier tipo de medio y en una extensa área geográfica. Funcionamiento del Protocolo IP MóvilIgual que todo protocolo, éste también consiste en la consecución de una serie de operaciones: Los agentes local y externo anuncian su presencia mediante el nodo móvil mediante mensajes de anuncio, los cuales son generados periódicamente en la red. Opcionalmente el nodo móvil puede solicitar tales mensajes a un agente cercano a través de un mensaje de solicitud de agente . El nodo móvil recibe el mensaje de anuncio y determina si se encuentra en su red local o por el contrario al moverse ha ido a parar a una red externa. Si el nodo móvil detecta que se encuentra en su red local operará sin funciones de movilidad. Por otro lado, si regresa tras haber sido registrado en otra red procede a “desregistrarse” a través de su agente local. Si el nodo móvil detecta que se encuentra en una red externa, obtiene su dirección de remite ( care-of-address ) en la nueva red. Esta dirección puede ser la del agente externo o una dirección de remite colocada ( colocated care-of-address ). Si el nodo móvil se encuentra fuera del alcance de ningún tipo de agente, el nodo móvil debe obtener su dirección de remite como una dirección IP local por algún método, como podría ser el DHCP ( Dynamic Host Configuration Protocol, configuración dinámica de host ). En este caso se trata de una dirección de remite “colocada”. El nodo móvil registra su dirección de remite con su agente local. Este proceso se realiza enviando una solicitud de registro al agente local y recibiendo de éste un mensaje de contestación. Todo paquete destinado al nodo móvil es interceptado por el agente local y enviado mediante tunneling por éste hacia la dirección de remite. Al otro lado del túnel el agente externo recibe el paquete y lo envía al nodo móvil. Si éste posee una dirección de remite colocada, el agente externo no interviene en la recepción del paquete. Por su parte, los paquetes originados por el nodo móvil pueden ser transportados a la dirección IP de destino sin pasar necesariamente por el agente local. FasesEn los puntos que exponemos a continuación tratamos de forma más concreta los procedimientos que sigue el desarrollo del protocolo en cuestión, y que apuntábamos en el punto anterior. Descubrimiento del AgenteEs un procedimiento por el que el nodo móvil determina si se encuentra en su red local o si por el contrario y debido a su movimiento se encuentra en una red externa. Asimismo se utiliza para obtener la dirección de remite necesaria para el nodo móvil. Este procedimiento es sencillo y utiliza dos tipos de mensajes mencionados anteriormente: el anuncio de agente y de solicitud de agente. En primer lugar lo que vamos a necesitar es un anuncio por parte del agente local o bien por parte del agente externo de la disponibilidad para aceptar un nodo móvil en su red. El agente local deberá estar siempre dispuesto para servir a sus nodos móviles. Para evitar posibles saturaciones que le impidan cumplir con su compromiso se permite una configuración de un agente local a una determinada población de agentes móviles. Puede ocurrir que un agente externo no pueda servir a un nodo móvil que no pertenece a su red. A pesar de ello el agente externo no puede parar de emitir los mensajes para que el nodo móvil identifique que se encuentra dentro de su red de cobertura. Este mensaje de anuncio consiste en un mensaje ICMP (Internet Control Message Protocol) el cual ha sido extendido para permitir abarcar esta nueva funcionalidad. Anuncio de AgenteLa primera acción a realizar para permitir la movilidad de un nodo es la de anunciar, por parte del agente local o externo, la disponibilidad para aceptar al nodo móvil en su red. El nodo móvil utiliza mensajes de anuncio para determinar su punto de conexión actual a Internet. El agente local deberá estar siempre listo para servir a sus nodos móviles. Para evitar una posible saturación debida al exceso de móviles en una determinada red, es factible configurar múltiples agentes locales en una única red local, asignando a cada agente local una porción de la población de nodos móviles. Por otro lado, es posible que un agente externo no tenga capacidad para servir a un nodo móvil no perteneciente a su red. Aún en ese caso, el agente externo debe continuar emitiendo mensajes de anuncio para que el nodo móvil sepa que se encuentra dentro de su área de cobertura o que no ha fallado. El mensaje de anuncio consiste en un mensaje ICMP de anuncio de router al cual se la ha añadido una extensión para permitir la gestión de los nodos móviles. Los campos de la extensión de anuncio de agente son los siguientes: Type : 16 Length : (6+4*N), donde N es el número de direcciones de cuidado anunciadas. Sequence number : número total de mensajes de anuncio enviados desde que el agente fue inicializado. Registration lifetime : tiempo de vida máximo (S) durante el cuál este agente acepta una solicitud de registro. _R_ : registro solicitado. Es conveniente registrar con un agente externo en vez de usar una dirección de remite colocada. _B_ : el agente externo no puede aceptar nuevos registros, al estar ocupado ( Busy ). _H_ : este agente ofrece servicios de agente local ( Home Agent ) en esta red. _F_ : este agente ofrece servicios de agente externo ( Foreign Agent ) en esta red. _M_ : el agente soporta encapsulado mínimo. _G_ : el agente soporta encapsulado GRE. _V_ : el agente soporta la compresión de cabecera Van Jacobson. Reserved : reservado. Care-of addresses : la dirección de remite anunciada por el agente externo. Campos del mensaje de anuncio de agente Para que un nodo móvil pueda averiguar si se encuentra en su red local o no, ha de verificar los bits F y H de alguno de los mensajes de anuncio que capture, y además sabrá si el agente actúa como agente local o externo. La obtención de su dirección de remite se obtendrá a partir del campo de datos Care-of address indicado anteriormente. Solicitud de AgenteEstos son los mensajes que realiza el nodo móvil cuando no puede, o quiere, esperar la transmisión periódica de mensajes de anuncio de agente. Es decir, este mensaje busca forzar la transmisión de un mensaje de anuncio a cualquier agente ubicado en el mismo enlace. El formato de este tipo de mensajes es igual al que explicábamos al adjuntar la figura en el apartado anterior, con la salvedad que los mensajes de solicitud de agente deben tener su campo de Tiempo de Vida a 1 ( Time To Live -TTL). RegistroHay varias circunstancias bajo las cuales un nodo móvil debe registrarse. La primera de ellas es cuando detecta que su punto de conexión a Internet ha variado respecto al que tenía anteriormente. También deberá registrarse si su registro anterior está a punto de caducarse (aunque no haya cambiado el punto de unión). Y, por último, cuando el nodo móvil esté en una red externa y detecte que su nodo externo se ha reiniciado. El procedimiento de registro sirve para pedir los servicios de un agente externo. A continuación el nodo móvil comunica a su nodo local su “dirección de remite” en la red. Por el contrario, si el nodo móvil detecta que ha regresado a su red local debe iniciar el proceso para desregistrarse con su nodo local, para sí poder funcionar como cualquier otro nodo fijo. Son tres los pasos que componen el procedimiento de registro: El nodo móvil envía un mensaje de petición de registro. Según el caso, este mensaje se puede enviar al agente local o al externo (previa aceptación del mismo). El agente recibe la petición de registro y envía al nodo móvil un mensaje de contestación de registro, para informar si su petición de registro ha sido aceptada o no. Si el nodo móvil no recibe la contestación de registro en un período razonable de tiempo procederá a retransmitir las peticiones de registro con intervalos cada vez más largos entre ellos, hasta que al fin reciba contestación. Para poder llevar a cabo este procedimiento es necesaria la cooperación entre los agentes local y externo, intercambiando mensajes de petición de registro. Petición de RegistroEs el mensaje que el nodo móvil envía a su agente local para poder registrarse, y así éste podrá crear o modificar la entrada del nodo móvil en su lista de nodos con movilidad. Su formato se presenta en la siguiente figura: Campos del mensaje de petición de registro Los diferentes campos que conforman el mensaje de petición de registro son los siguientes: Type : 1 (Petición de registro). _S_ : El nodo móvil solicita que el agente local mantenga sus anteriores entradas de movilidad. _B_ : El nodo móvil pide, solicita al agente local que mande hacia él los paquetes broadcast (mensaje uno a todos) que se reciban en la red local. _D_ : El nodo móvil informa al agente local que desencapsulará los paquetes que le sean enviados a su “dirección de remite”. Esto implica que el nodo móvil está utilizando una “dirección de remite colocada”. _M_ : el nodo móvil solicita que el agente local utilice encapsulado mínimo para los paquetes destinados a él. _G_ : El nodo móvil pide al agente local que utilice encapsulado GRE para los paquetes destinados a él. _V_ : El nodo móvil solicita que el agente local emplee la comprensión de cabeceras de Van Jacobson. Reserved : Reservado. Lifetime : Número de segundos restantes antes de la caducidad del registro actual. Home Address : Dirección IP del nodo móvil. Home Agent : Dirección IP del agente local del nodo móvil. Care-of Address : “dirección de remite” = dirección IP a la salida del túnel. Identification : Número de 64 bits creado por el nodo móvil para asociar peticiones de registro con respuestas de registro. También sirve para proteger contra respuestas de registro fraudulentas. Extensions : Extensiones. Respuesta de RegistroComo ya hemos apuntado anteriormente, tras la recepción de una petición de registro el agente local devuelve al nodo móvil un mensaje de respuesta de registro. Si el nodo móvil solicita el servicio a través de un agente externo, será éste quién reciba la respuesta de registro y la envíe a continuación al nodo móvil. Por el contrario, si el nodo móvil está utilizando una “dirección de remite colocada” será él mismo quien reciba el mensaje de respuesta de registro. Este mensaje informa al nodo móvil del resultado de su petición de registro y del tiempo de vida de tal registro, que puede ser igual o inferior al solicitado por el nodo móvil. El agente externo no puede modificar, en ningún caso, el tiempo de vida asignado por el agente local. En la siguiente figura se muestra el formato de este mensaje. Formato del mensaje de respuesta de registro Los campos del mensaje son los siguientes: Type : 3 (Contestación de registro). Code: Código indicador del resultado de la petición de registro. Lifetime : Tiempo de vida, en segundos, de la entrada del nodo móvil en la lista de movilidad del agente local. Home Address : Dirección IP del nodo móvil. Home Agent : Dirección IP del agente local del nodo móvil., Identification : Número de 64 bits creado por el nodo móvil para asociar peticiones de registro con contestaciones de registro. También sirve para proteger contra contestaciones de registro fraudulentas. Extensions : Extensiones. Posibilidades Opcionales del Procedimiento de RegistroAdemás de las acciones anteriormente descritas, el procedimiento de registro permite también llevar a cabo otras interesantes funciones que se enumeran a continuación: Descubrir la dirección de un agente local si el nodo móvil no ha sido configurado con esta información. Seleccionar diferentes tipos de encapsulado de los paquetes. Utilizar la compresión de encabezados de Van Jacobson. Mantener varios registros simultáneos para que cada dirección de remite activa reciba una copia de los paquetes destinados al nodo móvil. Desregistrar ciertas direcciones de cuidado manteniendo otras activas. EncaminamientoEn este apartado presentaremos los diferentes modos en que un paquete puede ser encaminado de su dirección IP de origen hasta la dirección IP de destino, distinguiendo entre las dos posible opciones: que el nodo móvil esté conectado a su red local, o bien que se encuentre en una red externa. Si el nodo móvil se encuentra en su red local, actúa como si de cualquier otro nodo fijo se tratase. Por lo tanto, las reglas para el encaminamiento de paquetes en este caso son las mismas que para el encaminamiento de paquetes IP hacia cualquier nodo o router convencional. En el caso que el nodo móvil se encuentre en una red externa, distinguiremos dos situaciones: Nodo móvil como destino. Nodo móvil como origen. Nodo móvil como destinoEl protocolo IP Móvil requiere que los paquetes enviados desde la red local hasta el nodo móvil sean encapsulados. Esto altera el encaminamiento habitual ya que los paquetes atraviesan un nodo intermedio antes de llegar a su destino. Este nodo realizará el desencapsulado y enviará el paquete original al destino. Las operaciones que comprenden el envío de un paquete hacia un nodo móvil en una red externa son: Un router en la red local, normalmente el agente local, anuncia que existe conectividad hasta el prefijo de red equivalente al de la dirección local del nodo móvil. Es decir, todo paquete destinado al nodo móvil es encaminado hacia su red local y recibido por su agente local. El agente local intercepta el paquete y consulta su entrada en su lista de movilidad para conocer las “direcciones de remite” registradas. El agente local envía una copia del paquete, encapsulándolo, hacia cada “dirección de remite” a través de túneles ( tunneling ). En cada dirección de remite se extrae el paquete original y es entregado al nodo móvil. Si se trata de una dirección de remite de un agente externo, éste deshace el encapsulamiento del paquete. A continuación, consulta el campo de dirección IP de destino para comprobar si coincide con alguno de los nodos móviles a los que está prestando servicio, y si es así, el agente envía el paquete al nodo. Si la dirección es colocada , el nodo móvil no recibe los servicios de ningún agente externo y, por lo tanto, efectúa el mismo las operaciones de desencapsulamiento. Nodo móvil como origenSi el nodo móvil depende de un agente externo, existen dos alternativas a la hora de determinar un router adecuado para dar salida a los paquetes: El propio agente externo, según especifica el campo IP Source Address del mensaje de anuncio de agente. Cualquier router cuya dirección IP aparezca en los campos Router Address del mensaje de anuncio de router. Siempre y cuando el nodo móvil sea capaz de determinar la dirección de la capa de enlace del router deseado, sin tener que enviar peticiones ARP (Address Resolution Protocol) que contengan su dirección local. Si el nodo móvil posee una “dirección de remite colocada”, también tiene dos alternativas para elegir router : Escoger algún router que esté enviando mensajes de anuncio de router (no de agente) en la red en la que se encuentra. Mediante el mismo mecanismo por el que obtuvo su dirección de remite colocada puede obtener la dirección de un router adecuado. Por ejemplo, el protocolo DHCP ofrece todo tipo de información al nodo móvil, incluida la dirección de un router . Contrariamente a los nodos móviles dependientes de un agente externo, los nodo móviles con una “dirección de remite colocada” pueden enviar peticiones ARP con su dirección local. Resolución de Problemas: TunnelingLos paquetes o datagramas son encapsulados (normalmente con datagramas IP) para alterar el normal enrutamiento (o encaminamiento), para que estos sean entregados a destinatarios intermedios que no constan en el campo de dirección de destino en la cabecera original IP. Una vez el datagrama encapsulado llega a este nodo intermedio se desencapsula, dejando el datagrama original IP, el cual es entonces entregado al destino, indicado en el original campo de dirección de destino. Este proceso de encapsular y desencapsular es frecuentemente llamado como “tunneling” del datagrama. Normalmente tenemos: Fuente —-&gt; encapsulador —-&gt; desencapsulador —-&gt; destino El nodo encapsulador es generalmente considerado el punto de entrada al túnel, y el nodo desencapsulador es punto de salida. Pueden haber muchas parejas de fuente-destino usando el mismo túnel. El protocolo Mobile IP ha especificado el uso del encapsulado como un modo de entregar datagramas desde un nodo móvil (“home network”) a un agente que puede entregar datagramas localmente cuando el nodo esté lejos de su red local. El uso del encapsulado también es conveniente cuando la fuente (o un router intermediario) de un datagrama IP quiere decidir la ruta por la cual será entregado a su destino. Por esto, las técnicas de encapsulado IP son especialmente útiles para realizar transmisiones multicast, e incluso llevar a cabo acciones de seguridad y privacidad en Internet. El protocolo IP Móvil, requiere que los agentes locales, los agentes externos y los nodos móviles, que tengan una dirección de remite colocada, soporten el encapsulado IP-in-IP. Ejemplo del TunnelingEl manejo del caso general de lograr la interacción de dos redes diferentes es extremadamente difícil. Sin embargo, hay un caso especial común que puede manejarse. Este caso se da cuando el host de origen y el de destino están en la misma clase de red, pero hay una red diferente en medio. Como ejemplo, piénsese en un banco internacional con una Ethernet basada en TCP/IP en París, una Ethernet basada en TCP/IP en Londres y una WAN PTT en medio, como se muestra en la siguiente figura: Tunneling de un paquete de París a Londres La solución a este problema es el proceso de túnel o tunneling. Para enviar un paquete IP al host 2, el host 1 construye el paquete que contiene la dirección IP del host 2, lo inserta en un marco Ethernet dirigido al enrutador multiprotocolo de París, y lo pone en el Ethernet. Cuando el enrutador multiprotocolo recibe el marco, retira el paquete IP, lo inserta en el campo de carga útil del paquete de capa de red de la WAN, y dirige este último a la dirección de la WAN del enrutador multiprotocolo de Londres. Al llegar ahí, el enrutador de Londres retira el paquete IP y lo envía al host 2 en un marco Ethernet. La WAN puede visualizarse como un gran túnel que se extiende de un enrutador multiprotocolo al otro. El paquete IP simplemente viaja de un extremo del túnel al otro. No tiene que preocuparse por competir con la WAN. Tampoco tienen que hacerlo los hosts de cualquiera de los Ethernet. Sólo el enrutador multiprotocolo tiene que entender los paquetes IP y WAN. De hecho, la distancia completa entre la mitad de un enrutador multiprotocolo y la mitad del otro actúa como una línea serie. Por ejemplo, considérese una persona que conduce su coche de París a Londres. En Francia, el coche se mueve por su propia energía, pero al llegar al Canal de la Mancha, se carga en un tren de alta velocidad y se transporta a Inglaterra a través del Channel, el túnel subterráneo que une a ambos países, ya que los coches no pueden conducirse a través del Channel. En efecto, al automóvil se transporta como una carga, como se muestra en la figura. En el otro extremo, se libera el coche en las carreteras inglesas y nuevamente continúa moviéndose con sus propios medios. El proceso de túnel de paquetes a través de una red externa funciona de la misma manera. Paso de un coche a través del túnel Francia-Inglaterra Las ventajas de la encapsulación queda claro que son muchas, aunque podemos observar los siguientes problemas: Los datagramas encapsulados son normalmente más largos que los datagramas originarios. La encapsulación, como ya hemos comentado antes, no se puede utilizar a menos que el nodo de la salida del túnel pueda desencapsular el datagrama. Encapsulado IP-in-IPEl encapsulado IP-in-IP consiste en insertar una cabecera IP adicional ( outer IP header ) antes de la cabecera IP original ( inner IP header ) del paquete inicial. Si es necesario, también es posible insertar otras cabeceras entre las dos anteriores como por ejemplo la cabecera de autenticación IP. Hay que tener en cuenta que las opciones de seguridad de la cabecera original IP quizás afecten la elección de opciones de seguridad para la cabecera IP insertada en el encapsulamiento. Los campos de la cabecera IP del encapsulado son puestos por el encapsulador: Versión : 4. IHL (Internet Header Length) : es la longitud de esta cabecera medida en palabras de 32 bits. Total Length : muestra la longitud total del IP datagrama encapsulado, incluyendo todas las cabeceras y la información o carga útil (payload). Identification, Flags, Fragment Offset : el bit “Don’t Fragment” indica si el paquete debe o no fragmentarse. Si este bit está a uno (no fragmentarse) en la cabecera IP original, éste se deberá poner a uno en la cabecera adicional; pero si el bit “Don’t Fragment” no está puesto a uno (fragmentarse) en la cabecera original, éste quizás se ponga a uno. TTL (Time To Live) : este campo de tiempo de vida se utiliza para que un datagrama no se quede erróneamente dando vueltas por Internet indefinidamente. Protocol : 4. Header Checksum (suma de comprobación de cabecera). Source Address : muestra la dirección del encapsulador, es decir, del punto de entrada al túnel. Destination Address : muestra la dirección del desencapsulador, es decir, del punto de salida del túnel. Options : cualquier opción presente en la cabecera original en general no es grabada en la cabecera insertada. Aunque, nuevas opciones específicas del camino de túnel sean añadidas. La cabecera exterior contiene información sobre los extremos del túnel. La cabecera interior contiene información sobre los nodos origen y destino del paquete inicial y no puede ser modificada en ningún caso, salvo para decrementar el tiempo de vida (TTL) del paquete, aunque tan solo una vez dentro del túnel, a pesar de que pueda atravesar varios routers. Este campo como hemos visto tiene una copia en la cabecera exterior. Si el TTL vale 0, el datagrama será descartado, y un mensaje ICMP time exceded message se enviará al nodo origen. El encapsulador probablemente use cualquier mecanismo IP existente para la entrega de la información encapsulada al punto de salida del túnel. En particular, hace que las IP options sean permitidas, y hace que la fragmentación sea permitida a menos que el bit “Don’t Fragment” esté a uno en la cabecera IP original. Esta restricción es necesaria para que los nodos que utilizan el path MTU Discovery puedan obtener la información que buscan. Después de que el datagrama encapsulado ha sido enviado, puede ser que el encapsulador reciba un mensaje ICMP desde cualquier router del túnel. La acción del encapsulador dependerá del tipo de mensaje ICMP recibido. Cuando este mensaje contiene suficiente información, el encapsulador deberá crear un mensaje similar ICMP, que será enviado de vuelta al que envió el mensaje original. Encapsulado IP mínimoEl encapsulado suele conllevar el duplicado innecesario de numerosos campos de la cabecera IP interna. El encapsulado mínimo intenta minimizar al máximo la información de encapsulado para disminuir el tamaño del paquete resultante. Esto se realiza añadiendo una cabecera IP más pequeña que en el encapsulado IP-in-IP, y la cabecera IP original es modificada como indicamos a continuación: El campo Protocolo es cambiado por el número 55 que indica que es encapsulado mínimo. El campo de dirección de destino es cambiado por la dirección IP del punto de salida del túnel. El Total Length (longitud total) es incrementado por el tamaño de la cabecera añadida al datagrama. El Header Checksum (suma de comprobación de cabecera) se actualiza por su nuevo valor correspondiente. Al desencapsular un paquete con este encapsulado mínimo, se deberán restaurar los campos modificados en la cabecera original con los datos de la cabecera de encapsulado mínimo, actualizando los campos antes nombrados. La cabecera añadida antes de la cabecera IP original consta de los siguientes campos: Protocol : campo copiado de la cabecera IP original. Original Source Address Present : (es un bit). “0”: El campo dirección de la fuente original no está presente. “1”: El campo dirección de la fuente original está presente. Bit reservado: puesto a cero. Ignorado en la recepción. Header checksum . Dirección del destino original. Dirección de la fuente original. A pesar de todo, el encapsulado mínimo no está ampliamente difundido ya que presenta ciertas desventajas, concretamente, no funciona con paquetes ya fragmentados, ya que no hay sitio en la cabecera añadida para almacenarinformación de la fragmentación. Además, este encapsulado fuerza que el valor TTL sea decrementado en cada router dentro del túnel por lo que, puede suceder que los paquetes caduquen antes de llegar a su destino. Encapsulado GRELa encapsulación de enrutamiento genérico GRE ( Generic Routing Encapsulation ) es el más flexible de los tres presentado, ya que permite la encapsulación de cualquier tipo de paquete, incluidos los paquetes IP. El formato de un paquete GRE consta de una cabecera externa, seguida de la cabecera GRE, y de los datos IP. Contrariamente a los encapsulados IP-in-IP y mínimo, este encapsulado ha sido diseñado para prevenir encapsulamientos recursivos (encapsular de nuevo lo encapsulado). Concretamente, el campo recur en la cabecera GE es un contador que informa del número de encapsulados adicionales que son permitidos. En el protocolo IP versión 6 se está estudiando implementar un mecanismo similar a éste. SeguridadLas redes y nodos móviles son particularmente propensos a recibir ataques que comprometan la seguridad. En la mayoría de los casos los nodos móviles se conectarán a Internet mediante conexiones inalámbricas. Este tipo de conexiones es especialmente vulnerable a escuchas silenciosas, ataques activos de respuesta y otro tipo de ataques activos. Códigos de Autentificación de MensajesLos agentes domésticos y nodos móviles deben ser capaces de realizar autentificación. El algoritmo por defecto es el codificado MD5, con una clave de 128 bits. El modo operación por defecto es el uso ‘prefijo+sufijo’ en el que los datos son precedidos y seguidos por la clave de 128 bits. El agente externo también debe soportar la autentificación usando codificado MD5 y claves de 128 o más bits, con distribución explícita. También se permite el uso de otros algoritmos de autentificación y de distribución de claves. El registro de un nodo móvil en el agente doméstico es un momento crítico en que se debe aplicar autentificación de mensajes, ya que si un usuario se registra en nombre de otro, le robará su tráfico, pues el agente doméstico se encargará de reenviarle todo el tráfico a la Care-of-Address que haya utilizado en el proceso de registro. PrivacidadLos usuarios que tengan datos privados, que no desea que sean observados por nadie más, deben usar mecanismos de seguridad (encriptación) ajenos al protocolo de IP Móvil y que, por lo tanto, no están especificados. Protección de Duplicado en las Peticiones de RegistroEn la peticiones de registro hay un campo de identificación que permite al agente doméstico verificar que un mensaje de registro ha sido generado recientemente por el nodo móvil, y que no es una petición que haya sido escuchada con anterioridad por otro usuario que la está duplicando ahora. Existen dos algoritmos para este tipo de protección: marcas de tiempo ( timestamp ) y ‘ nonces ‘. Todos los agentes domésticos y nodos móviles deben implementar la protección contra duplicado basada en marcas de tiempo, la implementación del segundo algoritmo es opcional. El nodo móvil y el agente doméstico deben acordar qué método de protección van a usar. Independientemente del método usado, los 32 bits menos significativos de la identificación deben ser copiados sin cambiarse de la petición de registro a la contestación. El agente externo usa estos bits (y la dirección fija del nodo móvil) para emparejar las peticiones con las respuestas correspondientes. A su vez, el nodo móvil debe comprobar que los 32 bits menos significativos de la respuesta deben ser idénticos a los bits enviados en la petición de registro. Protección de Duplicados mediante uso de Marcas de TiempoEl principio en que se basa este método es que el nodo que genera el mensaje inserta la hora actual y el nodo receptor comprueba que la marca de tiempo es lo suficientemente cercana a su hora local como para ser cierta. Obviamente ambos nodos deben tener relojes correctamente sincronizados. Como cualquier otro mensaje los mensajes desincronización de tiempo deben estar protegidos contra manipulación por un mecanismo de autentificación determinado por ambos nodos. Protección de duplicados usando ‘ nonces ‘Una posible traducción de ‘nonces’ es la de algo que sólo se produce una vez u ocasional, en este caso se le llama nonce a un número aleatorio. El mecanismo es el siguiente: el nodo A incluye un número aleatorio o nonce nuevo en cada mensaje que envía al nodo B, y comprueba que el nodo B devuelve el mismo número en el siguiente mensaje al nodo A. Ambos mensajes usan un código de autentificación para protegerse de modificaciones producidas mediante un ataque. Al mismo tiempo B puede estar mandando su propios nonces en todos los mensajes que envía a A, de tal forma que los dos pueden comprobar que están recibiendo mensajes recientes. El agente doméstico se supone que tendrá recursos para computar número pseudo-aleatorios que puedan ser usados como nonces . Así pues, inserta un nuevo número aleatorio en los 32 bits más significativos del campo identificación de cada respuesta de registro. El agente doméstico copia los 32 bits menos significativos de la identificación de la petición de registro en los 32 bits menos significativos de la contestación. Cuando el nodo móvil recibe una contestación de registro del agente doméstico, se guarda los 32 bits más significativos de la identificación para ser usados como los 32 bits más significativos de la siguiente petición de registro. Normativa ReguladoraEl crecimiento de las comunicaciones móviles y en especial el de los celulares que se está produciendo en los últimos años no tiene precedentes. Para el caso de España, la cuota de penetración ya supera el 50% y el número de líneas móviles ya supera al de las fijas. Por otra parte, la evolución de la redes celulares actuales de segunda generación (2G) hasta la tercera generación (3G) para por ofrecer velocidades más elevadas y acceso por conmutación de paquetes, como es el caso de GPRS ( General Packet Radio Service ). Todo ello a fin de prepararse para un horizonte en que el tráfico de datos va a ser superior al de voz; apareciendo el acceso a redes IP en general y a Internet en concreto como principal artífice de esta situación. Si a todo ello añadimos el trabajo que se está realizando para soportar el transporte de voz sobre IP está bastante maduro, con soluciones comerciales disponibles, puede entenderse porque la opción de una red de acceso de comunicaciones celulares basada totalmente en IP va ganando peso. El siguiente paso lógico sería que esta red de manera natural asumiera todas las funciones necesarias para el soporte de la movilidad. En esta dirección se están moviendo los dos grupos que desarrollan la 3G: 3GPP ( Third Generation Partnership Project ) y 3GPP2 ( Third Generation Partnership Project 2 ). Cada uno siguiendo su camino, los grupos 3G estudian como convertir sus redes de acceso enredes totalmente IP basadas en routers y en como adoptar el trabajo que está realizando el IETF ( Internet Engineering Task Force ) para ofrecer movilidad mediante IP. El IETF ha estado trabajando en una solución universal para conseguir la movilidad en IP conocida como MobileIP. Se trata pues, de una solución general no optimizada para ningún tipo de red de acceso y es aquí donde surgen los problemas. Uno de los requisitos clave que demandan las redes de 3G y de manera general las redes celulares es el soporte de la micro movilidad; entendiendo por micro movilidad, la posibilidad de cambiar de una manera frecuente y rápida de punto de acceso dentro de una red. Como se verá más adelante, MobileIP tiene serias limitaciones para cumplir con este requisito. Actualmente existe una sinergia importante entre los grupos de 3G y el IETF: por un lado se intenta ver cómo jemorar el protocolo MobileIP para cumplir con los requisitos de 3G sin perder de vista su carácter de solución universal y por otro se ha creado un nuevo grupo especializado en micro movilidad. Dejando a un lado el IETF y los grupos de 3G también cabe mencionar al MWIF ( Mobile Wireless Internet Forum ). Creado en febrero del 2000, este foro está formado por empresas significativas en ámbitos como las comunicaciones móviles, las redes de datos, el software o la electrónica. Su propósito es el desarrollo de unas especificaciones clave que permitan la utilización de IP en cualquier tipo de redes sin hilos buscando aunar los esfuerzos de otros grupos como el IETF, 3GPP o 3GPP2. Grupos de Trabajo RelacionadosDejando a un lado las redes ad-hoc y la movilidad de usuarios entre ISPs (Internet Service Providers), temas que se tratan respectivamente en los grupos de trabajo MANET ( Mobile Ad-hoc Networks ) y ROAMOPS ( Roaming Operations ) el foco de trabajo en movilidad en IP ha sido y es el grupo MOBILEIP ( IP Routing for Wireless/Mobile Hosts ). El trabajo de MOBILEIP gira en torno al protocolo MobileIP (MIP). Este protocolo ofrece el mantenimiento de la dirección IP independientemente de la localización de la máquina que la posea, con un encaminamiento trasparente de los paquetes IP e intentando aumentar en lo mínimo los flujos de señalización. Todo esto manteniendo activas las conexiones TCP y las vinculaciones con los puertos UDP. Existen dos versiones de este protocolo, una estandarizada para IPv4 y otra que todavía tiene el carácter de draft par aIPv6 pero que se espera su propuesta como estándar a corto plazo. Dejando a un lado la estandarización de la versión para IPv6 de MobileIP y la revisión de la versión para IPv4, la actividad actual del grupo gira en torno a la solución de dos grandes problemas: la seguridad y la mejora de prestaciones para conseguir traspasos rápidos. En verano del 2000 se produjeron una serie de discusiones en torno a las diferentes, y muy numerosas, propuestas presentadas en forma de draft para conseguir un traspaso rápido. Dentro de las propuestas se podían diferenciar dos grupos. El primero las dirigidas a solucionar el problema de la micro movilidad, con un grado de compatibilidad y cooperación respecto a MobileIP más o menos grande. En la mayoría de los casos la red de referencia era de tipo celular. Dentro de este grupo destacaron los protocolos HAWAII ( Handoff-Aware Wireless Access Internet Infraestructure ) y CellularIP, del que se hablará más adelante. En el segundo grupo figuraban las soluciones basadas completamente en MobileIP. Se trataba de soluciones que, respetando el carácter de solución universal no ligada a ninguna tecnología de Mobile IP, pretendían mejorar su rendimiento para conseguir traspasos más rápidos. El resultado de estas discusiones fue una refundación del grupo de trabajo MOBILEIP que dejaba fuera las propuestas del primer grupo con objeto de mantener el carácter universal de MobileIP como solución para el soporte de la movilidad. Además se crearon dos equipos de trabajo para buscar una solución común para MIPv4 y otra para MIPv6. El trabajo de estos grupos ha cuajado de momento en sendos drafts . Todo esto no significa que MOBILEIP deje de lado toda la problemática de las redes celulares. Existe una estrecha relación con los grupos de 3G y como ejemplo de ella puede verse el draft en el que se plantean las extensiones necesarias para que Mobile IP pueda administrar la movilidad en redes cdma2000. Otra consecuencia de la refundación fue la reciente aparición de un nuevo grupo, SEAMOBY ( Context and Micro-mobility routing ). Los objetivos de SEAMOBY son el desarrollo de un protocolo que soporte la micro movilidad, con traspasos rápidos en la red de acceso y paging , y la provisión de mecanismos que permitan el intercambio de información de estado, como pueden ser el nivel de calidad de servicio asociado al usuario o un contexto de seguridad. Otros GruposAparte de los grupos comentados en el apartado anterior, cuya dedicación es exclusiva de los temas de movilidad, existen toda una serie de grupos cuyo trabajo tienen una relación significativa con esta problemática. Destacaremos dos: ROHC ( Robust Header Compression ) y AAA ( Authentication, Authorization and Accounting ). El objetivo de ROHC es conseguir un sistema de compresión que funcione correctamente sobre enlaces con tasas de error elevadas y retardos importantes. La motivación principal es el envío de información en tiempo real (voz o vídeo de baja calidad) sobre enlaces celulares. La combinación de protocolos IP/UDP/RTP/TCP utilizada para el transporte de tráfico real-time conlleva un alto overhead . Para trabajar eficientemente sobre enlaces de baja velocidad, como son los de las redes celulares, es necesario utilizar métodos de compresión. Una posible solución pasaría por la utilización de los algoritmos tradicionales de compresión de cabeceras pero la elevada tasa de error así como los elevados retardos que se puedan dar en una red celular hacen que su comportamiento no sea el idóneo. De ahí la necesidad de un nuevo tipo de compresión. En los draft se especifican los requerimientos que debería cumplir esta nueva codificación y se da una posible especificación de ella respectivamente. El AAA es el grupo encargado de desarrollar los requerimientos para la autenticación, autorización y contabilidad. Estas funciones son de vital importancia para control del acceso a cualquier sistema. El AAA trata el caso de un sistema con terminales como un caso particular con unas necesidades propias que requieren de unas extensiones determinadas. Esto se ha traducido en un listado de requerimientos formulado por el grupo MOBILEIP y en draft del AAA sobre las extensiones a realizar para cumplir con estos requerimientos. Integración de los Protocolos del IETF en 3GLas aproximaciones realizadas por los dos grupos de 3G para integrar los protocolos desarrollados por el IETF están siendo diametralmente opuestas. Por un lado el 3GPP2 cuenta ya desde hace más de un año con un estándar de lo que ellos denominan WirelessIP . En este documento se describen los requerimientos para soportar redesd e paquetes inalámbricas en las redes de 3G basadas en cdma2000; diferenciando dos alternativas: SimpleIP , basado en el protocolo PPP ( Point to Point Protocol ); y MobileIP basado en el protocolo del mismo nombre. El documento también propone la utilización de servidores RADIUS (R emote Authentication Dial In User Service ) para labores de AAA y la utilización de Diffserv para ofrecer calidad de servicio. Se trata pues de utilizar las soluciones ofrecidas por el IETF, aunque no estén optimizadas para sistemas celulares. Paralelamente el 3GPP2 tiene abierto otro proyecto en fase de definición denominado A//IP , que consiste en el desarrollo de una red que se basa en IP como principal mecanismo para el transporte y la conmutación. El camino por el que ha optado el grupo 3GPP es mucho más ambicioso. En este report técnico el 3GPP propone una arquitectura basada totalmente en IP, A//IP , para el transporte de todos los datos de usuario y señalización. El documento tiene una doble vertiente: la identificación de los problemas clave a resolver y la proposición de un plan de trabajo para ofrecer una A//IP release 200 del estándar UMTS ( Universal Mobile Telecommunications System ). Ventajas e Inconvenientes de Móvil IPVentajas No tiene limitaciones geográficas: un usuario puede usar una computadora palmtop o laptop en cualquier parte sin perder su conexión a su red. No requiere una conexión física: este nuevo protocolo encuentra los routers IP y se conecta automáticamente. No requiere modificación a otros router o clientes ya que deja otros protocolos intactos. Soporta autentificación, la cual se realiza para asegurarse que los derechos estén protegidos. El acceso de la red se asegura en todo momento desde todas las ubicaciones. Inconvenientes Problemas de autentificación con el agente externo, cuando éste pertenece a otra organización. Falta de protocolos de manejo de claves que sean estándares Internet. El encaminamiento en triángulo es ineficiente. La creación de túneles es un coste añadido e incrementa el overhead por paquete. Cuello de botella en el Agente Local. Implicaciones de seguridad en cortafuegos. PLC (Power Line Communications)IntroducciónDesde los años 40 se consideró la posibilidad de aprovechar la red eléctrica como red de comunicaciones. Finalmente, en 1997 se presentó un sistema que permitía el acceso a Internet desde la red eléctrica, que pasó a denominarse PLC (Power Line Communications). Los primeros problemas para la utilización de esta tecnología fueron debidos al ruido inherente a la red eléctrica de baja tensión, capaz de alterar la información transmitida, así como problemas legales de regulación del espectro de frecuencias en las que trabaja y de las emisiones electromagnéticas del sistema, dadas las potenciales interferencias sobre otros aparatos electrónicos. En los últimos años la tecnología ha evolucionado muy rápidamente, permitiendo velocidades de acceso competitivas con tecnologías alternativas, y en la actualidad ya existen ofertas comerciales de servicios de telecomunicaciones basados en PLC en algunos países, así como multitud de experiencias piloto en otros muchos, entre ellos España. El sector eléctrico español se encuentra en un proceso de rápida evolución, tanto en las estructuras de capital de las empresas eléctricas como en el marco regulatorio, lo que ha introducido planteamientos totalmente innovadores en su funcionamiento, con el propósito de fomentar la competencia entre las empresas. Esta liberalización del sector eléctrico empuja a las empresas del sector a buscar nuevas oportunidades de negocio para compensar las pérdidas de cuotas del mercado. El Sector de las Telecomunicaciones se encuentra también en un proceso de cambio acelerado, pasando en pocos años de una situación de monopolio a otra de amplia liberalización. La creciente orientación de las políticas económicas hacia la satisfacción de las demandas y necesidades de los usuarios, justifica la introducción de la competencia en un sector tan complejo y variado como el de las telecomunicaciones en constante y rápida evolución tecnológica. No cabe duda de que la liberalización de las telecomunicaciones es ya uno de los motores del crecimiento económico y de la nueva economía de servicios basada en la sociedad de la información. Tradicionalmente las Empresas Eléctricas han instalado, operado y mantenido redes privadas de telecomunicación fundamentalmente para el control u operación de la propia red eléctrica. El resto de los servicios demandados por las necesidades administrativas y societarias ha estado restringido por la legislación a ser prestado por el operador nacional que actuaba en condición de monopolio natural. Este hecho influyó en el desarrollo de la infraestructura y en el uso a veces de equipos especializados, previstos para estas condiciones de operación específicas. Los medios de transmisión empleados son muy diversos: equipos de ondas portadoras usando los propios cables de energía, cables pilotos, cables coaxiales, enlaces vía radio, fibras ópticas, satélites de comunicaciones, enlaces por infrarrojos, etc, y su utilización ha seguido los dictados de las propias necesidades de cada compañía y la oferta tecnológica existente en cada momento. En cualquier caso, la ruptura de las barreras legales que supone la liberalización de las telecomunicaciones implica la posibilidad de poder usar la capacidad excedente de las redes privadas, proporcionada por la digitalización y los avances tecnológicos, para proporcionar servicios a terceros. Además la Disposición Adicional 14 a la Ley del Sector Eléctrico permite que la Empresa Eléctrica, que en principio tiene objeto social exclusivo, ponga en valor la infraestructura de que es titular con fines de telecomunicaciones. Y por supuesto nada impide que cedan el uso de dichas infraestructuras a un tercero para que las explote dado que además en la legislación de telecomunicaciones de España en la actualidad, existe la obligación de separación de cuentas por los operadores de telecomunicaciones que desarrollen actividades en otros sectores económicos. Las Directivas armonizadoras del Consejo Europeo, han decidido promover el desarrollo de la Sociedad de la Información para todos, facilitando una mayor competencia en el segmento de acceso al hogar. El servicio Internet va a ser declarado universal y se deberán facilitar todas las infraestructuras que favorezcan su desarrollo e implantación. Esta consecuencia del rápido y universal desarrollo de la red Internet hacen que el acceso de banda ancha sea el negocio de más rápido crecimiento en las telecomunicaciones en los próximos años. La tecnología Power Line Communications, “PLC”, posibilita la transmisión de voz y datos a través de los cables eléctricos, convirtiendo cualquier enchufe de la casa en conexión potencial a todos los servicios de telecomunicaciones. El cliente sólo necesitará conectar un pequeño módem para acceder a Internet, telefonía y datos al mismo tiempo y a alta velocidad (banda ancha). La naturaleza y ubicuidad de la red de baja tensión permitirá también lograr una comunicación permanente y a bajo coste entre todos los aparatos electrónicos de la casa, dando lugar a nuevos y eficientes servicios de seguridad, control del consumo a distancia, domótica y teleasistencia, entre otros. Actualmente muchas compañías desarrollan y comercializan sus propias soluciones para la creación de redes domésticas sin necesidad de tender cableados adicionales, y sin el todavía elevado gasto que supone las redes inalámbricas. Crear una red local en un domicilio es bastante sencillo. Siguiendo el enfoque de estas soluciones, para conectar dos ordenadores sólo se necesitan dos enchufes. El siguiente paso es más difícil. Cientos de equipos de investigadores llevan varios años intentando estabilizar las tecnologías que permitan conectar estas pequeñas redes locales con la red de redes, o dicho de otra forma, se trata de conectarse a Internet a través del enchufe eléctrico de casa. Y también, de llamar por teléfono desde ese mismo enchufe. La solución, de tener éxito finalmente en las experiencias precomerciales que ya se están llevando a cabo, podría también penetrar – nada más fácil, en principio- los muros empresariales. Características TécnicasExiste interés generalizado en el mercado por los accesos a Internet de banda ancha, ya que este tipo de acceso es el que va a permitir que las diferentes compañías dejen de ser meros ISP para convertirse en auténticos proveedores de servicios multimedia. Power Line Digital (PLC) podrá alcanzar velocidades entre 1 y 1,5 megas de ancho de banda en la casa de cada usuario particular (en principio). Esto hace posible que se ofrezcan servicios de Internet bajo un modelo de tarifa plana, así como otro tipo de transmisión de datos y hasta telefonía IP. La técnica es bastantes sencilla y tiene algunos puntos de similitud con los sistemas xDSL. Basta acondicionar parte de las actuales infraestructuras eléctricas para que puedan transmitir señales regulares de baja frecuencia y otras por encima de la banda de 1 MHz, sin que se vea afectado el rendimiento eléctrico. Arquitectura de PLC La red eléctrica transporta electricidad a una frecuencia de 50 Hz. En PLC se añaden frecuencias en la banda que va desde 1,6MHz hasta 30MHz para el transporte de los datos. Unos filtros instalados en el transformador de baja tensión separan las frecuencias altas de datos, de la frecuencia de 50Hz de la electricidad. Por otro lado, en el enchufe del abonado, cuando se conecta un dispositivo de transmisión de datos (un PC, teléfono, etc) a la red, se hace a través de un módem adaptador. Organización del espectro de PLC Power Line emplea una red conocida como High Frequency conditioned Power Network (HFCPN) para transmitir simultáneamente energía e información. Una serie de unidades acondicionadoras son las que se encargan del filtrado y separación de ambas señales. Así pues estas unidades acondicionadoras separarían la electricidad, que alimenta a los electrodomésticos, de las señales de alta frecuencia, que van a un módulo o unidad de servicio, donde se reconvierten en canales de vídeo, datos, voz, etc. En las subestaciones eléctricas locales hay servidores de estación base que se conectan a Internet generalmente a través de fibra óptica. Esto quiere decir que nos e utiliza toda la red eléctrica para la transmisión de datos. La red eléctrica consta de tres partes bien diferenciadas: los tramos de baja tensión, los de media y los de alta tensión. Los de baja tensión – equivalentes a la “última milla” o bucle de abonado en las redes telefónicas- conecta los hogares con las subestaciones de distribución local. Es precisamente este tramo el único que se utiliza en PLC. Tramos en la implantación de PLC Las estaciones base de PLC tienen una estructura típica de rack. Una localización puede llegar a contener unas doce unidades emisoras del tipo estación base, cada una capaz de comunicar un canal. Los datos llegan a estas estaciones que las incorporan a la señal eléctrica. Una estación estándar sirve a unos cincuenta usuarios, ofreciéndoles un espectro cercano a los 20 MHz en el caso de clientes próximos, o entre 6 y 10 MHz para clientes lejanos. El servidor opera con un sistema basado en IP para crear redes LAN en cada área de servicio. Las unidades acondicionadoras situadas en los hogares de los abonados, que también pueden recibir el nombre de módem eléctricos, tienen en su interior dos filtros. El primero de ellos, el de baja banda, libera la corriente eléctrica de 50 Hz para su distribución a todos los enchufes de la casa. Este filtro además sirve para limpiar los ruidos generados en la red por los electrodomésticos conectados en casa del usuario. Si se dejaran pasar esos ruidos, al unirse a los procedentes de otros usuarios de la red, acabarían por introducir distorsiones muy significativas. En segundo lugar, el filtro de banda es el que libera los datos y facilita el tráfico bidireccional entre el cliente y la red. En la actualidad no existen estándares tecnológicos para el PLC de acceso. Éste es uno de los principales problemas de esta tecnología, al no permitir la interoperabilidad entre los equipos suministrados por los distintos fabricantes. Tampoco existe una regulación en cuanto a la utilización de frecuencias, aunque CENELEC y ETSI tienen previsto publicar este año una recomendación conjunta acerca del uso de frecuencias por los sistemas de última milla y los sistemas domésticos que al menos garantice la coexistencia de ambos tipos de sistema, dado que la red eléctrica es continua y las señales de ambos se mezclan y extienden por toda la red que depende de un mismo transformador. La situación actual de la tecnología PLC queda reflejada en el siguiente cuadro: En la práctica se están consiguiendo velocidades de hasta 45 Mbps compartidos en algunas de las pruebas realizadas, aunque las velocidades realmente disponibles a nivel comercial todavía sean muy inferiores. La mayor limitación actualmente para conseguir velocidades de transmisión mayores sigue siendo el ruido inherente a la red eléctrica de baja tensión. En el caso de la red de media tensión, se están obteniendo velocidades de hasta 135 Mbps para la conexión de centros transformadores. Características del Chip DS2 Flujo de datos de 45 Mbps; 27 Mbps en Bajada y 18 en Subida. Full dúplex, punto a multipunto, paquete orientado a enlace de comunicaciones. Cumple los estándares del ETSI y CENELEC para acceso y LAN. Eficiencia de la modulación hasta 7,25 bps/Hz. 1280 portadoras OFDM adaptativas para conseguir el máximo flujo sobre cualquier red. Monitorización de la SNR (relación señal ruido) del canal continuamente. Ratio adaptativo por portadora dependiendo de las condiciones SNR del canal. Empleo de control de errores mediante códigos bloque Reed Solomon. Opera por debajo de -1 dB de SNR. Ratios de error optimizados para TCP/IP, y programable para cualquier aplicación. Mensajes Broadcast disponibles. Soporte de encriptación y SNMP. Control de QoS. Hasta 254 usuario, (512 en versiones posteriores). Comparativa con otras TecnologíasLa comparativa da una idea de las posibles diferencias entre las distintas tecnologías. Se espera que el precio de la tecnología PLC sea bastante inferior al de los actuales ADSL y Cable en el mismo rango de velocidades, que junto con su mayor velocidad de conexión la convierten en una llamativa tecnología. En el cuadro se reflejan las principales características: Las categorías anteriormente mencionadas sólo son indicativas, ya que las fronteras entre las tecnologías están cambiando. Es también significativo el hecho de que las tecnologías de banda ancha actualmente en el mercado pueden dar resultados comparables o mejores que la PLC, pero en general no tienen una infraestructura totalmente implantada con la que poder alcanzar un mercado de masas en un corto período de tiempo. Modos de Operación y ServiciosDentro de la tecnología PLC se distinguen la red externa de transmisión y la red interna de comunicación dentro del hogar o del negocio del usuario final. La red externa o tecnología de acceso a la “última milla” permite el transporte de señales hasta el usuario final vía el centro de transformación local y la red eléctrica. Los servicios típicos de telecomunicaciones que podrían proporcionarse son: Telefonía: Incluye la prestación de servicios de voz y fax sobre el protocolo de Internet IP. Acceso a Internet: Dependiendo de las diversas tecnologías empleadas es posible alcanzar velocidades aseguradas de unos 20 Mbps. Vídeo bajo demanda: Aunque es posible, esta opción por las necesidades tan elevadas de ancho de banda que requiere, parece algo más lejana de implantarse. La red interna de comunicaciones o tecnología de uso doméstico integra la conexión y el control de dispositivos mediante un único interfaz dentro del edificio. Esta red interna es utilizada para la transmisión de la señal a alta velocidad proveyendo soluciones de comunicación interna. Como ejemplos de los posibles servicios que se pueden obtener están: La implantación de una red e área local de ámbito doméstico. Control de seguridad remoto a través de dispositivos dotados de cierta inteligencia. Gestión y control remoto de electrodomésticos. SeguridadCualquier línea conductora es, por definición, una antena lo que nos lleva a pensar en seguridad. La seguridad es uno de los aspectos menos investigados de PLC. Los problemas técnicos se traducen en dinero. Para filtrar y limpiar las líneas hacen falta equipos costosos, y aún así siempre hay un equilibrio entre la velocidad y el aislamiento: cuanto más se filtre la línea, más difícil es transmitir a altas velocidades. Las soluciones a estos problemas de confidencialidad, ya que todavía no hay estándares al respecto, pasan por soluciones propietarias de cifrado implantadas por las empresas que proporcionan los servicios, como el cifrado por hardware propuesto por la empresa DS2 en su chip. Por último, decir que esta tecnología es totalmente compatible con las tecnologías de cifrado IPSEC y también se ha tratado el problema de compatibilidad con VLANs basadas en el protocolo 802.1q. Este protocolo consiste en añadir un encabezado a la trama Ethernet para identificar la VLAN que le corresponde. Por supuesto, dicho protocolo es soportado por los switches actuales, siendo dicha información adicional manejada e intercambiada entre ellos exclusivamente y no por el usuario final, por lo que no se necesitan tarjetas Ethernet especiales. Normativa ReguladoraDisponer de un marco regulatorio estable, es esencial para el desarrollo y la aplicación práctica de la tecnología PLC. Además, como toda nueva tecnología necesita estándares (bandas de frecuencia, potencia, límites EMC, etc), para poder desarrollarse comercialmente de una forma competitiva al permitir la interoperabilidad entre distintos fabricantes. En Europa no existe una regulación unificada al no haberse aprobado ninguna Norma sobre el PLC de banda ancha. El principal problema está surgiendo con la regulación del espectro que evite problemas de interferencias. Se trata de fijar los límites a imponer, sobre la posible acumulación de las emisiones EMC (sobre todo en condiciones atmosféricas adversas) según PLC vaya implantándose. Organismos encargados de la estandarización de PLC En los aspectos regulatorios/normativos se pueden anotar las siguientes consideraciones: El Parlamento Europeo ha aprobado el texto de compromiso propuesto por el Consejo de la UE sobre el nuevo marco regulador de las comunicaciones electrónicas (Directiva Marco, Directiva de Acceso a Interconexión, Directiva de Autorizaciones, Directiva de Servicio Universal y Decisión sobre el Espectro). Estos textos, una vez aprobados formalmente por el Consejo, deberán publicarse en el DOCE. El texto de compromiso sobre la Directiva relativa a un “marco regulador común de las redes y servicios de comunicaciones electrónicas”, incluía en su artículo 2 relativo a la definición de “red de comunicación electrónica”, la tecnología “Power Line communications” (PLC). El proceso de normalización europeo es complejo y lento. Existe un borrador de mandato de la Comisión Europea a CEN (European Committee for Standardization), CENELEC (European Committee form Electrotechnical Standardization) y ETSI (European Telecommunication Stardardization Institute) para la elaboración de normas armonizadas que cubran los aspectos de EMC de las redes de telecomunicaciones que usan cables coaxiales, pares de cobre, líneas eléctricas o cualquier otro tipo de medio físico. La ETSI TS (101867 recomienda las condiciones de separación de las bandas de acceso y domésticas: como bandas a utilizar por el PLC de las aplicaciones de acceso (PLC Access) establece el espectro de frecuencias comprendidas entre 1,6 – 10 MHz, reservando al PLC de las aplicaciones de uso doméstico (PLC Inhouse) la banda de 10 – 30 MHz. El SC205A de CENELEC está trabajando en la ES 59013, que define el espectro de frecuencias a utilizar por las aplicaciones de acceso y las domésticas, fijando la frecuencia de separación en 13,5 MHz en lugar de los 10 MHz de la ETSI. Esto ha motivado la creación de un grupo conjunto de CENELEC/ETSI/CISPR para alcanzar un consenso final que resuelva los problemas de coexistencia entre los equipos PLC de acceso y domésticos. Las particularidades del PLC como red de telecomunicaciones en cuanto aprovecha la infraestructura eléctrica existente, implica la aplicación de la doctrina relativa a la correlación entre derechos y obligaciones de Servicio Público. Además, por dicha particularidad, la CMT podrá imponer una cláusula específica en la licencia a otorgar. Ventajas e Inconvenientes de PLCVentajasPodemos destacar los siguientes puntos en relación a los beneficios que nos puede aportar esta tecnología: Como la PLC se ha posicionado como un servicio de tipo IP utilizará routers de paquetes en vez de los de conmutación de circuitos típicos, de os suministradores de telecomunicaciones tradicionales, manteniendo así los costes de los equipos de IT bajos. Como la electricidad se suministra a través de una conexión permanente, los servicios de transmisión de datos ofrecidos por la infraestructura eléctrica también están conectados permanentemente (no es necesario marcar el número de conexión) convirtiéndose en el ideal para el número creciente de servicios en línea. Las compañías eléctricas podrían pues comercializar un servicio básico de conexión a Internet con una suscripción mensual de tarifa plana, al igual que algunos operadores de cable. Pagar una tarifa estándar, sin tener en cuenta el nivel de utilización, será un gran atractivo para los clientes. Al dar a los clientes de las compañías eléctricas acceso a Internet mediante la red que ya les suministra la electricidad, esta tecnología se pone virtualmente al alcance de cualquiera, con un potencial mercado de masas sin necesidad de las inversiones necesarias para enterrar el cableado hasta los hogares. Ya existen varias tecnologías que transforman los cables eléctricos existentes en un cableado LAN (Local Area Network). Lo que hace diferente a la PLC es la alta velocidad de transmisión de datos que se puede conseguir y el hecho de que esté diseñada para trabajar en el exterior del hogar o del edificio. Por tanto, podrían instalarse sistemas sofisticados de automatización doméstica que permitiesen el acceso y el control remotos de aparatos electrodomésticos, alarmas antirrobo, etc. PLC podría también facilitar a las compañías eléctricas la oportunidad de ofrecer servicios de valor añadido orientados sectorialmente, tales como la gestión de la energía (enlazando contadores “inteligentes”, controladores programables y dispositivos “inteligentes” de control de la demanda/suministro, de modo que la empresa eléctrica suministadora del servicio pudiera introducir tarifas innovadoras que premiasen el uso sensato de la energía, la información remota (la conexión permanente ofrecida por PLC se podría optimizar para proporcionar información en tiempo real o indicadores de estado en apoyo de algunas aplicaciones de seguridad para sistemas de alarma/vigilancia) y la automatización de la distribución (la lectura remota automática de los contadores mejoraría el control y ayudaría al proveedor en la gestión de los picos de demanda eléctrica). Algunas empresas eléctricas han empezado a usar recientemente estas técnicas. InconvenientesLa red eléctrica no ha sido diseñada para transmitir datos, sólo para transmitir energía, y esto hace que presente varias limitaciones y problemas de seguridad. Los obstáculos son fundamentalmente técnicos: En primer lugar, hay que elegir un tipo de modulación que sea el más adecuado para la red eléctrica. En PLC se emplea la modulación OFDM (Orthogonal Frequency Divison Multiplexing). Otro de los problemas reside en el número máximo de hogares por transformador. Como las señales de datos de Power Line no pueden sobrevivir a su paso por un transformador, sólo se utilizan en la última milla. El modelo europeo de red eléctrica suele colocar un transformador cada 150 hogares aproximadamente. Si se juntan estos dos factores, se comprueba que es necesario que todos los transformadores vengan dotados de servidores de estación base Power Line. Y cuanto menor es el número de usuarios por cada transformador, más se elevan las inversiones necesarias. En tercer lugar, están las interferencias. Al poco tiempo de realizarse las primeras pruebas se comprobó que algunas de las frecuencias no se podían usar porque generaban interferencias en otros servicios preexistentes. Por ejemplo, el uso de determinadas frecuencias en las cercanías de un aeropuerto podía interferir, y de hecho interfería, con las frecuencias de la torre de control y las de los radares de aproximación. También se puede llegar a interferir con las transmisiones convencionales de radio en FM o incluso En DAB, o con las de los servicios de emergencia, como bomberos o policía. En la actualidad muchas compañías eléctricas están realizando intensivas pruebas de campo. La suciedad electromagnética de los cables es otro de los problemas significativos. Si piensa que el famoso espacio radioeléctrico está lleno de ondas de radio en constante peligro de interferencia, tendría que ve cómo está el tendido eléctrico. Es un problema de aislamiento. Compare un cable eléctrico con un cable de antena de televisión. El primero sólo está recubierto de plástico. El cable de antena tiene varias capas de plástico y una malla metálica intermedia que lo aísla de posibles interferencias. Cualquier línea conductora es, por definición, una antena. Eso quiere decir que la instalación eléctrica de una casa actúa como tal, y es muy sensible a las interferencias que se produzcan en las frecuencias de transmisión de datos, alrededor de los 30 MHz. La red eléctrica no está protegida contra las ondas de radio, pero tampoco contra el ruido electromagnético que puede introducir una afeitadora, la televisión o el propio PC. Todos estos aparatos se protegen a sí mismos de lo que pueda venir de la línea eléctrica (somo una subida de tensión) con filtros y fusibles, pero nadie se preocupa de lo que vierten en ella. Bibliografía Scribd (Ibiza Ales)"},{"title":"Bloque 3","date":"2019-01-16T15:11:23.000Z","path":"bloque-3/index.html","text":"B3 Desarrollo de sistemas Concepto del ciclo de vida de los sistemas y fases. Modelos de ciclo de vida.IntroducciónEn este tema se va a dar una visión de lo que se llama ciclo de vida del software, así como distintos modelos de representación del mismo. ¿Para qué un ciclo de vida? En un departamento de sistemas de información de una empresa es necesario establecer lo que llamamos un marca de referencia común que pueda ser empleado por todas las personas que participan en el desarrollo de un proyecto informático: directivos, consultores externos e internos, jefes de proyecto, analistas, programadores, usuarios, etc. En este marco de referencia deben estar claramente definidos los procesos, actividades y tareas a desarrollar. Veamos primeramente dos definiciones publicadas por dos organismos internacionales: Norma IEEE 1074: Se entiende por ciclo de vida del software una aproximación lógica a la adquisición, el suministro, el desarrollo, la explotación y el mantenimiento software. Norma ISO 12207-1: Se entiende por modelo de ciclo de vida un marco de referencia que contiene los procesos, las actividades y las tareas involucradas en el desarrollo, la explotación y el mantenimiento de un producto de software, abarcando la vida del sistema desde la definición de requisitos hasta la finalización de su uso. La Evolución del SoftwareEl concepto de ciclo de vida se utilizó para modelar el proceso de ingeniería del software que, a su vez, apareció como solución a la llamada “crisis del software”. Veamos un poco de historia. El desarrollo del software ha ido en paralelo con la evolución de los sistemas informáticos, los cuales han ido mejorando notablemente debido al aumento de la potencia del hardware, a la reducción de su tamaño y la disminución de su coste económico. Siguiendo a Presmann podemos distinguir las siguientes etapas en la evolución del software: 1ª Etapa: Primeros años de la informática (1950-1965). El hardware sufrió numerosos cambios. Se desarrollaba software sin planificación y sin metodologías sistemáticas. En casi todos los sistemas se utilizaba programación en “batch”. La mayoría del software era desarrollado y utilizado por la misma persona. 2ª Etapa: (1965-1975). Aparición de la multiprogramación y de los sistemas multiusuarios. Como consecuencia de la interactividad de los sistemas aparecen nuevos tipos de aplicaciones. Surgen, asimismo, los sistemas de tiempo real. También los primeros gestores de BD. 3ª Etapa: (1975-1985). Aparecen los sistemas distribuidos, redes de área local LAN y de área global WAN. Hay una fuerte presión sobre los desarrolladores de software. Los ordenadores personales impulsan el crecimiento de muchas compañías de software. 4ª Etapa: (1985- ). Tecnologías de cuarta generación. Tecnologías orientadas a objetos. Características especiales del SoftwareExisten características propias del software que lo diferencian de otros productos: El software no se fabrica en un sentido clásico, sino que se desarrolla: Si bien existen similitudes con la fabricación del hardware, se trata de actividades fundamentalmente diferentes. Tanto en una como en otra la buena calidad se adquiere mediante un buen diseño pero la fabricación del hardware es muy diferente de la del software y puede introducir problemas de calidad que no existen o son fácilmente corregibles en el software. Ambas actividades requieren la construcción de un produce pero con métodos muy diferentes. Los costes del desarrollo del software están en la ingeniería por lo que no se pueden gestionar como si fueran clásicos proyectos de fabricación. El software no se “estropea”: El hardware se deteriora con el paso del tiempo y con el uso. Los errores no detectados del software provocarán fallos al principio de su vida. Sin embargo, una vez corregidos deberían desaparecer los fallos y no aparecer nuevos. No obstante la realidad suele ser diferente. El software sufre a lo largo de su vida modificaciones y al introducir estos cambios suelen producirse nuevos fallos que, a su vez, tienen que ser corregidos y así sucesivamente. La mayor parte del software se construye a medida, en lugar de ensamblar componentes como hace la industria: En la fabricación del hardware, la reutilización de componentes es una parte natural del proceso de ingeniería. En el mundo del software es algo que sólo ha comenzado a lograrse recientemente. La “Crisis del Software”Desde que se empezó a desarrollar software a gran escala empezaron a ser comunes una serie de problemas: La planificación resultaba ser muy imprecisa. Los plazos de entrega eran superados en la mayoría de los casos. El coste final de los proyectos era frecuentemente mucho mayor que el previsto. La productividad era muy baja. La calidad el producto entregado era, asimismo, muy baja. El cliente solía quedar insatisfecho del producto. El software era difícil de mantener. Hay que añadir que el conjunto de problemas encontrados en el desarrollo del software no se limitan al software que “no funciona correctamente”. La llamada crisis abarca los problemas asociados a cómo desarrollar software, como mantener el volumen cada vez mayor de software existente y cómo poder mantenernos al corriente de la demanda creciente de software. Ingeniería del SoftwareLa ingeniería del Software surge de la necesidad de sistematizar el desarrollo del software afectado por la llamada “crisis del software”, aplicando principios de ingeniería para poder obtener software de calidad. ¿Qué es software de calidad? Si asumimos que el software cumple con la funcionalidad requerida, para que sea de calidad deberá tener las siguientes características: El software debe ser mantenible. Deberá estar escrito y documentado de forma tal que las modificaciones se puedan realizar con el menor coste posible. Esto es fundamental ya que la mayor parte del coste asociado del software se produce después de su puesta en funcionamiento. El software debe ser fiable. Es decir, debe comportarse como esperan los usuarios y no debe fallar más de lo permitido por la especificación. El software debe ser eficiente. Debe ofrecer una interfaz de usuario apropiada. Concepto del Ciclo de Vida y FasesPodemos definir ciclo de vida de un sistema de información como el conjunto de etapas por las que atraviesa el sistema desde su concepción, hasta su retirada de servicio pasando por su desarrollo y explotación. A veces también se habla de “ciclo de desarrollo”, que es un subconjunto del anterior que empieza en el análisis y finaliza con la entrega del sistema al usuario. Existen diferentes modelos de ciclo de vida o sea distintas pautas a seguir en el desarrollo de los sistemas de información. Más adelante estudiaremos dos, el llamado modelo clásico o en cascada y el modelo en espiral. Tres son los objetivos básicos que cualquier modelo de ciclo de vida debe cubrir: Definir las actividades a realizar y su orden. Asegurar la consistencia con el resto de los sistemas de información de la organización. Proporcionar puntos de control para la gestión del proyecto (presupuesto y calendario). Procesos del Ciclo de Vida SoftwareSegún la Norma ISO 12207-1, las actividades a realizar durante el ciclo de vida del software se agrupan en cinco procesos principales, ocho procesos de soporte y cuatro procesos de la organización, así como un proceso especial que permite adaptar el ciclo de vida a cada proyecto en concreto. A destacar que la norma no recomienda ningún modelo concreto de ciclo de vida, ni de gestión del software, ni detalla cómo realizar ninguna de las actividades. Procesos PrincipalesSon aquellos que resultan útiles a las personas que inician o realizan el desarrollo, la explotación o el mantenimiento del software a lo largo del ciclo de vida. Estas personas son los compradores, los proveedores, el personal de desarrollo, los usuarios y el personal encargado del mantenimiento del software. Proceso de adquisición : Contiene las actividades y tareas que el comprador, el cliente o el usuario realizan para adquirir un sistema o un producto software. Aquí están incluidos la preparación y publicación de una solicitud de ofertas, la selección del proveedor del software y la correspondiente gestión de los procesos desde la adquisición hasta la aceptación del producto. Proceso de suministro : Contiene las actividades y tareas que el suministrador o proveedor realiza. Comienzan con la preparación de una propuesta para responder a una petición de oferta de un comprador o con la firma de un contrato con el comprador para proporcionarle un producto software. Trata, asimismo de la identificación de los procedimientos y de los recursos necesarios para gestionar y garantizar el éxito del proyecto, incluyendo el desarrollo de los planes del proyecto y la ejecución de dichos planes hasta la entrega del producto software al comprador. Proceso de desarrollo : Contiene las actividades de análisis de requisitos, diseño, codificación, integración, pruebas e instalación y aceptación. Vamos a resumir someramente estas actividades: Análisis de requisitos del sistema : Aquí son especificados todos los requisitos del Sistema de Información, funciones y capacidades que debe cumplir, requisitos de seguridad, interfaces, de mantenimiento, etc. Diseño de la arquitectura del sistema : Se identifican los principales componentes hardware y software. Análisis de los requisitos de software : Se establecen dichos requisitos, incluyendo el nivel de calidad que debe cumplir el sistema. Diseño de la arquitectura del software : El diseñador debe transformar el análisis anterior en una arquitectura en la que se puedan identificar sus componentes principales. Diseño detallado del software : Aquí se realiza un diseño detallado de cada componente software, de las BD y manuales de usuario. Codificación y pruebas unitarias : Se desarrollan y se documentan los componentes del punto anterior. Finalmente se realizan las pruebas unitarias de cada uno de ellos para asegurarse de que cumplen los requisitos exigidos. Pruebas de integración : Se integran los componentes del software realizando las correspondientes pruebas. Prueba del software : Las pruebas se planifican y diseñan de forma sistemática para poder detectar el máximo número y variedad de defectos con el mínimo consumo de tiempo y esfuerzo. Integración del sistema : Aquí se realizan las pruebas conjuntas de los elementos hardware y software. Implantación del software desarrollado en el entorno de explotación final . Cuando se sustituya a un software ya existente, puede ser recomendable un período de tiempo en el que convivan los dos sistemas. Proceso de aceptación del software . Proceso de explotación : Comprende la propia explotación del software y el soporte operativo a los usuarios del sistema. Proceso de mantenimiento : Aparece cuando, tarde o temprano, el software requiere modificaciones, bien por errores, necesidades de mejora, etc. Procesos de SoporteSirven de apoyo al resto de procesos y pueden aplicarse en cualquier punto del ciclo de vida. Proceso de documentación : Comprende todas las actividades que permiten desarrollar, distribuir y mantener la documentación necesaria para todas las personas involucradas: consultores, jefes de proyecto, analistas, programadores, usuarios, etc. Proceso de gestión de la configuración : Controla las modificaciones y las versiones de los elementos de configuración del software del sistema. Proceso de aseguramiento de la calidad : Comprueba que los procesos y los productos software del ciclo de vida cumplen con los requisitos especificados y se ajustan a los plantes establecidos. Proceso de verificación : El objetivo es demostrar la consistencia, completitud y corrección del software entre las fases del ciclo de desarrollo de un proyecto (por ejemplo, si el código es coherente con el diseño). Este proceso puede ser responsabilidad de una empresa de servicios y, en este caso se conoce como proceso de verificación independiente. Proceso de validación : El objetivo es determinar la corrección del producto final respecto a las necesidades del usuario. Al igual que el anterior, este proceso puede ser ejecutado por una organización de servicios, denominándose proceso de validación independiente. Proceso de revisión conjunta : Para evaluar el estado del software y sus productos en una determinada actividad del ciclo de vida o una fase de un proyecto. Las revisiones conjuntas se celebran tanto a nivel de gestión como a nivel técnico del proyecto a lo largo de todo su ciclo de vida. Un mecanismo habitual de revisión son las reuniones y la responsabilidad es generalmente compartida entre un grupo de personas pertenecientes a la organización. Proceso de auditoría : Permite determinar, en los hitos preestablecidos, si se han cumplido los requisitos, los planes y, en suma, el contrato. Proceso de resolución de problemas : Permite analizar y solucionar los problemas, sean éstos diferencias con los requisitos o con el contrato. Aporta un medio oportuno y documentado para asegurar que los problemas detectados son analizados y solucionados. Procesos de la OrganizaciónSon los utilizados por una organización para llevar a cabo funciones como la gestión, formación del personal o procesos de mejora continua. Proceso de gestión : Contiene las actividades y las tareas genéricas que puede emplear una organización que tenga que gestionar sus procesos. Incluye actividades como la planificación, el seguimiento y control, la revisión y evaluación. Proceso de infraestructura : Establece la infraestructura necesaria para cualquier otro proceso: hardware, software, herramientas, técnicas, etc para el desarrollo, explotación y mantenimiento. Proceso de mejora : Para mejorar los procesos del ciclo de vida del software. Proceso de formación : Para mantener al personal con la adecuada formación, lo que conlleva el desarrollo del material de formación, así como la implementación del plan de formación de la organización. Proceso de AdaptaciónSirve para realizar la adaptación básica de la norma ISO 12207-1 respecto a los proyectos software. Como es sabido, las variaciones en las políticas y procedimientos de la organización, los métodos y estrategias de adquisición, el tamaño y complejidad de los proyectos, los requisitos del sistema y los métodos de desarrollo, entre otros, influencian la forma de adquirir, desarrollar, explotar o mantener un sistema. Dado que los procesos se aplican durante el ciclo de vida del software, y además se utilizan de diferentes formas por las diferentes organizaciones y con distintos puntos de vista y objetivos, es preciso comprender los procesos, las organizaciones y sus relaciones bajo diferentes puntos de vista: Contrato: El comprador y el proveedor negocian y firman el contrato, empleando los procesos de adquisición y suministro. Gestión o dirección: El comprador, el proveedor, el desarrollador, el operador y el personal de mantenimiento gestionan sus respectivos procesos en el proyecto software. Explotación: El operador proporciona el servicio de explotación del software a los usuarios. Ingeniería: El desarrollador o el personal de mantenimiento llevan a cabo sus respectivas tareas de ingeniería para producir o modificar los productos de software. Soporte: Los grupos de soporte (el de gestión de la configuración, el de aseguramiento de la calidad, el de auditoría, etc) proporcionan servicios de apoyo a otros grupos en el cumplimiento de tareas únicas y específicas. Modelo en CascadaEste modelo nació durante los años setenta y supuso un gran avance con respecto a los modelos que habían sido utilizados hasta entonces. Este modelo se compone de una serie de fases que se suceden secuencialmente, generándose en cada una de las fases resultados que constituyen la entrada de la fase siguiente. Estas fases pueden diferir, pero suelen comprender: Planificación Especificación de Requisitos Diseño Codificación Pruebas e Integración Implantación y Aceptación Mantenimiento La fase de especificación de requisitos es conocida también como análisis funcional, la fase de diseño se denomina análisis orgánico y la fase de codificación se llama programación. El número de fases es irrelevante, lo que caracteriza verdaderamente a este modelo es la secuencialidad entre las fases y la necesidad de completar cada una de ellas para pasar a la siguiente. El sistema está terminado cuando se han realizado todas las fases. El modelo en cascada ayudó a eliminar muchos de los problemas que se planteaban antes de su utilización, además ha sido la base para la normalización y la adopción de estándares. A medida que ha sido utilizado se han detectado en él debilidades e inconsistencias que se han intentado corregir con diversas modificaciones y extensiones al modelo inicial. Fases del Modelo en CascadaVamos a analizar cada una de las posibles fases: Planificación De esta fase depende en gran medida un desarrollo efectivo en lo referente a costos y plazos de entrega. Hay que fijar los siguientes puntos: Ámbito del trabajo a realizar. Recursos necesarios. Tareas a realizar. Referencias a considerar. Coste aproximado del desarrollo del proyecto. Formación del equipo de desarrollo. Ordenación y calendario de las actividades. La planificación se llevará a cabo con un nivel de detalle adecuado a la complejidad, tamaño y grado de estructuración del proyecto. Para proyectos de gran tamaño la planificación es imprescindible y en ocasiones sirve para determinar el modelo de ciclo de vida a seguir en el proyecto. La estimación de recursos, costes y calendario se determina a partir de la experiencia acumulada por parte del jefe de proyecto y de la información histórica de otros proyectos realizados dentro o fuera de la organización. Esta es una de las fases más difíciles de realizar, pero en la actualidad se cuenta con técnicas y herramientas automatizadas para el control y la gestión del proceso de producción de los sistemas de información. Especificación de Requisitos – Análisis Funcional Una vez terminada la planificación del proyecto, la fase de análisis de requisitos es la primera del proceso de desarrollo del sistema. En esta fase es preciso analizar, entender y documentar el problema que el usuario trata de resolver con el sistema de información o aplicación a desarrollar. Es necesario especificar en detalle las funciones, objetivos y restricciones del sistema propuesto para que el usuario y los desarrolladores puedan tomar estas especificaciones como punto de partida para acometer el resto del sistema. El proceso de recogida de requisitos es la tarea más delicada de esta fase en la cual el analista del sistema debe llegar a comprender el dominio de la información y adaptar las necesidades del usuario a unas especificaciones formales listas para poder ser utilizadas por los desarrolladores. Se trata de definir “qué” debe hacer el sistema identificando la información a procesar, las funciones a realizar, el rendimiento deseado del sistema, las interfaces con otros sistemas o las restricciones de diseño entre otros aspectos. Es fundamental en esta fase la participación e implicación del usuario del sistema. Para el análisis de las necesidades a cubrir y los requisitos a satisfacer por el sistema, su priorización, comprobar que el sistema se ajusta a las necesidades del usuario y plantear alternativas viables no solo a nivel técnico sino desde el punto de vista de costes y riesgos, hay que utilizar todas aquellas técnicas o elementos a nuestro alcance como, por ejemplo, realización de entrevistas con los usuarios, utilización de información referente al sistema actual si es que existe, utilización de técnicas de diagramación para facilitar la comprensión del sistema, técnicas de análisis coste-beneficio, técnicas de prototipado rápido o técnicas de análisis estructurado. Las tareas asociadas a esta fase y los resultados que se obtienen serán independientes del entorno tecnológico del sistema de información. Diseño – Análisis Orgánico A partir, como siempre, de las especificaciones de la fase anterior y una vez elegida la mejor alternativa, se debe comenzar a crear la solución al problema descrito atendiendo a aspectos de interfaz de usuario, estructura del sistema y de decisiones sobre la implantación posterior. Esta fase aborda el “cómo”, es decir, deberá diseñar las estructuras de datos, la arquitectura del sistema, los detalles que permitan la codificación posterior y las pruebas a realizar. Para el diseño del sistema habrá que trasladar las especificación de requisitos a un conjunto de representaciones ya sean gráficas, tabulares o basadas en lenguajes que constituirán la estructura de datos lógica y física, la arquitectura y los procedimientos. Otras cuestiones que se abordan en esta fase son los requisitos de comunicaciones, algoritmos, seguridad y control. Al igual que en las fases anteriores la de diseño conlleva una documentación en la que se recogen sus resultados. En esta fase hay que tener en cuenta el entorno del sistema referente a hardware y software de base. Codificación – Programación En esta fase se traducen las especificaciones de diseño a un lenguaje de programación capaz de ser interpretado y ejecutado por el ordenador. Existen lenguajes de distintos grados de complejidad o eficacia y la utilización de uno u otro determinará la forma de trabajo de esta fase. En todo caso, el lenguaje vendrá determinado por el entorno lógico del sistema. El programador deberá velar por la claridad de su estilo para facilitar cualquier interpretación posterior de los programas. Asimismo se respetarán los estándares de la organización en cuanto a nomenclatura y formato. Es imprescindible que los programas incorporen comentarios escritos que ayuden a su comprensión y que se acompañen de la documentación externa necesaria que describa su objeto, los algoritmos que incluye, sus entradas y salidas y cualquier otro elemento relevante. Muchas son las técnicas aplicables a la programación, como, por ejemplo, las técnicas estructuradas ampliamente extendidas desde hace años. Más reciente es la generación automática de código, que a partir de especificaciones formales algunas herramientas CASE (Computed Aided Software Engineering) facilitan con un mayor o menor grado de optimización, según los casos, código en los lenguajes de programación más utilizados. Pruebas e Integración Una vez que los programas han sido desarrollados, es preciso llevar a cabo las pruebas necesarias para asegurar la corrección de la lógica interna de los mismos y comprobar que cubren las funcionalidades previstas. La integración de las distintas partes que componen la aplicación o el sistema es precisa en proyectos complejos o de grandes dimensiones que hayan sido descompuestos por razones de facilidad de gestión y control. La integración debe solucionar posibles problemas de interacción y garantizar el buen funcionamiento del conjunto. En esta fase se debe proporcionar la documentación que ilustre los procedimientos de usuario en la utilización y funcionamiento del sistema y si fuera preciso se organiza un plan de formación para usuarios con el material didáctico necesario. Como en las fases anteriores existen técnicas y herramientas para la realización de las tareas de esta fase. Implantación y Aceptación En esta fase se trata de conseguir la aceptación final del sistema por parte de los usuarios del mismo y llevar a cabo las actividades necesarias para su puesta en producción. Para ello se tomarán como punto de partida los componentes del sistema probados de forma unitaria e integrada en la fase anterior y se probarán una vez más esta vez con el fin de verificar que cumplen los requisitos de usuario y que el sistema es capaz de manipular los volúmenes de información requeridos en los tiempos y velocidades deseados, que las interfaces con otros sistemas funcionan, etc. En estas pruebas participará el usuario que si está conforme deberá aceptar formalmente el sistema. Mantenimiento Esta fase comienza una vez que el sistema es entregado al usuario y continúa mientras permanece activa su vida útil. El mantenimiento puede venir propiciado por: Errores no detectados previamente. Modificaciones, mejoras o ampliaciones solicitadas por los usuarios. Adaptaciones requeridas por la evolución del entorno tecnológico o cambios normativos. En el primer caso se habla de mantenimiento “correctivo” puesto que debe solventar defectos en el sistema. El segundo caso se denomina mantenimiento “perfectivo” puesto que se produce una modificación de los requisitos iniciales aumentando las funcionalidades. El último de los casos se conoce por mantenimiento “adaptativo”, con el paso del tiempo es muy posible que la situación inicial respecto de la cual se concibió el sistema cambie. Para la realización del mantenimiento se siguen los mismos pasos que para la realización del sistema, pero en el contexto de un sistema existente. Es fundamental que las variaciones producidas en esta fase queden reflejadas en todas las fases anteriores y no simplemente en la fase de codificación. De lo contrario esta práctica conduciría a sistemas intratables al cabo de varias modificaciones sin actualización de la documentación afectada. La fase de mantenimiento lleva asociada su propia documentación reflejando los cambios, su objeto, la fecha, el autor y cualquier dato que pueda ayudar al control de dichos cambios o a procesos de mantenimiento posteriores. La DocumentaciónEl modelo de ciclo de vida en cascada está regido por la documentación, es decir, la decisión del paso de una fase a la siguiente se toma en función de si la documentación asociada a esa fase está completa o no. El concepto de documentación debe entenderse en sentido amplio como todos los productos resultantes de las tareas realizadas en cada fase ya sean informes, programas, juegos de pruebas, etc, se podría definir la documentación como aquello que se construye y ha de mantenerse durante la vida del sistema. A pesar de posibles críticas a esta orientación se recogen a continuación las características que deben incluir los documentos asociados a cada fase. No hay que olvidar que el objetivo final es construir con éxito un sistema de información y que la documentación nos ayudará a conseguirlo pero no es un fin en sí misma. En la fase de planificación se elaborará documentación que tendrá el carácter de marco básico de referencia del proyecto y deberá incluir: Descripción y alcance de las tareas que se van a llevar a cabo. Identificación de los principales métodos, instrumentos y procedimientos de trabajo que se utilizarán. Procedimientos de seguimiento y control de los trabajos y mecanismos de revisión y aprobación de los mismos. Calendario de tareas y su ordenación temporal. Asignación de recursos. Organización de las personas que intervienen en el proyecto. En la fase de especificación de requisitos será necesaria la existencia de documentación en el que se recojan las necesidades del usuario respecto al sistema: funcionalidades, rendimientos, limitaciones y restricciones, los interfaces de usuario. La descripción de los requisitos de los usuario debe ser descrita de forma que se pueda verificar su cumplimiento mediante inspecciones o pruebas. Este documento reflejará el punto de vista del analista del sistema respecto de las especificaciones que el usuario ha realizado, si la comprensión por parte del analista no es adecuada el usuario debe rechazar el documento. Este documento es crucial porque sobre sus presupuestos se construirá el sistema de información, por tanto se detallará la naturaleza de la información, su contenido y su estructura lógica. La representación de las operaciones y procesos, sus entradas y salidas y cualquier característica relevante a nivel funcional referente al entorno tecnológico del sistema. En la documentación asociada a la fase de diseño se profundizará más, llegando a describir los componentes del sistema: estructuras de datos, unidades de tratamiento o módulos de procesamiento e interfaces al máximo nivel de detalle. Se concretará la descripción técnica y cuestiones relacionadas con la implantación del sistema: arquitectura general, fichero y/o BD, pantallas, informes o comunicaciones con otros sistemas. La fase de codificación debe proporcionar como documentación, el código fuente de todos los módulos incluidas las funciones auxiliares, el código para la creación de estructuras de datos e interfaces externas y cualquier tipo de módulos o rutinas relacionadas con el sistema. Además del código de cada módulo en el soporte físico y formato de representación previamente establecido por el usuario, se acompañará el listado del código con sus comentarios internos y comentarios externos sobre la definición de las estructuras de datos, de los algoritmos, sobre el manejo de excepciones y la gestión de errores. La fase de pruebas e integración llevará documentación que describa el plan de pruebas a nivel unitario e integrado y los resultados de dichas pruebas. La fase de implantación y aceptación vendrá documentada con el plan de pruebas del sistema a nivel global y sus resultados. Por último si se realizan modificaciones en la fase de mantenimiento deberán reflejarse en la documentación correspondiente que haya podido ser afectada. La documentación asociada al sistema no estaría completa sin un manual de usuario que contenga la descripción funcional de todos los procedimientos para facilitar la operativa del sistema al usuario. Recogerá los procedimientos de instalación, administración y gestión de la configuración, operaciones especiales, funcionamiento, ayudas incorporadas, tratamiento de errores, comandos y sentencias de control. A veces también puede incluir una guía de referencia rápida con el resumen de todas estas instrucciones. Crítica del ModeloLas principales críticas al modelo se centran en sus características básicas, es decir, secuencialidad y utilización de los resultados de una fase para acometer la siguiente de modo que el sistema sólo se puede validar cuando está terminado. Los proyectos reales raramente siguen el flujo secuencial que propone el modelo. Siempre ocurren interacciones y en las últimas fases sobre todo se pueden realizar en paralelo algunas áreas como por ejemplo: codificación y pruebas. Una aplicación del modelo en sentido estricto obligaría a la “congelación” de los requisitos de los usuarios, supuesto este completamente alejado de la realidad. El modelo no contempla la posibilidad de realimentación entre fases. El modelo en su formulación pura no prevé revisiones o validaciones intermedias por parte del usuario, así los resultados de los trabajos sólo se ven al final de una serie de tareas y fases de tal forma que si se ha producido un error en las primeras fases éste sólo se detectará al final y su corrección tendrá un costo muy elevado, puesto que será preciso rehacer todo el trabajo desde el principio. El modelo no dispone de resultados parciales que permitan validar si el sistema cumple con los requisitos desde las primeras fases, dándose el caso de sistemas perfectamente formalizados y documentados que no cumplen los requisitos del usuario. Extensiones al Modelo en CascadaActualmente, después de la experiencia obtenida con la aplicación del modelo en cascada y gracias a avances tecnológicos como por ejemplo lenguajes de cuarta generación o herramientas CASE existen modelos de ciclo de vida alternativos al modelo en cascada. En la práctica se llegan a realizar incluso modelos mixtos. En este punto no se tratarán estos casos, sino que se citarán algunas innovaciones aplicables al modelo en cascada que permiten mejorar algunas de las deficiencias del modelo y aumentar su eficacia: Utilización de herramientas CASE y otras facilidades. Al hablar de las fases del ciclo de vida se mencionaban las técnicas estructuradas, en principio dichas técnicas no se utilizaban en el modelo en cascada, sin embargo, hoy en día no se plantea la realización de un sistema “artesanalmente”. Introducción de una fase intermedia entre especificación de requisitos y el diseño denominado diseño rápido que sirva para validar las especificaciones por parte del usuario, estableciéndose un proceso iterativo de modificación de la especificación hasta que el usuario esté satisfecho con la misma. Técnicas de diseño en grupo. Es deseable que el usuario pueda participar al máximo en la definición de los requisitos puesto que se evitarán errores y confusiones ya en las primeras etapas. Utilizar técnicas de análisis de riesgos y de coste-beneficio para pasar a la fase siguiente y abandonar planteamientos rígidos de paso a la fase siguiente cuando se ha completado la documentación. Dotar de autonomía al jefe de proyecto para que de acuerdo con su experiencia y las características del proyecto decida comenzar una fase sin haber terminado la anterior al 100%, pero que le permite, por ejemplo, realizar una maqueta para la validación por parte del usuario. Codificación y pruebas de los módulos de más alto nivel en primer lugar, seguida de la de los módulos más detallados o de más bajo nivel. Esta aproximación puede propiciar el diálogo entre el analista y el usuario en fases posteriores a la especificación de requisitos estableciendo así un proceso de realimentación entre las fases de Implantación y Especificación de requisitos. Realización de fases en paralelo como codificación y pruebas. La codificación se puede así ver beneficiada por los resultados de las pruebas y detección de errores. Reutilización de código ya probado. A veces con pequeñas modificaciones se pueden llegar a utilizar programas existentes. Revisiones de código. Se trata de inspecciones para localizar lo más pronto posible dentro del ciclo todos los errores de diseño y codificación. Modelo en Espiral del Ciclo de VidaIntroducciónA partir de la experiencia de la aplicación del modelo en cascada se desarrolló el modelo en espiral del ciclo de vida del software y se implementó en algunos grandes proyectos. En este modelo subyace el concepto de que cada ciclo implica una progresión que aplica la misma secuencia de pasos para cada parte del producto y para cada uno de sus niveles de elaboración, desde la concepción global de la operación hasta la codificación de cada programa individual. El modelo en espiral se ilustra en la siguiente figura. En ella podemos apreciar: La dimensión radial: representa el coste acumulativo en el que se ha incurrido en las etapas realizadas hasta el momento actual. La dimensión angular : representa el progreso hecho en completar cada ciclo de la espiral. Describamos a continuación como sería un típico ciclo de espiral. Considerando cada cuadrante de izquierda a derecha en el sentido de las agujas del reloj. CUADRANTE 1 : Cada ciclo en espiral empieza con la indentificación de: Los objetivos de la parte del producto que va a ser elaborada (rendimiento, funcionalidad, disponibilidad para acomodarse a nuevos cambios, etc.) Las alternativas para implementar esta parte del producto (diseño A, diseño B, compra del software ya desarrollado, reutilización de un software ya existente, etc). Las restricciones impuestas: costes, calendario de realización, interfaces, etc. CUADRANTE 2 : Aquí se evalúan las opciones relativas a los objetivos y las restricciones. Este proceso, con frecuencia, identificará áreas de incertidumbre que son fuentes significativas de riesgo. Esto llevaría implícito la formulación de una estrategia económicamente efectiva para resolver las fuentes de riesgo: prototipado, simulación, benchmark, modelos analíticos o combinaciones de éstas y otras técnicas de resolución de riesgos. Si los riesgos de rendimiento o los riegos del interfase de usuario tienen mucha más importancia que los riesgos de desarrollo de programas o los riesgos de interfase de control interno, entonces el paso siguiente podría ser un paso de desarrollo evolutivo: un esfuerzo mínimo para especificar la naturaleza global del producto, un plan para el siguiente nivel de prototipado y el desarrollo de un prototipo más detallado para continuar la resolución de las mayores fuentes de riesgo. Si este prototipo es operacionalmente útil y suficientemente robusto para servir como una base de bajo riesgo para el evolución del producto, entonces, los subsiguientes pasos dirigidos por el riesgo (risk-drive) podrían ser una serie de prototipos cada vez más evolucionados, moviéndonos hacia la derecha de la figura. Así, consideraciones de riesgo, pueden llevar a la realización del proyecto utilizando sólo un subconjunto de todos los pasos potenciales en el modelo. CUADRANTES 3 y 4 : Si los esfuerzos previos de prototipado han resuelto ya todos los riesgos de rendimiento o los riesgos de interface de usuario y dominan los riesgos de desarrollo en programas o los riesgos de control de interface, el paso siguiente sería el desarrollo según el modelo en cascada. Cada nivel de especificación del software en la figura, entonces, seguido por un paso de validación y planificación del siguiente ciclo. Esta implementación, dirigida por el riesgo, de un subconjunto del modelo en espiral, permite al modelo acomodarse a cualquier mezcla de estrategias de desarrollo de software: orientado por las especificaciones, orientado por la simulación, orientado por transformaciones automáticas o cualquier otro enfoque de desarrollo. En tales casos, la estrategia mixta apropiada se escoge, considerando, la relación relativa de magnitudes de los riesgos y la efectividad relativa de las distintas alternativas en la resolución de estos riesgos. De forma similar, consideraciones de gestión de riesgo, permiten determinar la cantidad de tiempo y esfuerzo que debe dedicarse a otras actividades del proyecto tales como: planificación, gestión de la configuración, garantía de la calidad, verificación formal, y prueba. Un aspecto importante del modelo espiral, como en otros muchos modelos, es que cada ciclo se completa por una revisión que involucra a las personas u organizaciones principales relacionadas con el proyecto. Esta revisión cubre todas las actividades desarrolladas durante el ciclo previo, incluyendo los planes para el próximo ciclo y los recursos que se requieren para llevarlos a cabo. El principal objetivo de la revisión es asegurar que todas las partes implicadas están de acuerdo con el camino a seguir en la siguiente fase. Los planes para las fases sucesivas pueden incluir también el desarrollo del producto por medio de incrementos sucesivos o la división en componentes que pueden ser desarrollados por distintas personas u organizaciones. En este último caso, aparecen unos ciclos espirales paralelos (uno para cada componente) añadiendo una tercera dimensión al concepto presentado en la figura. Además, la etapa de revisión y compromiso puede extenderse, desde una simple revisión informal del diseño de un programa a una revisión de los requerimientos principales implicando, en este caso, a clientes, usuarios, desarrolladores y organizaciones de mantenimiento. Ejemplo de Aplicación del Modelo en EspiralEl modelo en espiral se aplicó en un proyecto muy complejo: la definición y desarrollo del Sistema de Productividad de Software (TRW-SPS) un entorno integrado de ingeniería de software de la empresa TRW. El objetivo era mejorar la productividad de las tareas de desarrollo de software realizadas por la empresa. En primer lugar se realizó un “ciclo 0” de la espiral para determinar la viabilidad de conseguir un incremento significativo de productividad en el desarrollo software. Ciclo 0: Estudio de viabilidad Participaron cinco personal, a tiempo parcial, durante un período de dos a tres meses. Durante este ciclo los objetivos y las restricciones se consideraron a un nivel muy alto, expresándoles en términos cualitativos (“mejora significativa”, “coste razonable”, etc). Se consideraron alternativas en cuatro áreas: gestión de los proyectos, gestión del personal, tecnología e instalaciones. Como áreas de riesgo, se consideró la posibilidad de que la compañía realizase una inversión importante para encontrarse con: Mejoras de productividad escasamente significativas. Mejoras potenciales incompatibles con aspectos de la cultura de la empresa. El análisis de riesgos condujo a la conclusión de que se podrían obtener significativas mejoras en la productividad, a un coste razonable, por medio de un conjunto integrado de iniciativas. Ciclo 1: Concepción de la operación En este ciclo se invirtieron más recursos (12 meses hombre en lugar de los 2 meses hombre del Ciclo 0); se consideraron objetivos más específicos (conseguir el doble de productividad en la producción de software en 5 años a un coste máximo de 10.000$ por persona); surgieron nuevas restricciones como la preferencia por la utilización de productos desarrollados por la propia empresa, en especial la red local de TRW. Para las áreas de riesgo también se fue más específico que en el Ciclo 0 (“comprobación que la red TRW LAN ofrecía una relación precio/rendimiento dentro de la restricción de 10.0000$ por persona”). Para la resolución de riesgos se realizaron tareas más extensivas, tales como la realización de benchmarks y análisis de un prototipo de la TRW LAN. La fase de compromisos no se limitó a la aceptación del plan. Se acordó la aplicación del entorno de desarrollo de software producido a un proyecto piloto que implicaba a más de 100 personas. Se decidió la formación de un comité de seguimiento para garantizar la coordinación de las distintas actividades y para evitar que el prototipo de entorno de desarrollo no se optimizase, excesivamente, en función de las características del proyecto en el que se iba a probar. Se recomendó que no solamente se desarrollase el prototipo, sino que también se elaborase una especificación de requerimientos y un diseño siguiendo una orientación orientada al riesgo. Ciclo 2: Especificación de los requerimientos de alto nivel Se tomaron las decisiones al comienzo del ciclo al observarse que muchos de los requisitos del sistema dependían de la decisión sobre el SO a utilizar y de si el sistema a elaborar iba a ser un sistema orientado al host o totalmente portable. Se decidió escoger UNIX y un sistema orientado a un host UNIX. Otros ciclos posteriores Las etapas posteriores se realizaron según las características generales de la implantación del modelo en este caso: Desarrollo de especificaciones, postergando la elaboración de los elementos de software de bajo riesgo hasta que no se hubieran estabilizado los elementos de software de alto riesgo. Incorpora la construcción de prototipos como técnica de reducción de riesgos en cualquiera de las etapas del proyecto. Permite la “vuelta atrás” a etapas anteriores cuando se encuentran alternativas más atractivas o se identifican nuevas fuentes de riesgo que requieren solución. Evaluación del ModeloVentajas La ventaja principal del modelo en espiral es que su rango de opciones acomoda las buenas características de los otros modelos de desarrollo de software, y su procedimiento dirigido por el riesgo, evita muchas de sus dificultades. En situaciones apropiadas, el modelo en espiral es equivalente a uno de los modelos de proceso existentes. Si un proyecto tiene un riesgo bajo en áreas tales como el establecimiento de una interfaz de usuario no adecuada o en el cumplimiento de requerimientos rigurosos de ejecución y si, por el contrario, tiene un alto riesgo en la predicción y control del presupuesto y del calendario de elaboración, entonces estas consideraciones conducen el modelo en espiral a uno equivalente al modelo en cascada. El modelo en espiral tiene otras ventajas adicionales: Concentra su atención en opciones que consideran la reutilización de software existente. Los pasos implicados en la identificación y evaluación de alternativas potencian estas opciones. Permite preparar la evolución del ciclo de vida, crecimiento y cambios del producto software. Proporciona un mecanismo de incorporación de objetivos de calidad en el desarrollo de producto software. Este mecanismo se deriva del énfasis puesto en la identificación de todos los objetivos y restricciones durante cada una de las vueltas de la espiral. Es especialmente adecuado para la temprana eliminación de errores y alternativas poco atractivas. No implica procedimientos separados para el desarrollo y la mejora del software. Proporciona un marco viable para integrar los desarrollos de sistemas software-hardware. El centrar la atención en la gestión del riesgo y en la eliminación de alternativas no atractivas lo antes posible y con el menor coste es, igualmente, aplicable al software y al hardware. Se adapta al diseño y programación orientada a objetos y posiblemente con estos métodos es cuando permite obtener mejores resultados: el modelo en espiral potencia la utilización de desarrollos incrementales, y cuando sea necesario, la reelaboración de partes ya desarrolladas. La abstracción, encapsulación, modularidad y jerarquización, elementos fundamentales de los métodos orientados a objeto, facilitan los desarrollos incrementales y hacen menos traumáticas las reelaboraciones. Dificultades Adaptar su aplicabilidad al software contratado El modelo en espiral actualmente trabaja bien en desarrollos de software internos pero necesita un trabajo adicional para adaptarlo al mundo de los contratos de adquisición del software. Los desarrollos internos de software tienen bastante flexibilidad y libertad para acomodarse a confirmaciones etapa por etapa; para diferir confirmaciones a determinadas opciones; para establecer miniespirales con las que resolver caminos críticos; para ajustar niveles de esfuerzo, o para acomodar prácticas tales como el prototipado, o el desarrollo evolutivo. En el mundo de la adquisición de software es muy difícil conseguir estos grados de flexibilidad y libertad sin perder responsabilidad y control y es muy difícil definir contratos que no especifiquen por adelantado los productos a obtener. Recientemente, se ha progresado en el establecimiento de mecanismos de contratación más flexibles pero todavía se necesitan mejoras para conseguir que los compradores se sientan completamente cómodos utilizándolos. Dependencia de la experiencia en la evaluación de riesgos El modelo en espiral da un papel relevante a la habilidad de los desarrolladores de software para identificar y gestionar las fuentes de riesgo del proyecto. Un buen ejemplo de esto es la especificación dirigida por el riesgo en el modelo en espiral. En ella se debe llegar a un alto nivel de detalle en los elementos de alto riesgo dejando, los de bajo riesgo, para una elaboración en etapas posteriores. Otro problema es que la especificación será, en exceso, dependiente de las personas. Por ejemplo, un diseño producido por un experto puede ser implantado por no expertos; en este caso, el experto que no necesita mucha documentación, debe producir suficiente documentación adicional para que los no expertos no se extravíen. Con una aproximación convencional, dirigida por la documentación, el requerimiento de llevar todos los aspectos de la especificación a un nivel uniforme de detalle elimina algunos problemas potenciales y permite una adecuada revisión de algunos aspectos por revisores inexpertos. No obstante, esto produce pérdidas de tiempo a los expertos que deben buscar los aspectos críticos en medio de un gran volumen de detalles no críticos. El Plan de Gestión de RiesgoIncluso si una organización no está lista para adoptar el procedimiento completo en espiral, una característica técnica del mismo que puede ser fácilmente adaptada a cualquier modelo de ciclo de vida proporciona muchos de los beneficios de dicho procedimiento. Se trata del Plan de Gestión del Riesgo. Este plan, básicamente, asegura que en cada proyecto se haga una identificación temprana de sus factores de riesgo más altos; desarrolla una estrategia para resolver los factores de riesgo; identifica y establece una agenda para resolver nuevos factores de riesgo a medida que afloran y, por último, muestra los progresos respecto al plan en revisiones mensuales. El plan de gestión de riesgo y las técnicas para gestión de riesgo de Software facilitan adaptar concepto del modelo en espiral a los procedimientos de adquisición y desarrollo de software más asentados. Se pueden sacar las siguientes cuatro conclusiones: El modelo en espiral, por su naturaleza de estar dirigido por el riesgo, es más adaptable a un amplio rango de situaciones que los procedimientos dirigidos por la documentación, tales como el modelo en cascada o los procedimientos dirigidos por el código, tales como el modelo de desarrollo evolutivo. Es particularmente aplicable a sistemas de software muy grandes y complejos. El modelo en espiral ha tenido éxito en su mayor aplicación realizada hasta ahora: el desarrollo del TRW-SPS. Proporciona la flexibilidad necesaria para acomodar un rango de alternativas técnicas y objetivos de usuario muy dinámico. El modelo en espiral no está, todavía, tan elaborado como los modelos más establecidos. Por esta razón, el modelo en espiral puede ser aplicado por personal experto, pero necesita elaboración posterior en áreas como contratación, especificaciones, puntos de control, revisiones, calendarios e identificación de áreas de riesgo para ser completamente aplicable en todas las situaciones. Algunas implementaciones parciales del modelo en espiral como el Plan de Gestión del Riesgo, son compatibles con la mayoría de los modelos de proceso actuales y son muy útiles para superar las mayores fuentes de riesgo de proyectos. Tabla de los 10 factores de riesgo del software más importantes: Tabla del Plan de Gestión del Riesgo del Software: Bibliografía Scribd (Ricardo Costanzi) Gestión del proceso de desarrollo: objetivos, actores y actividades. Técnicas y prácticas de gestión de proyectos.Introducción a la Gestión del Proceso de DesarrolloLos fundamentos de gestión consisten en determinar el tamaño del producto (incluyendo funcionalidad, complejidad y otras características del producto), asignando los recursos apropiados a un producto de ese tamaño, creando un plan para aplicar esos recursos y luego controlando y dirigiendo los recursos para impedir que el proyecto se desvíe. En algunos casos los altos cargos delegan explícitamente estas tareas de gestión a los responsables técnicos, y en otros casos simplemente las dejan vacantes, ocupándose de ellas un responsable o desarrollado motivado. Definiciones Proceso . Conjunto de actividades con un objetivo, que transforman un conjunto de entradas en un conjunto de salidas, agregando valor. Proceso de Software . Proceso que transforma Requerimientos de Procesamiento de Información en Elementos de Software (componentes computacionales y documentos) que satisfacen los requerimientos de procesamiento de información, ofreciendo servicios a una organización. Proceso de desarrollo de software . Proceso cuyo objetivo es generar un nuevo sistema informático que satisfaga un conjunto de requerimientos iniciales de procesamiento de información en una organización. Procesos de Administración de Proyectos . Procesos orientados a organizar y guiar al equipo de desarrollo para cumplir los compromisos negociados con el cliente (fechas de entrega y costes). Procesos de Ingeniería de Software . Procesos orientados a obtener componentes de software (código y documentos) entregables al cliente, sin errores y con el menor esfuerzo posible. Características de la Gestión del Proceso de DesarrolloLas características principales de la gestión del proceso de desarrollo son las siguientes: El producto a desarrollar es intangible. El producto tiene su propia flexibilidad. La ingeniería de software no es reconocida como una disciplina de la Ingeniería con el mismo estatus de la mecánica, eléctrica, matemáticas, etc. El proceso de desarrollo de software no está estandarizado. De estas características se deduce la importancia de la propia gestión, ya que la Ingeniería de software es una actividad económica importante, que está sujeta a restricciones económicas. No hay que olvidar que los proyectos bien gestionados a veces fallan. Los proyectos mal gestionados siempre fallan. Causas de Fallos en los ProyectosDentro de las principales causas por las que puede fallar un proyecto, se encuentra el hecho de que los componentes del proyecto no respetan o no conocen bien las herramientas y las técnicas del análisis y diseño de sistemas, además de esto puede haber una mala gestión y dirección del proyecto. Además existen una serie de factores que pueden hacer que el sistema sea mal evaluado: Requerimientos Incompletos. Usuarios no Involucrados. Carencia de Recursos. Expectativas Irreales. Falta de soporte ejecutivo. Cambios de Requerimientos. Falta de planificación. Obsoleto antes de ser completado. Carencia de supervisión por parte de la gerencia de TI. Desconocimiento de la tecnología. No resuelve problemas de negocio. Requerimientos planificados de manera irreal. Carencia de entrenamiento en Administración de Programas. Mala Estimación. Aunque estos factores pueden influir de manera muy trascendente en la mala realización de un proyecto, generalmente están acompañados de otro tipo de problemas. Pero, ¿cuáles de estos errores de gestión de proyectos ocasionan que no se cumplan los requisitos, que se sobrepase los tiempos de entrega o se aumenten repetidas veces los costes? La respuesta a esta pregunta puede ser hallada en dos fuentes principalmente, deficiencias en las herramientas y las técnicas de análisis del diseño de sistemas o la mala gestión de los proyectos. En el caso de las necesidades no satisfechas o no identificadas, el error puede aparecer debido a que se omiten datos durante el desarrollo del proyecto, es por esto que es muy importante no saltar ninguna etapa del ciclo de vida del desarrollo de sistemas. Otra causa de insatisfacción de necesidades es la mala definición de las expectativas de un proyecto en sus orígenes, ya que si no están bien definidos los requerimientos máximos y mínimos que el proyecto debe satisfacer desde el comienzo, los desarrolladores verán afectados su trabajo por el síndrome de las necesidades que crecen el cual les dejará hacer cambios en el proyecto en cualquier momento sin detenerse a pensar si esos cambios serán buenos para el proyecto como un todo, por supuesto todas estas modificaciones acarrearan alteraciones en los costes y en los tiempos de entrega. El coste de un proyecto puede aumentar durante el desarrollo de este debido a varias causas: Para comenzar un proyecto generalmente se exige un estudio de viabilidad en el cual no se incluyen datos completamente precisos de la cantidad de recursos que cada tarea consumirá, y es en base a este estudio que se hacen estimaciones de los recursos totales que el proyecto va a necesitar. El uso de criterios de estimación poco eficientes por parte de los analistas. El aumento en los tiempos de entrega debido a que los directores del proyecto en ocasiones no gestionan bien los tiempos de entrega de cada tarea del proyecto y cuando tienen un retraso no son capaces de alterar los plazos de entrega finales creyendo que podrán recuperar el tiempo perdido, pero no siempre es posible acelerar otras tareas para ahorrar tiempo en la entrega final. Para evitar todos estos problemas, se debe tener al mando del proyecto un director que conozca las herramientas de diseño y análisis de sistemas y tenga una buena formación en las funciones de dirección. Objetivos de la Gestión del Proceso DesarrolloLos objetivos de la gestión son al final objetivos de calidad, es el primer paso de cualquier metodología de mejora, estos se pueden definir respondiendo la pregunta… ¿Cuáles son los puntos que queremos mejorar en la gestión? Seguramente habrá muchos puntos que son susceptibles de mejora, sin embargo hay que considerar solo unos pocos y sobre todo aquellos que sean los que más nos interesa modificar. Al establecer los objetivos debemos procurar definirlos de manera clara, concreta y deben ser cuantificables. Básicamente estos podrían ser: Reducir la diferencia entre la fecha real y la fecha acordada. Reducir la diferencia entre el esfuerzo real y el esfuerzo acordado. Reducir el número de errores funcionales y no funcionales de los sistemas en entornos de producción (tendencia a cero errores). Aumentar la productividad de los equipos de desarrollo (Relación productos-esfuerzo global de proyectos). En mejora de procesos hay tres cosas que esperamos que ocurran: En la figura se muestra como en principio se establecen objetivos de fecha y costo que no se cumplen. La mayor parte del trabajo no está dentro del objetivo (a la izquierda de la gráfica). En la parte de abajo lo que ocurre es que el objetivo que se establece se acerca más a la realidad del desempeño del equipo de trabajo que desarrolla las tareas, volviendo más certera la estimación. En la siguiente figura lo que esperamos que se transforme es el control. Aquí se muestra como (en la parte de arriba) una parte importante de los resultados, a pesar de estar centrados en los objetivos, se sale de los objetivos. Disminuir (abajo) esta curva significa que nuestro proceso está bajo control. En la siguiente figura, con más certeza en la información para fijar objetivos y con más control en nuestros procesos, podemos mejorar la efectividad con objetivos más agresivos y con altas posibilidades de cumplirlos. El Modelo CMM (Capacity Maturity Model)Existen diferentes modelos de calidad entre los cuales se encuentran: CMM, SPICE, Bootstrap y Thrillium, los cuales se concentran en evaluar la capacidad de los procesos de software y la madurez de las organizaciones de software. Estos modelos constituyen un marco de referencia que permite calificar a las organizaciones de desarrollo de software. Está comprobado que ISO9000 no es adecuado para evaluar capacidad de procesos de software, es por eso que el mismo ISO creó el proyecto SPICE. El modelo CMM ubica a las organizaciones en uno de cinco niveles de madurez según se muestra en la figura: En el nivel 1 la organización es reactiva, los administradores se dedican a resolver crisis inmediatas y los tiempos calendario y los costes son excedidos básicamente porque no están basados en estimaciones reales. Cuando hay metas calendario agresivas, regularmente la funcionalidad y calidad del producto son comprometidos a fin de cumplir con las fechas. No existe un proceso definido y cuando un proyecto sale bien, no hay manera de reproducir su forma de trabajo en proyectos subsecuentes. En el nivel 2 ya hay un proceso definido, se tienen identificadas las entradas y salidas de cada etapa y se establecen controles en las entradas y salidas de cada etapa. En el nivel 3 se define el cómo de cada etapa, habiendo probado en la VIDA REAL las técnicas y descubierto la mejor forma de aplicarlas. Se identifican los controles internos necesarios para cada etapa. En el nivel 4 se aplican los controles internos de cada etapa y se llevan a cabo mediciones y estimaciones en base a información estadística. Finalmente en el nivel 5 se lleva a cabo un proceso de mejora continua REAL en el que se hace reingeniería de procesos. Actividades de GestiónSon las actividades que permiten asegurar que el software se lleva a cabo a tiempo y de acuerdo a la planificación. Así como de acuerdo a los requerimientos del software: Planificación de las actividades del equipo de desarrollo dentro del proyecto. Obtener los recursos (tanto físicos como humanos) necesarios para la ejecución del proyecto. Organizar funciones y responsabilidades de las personas dentro del equipo de desarrollo. Revisar cumplimento de planes y compromisos. Supervisar/auditar la ejecución de las actividades dentro del desarrollo y revisar las características necesarias de los productos que se generan dentro del proyecto. Administrar/controlar los cambios en los productos generados dentro del proceso de desarrollo. Medir y registrar el desempeño durante la ejecución del proyecto. Anticipar posibles problemas durante la ejecución del proyecto y prevenirlos. Evaluar y retroalimentar el desempeño de los miembros del equipo de desarrollo. Estas actividades se pueden agrupar como se muestra en el diagrama general de grupos de procesos: Todos los proyectos, que se gestionan como tales, tienen una serie de fases comunes, no tanto porque se realicen tareas iguales, sino porque el objetivo de cada fase con relación al producto a obtener es común a cualquier proyecto. Así tenemos dos grandes fases: Planificación y Ejecución. Estas fases se subdividen en otras menores. Veamos cada una de ellas por separado. IniciaciónEl origen de un proyecto suele ser difuso. Normalmente alguien identifica un problema o una necesidad. Este problema-necesidad hace muy interesante el nacimiento de un proyecto, ya que podemos observar como ante el problema que se plantea unos gerentes lo ven como un impedimento para alcanzar sus metas, mientras otros, pensando que el mismo problema también la tienen sus competidores, lo ven como una oportunidad para dar una solución correcta y posicionarse mejor en el mercado. Ya sea visto como problema u oportunidad, lo primero que hay que hacer es obtener una descripción clara de éste. La pregunta clave a responder es: ¿Cuál es el problema, o dónde está la oportunidad? Evidentemente aquí hay que trabajar con los usuarios, directores de empresa y clientes, pues ellos son los que conocen su negocio y será de ellos de quien tendremos que obtener la información para responder a esta pregunta. La definición del problema suele ocupar muy poco tiempo, por esto muchas veces no se le da la importancia central que tiene. Hay que tener en cuenta que todo el proyecto se basará en esta definición y es mejor que quede clara. La definición del problema debe ser revisada por todos los implicados en el problema: usuarios, directivos y clientes. Normalmente al definir el problema debemos hurgar en la organización, sus objetivos y fines. También debemos, una vez clarificado el problema, identificar los beneficios que se obtendrán si lo solucionamos. Hay que evitar “las soluciones en busca de un problema”, es decir cuando alguien ha visto una aplicación en marcha, o un sistema, y quiere algo similar. Muchas veces se esconde la idea intuitiva de que aquello resolverá un problema o generará una oportunidad. Lo mejor es sacar a flote el problema o la oportunidad y entonces definirlo en términos claros. También es peligrosa la situación en la que los únicos interesados en el problema y su solución son los implicados en el proyecto. Muchas veces los técnicos desean aplicar nuevas técnicas o herramientas y organizan un proyecto en torno a éstas. En todo caso lo que se debe hacer es buscar en la empresa, identificando alguna aplicación que no sea compleja y que sea útil a los objetivos de la misma. Los siguientes puntos nos dan una idea de la forma de pensar, así como las tareas a realizar durante esta fase: Estudiar el sistema actual. Discutir y analizar lo que se desea obtener. Clarificar las áreas de la empresa que se verán afectadas. Definir el problema y sus componentes, aclarando: qué es fundamental, qué es deseable y qué es opcional. Visualizar el producto o sistema a proporcionar, así como su adaptación a la organización. Identificar al responsable del proyecto. Crear una declaración clara de lo que se va a hacer. Obtener el sí de los implicados: “Sí, tenemos exactamente ese problema”. En todas las fases y en esta de forma especial se debe estimar los costes previsibles del proyecto y, sobre todo, el coste de la siguiente fase, la planificación. En muchas organizaciones, una vez definido el problema, éste se añade a la lista de los problemas pendientes de resolución. Así un comité de dirección selecciona el próximo problema a resolver, o sistema a desarrollar. PlanificaciónEl objetivo de toda planificación es la de clarificar el problema a solucionar, definir el producto a obtener, o servicio a proporcionar, estimar los costes económicos en que va a incurrir, así como los recursos humanos y de cualquier otro tipo que se requieran para alcanzar la meta. La función principal es la de atender a las necesidades que aparecerán a lo largo del desarrollo, anticipando el curso de las tareas a realizar, la secuencia en que se llevarán a cabo, los recursos y el momento en que serán necesarios. Hay que tener en cuenta que normalmente hay más bienes o servicios que desearíamos obtener, que recursos disponibles para obtenerlos, por lo que las empresas deben seleccionar entre varias alternativas. Así una mala definición de un proyecto puede provocar que la empresa comprometa sus recursos en un bien del que hubiera podido prescindir en favor de un sustituto más económico. La planificación del proyecto es la fase en la que se deberán identificar todas las cosas necesarias para poder alcanzar el objetivo marcado. En esta fase se han de concretar los tres cimientos sobre los que se apoyará el desarrollo de todo el proyecto, estos son: Calidad: viene dadas por las especificaciones. Coste económico: valorado en el presupuesto. Duración: asignada en el calendario de trabajo. Así como en la fase anterior nos centrábamos en identificar el problema, aquí tenemos que identificar diferentes soluciones y los costes asociados a cada una de ellas. Aunque muchos autores separan el análisis de la aplicación de la propia planificación, por entenderse que la primera es una técnica, mientras que la planificación es una tarea de gestión, cronológicamente se han de realizar de forma simultánea, aunque, se debería partir de una especificación seria del problema, antes de planificar las tareas, costes y recursos necesarios para desarrollar la aplicación. Otro asunto es que cada trabajo que se realiza se debe planificar antes de acometerlo. Así, antes de realizar el análisis se deberá hacer una planificación de los trabajos asociados a éste, pero difícilmente se podrá realizar la planificación de todo el proyecto. Las tareas a realizar para planificar el proyecto, las podemos agrupar en: Estimar el tamaño de la aplicación a desarrollar. Estimar el coste en recursos humanos. Identificar las tareas a realizar. Asignar recursos a cada tarea. Crear un calendario de las tareas. Realizar un estudio económico. Reunir todo en un documento, Estudio de viabilidad. Estas tareas se realizan de forma secuencial o iterativa entre ellas. Esta sería una iteración de las tareas: Establecer las restricciones del proyectohacer las suposiciones iniciales de los parámetros del proyectowhile el proyecto no termina o ha sido cancelado loopDescribe la planificación de tiempos del proyectoInicia las actividades de acuerdo a la planificaciónEspera (a que se lleve a cabo el desarrollo)Revisa el progreso del proyectoRevisa los parámetros estimados del proyectoActualiza la planificación del proyectoRenegocia las restricciones del proyecto y los tiempos de entregaif (aparecen problemas) theninicia una revisión técnica y sus posibles solucionesend ifend loop Las actividades en un proyecto deben ser organizadas para producir resultados tangibles para que la administración pueda juzgar el progreso. Los “Milestones” son los puntos finales de alguna actividad. Los “deliverables” son los resultados del proyecto que serán entregados a los clientes. El proceso de “cascada” permite una definición precisa de los “milestones”. EjecuciónEn esta fase, se trata de llevar a cabo el plan previo. Se verá fuertemente influida por la planificación. Una mala planificación, llevará a una mala ejecución, ya que si se planifica que costará menos tiempo del real, los usuarios presionarán a los desarrolladores, con lo que éstos trabajarán en peores condiciones, del mismo modo, si se planifica un coste inferior, los administradores de la empresa presionarán al personal del proyecto, con lo que éstos trabajarán con más estrés. Esta fase se caracteriza fundamentalmente porque en ella se ha de organizar el equipo de desarrollo, los mecanismos de comunicación, la asignación de roles y de responsabilidades a cada persona. Tareas fundamentales son: Identificar las necesidades de personal, que aunque ya venían de la fase de planificación, habrá que ajustarla a las disponibilidades actuales. Establecimiento de la estructura organizativa. Definir responsabilidades y autoridad. Organizar el lugar de trabajo. En muchas ocasiones el comienzo de un proyecto tiene tareas como instalación de equipamientos, acondicionamiento de locales, … Puesta en funcionamiento del equipo. Cuando las personas que van a trabajar en un proyecto no se conocen, es oportuno organizar reuniones más o menos informales para que se conozcan, esto evitará malentendidos y conflictos durante la ejecución del proyecto. Divulgación de los estándares de trabajo y sistemas de informes. Al comenzar el proyecto, las personas están más receptivas que cuando se encuentran en un trabajo rutinario o cuando el objetivo se transforma en algo obsesivo. Ésta es una razón de peso para introducir los nuevos métodos de trabajo. Es posible que sea el cliente el que marque los estándares. ControlEn este momento, ya tenemos el proyecto con su calendario, etc, las especificaciones claras, los recursos y personas en situación de trabajo. Las personas deben llevar a término cada una de las tareas que se les ha asignado en el momento que se le haya indicado. El objetivo principal en esta fase es; establecer visibilidad adecuada del progreso real del proyecto, de manera que la administración pueda tomar acciones efectivas cuando la ejecución del proyecto de software se desvía significativamente de los planes. Por su parte el responsable del proyecto debe realizar las siguientes actividades: Revisar cumplimiento de planes y compromisos. Tomar medidas del rendimiento. Revisar los informes que le llegan de los empleados. Mantener reuniones para identificar los problemas antes de que aparezcan. Administrar / controlar los cambios en los productos generados dentro del proceso de desarrollo. En caso de desviaciones poner en práctica las acciones correctivas necesarias. Coordinar las tareas. Motivar y liderar a los empleados. Recompensar y disciplinar. CierreÉsta fase es la opuesta a la de puesta en marcha. En ésta se trata primero dar por finalizado el proyecto y entregar el producto, o dejar de producir el servicio encomendado. Las actividades a realizar son las siguientes: Hacer entrega definitiva del producto al cliente. Revisar las desviaciones del proyecto, identificar causas e indicar formas diferentes de actuación en futuros proyectos. Reasignar el personal a los nuevos proyectos o reintegrarlos en los departamentos de partida. Es interesante documentar las relaciones entre los empleados para futuros proyectos. Gestión de RiesgosGestión de riesgos concierne con la identificación de riesgos y la escritura de planes para minimizar el efecto de estos en el proyecto. Un riesgo se relaciona con la probabilidad de que ocurra alguna circunstancia adversa al proyecto: Los riesgos de un proyecto afectan a la planificación o a los recursos. Los riesgos del producto afectan a la calidad o al desempeño del software por desarrollarse. Los riesgos del negocio son aquellos que afectan a la organización que desarrolla el software. Identificación de los Riesgos Identifica riesgos en el proyecto, en el producto y en el negocio: Riesgos en la tecnología. Riesgos en la gente. Riesgos organizacionales. Riesgos en los Requerimientos. Riesgos de estimación. Análisis de Riesgos Calculo de la posibilidad de que ocurran estos riesgos y de sus consecuencias: Determina la probabilidad y la seriedad de cada riesgo. Las probabilidades pueden variar entre muy alta, alta, moderada, baja o muy baja. Los efectos de los riesgos pueden ser: catastróficos, serios, tolerables o insignificantes. Planificación de Riesgos Trazar planes para evitar o minimizar el efecto de los riesgos. Considera cada riesgo y desarrolla una estrategia para manejarlo: Estrategias de evasión: La probabilidad de que el riesgo que presente se minimizara. Estrategias de minimización: El impacto del riesgo en el producto o en el proyecto se reducirá. Planes de contingencia: Si el riesgo se presenta, el plan de contingencia se encarga de tratar este riesgo. Monitorización de Riesgos Monitorizar los riesgos durante el proyecto: Determina regularmente cada riesgo identificado y decide si es probable o no que se presente. Determina si los efectos que produciría el riesgo ha cambiado. Cada riesgo clave debe discutirse en las reuniones de avance del proyecto. Desarrollo en FasesEl proceso de desarrollo de software no es solamente escribir líneas de código, compilar y ejecutar. Lo anterior es sólo una etapa (importante) de dicho proceso. En un proceso, se debe definir quién hace qué cosa cuando y cómo para alcanzar un cierto objetivo. En la ingeniería de software el objetivo principal es construir un producto de software o mejorar alguno ya construido, tomando en cuenta los requerimientos de los clientes (usuarios). Un proceso, provee de una guía para el desarrollo eficiente de un software de calidad. Tal proceso es una guía para todos los participantes en el desarrollo (usuarios, desarrolladores, responsables de proyecto, etc.) y permite construir software más ordenado y con un tiempo de vida relativamente largo. Para realizar un proyecto, empezaremos por ver cuales son los objetivos que queremos alcanzar y luego pensaremos qué cosas tenemos que hacer para alcanzar estos fines. Esta descomposición pasará por identificar las fases de nuestro proyecto y el esfuerzo a aplicar en cada una de ellas. A su vez estas fases se descompondrán en tareas. También tendremos que marcar unos puntos (hitos) de control que nos permitan saber si el proceso va de acuerdo a lo previsto. Normalmente todas las fases y tareas terminan en la generación de uno o varios documentos. A éstos se les llama entregables . Este nombre se debe a que pasan de manos del desarrollador a manos del controlador del proyecto o cliente. En los proyectos informáticos se suele asociar los hitos a la consecución de un entregable. Descomposición en Actividades del Proyecto (WBS)Empezaremos por ver la herramienta que se utiliza a la hora de descomponer y documentar el trabajo de un proyecto, como un conjunto de tareas. Habitualmente se le conoce como WBS (Work Breakdown Structure) que literalmente significa estructura de descomposición del trabajo. Es un método de representar de forma jerárquica los componentes de un proceso o producto. Puede ser utilizado para documentar la descomposición de un proceso, la descomposición de un producto, o de forma híbrida. Entregables de un Proyecto InformáticoLos entregables son: “Productos que, en un cierto estado, se intercambian entre los clientes y los desarrolladores a lo largo de la ejecución del proyecto informático”. Los entregables se clasifican como relativos al objetivo y relativos a la gestión del proyecto. Son relativos al objetivo todos aquellos documentos que hacen referencia exclusivamente al sistema de información y al subsistema informático en desarrollo. Pertenecen a este conjunto los requisitos del sistema, la especificación del sistema, la documentación del diseño, el código fuente, los programas ejecutables, los manuales de usuario, etc. Los entregables relativos a la gestión del proyecto hacen referencia a aquellos documentos que se refieren a la situación en que se encuentra un proyecto, previsiones de costes, gastos realizados, informe sobre entornos de trabajo, etc, siendo su objetivo el poder controlar el proyecto. Pertenecen a esta clase la planificación del proyecto, los presupuestos, los documentos de control de la planificación o de la calidad, los estudios de riesgos durante el desarrollo, etc. Se deberá definir de forma clara el conjunto mínimo de entregables necesario para dar por terminada cada fase de desarrollo. Aunque algunos entregables se desarrollan a lo largo de varias tareas. Los entregables nos proveen de: Un conjunto de componentes que formarán el producto una vez finalizado el desarrollo. Los medios para medir el progreso y la calidad del producto en desarrollo. Los documentos necesarios para la siguiente etapa. Entregables más UsualesDado que como hemos visto los entregables juegan un papel central en el desarrollo de un subsistema informático, vamos a listar los más importantes: Estudio de viabilidad: Descripción breve del sistema propuesto y sus características. Descripción breve de las necesidades del negocio en el sistema propuesto. Propuesta de organización del equipo de desarrollo y definición de responsabilidades. Estudio de los costes, que contendrán estimaciones groseras de planificación y fechas, tentativas, de entrega de los productos. Estudio de los beneficios que producirá el sistema. Análisis: Captura de requisitos: Análisis del sistema actual (si existe). Requisitos nuevos de los usuarios. Descripción del sistema propuesto. Especificación del sistema: Descripción del sistema (DFD’s, etc.). Requisitos de datos. Requisitos de telecomunicaciones. Requisitos de hardware. Plan de pruebas de integración. Diseño: Descripción detallada del sistema, contendrá: Programas, módulos reutilizables y objetos. Ficheros y bases de datos. Transacciones. Diccionario de datos. Procedimientos. Carga del sistema y tiempos de respuesta. Interfaces, tanto humanos como de máquinas. Descripción de los controles del sistema propuestos. Diseños alternativos recomendados. Estándares de programación y diseño de programas, recomendados. Técnicas de implementación recomendadas: codificación propia, compra de paquetes, contratación externa, etc. Plan de pruebas de programas. Codificación: Documentación del diseño final del sistema y de cada programa. Diagramas definitivos del sistema y de los programas. Descripción detallada de la lógica de cada programa. Descripción de las Entradas y Salidad (ficheros, pantallas, listados, etc.). Listado de los programas, conteniendo comentarios. Cadenas de ejecución si es necesario (JCL, scripts, etc). Resultado de las pruebas de cada unidad. Resultado de las pruebas de cada programa. Resultado de las pruebas de la integración. Guía para los operadores del sistema. Programa de entrenamiento de los operadores. Manual de usuario del sistema. Pruebas: Plan de pruebas del sistema (actualizado). Informe de los resultados de las pruebas. Descripción de las pruebas, el resultado esperado, resultado obtenido y acciones a tomar para corregir las desviaciones. Instalación: Planes detallados de contingencias de explotación, caídas del sistema y recuperación. Plan de revisión post-instalación. Informe de la instalación. Carta de aceptación del sistema. Mantenimiento: Listado de fallos detectados en el sistema. Listado de mejoras solicitadas por los usuarios (si no dan lugar a nuevos proyectos). Traza detallada de los cambios realizados en el sistema. Actas de las revisiones regulares del sistema y aceptación de los niveles de soporte. A todos estos documentos hay que añadir en todas las fases documentos con la estimación y planificación de la próxima fase y del resto del proyecto. También habrá que ir actualizando el índice de todo el material relacionado. Descomposición en Fases del Desarrollo de una AplicaciónLa descomposición por fases (actividades) se basa en referencias históricas de la empresa que asocian una cantidad media de horas de trabajo a una actividad concreta, de modo que dado un proyecto concreto podemos estimar la cantidad de esfuerzo que se dedicará a esa actividad. En ésta se ha de tener en cuenta el tipo de proyecto, el lenguaje de desarrollo y la madurez de la organización. Podemos plantear la descomposición desde el enfoque de entregables y asociar las tareas a la producción de un entregable concreto. Este enfoque tiene la ventaja de que la culminación de una tarea indica que ha concluido un producto y viceversa. Dado que, como veremos, no es aconsejable el tener tareas que duren más de una semana, se plantean problemas con algunos entregables que cuestan más. El planteamiento de descomponer por procesos o actividades puede resultar más natural en algunos casos. Es más fácil conseguir tareas acotadas en el tiempo. Tiene la desventaja de que el proyecto no será tan fácil de controlar ya que en muchos casos será la palabra de los realizadores la única constancia de que la tarea está terminada o al “90%”. En cualquier caso, los proyectos se planifican con dos horizontes, el de la próxima fase y el del proyecto completo. En el horizonte de la próxima fase se realiza con mayor nivel de detalle, mientras que según se alejan las fases se aplica un menor nivel de detalle. La descomposición del proyecto con mayor nivel de refinamiento no puede basarse en datos recogidos de forma analítica, sino que hace falta una aportación personal de los miembros del equipo de trabajo, tanto para identificar tareas como para asignarles esfuerzos. Se suele aconsejar el trabajo en grupo donde todos puedan aportar sus conocimientos y experiencias previas. Hay que tener en cuenta que si identificamos las tareas y se las imponemos a los desarrolladores, éstos funcionarán en una situación de sumisión lo que puede tener efectos perniciosos tanto para los plazos de entrega como para la calidad del software. Por otra parte el dejar que sean los propios desarrolladores los que identifiquen tareas y recursos, dentro de un marco razonable (puntos de función) les llevará a una situación de compromiso personal, pasando a interiorizar los objetivos y como consecuencia obtendremos mejores resultados. La tarea fundamental de los desarrolladores es escuchar a los clientes o usuarios y traducir sus requisitos a un lenguaje comprensible por la máquina, de modo que el subsistema informático se adapte a las necesidades expresadas. Así para cualquier tarea podremos encontrar las siguientes subtareas: Documentarse, Buscar o Investigar. Organizar, Escribir Documentos. Verificar, Comprobar. Revisar, Actualizar Documentos. Entregar, Finalizar. Además de lo anterior hay que tener en cuenta que al ir desarrollando el sistema obtenemos información que nos será útil a la hora de identificar nuevas tareas. Así, el análisis estructurado nos provee de una descomposición del proyecto por productos: transacciones, archivos, entradas, salidas, etc. El Diseño de programas nos descompone el sistema por módulos, el Diseño de BD descompone por tablas, archivos, etc, y los diseños de interfaz de pantallas, listados, mensajes, etc. Así, por ejemplo, una entrada puede ser que requiera de una reunión con el usuario, un estudio de ésta y la posterior presentación y aprobación de la propuesta a desarrollar. Tareas Usuales de un Proyecto Informático Estudio de viabilidad: Analizar el sistema propuesto y escribir una descripción. Definir y documentar posibles tipos de sistemas. Hacer un análisis de coste de sistemas similares. Hacer una estimación del tamaño del sistema, la planificación y los costes (tener en cuenta los entregables más importantes). Definir cualitativa y cuantitativamente los beneficios del sistema propuesto. Realizar una planificación inicial del plazo de recuperación de la inversión. Realización de una estimación detallada de costes, planificación, recursos, etc, de la siguiente fase (Análisis). Asignar director del proyecto. Composición del documento de estudio de viabilidad. Presentación del documento de viabilidad a la dirección para su aprobación. Análisis: Captura de requisitos: Definir el ámbito del sistema propuesto. Funciones Usuarios Restricciones Entrevista a todos los usuarios propuestos y actuales: Determinar: Utilización del sistema actual. Deficiencias del sistema actual. Requisitos nuevos del sistema. Documentar: Descripción del sistema actual. Deficiencias del sistema actual. Producir el documento de requisitos del nuevo sistema: Incluir: Requisitos del usuario priorizados. Resoluciones sobre las deficiencias del sistema actual. Producir una lista de los beneficios tangibles e intangibles (un refinamiento de la lista del estudio de viabilidad). Realización de una estimación detallada de costes, planificación, recursos, etc, de la siguiente fase (Especificación del sistema). Producir una estimación revisada de costes, planificación, recursos, etc, para el resto del proyecto. Producir el documento de definición de requisitos, esta tarea incluye la construcción de un prototipo. Realizar una revisión final del documento de requisitos. Toma la decisión de continuar o no con el proyecto. Definir las responsabilidades en la próxima fase para el director, miembros del equipo de desarrollo y otros. Especificación del sistema: Definir el tipo de sistema propuesto: Transformar las restricciones físicas, ambientales y operacionales a características del sistema. Por ejemplo: ¿Sistema basado en transacciones? ¿Distribuido o centralizado?¿Estaciones de trabajo o terminales? Esquematizar el sistema propuesto: Transformar los requerimientos del usuario de la fase anterior en unas especificaciones funcionales (DFD, Organigramas, etc). Construir el diccionario de datos (DD): Describir todos los elementos del DFD incluyendo funciones y datos; asegurarse de que todas las relaciones inter-funcionales y entre datos sean documentadas. Si existe DD de la empresa, hacerlo compatible con el que estamos realizando. Revisar y expandir el análisis de coste beneficio: Actualizarlo con la información nueva y verificar que los beneficios esperados se mantienen y que el plazo de recuperación de una inversión sigue siendo aceptable. Realización de una estimación detallada de costes, planificación, recursos, etc, de la siguiente fase (Diseño del sistema). Producir una estimación revisada de costes, planificación, recursos, etc, para el resto del proyecto. Producir el documento de especificación del sistema. Realizar una revisión final del documento de especificación del sistema. Tomar la decisión de continuar o no con el proyecto. Definir las responsabilidades en la próxima fase para el director, miembros del equipo de desarrollo y otros. Diseño: Producir el diseño global del sistema, contendrá: Definir los programas y sus principales funciones. Definir los principales flujos de datos entre programas y funciones. Diseñar el esquema de datos lógico y físico. Definir las fronteras con paquetes software, si existen. Definir los entornos de hardware y software, proponiendo alternativas. Localización de paquetes software: Buscar paquetes software apropiados que puedan implementar parte, o toda la funcionalidad requerida del sistema de forma rentable y que, si se implementa, ofrezca un entorno compatible con los objetivos de la organización. (Puede realizarse antes del diseño, o de forma simultánea a la tarea anterior). Desarrollar un diseño detallado del sistema, para cada alternativa de diseño planteada: Crear una descripción narrativa detallada del diseño para todo el sistema y cada una de sus partes (programas, funciones y datos). Actualizar el diccionario de datos. Definir los componentes hardware específicos (Capturadores de datos, sistemas de comunicación, etc) y sus funciones. Validar el diseño con las especificaciones del sistema. Documentar el entorno hardware y software necesarios para esta alternativa. Revisar y expandir el análisis de coste beneficio para cada alternativa: Actualizar con la información nueva. Verificar que los beneficios esperados se mantienen y que el plazo de recuperación de la inversión sigue siendo aceptable. Evaluar las alternativas de diseño, para cada alternativa, documentar: Requerimientos de usuario que se alcanzan con esta alternativa. Nivel de aceptación esperado de los usuarios. Realización de una estimación detallada de costes, planificación, recursos, etc, de la siguiente fase (Codificación) con esta alternativa. Producir una estimación revisada de costes, planificación, recursos, etc, para el resto del proyecto. Alternativa recomendada. Desarrollo de un plan de test del sistema: Crear datos de entrada del test. Producir el listado de los resultados esperados. Producir el listado de los criterios de test. Desarrollar la planificación de test del sistema. Desarrollar un plan de test diferenciado para cada alternativa. Identificar las necesidades de entrenamiento y documentación de los usuarios. Definir las guías de: Documentación completa de usuario. Manuales de operador. Documentos y planificación de formación para usuarios y operadores. Producir el documento de diseño del sistema. Realizar una revisión final del documento de diseño del sistema. Tomar la decisión de continuar o no con el proyecto. Recomendar una alternativa. Definir las responsabilidades de la próxima fase para el director, miembros de los equipo de programación y test, así como de otros implicados. Codificación: Producir un plan de trabajo: Creación de la lista detallada de tareas necesarias para realizar la codificación y test de todos los componentes del sistema. Producir una planificación para las tareas anteriores con las fechas más tempranas y más tardías, así como la asignación de responsabilidades. Instaurar los procedimientos para recoger los progresos y estados del proyecto. Instaurar los procedimientos para recoger tiempos, si resulta apropiado. Obtener la aprobación del plan de trabajo por parte de la dirección. Realización del diseño detallado de cada programa: Diseñar detalladamente los diagramas: De estructura de los programas. De estructura de los ficheros. Pantallas, informes, y otras composiciones. Esquemas de la base de datos. Composición de las tablas y sus diseños. Pseudocódigo de la lógica del programa (Dependerá de los métodos de diseño utilizados). Codificar, documentar y pasar los test en cada programa: Codificar el programa. Realizar las pruebas de unidad, hasta que los programas se adapten a las especificaciones descritas en las etapas anteriores. Actualizar todo lo necesario en el sistema y en el DD de la organización. Realizar el test de integración: Poner todos los programas probados en la librería de pruebas de integración. Realizar el test de integración de cada programa. Documentar todos los resultados del test de integración. Terminar los manuales de operador y usuario, así como los de formación. Realización de una estimación detallada de costes, planificación, recursos, etc, de la siguiente fase (Prueba del sistema). Producir una estimación revisada de costes, planificación, recuros, etc, para el resto del proyecto. Confeccionar el documento de diseño de programas y codificación. Realizar revisiones del documento de diseño de programas y codificación. Obtener los resultados finales de la integración completa del sistema y de las pruebas de integración. Definir las responsabilidades en la próxima fase para el director, miembros del equipo de test, así como de otros implicados. Pruebas: Realizar el test del sistema: Hacer el test de sistema de acuerdo al documento de test del sistema. Verificar la operatividad de los manuales de usuario y operador, utilizándolas en los cursos de formación de los usuarios y operadores que realicen el test del sistema. Verificar los documentos de entrenamiento de usuarios y operadores, utilizándolos en los cursos de formación de los usuarios y operadores que realicen el test del sistema. Documentar completamente los resultados del test del sistema. Revisar la planificación de instalación: Disponibilidad de los recursos. Revisión de los factores de contingencia que puedan afectar a la instalación: Procesos especiales de final de mes y fin de año. Vacaciones y fiestas. Disponibilidad de soporte por parte de otros proveedores. Revisión final del calendario de instalación. Esbozar el plan de contingencia ante caídas del sistema: Criterios para las caídas. Identificación de recursos para contingencias. Horario para recuperaciones o abandonos. Desarrollar un acuerdo de nivel de servicio: Criterios de rendimiento de usuario, precisión y volumen. Criterios de apoyo de los proveedores: Tiempo medio entre fallos. Tiempo medio de reparación. Criterios de calidad del sistema. Frecuencia con la que se medirán los criterios. Producir los documentos de test en la entrega. Revisión y aprobación de los documentos de entrega. Aprobación de la documentación del sistema. Documentación de programas. Manuales de operador. Manuales de usuario. Manuales de formación. Documentación de ayuda. Aprobación del plan de instalación. Aprobación de los planes de contingencia, recuperación y caídas. Finalización del sistema completamente probado: Documento de finalización del desarrollo del sistema. Documento de finalización de los usuarios. Documento de finalización del CPD. Documento de finalización de garantía de calidad. Documento de finalización de finanzas. Instalación: Instalación de hardware y software nuevo. Formar a los primeros usuarios y operadores. Desarrollar los planes de contingencia, recuperación y caída. Desarrollar los procedimientos de mantenimiento y versiones. Establecer procedimientos para: Versiones regulares. Versiones de emergencia. Versión por configuración, si existen diferentes tipos de hardware. Llevar a cabo cualquier conversión de datos necesaria. Llevar a cabo la instalación del sistema nuevo a producción: Instalación completa desde cero. Instalación en paralelo. Instalación por fases. Planificar y programar las revisiones post-instalación. Establecer los criterios de: Rendimiento del sistema. Calidad del sistema. Satisfacción del usuario. Calidad y facilidad de Gestión de: manuales de usuario y operador, formación de usuarios y operadores e información y datos producidos. Fluidez de la instalación. Costes de desarrollo, instalación, operaciones y mantenimiento. Establecer planificación y calendario de las revisiones, asegurando la disponibilidad del personal y documentación. Llevar a cabo las revisiones post-instalación: Crear el informe de la revisión post-instalación. Obtener la aprobación firmada de los informes de: Usuarios finales del sistema. Operadores del sistema. Auditoria y garantía de la calidad. Desarrollo de sistemas. Soporte de sistemas y mantenimiento. Finanzas. Obtener la carta de aprobación del sistema. Establecer el calendario para otras revisiones post-instalación si es necesario. Mantenimiento: Implementar los cambios del sistema: Utilizar los procedimientos de implementación de versiones. Implementar versiones de emergencias. Asegurarse de que el sistema continúa solucionando las necesidades de los usuarios: Utilizar los acuerdos de niveles de soporte, en estos acuerdos se establecen los requerimientos de soporte y objetivos de funcionamiento: Revisiones regulares de requerimientos del nivel de acuerdo. Revisiones regulares de como el sistema está alcanzando sus objetivos. Llevar a cabo revisiones regulares del sistema: Utilizar los procedimientos y contenido de las revisiones post-instalación. Estas tareas se han enumerado a modo de lista de comprobación, de forma que serán los desarrolladores los encargados de identificar las tareas apropiadas a cada proyecto así como los recursos necesarios, teniendo en cuenta la estimación previa del esfuerzo. Tareas y Funciones de los Distintos AgentesSon personas y organizaciones que participan activamente en el proyecto o cuyos intereses pueden ser afectados positiva o negativamente tanto por el resultado de la ejecución del proyecto como por su terminación exitosa. Los principales agentes en cada proyecto pueden ser: Jefe de proyecto. Responsable de la planificación y ejecución el proyecto. Equipo de desarrollo. Encargado de realizar el proyecto. Cliente. Es el que arriesga su dinero en el desarrollo, es decir, el que pagará por el sistema. Usuarios. Personas que utilizarán el sistema a nivel operativo y que normalmente pertenecen al cliente. Nos dan pistas sobre el problema a nivel de funcionamiento. Son responsables de que el sistema funcione de manera eficiente. El Jefe de ProyectoLa misión del jefe de proyecto es: Con el cliente: Dejarlo satisfecho. Incrementar su competitividad y/o desempeño interno a través de la solución que se le entregue. Con el negocio: Lograr rentabilidad. Aprovechar recursos al máximo. Con los recursos humanos: Crecimiento profesional. Satisfacción interna y externa. Al jefe de proyecto se le concede una amplia autoridad sobre los recursos del proyecto y puede adquirir nuevos recursos ya sea dentro o fuera de la organización. Todo el personal del proyecto está bajo su autoridad mientras dure el proyecto. Debe combinar conocimiento técnico en la materia además de habilidades de dirección para poder dirigir a todo el personal del proyecto. Las interacciones que tiene el jefe de proyecto dentro de su organización son: Con su equipo de trabajo. Con el ejecutivo de cuenta. Con el departamento de calidad. Las interacciones que tiene el jefe de proyecto con el cliente son: Con el usuario operativo. Con el departamento de sistemas. Con el experto funcional del negocio. Con otras áreas. Responsabilidades del Jefe de Proyecto Conocer los criterios de negociación acordados con el cliente. Notificar al ejecutivo de cuenta los cambios al alcance para que los renegocie adecuadamente. Evaluar el desempeño de cada persona en base al cumplimiento de los compromisos acordados. Informar a la dirección de manera justificada y en tiempo sobre la planificación de la asignación y liberación de recursos. Informar los requerimientos de formación, evaluaciones y vacaciones de los miembros del equipo de trabajo al director. Informar de avances e incidentes del proyecto. Asegurar que el proyecto sea rentable y productivo. Dar seguimiento al plan de trabajo y corregir desviaciones a tiempo. Asegurar la obtención de los recursos materiales indispensables para desarrollar el proyecto. Autoridad del Jefe de Proyecto Organizar al equipo de trabajo como sea más conveniente y productivo. Definir las expectativas de crecimiento para los miembros del equipo de trabajo. Implementar las medidas necesarias para que las expectativas de cada persona se cumplan. Prescindir de los servicios de un miembro del equipo de trabajo, si hay motivos para apoyar esta decisión. Bibliografia Scribd (tfandos) Planificación del desarrollo. Técnicas de planificación. Metodologías de desarrollo. La metodología Métrica.Técnicas de PlanificaciónIntroducción a la Planificación del DesarrolloEl objetivo de la Planificación del proyecto Software es proporcionar un marco de trabajo que permita al gestor hacer estimaciones razonables de recursos, costos y planificación temporal. Estas estimaciones se hacen dentro de un marco de tiempo limitado al comienzo de un proyecto de software, y deberían actualizarse regularmente a medida que progresa el proyecto. Además las estimaciones deberían definir los escenarios del mejor caso, y peor caso, de modo que los resultados del proyecto pueden limitarse. El Objetivo de la planificación se logra mediante un proceso de descubrimiento de la información que lleve a estimaciones razonables. Existen diversos tipos de planes: Aunque hay que tener en cuenta que la planificación conlleva una serie de problemas añadidos: Es difícil estimar la longitud y dificultad de las tareas, por lo que la estimación del coste es más difícil. La productividad no es proporcional al número de personas trabajando en una tarea. Incluir personal en un proyecto en avance, retrasa el proyecto por sobrecargas en la comunicación. Lo inesperado siempre sucede. Es necesario considerar siempre contingencias. Herramientas Básicas Usadas en la PlanificaciónAunque desde la antigüedad se han realizado proyectos de gran envergadura como por ejemplo la construcción de edificios públicos, guerras, viajes, etc, no es hasta principios de este siglo cuando aparece el conocido diagrama de Gantt en el que se refleja de forma esquemática las tareas, su duración y las fechas en que se deberán realizar. Trabajando sobre este diagrama el director de proyecto realizaba planificaciones y seguimiento de un proyecto. Dada la evolución tecnológica los seres humanos cada vez abordamos proyectos más complejos, pero por otra parte creamos técnicas más evolucionadas, completas y automáticas para gestionar estos proyectos. La construcción del misil Polaris, así como la solución de los problemas en la gestión de la producción de Dunlop llevaron al desarrollo de las técnicas conocidas como PERT (Técnica para la Evaluación y Revisión de Programas) y CMP (Método del Camino Crítico) que aportan a la programación de proyectos técnicas matemáticas. Estas técnicas surgieron de la necesidad de obtener algoritmos automatizables que ayudasen a los gestores de proyectos complejos en la construcción de calendarios (programas). En el siguiente punto veremos mediante un ejemplo como se aplica en el CPM. Diagrama de Gantt. Aplicación de la Técnica CPMEl CPM se realiza sobre un proyecto en cuatro etapas, a continuación se describe cada una de ellas. Etapa 1. Identificar las Tareas Se deben identificar cada una de las tareas que forman parte del desarrollo del proyecto. Si queremos realizar el proceso de forma manual, rellenaremos una ficha por cada actividad identificada. El formato de la ficha será el que se muestra en la figura. Etapa 2. Añadir Recursos y Tiempos A cada actividad se le asignarán recursos (personas, material, equipos, etc) y tiempo estimado para su realización, completando la ficha. Etapa 3. Ordenar las Tareas En esta etapa se tienen que organizar las tareas en base al orden técnico de ejecución. Así sabemos que hay que hacer las especificaciones antes de diseñar el programa. Nos podemos plantear las siguientes preguntas para ordenar las tareas: ¿Qué se puede hacer ahora? ¿Qué debe haberse hecho antes de esto? ¿Qué podría hacerse a la vez? ¿Qué debe seguir a lo que hacemos ahora? Si se trata de calcular el Camino Crítico de forma manual será interesante pinchar todas las tareas en un tablero de corcho, señalando mediante cuerdas la ordenación de las tareas. Esta representación es conocida como red de procedencia , aunque su apariencia es diferente al gráfico PERT en algunos programas informáticos se describe erróneamente con este nombre. Si tenemos un diagrama complejo, y queremos realizar los cálculos de forma manual se puede utilizar el método que se describe a continuación. Este diagrama se compone de nodos y arcos, similares a las pegatinas comentadas anteriormente. Los nodos representan a las tareas y la información necesaria para calcular sus fecha de realización. Los arcos indican las precedencias entre tareas. Vamos a representar cada nodo (tarea) como se ve en la figura. Donde: DESCRIPCIÓN DE LA ACTIVIDAD es el nombre que le hemos dado a la actividad. Por ejemplo: Codificación Programa A. Etiqueta actividad es un número arbitrario y que identifica unívocamente a cada actividad. Duración es el tiempo que calculamos que se tardará en completar la tarea, teniendo en cuenta el esfuerzo y los recursos asignados a la tarea. Por ejemplo una tarea que estimamos requerirá seis días-hombre de esfuerzo, si se realiza entre tres personas podría tener una duración de dos días. Inicio temprano es la fecha en que se podrá comenzar la tarea si no se retrasa ninguna otra. Final temprano es, en el caso de iniciarse la tarea en el inicio temprano, lo antes que puede finalizar, respetando su duración. Inicio tardío es la fecha más retrasada en la que puede comenzar la tarea para que se pueda completar el proyecto en la fecha marcada como final del proyecto. Final tardío es la fecha más retrasada en la que puede terminar la tarea para que se pueda completar el proyecto en la fecha marcada como final del proyecto. Máximo tiempo disponible es el tiempo máximo que puede durar una tarea en caso de comenzar en su Inicio temprano y concluir en su Final tardío. Holgura es la diferencia entre el Máximo tiempo disponible y su Duración. Etapa 4. Cálculo del Camino Crítico Una vez tenemos todas las tareas con sus respectivas duraciones y las precedencias pasamos a dibujar una red en la que aparezca para cada tarea una caja similar a la vista en el punto anterior con casi todos los campos vacíos. Entre ellas aparecerán los arcos indicando precedencias, tendremos algo similar a la figura. Ahora calculamos las fechas tempranas. Para esto seguimos los siguientes pasos: En aquellas tareas que no tienen ningún predecesor se le asigna a Inicio temprano el valor 0. Si la tarea tiene predecesoras y todas estas tienen calculado su Final temprano se toma como Inicio temprano el máximo de todos ellos. El Final temprano de cada tarea se calcula como el Inicio temprano más la Duración . Repetiremos estos pasos hasta que todas las tareas tengan sus fechas tempranas. Para calcular las fechas tardías procederemos con los pasos que se describen a continuación: Se obtiene la fecha de finalización de proyecto más tardía. Esta puede venir dada por algún tipo de razones externas o puede que se nos pida que el proyecto termine lo antes posible, en este caso la fecha de finalización más tardía será el máximo de los “ Final temprano ” de todas las tareas. A aquellas tareas que no sean predecesoras de ninguna otra se les asigna como Final tardío la fecha de finalización más tardía del punto 4. El Inicio tardío se calcula restando al Final tardío la Duración . En aquellas tareas que son predecesoras de otras se calcula el Final tardío como el mínimo de los Inicio tardío de las tareas de que es predecesora. Los otros dos campos de cada tarea: Máximo tiempo disponible y Holgura se calculan mediante las siguientes fórmulas: Máximo tiempo disponible = Final tardío - Inicio tempranoHolgura = Máximo tiempo disponible - Duración Obtención del Camino Crítico Llamamos camino crítico de una planificación al conjunto de tareas que tienen Holgura cero. Siempre que se solicita que el proyecto tenga la duración mínima tendremos un camino crítico. Se le llama camino crítico porque suele ser un camino que parte de una tarea que no tiene predecesoras y atraviesa el grafo por tareas con holgura cero hasta terminar en una tarea que no es predecesora de ninguna otra. Puede darse el caso de que con el “camino crítico” se puedan construir varias secuencias, partiendo de tareas sin predecesoras y se alcancen tareas sin sucesoras. A las tareas del camino crítico se les llama tareas críticas y esto se debe a que un retraso en cualquiera de ellas lleva a un retraso del final del proyecto. Diferencia Fundamental entre el CPM y el PERTAunque en principio son similares los algoritmos de ambos métodos, la asignación de duraciones de las tareas en el PERT es algo más elaborada. En lugar de realizarse una sola estimación se realizan tres estimaciones: “tm” tiempo medio que se estima para la actividad. “to” tiempo optimista, el que resultaría de ir todo muy bien. “tp” tiempo pesimista, el que resultaría si todo fuese mal en esta tarea. A la tares se le asigna como duración el resultado de: Duración = (to + 4 tm + tp) / 6 Por otra parte el grafo se construye de forma dual a la vista. Los arcos modelan las actividades o tareas, mientras que los nodos modelan la relación de precedencia de las tareas. Así un nodo indica que los arcos que llegan a él anteceden a los que salen de él. Uso de Aplicaciones para la Planificación Control de ProyectosComo hemos indiciado estos algoritmos se hicieron pensando en el uso de sistemas de cómputo automático, así que no es de extrañar que existan muchas aplicaciones que den soporte a éstos. Entre las más conocidas que funcionan sobre PC están el CA-SuperProject y el Microsoft Project. Metodologías de Desarrollo: La Metodología Métrica 3Introducción a la Metodología Métrica 3Antes de describir una metodología, es necesario aclarar ciertas cuestiones, tales como: ¿Qué es una metodología? ¿Para qué sirve? Por que la respuesta a estas preguntas va a ser crucial para poder entender y evaluar toda la exposición siguiente sobre la metodología Métrica 3. ¿Qué es una Metodología? Según el Diccionario de la Lengua Española de la Real Academia, metodología es “Ciencia del método” o “Conjunto de métodos que se siguen en una investigación científica o en una exposición doctrinal”. Aunque existe otra un poco más certera; “Modo de decir o hacer con orden una cosa”, parece que esto aclara algo más, lo que veremos en el tema es un modo o manera concreto (Métrica 3) de hacer con orden una cosa (desarrollo de sistemas de información). El desarrollo de sistemas de información es una tarea de resolución de problemas, en la que el problema a resolver consiste en traducir: La situación-problema a un sistema de representación útil para nuestros propósitos (análisis de requisitos). La representación resultante de la fase anterior en otra que vaya acercando los objetos, conceptos y relaciones a una forma inteligible por nuestro dominio objetivo (máquinas, personas, otros sistemas de información); de esta manera realizaremos el diseño, desde el cual, en un proceso de traducción final, codificaremos los programas de ordenador, las instrucciones para los usuarios, las interfaces con otros sistemas, etc. ¿Para qué sirve? En primer lugar como guía; nos dice lo que tenemos que hacer, cómo, cuándo y quién tiene que hacerlo; además, lo hace de manera completa; podremos saltarnos los pasos que consideremos convenientes siempre que comprendamos la estructura del método y podamos evaluar la conveniencia de utilizar atajos. En segundo lugar, determina los puntos del proceso en los que debemos detenernos y comprobar cómo vamos, algo bastante importante y que evita la propagación de los errores a través del proceso. En tercer lugar, permite que los conocimientos adquiridos en el desarrollo de sistemas de información se plasmen en el método que la organización utiliza para ello mediante su continua revisión y adaptación y pasen a ser patrimonio de la organización y no solo de las personas que llevan a cabo la tarea. Objetivos La metodología MÉTRICA Versión 3 ofrece a las Organizaciones un instrumento útil para la sistematización de las actividades que dan soporte al ciclo de vida del software dentro del marco que permite alcanzar los siguientes objetivos: Proporcionar o definir Sistemas de Información que ayuden a conseguir los fines de la Organización mediante la definición de un marco estratégico para el desarrollo de los mismos. Dotar a la Organización de productos software que satisfagan las necesidades de los usuarios dando una mayor importancia al análisis de requisitos. Mejorar la productividad de los departamentos de Sistemas y Tecnologías de la Información y las Comunicaciones, permitiendo una mayor capacidad de adaptación a los cambios y teniendo en cuenta la reutilización en la medida de lo posible. Facilitar la comunicación y entendimiento entre los distintos participantes en la producción de software a lo largo del ciclo de vida del proyecto, teniendo en cuenta su papel y responsabilidad, así como las necesidades de todos y cada uno de ellos. Facilitar la operación, mantenimiento y uso de los productos software obtenidos. Características La nueva versión de MÉTRICA contempla el desarrollo de Sistemas de Información para las distintas tecnologías que actualmente están conviviendo y los aspectos de gestión que aseguran que un Proyecto cumple sus objetivos en términos de calidad, coste y plazos. Su punto de partida es la versión anterior de MÉTRICA de la cual se han conservado la adaptabilidad, flexibilidad y sencillez, así como la estructura de actividades y tareas, si bien las fases y módulos de MÉTRICA versión 2.1 han dado paso a la división en Procesos, más adecuada a la entrada-transformación-salida que se produce en cada una de las divisiones del ciclo de vida de un proyecto. Para cada tarea se detallan los participantes que intervienen, los productos de entrada y de salida así como las técnicas y prácticas a emplear para su obtención. En la elaboración de MÉTRICA Versión 3 se han tenido en cuenta los métodos de desarrollo más extendidos, así como los últimos estándares de ingeniería del software y calidad, además de referencias específicas en cuanto a seguridad y gestión de proyectos. También se ha tenido en cuenta la experiencia de los usuarios de las versiones anteriores para solventar los problemas o deficiencias detectados. Estructura En una única estructura la metodología MÉTRICA Versión 3 cubre distintos tipos de desarrollo: estructurado y orientado a objetos, facilitando a través de interfaces la realización de los procesos de apoyo u organizativos: Gestión de Proyectos, Gestión de Configuración, Aseguramiento de Calidad y Seguridad. La automatización de las actividades propuestas en la estructura de MÉTRICA Versión 3 es posible ya que sus técnicas están soportadas por una amplia variedad de herramientas de ayuda al desarrollo. Cada Proceso detalla las Actividades y Tareas a realizar. Para cada tarea se indican: Las técnicas y prácticas a utilizar. Los responsables de realizarla. Sus productos de entrada y salida. La estructura de procesos es la siguiente: Planificación (PSI) Desarrollo Estudio de Viabilidad (EVS) Análisis (ASI) Diseño (DSI) Construcción (CSI) Implantación y Aceptación (IAS) Mantenimiento (MSI) Las interfaces son las siguientes: Gestión de la Configuración. Aseguramiento de la Calidad. Gestión de Proyectos. Seguridad. Planificación de Sistemas de Información (PSI)Descripción y Objetivos El Plan de Sistemas de Información tiene como objetivo la obtención de un marco de referencia para el desarrollo de sistemas de información que responda a los objetivos estratégicos de la organización. Este marco de referencia consta de: Una descripción de la situación actual, que constituirá el punto de partida del Plan de Sistemas de Información. Dicha descripción incluirá un análisis técnico de puntos fuertes y riesgos, así como el análisis de servicio a los objetivos de la organización. Un conjunto de modelos que constituya la arquitectura de información. Una propuesta de proyectos a desarrollar en los próximos años, así como la prioridad de realización de cada proyecto. Una propuesta de calendario para la ejecución de dichos proyectos. La evaluación de los recursos necesarios para los proyectos a desarrollar en el próximo año, con el objetivo de tenerlos en cuenta en los presupuestos. Para el resto de proyectos, bastará con una estimación de alto nivel. Un plan de seguimiento y cumplimiento de todo lo propuesto mediante unos mecanismos de evaluación adecuados. La perspectiva del plan debe ser estratégica y operativa, no tecnológica. Es fundamental que la alta dirección de la organización tome parte activa en la decisión del Plan de Sistemas de Información con el fin de posibilitar su éxito. La dirección debe convencer a sus colaboradores más directos de la necesidad de realización del plan; de su apoyo de forma constructiva, mentalizándose de que la ejecución del mismo requerirá la utilización de unos recursos de los cuales son responsables. La presentación del Plan de Sistemas de Información y la constitución del equipo supone el arranque del proyecto y es fundamental que las más altas instancias de la organización estén implicadas en ambos, dando el apoyo necesario y aportando todo tipo de medios. Explicar el plan a las personas de la organización y a las unidades organizativas afectadas sobre las que recaerá el Plan, el apoyo de los altos directivos y la calificación de los recursos de las distintas unidades implicadas, serán factores críticos de éxito del Plan de Sistemas de Información. El nivel de detalle con el que se hará el estudio de la situación actual dependerá de la existencia de documentación actual, de si hay personas que conozcan dicha documentación y de la predisposición a una sustitución total o parcial por sistemas de información nuevos. En cualquier caso, como paso previo para detectar aspectos importantes que puedan afectar a la organización, es necesario investigar sus puntos fuertes, áreas de mejora, riesgos y amenazas posibles y hacer un diagnóstico de los mismos. Para la elaboración del Plan de Sistemas de Información se estudian las necesidades de información de los procesos de la organización afectados por el Plan, con el fin de definir los requisitos generales y obtener modelos conceptuales de información. Por otra parte se evalúan las opciones tecnológicas y se propone un entorno. Tras analizar las prioridades relacionadas con las distintas variables que afectan a los sistemas de información, se elabora un calendario de proyectos con una planificación lo más detallada posible de los más inmediatos. Además, se propone una sistemática para mantener actualizado el Plan de Sistemas de Información para incluir en él todos los cambios necesarios, garantizando el cumplimiento adecuado del mismo. A continuación se incluye un gráfico que representa la secuencia de actividades del proceso PSI. Aunque los resultados de la actividad Estudio de Información Relevante (PSI 3) deberán tenerse en cuenta para la definición de requisitos que se efectúa en la actividad Identificación de Requisitos (PSI 4), ambas podrán realizarse en paralelo, junto con el Estudio de los Sistemas de Información Actuales (PSI 5) . Actividad PSI 1: Inicio del Plan de Sistemas de Información El objetivo de esta actividad es determinar la necesidad del Plan de Sistemas de Información y llevar a cabo el arranque formal del mismo, con el apoyo del nivel más alto de la organización. Como resultado, se obtiene una descripción general del Plan de Sistemas de Información que proporciona una definición inicial del mismo, identificando los objetivos estratégicos en los que apoya, así como el ámbito general de la organización al que afecta, lo que permite implicar a las direcciones de las áreas afectadas por el Plan de Sistemas de Información. Además, se identifican los factores críticos de éxito y los participantes en el Plan de Sistemas de Información, nombrando a los máximos responsables. A continuación se incluye una table resumen con las tareas de la presente actividad: Actividad PSI 2: Definición y Organización del PSI En esta actividad se detalla el alcance del plan, se organiza el equipo de personas que lo va a llevar a cabo y se elabora un calendario de ejecución. Todos los resultados o productos de esta actividad constituirán un marco de actuación del proyecto más detallado que en PSI 1 en cuanto a objetivos, procesos afectados, participantes, resultados y fechas de entrega. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad PSI 3: Estudio de la Información Relevante El objetivo de esta actividad es recopilar y analizar todos los antecedentes generales que puedan afectar a los procesos y a las unidades organizativas implicadas en el Plan de Sistemas de Información, así como a los resultados del mismo. Pueden ser de especial interés los estudios realizados con anterioridad al Plan de Sistemas de Información, relativos a los sistemas de información de su ámbito, o bien a su entorno tecnológico, cuyas conclusiones deben ser conocidas por el equipo de trabajo del Plan de Sistemas de Información. La información obtenida en esta actividad se tendrá en cuenta en la elaboración de los requisitos. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad PSI 4: Identificación de Requisitos El objetivo final de esta actividad va a ser la especificación de los requisitos de información de la organización, así como obtener un modelo de información que los complemente. Para conseguir este objetivo, se estudia el proceso o procesos de la organización incluidos en el ámbito del Plan de Sistemas de Información. Para ello es necesario llevar a cabo sesiones de trabajo con los usuarios, analizando cada proceso tal y como debería ser, y no según su situación actual, ya que ésta puede estar condicionada por los sistemas de información existentes. Del mismo modo, se identifican los requisitos de información, y se elabora un modelo de información que represente las distintas entidades implicadas en el proceso, así como las relaciones entre ellas. Por último, se clasifican los requisitos identificados según su prioridad, con el objetivo de incorporarlos al catálogo de requisitos del Plan de Sistemas de Información. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad PSI 5: Estudio de los Sistemas de Información Actuales El objetivo de esta actividad es obtener una valoración de la situación actual al margen de los requisitos del catálogo, apoyándose en criterios relativos a facilidad de mantenimiento, documentación, flexibilidad, facilidad de uso, etc. En esta actividad se debe tener en cuenta la opinión de los usuarios, ya que aportarán elementos de valoración, como por ejemplo, su nivel de satisfacción con cada sistema de información. Se seleccionan los sistemas de información actuales que son objeto del análisis y se lleva a cabo el estudio de los mismos con la profundidad y el detalle que se determine conveniente en función de los objetivos definidos para el Plan de Sistemas de Información. Este estudio permite, para cada sistema, determinar sus carencias y valorarlos. Actividad PSI 6: Diseño del Modelo de Sistemas de Información El objetivo de esta actividad es identificar y definir los sistemas de información que van a dar soporte a los procesos de la organización afectados por el Plan de Sistemas de Información. Para ello, en primer lugar, se analiza la cobertura que los sistemas de información actuales dan a los requisitos recogidos en el catálogo elaborado en las actividades Estudio de la Información Relevante (PSI 3) e Identificación de Requisitos (PSI 4). Esto permitirá efectuar un diagnóstico de la situación actual, a partir del cual se seleccionan los sistemas de información actuales considerados válidos, identificando las mejoras a realizar en los mismos. Por último, se definen los nuevos sistemas de información necesarios para cubrir los requisitos y funciones de los procesos no soportados por los sistemas actuales seleccionados. Teniendo en cuenta los resultados anteriores, se elabora el modelo de sistemas de información válido para dar soporte a los procesos de la organización incluidos en el ámbito del Plan de Sistemas de Información. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad PSI 7: Definición de la Arquitectura Tecnológica En esta actividad se propone una arquitectura tecnológica que dé soporte al modelo de información y de sistemas de información incluyendo, si es necesario, opciones. Para esta actividad se tienen en cuenta especialmente los requisitos de carácter tecnológico, aunque es necesario considerar el catálogo completo de requisitos para entender las necesidades de los procesos y proponer los entornos tecnológicos que mejor se adapten a las mismas. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad PSI 8: Definición del Plan de Acción En el Plan de Acción, que se elabora en esta actividad, se definen los proyectos y acciones a llevar a cabo para la implantación de los modelos de información y de sistemas de información, determinados en las actividades Identificación de Requisitos (PSI 4) y Diseño del Modelo de Sistemas de Información (PSI 6), con la arquitectura tecnológica propuesta en la actividad Definición de la Arquitectura Tecnológica (PSI 7), el conjunto de estos tres modelos constituye la arquitectura de información. Dentro del Plan de Acción se incluye un calendario de proyectos, con posibles alternativas, y una estimación de recursos, cuyo detalle será mayor para los más inmediatos. Para la elaboración del calendario se tienen que analizar las distintas variables que afecten a la prioridad de cada proyecto y sistema de información. El orden definitivo de los proyectos y acciones debe pactarse con los usuarios, para llegar a una solución de compromiso que resulte la mejor posible para la organización. Por último, se propone un plan de mantenimiento para el control y seguimiento de la ejecución de los proyectos, así como para la actualización de los productos finales del Plan de Sistemas de Información. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad PSI 9: Revisión y Aprobación del PSI Esta actividad tiene como objetivo contrastar con los responsables de la dirección del Plan de Sistemas de Información la arquitectura de información y el plan de acción elaborados anteriormente, para mejorar la propuesta si se considera necesario y por último, obtener su aprobación final. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Estudio de Viabilidad del Sistema (EVS)Descripción y Objetivos Mientras que el Plan de Sistemas de Información tiene como objetivo proporcionar un marco estratégico que sirva de referencia para los Sistemas de Información de un ámbito concreto de una organización, el objetivo del Estudio de Viabilidad del Sistema es el análisis de un conjunto concreto de necesidades para proponer una solución a corto plazo, que tenga en cuenta restricciones económica, técnicas, legales y operativas. Las solución obtenida como resultado del estudio puede ser la definición de uno o varios proyectos que afecten a uno o varios sistemas de información y a existentes o nuevos. Para ello, se identifican los requisitos que se ha de satisfacer y se estudia, si procede, la situación actual. A partir del estado inicial, la situación actual y los requisitos planteados, se estudian las alternativas de solución. Dichas alternativas pueden incluir soluciones que impliquen desarrollos a medida, soluciones basadas en la adquisición de productos software del mercado o soluciones mixtas. Se describe cada una de las alternativas, indicando los requisitos que cubre. Una vez descritas cada una de las alternativas planteadas, se valora su impacto en la organización, la inversión a realizar en cada caso y los riesgos asociados. Esta información se analiza con el fin de evaluar las distintas alternativas y seleccionar la más adecuada, definiendo y estableciendo su planificación. Si en la organización se ha realizado con anterioridad un Plan de Sistemas de Información que afecte al sistema objeto de este estudio, se dispondrá de un conjunto de productos que proporcionarán información a tener en cuenta en todo el proceso. Las actividades que engloba este proceso se recogen en la siguiente figura, en la que se indican las actividades que pueden ejecutarse en paralelo y las que precisan para su realización resultados originados en actividades anteriores. Actividad EVS 1: Establecimiento del Alcance del Sistema En esta actividad se estudia el alcance de la necesidad planteada por el cliente o usuario, o como consecuencia de la realización de un PSI, realizando una descripción general de la misma. Se determinarán los objetivos, se inicia el estudio de los requisitos y se identifican las unidades organizativas afectadas estableciendo su estructura. Se analizan las posibles restricciones, tanto generales como específica, que puedan condicionar el estudio y la planificación de las alternativas de solución que se propongan. Si la justificación económica es obvia, el riesgo técnico bajo, se esperan pocos problemas legales y no existe ninguna alternativa razonable, no es necesario profundizar en el estudio de viabilidad del sistema, analizando posibles alternativas y realizando una valoración y evaluación de las mismas, sino que éste se orientará a la especificación de requisitos, descripción del nuevo sistema y planificación. Se detalla la composición del equipo de trabajo necesario para este proceso y su planificación. Finalmente, con el fin de facilitar la implicación activa de los usuarios en la definición del sistema, se identifican sus perfiles, dejando claras sus tareas y responsabilidades. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad EVS 2: Estudio de la Situación Actual La situación actual es el estado en el que se encuentran los sistemas de información existentes en el momento en el que se inicia su estudio. Teniendo en cuenta el objetivo del estudio de la situación actual, se realiza una valoración de la información existente acerca de los sistemas de información afectados. En función de dicha valoración, se especifica el nivel de detalle con que se debe llevar a cabo el estudio. Si es necesario, se constituye un equipo de trabajo específico para su realización y se identifican los usuarios participantes en el mismo. Si se decide documentar la situación actual, normalmente es conveniente dividir el sistema actual en subsistemas. Si es posible se describirá cada uno de los subsistemas, valorando qué información puede ser relevante para la descripción. Como resultado de esta actividad se genera un diagnóstico, estimando la eficiencia de los sistemas de información existentes e identificando los posibles problemas y las mejoras. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad EVS 3: Definición de Requisitos del Sistema Esta actividad incluye la determinación de los requisitos generales, mediante una serie de sesiones de trabajo con los usuarios participantes, que hay que planificar y realizar. Una vez finalizadas, se analiza la información obtenida definiendo los requisitos y sus prioridades, que se añaden al catálogo de requisitos que servirá para el estudio y valoración de las distintas alternativas de solución que se propongan. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad EVS 4: Estudio de Alternativas de Solución Este estudio se centra en proponer diversas alternativas que respondan satisfactoriamente a los requisitos planteados, considerando también los resultado obtenidos en el Estudio de la Situación Actual (EVS 2), en el caso de que se haya realizado. Teniendo en cuenta el ámbito y funcionalidad que debe cubrir el sistema, puede ser conveniente realizar, previamente a la definición de cada alternativa, una descomposición del sistema en subsistemas. En la descripción de las distintas alternativas de solución propuestas, se debe especificar si alguna de ellas está basada, total o parcialmente, en un producto existente en el mercado. Si la alternativa incluye un desarrollo a medida, se debe incorporar en la descripción de la misma un modelo abstracto de datos y un modelo de procesos, y en orientación a objetos, un modelo de negocio y un modelo de dominio. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad EVS 5: Valoración de las Alternativas Una vez descritas las alternativas se realiza una valoración de las mismas, considerando el impacto en la organización, tanto desde el punto de vista tecnológico y organizativo como de operación, y los posibles beneficios que se esperan contrastados con los costes asociados. Se realiza también un análisis de los riesgos, decidiendo cómo enfocar el plan de acción para minimizar los mismos y cuantificando los recursos y plazos precisos para planificar cada alternativa. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad EVS 6: Selección de la Solución Antes de finalizar el Estudio de Viabilidad del Sistema, se convoca al Comité de Dirección para la presentación de las distintas alternativas de solución, resultantes de la actividad anterior. En dicha presentación, se debaten las ventajas de cada una de ellas, incorporando las modificaciones que se consideren oportunas, con el fin de seleccionar la más adecuada. Finalmente, se aprueba la solución o se determina su inviabilidad. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Análisis del Sistema de Información (ASI)Descripción y Objetivos El objetivo de este proceso es la obtención de una especificación detallada del sistema de información que satisfaga las necesidades de información de los usuarios y sirva de base para el posterior diseño del sistema. Al ser MÉTRICA Versión 3 una metodología que cubre tanto desarrollos estructurados como orientados a objetos, las actividades de ambas aproximaciones están integradas en una estructura común. En la primera actividad, Definición del Sistema (ASI 1), se lleva a cabo la descripción inicial del sistema de información, a partir de los productos generados en el proceso Estudio de Viabilidad del Sistema (EVS). Se delimita el alcance del sistema, se genera un catálogo de requisitos generales y se describe el sistema mediante unos modelos iniciales de alto nivel. También se identifican los usuarios que participan en el proceso de análisis, determinando sus perfiles, responsabilidades y dedicaciones necesarias. Así mismo se elabora el plan de trabajo a seguir. La definición de requisitos del nuevo sistema se realiza principalmente en la actividad. Establecimiento de Requisitos (ASI 2). El objetivo de esta actividad es elaborar un catálogo de requisitos detallado, que permita describir con precisión el sistema de información, y que además sirva de base para comprobar que es completa la especificación de los modelos obtenidos en las actividades Identificación de Subsistemas de Análisis (ASI 3), Análisis de Casos de Uso (ASI 4), Análisis de Clases (ASI 5), Elaboración del Modelo de Datos (ASI 6), Elaboración del Modelo de Procesos (ASI 7) y Definición de Interfaces de Usuario (ASI 8). Hay que hacer constar que estas actividades pueden provocar la actualización del catálogo, aunque no se refleja como producto de salida en las tareas de dichas actividades, ya que el objetivo de las mismas no es crear el catálogo sino definir modelos que soporten los requisitos. Para la obtención de requisitos en la actividad Establecimiento de Requisitos (ASI 2) se toman como punto de partida el catálogo de requisitos y los modelos elaborados en la actividad Definición del Sistema (ASI 1), completándolos mediante sesiones de trabajo con los usuarios. Estas sesiones de trabajo tienen como objetivo reunir la información necesaria para obtener la especificación detallada del nuevo sistema. Las técnicas que ayudan a la recopilación de esta información pueden variar en función de las características del proyecto y los tipos de usuario a entrevistar. Entre ellas podemos citar la reuniones, entrevistas, Join Application Design (JAD) , etc. Durante estas sesiones de trabajo se propone utilizar la especificación de los casos de uso como ayuda y guía en el establecimiento de requisitos. Esta técnica facilita la comunicación con los usuarios y en el análisis orientado a objetos constituye la base de la especificación. A continuación se identifican las facilidades que ha de proporcionar el sistema, y las restricciones a que está sometido en cuanto a rendimiento, frecuencia de tratamiento, seguridad y control de accesos, etc. Toda esta información se incorpora al catálogo de requisitos. En la actividad Identificación de Subsistemas de Análisis (ASI 3), se estructura el sistema de información en subsistemas de análisis, para facilitar la especificación de los distintos modelos y la traza de requisitos. En paralelo, se generan los distintos modelos que sirven de base para el diseño. En el caso de análisis estructurado, se procede a la elaboración y descripción detallada del modelo de datos y de procesos, y en el caso de un análisis orientado a objetos, se elaboran el modelo de clases y el de interacción de objetos, mediante el análisis de los casos de uso. Se especifican, asimismo, todas las interfaces entre el sistema y el usuario, tales como formatos de pantallas, diálogos, formatos de informes y formularios de entrada. En la actividad Análisis de Consistencia y Especificación de Requisitos (ASI 9), se realiza la verificación y validación de los modelos, con el fin de asegurar que son: Completos, puesto que cada modelo obtenido contiene toda la información necesaria recogida en el catálogo de requisitos. Consistentes, ya que cada modelo es coherente con el resto de los modelos. Correctos, dado que cada modelo sigue unos criterios de calidad predeterminados en relación a la técnica utilizada, calidad de diagramas, elección de nombre, normas de calidad, etc. En la actividad Especificación del Plan de Pruebas (ASI 10), se establece el marco general del plan de pruebas, iniciándose su especificación, que se completará en el proceso Diseño del Sistema de Información (DSI). La participación activa de los usuarios es una condición imprescindible para el análisis del sistema de información, ya que dicha participación constituye una garantía de que los requisitos identificados son comprendidos e incorporados al sistema y, por tanto, de que éste será aceptado. Para facilitar la colaboración de los usuarios, se pueden utilizar técnicas interactivas, como diseño de diálogos y prototipos, que permiten al usuario familiarizarse con el nuevo sistema y colaborar en la construcción y perfeccionamiento del mismo. En el siguiente gráfico se muestra la relación de actividades del proceso Análisis del Sistema de Información, tanto para desarrollos estructurados como para desarrollos orientados a objetos, distinguiendo las que se pueden realizar en paralelo de aquellas que han de realizarse secuencialmente. Actividad ASI 1: Definición del Sistema Esta actividad tiene como objetivo efectuar una descripción del sistema, delimitando su alcance, estableciendo las interfaces con otros sistemas e identificando a los usuarios representativos. Las tareas de esta actividad se pueden haber desarrollado ya en parte en el proceso de Estudio de Viabilidad del Sistema (EVS), de modo que se parte de los productos obtenidos en dicho proceso para proceder a su adecuación como punto de partida para definir el sistema de información. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad ASI 2: Establecimiento de Requisitos En esta actividad se lleva a cabo la definición, análisis y validación de los requisitos a partir de la información facilitada por el usuario, completándose el catálogo de requisitos obtenido en la actividad Definición del Sistema (ASI 1). El objetivo de esta actividad es obtener un catálogo detallado de los requisitos, a partir del cual se pueda comprobar que los productos generados en las actividades de modelización se ajustan a los requisitos de usuario. Esta actividad se descompone en un conjunto de tareas que, si bien tienen un orden, exige continuas realimentaciones y solapamientos, entre sí y con otras tareas realizadas en paralelo. No es necesaria la finalización de una tarea para el comienzo de la siguiente. Lo que se tiene en un momento determinado es un catálogo de requisitos especificado en función de la progresión del proceso de análisis. Se propone como técnica de obtención de requisitos la especificación de los casos de uso de la orientación a objetos, siendo opcional en el caso estructurado. Dicha técnica ofrece un diagrama simple y una guía de especificación en las sesiones de trabajo con el usuario. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad DSI 3: Identificación de Subsistemas de Análisis El objetivo de esta actividad, común tanto para análisis estructurado como para análisis orientado a objetos, es facilitar el análisis del sistema de información llevando a cabo la descomposición del sistema en subsistemas. Se realiza en paralelo con el resto de las actividades de generación de modelos del análisis. Por tanto, se asume la necesidad de una realimentación y ajuste continuo con respecto a la definición de los subsistemas, sus dependencias y sus interfaces. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad ASI 4: Análisis de los Casos de Uso El objetivo de esta actividad, que sólo se realiza en el caso de Análisis Orientado a Objetos , es identificar las clases cuyos objetos son necesarios para realizar un caso de uso y describir su comportamiento mediante la interacción de dichos objetos. Esta actividad se lleva a cabo para cada uno de los casos de uso contenidos en un subsistema de los definidos en la actividad Identificación de Subsistemas de Análisis (ASI 3). Las tareas de esta actividad no se realizan de forma secuencial sino en paralelo, con continuas realimentaciones entre ellas y con las realizadas en las actividades Establecimiento de Requisitos (ASI 2), Identificación de Subsistemas de Análisis (ASI 3), Análisis de Clases (ASI 5) y Definición de Interfaces de Usuario (ASI 8). A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad ASI 5: Análisis de Clases El objetivo de esta actividad que sólo se realiza en el caso de Análisis Orientado a Objetos es describir cada una de las clases que ha surgido, identificando las responsabilidades que tienen asociadas, sus atributos, y las relaciones entre ellas. Para esto, se debe tener en cuenta la normativa establecida en la tarea Especificación de Estándares y Normas (ASI 1.3), de forma que el modelo de clases cumpla estos criterios, con el fin de evitar posibles inconsistencias en el diseño. Teniendo en cuenta las clases identificadas en la actividad Análisis de los Casos de Uso (ASI 4), se elabora el modelo de clases para cada subsistema. A medida que avanza el análisis, dicho modelo se va completando con las clases que vayan apareciendo, tanto del estudio de los casos de uso, como de la interfaz de usuario necesaria para el sistema de información. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad ASI 6: Elaboración del Modelo de Datos El objetivo de esta actividad que se lleva a cabo únicamente en el caso de Análisis Estructurado es identificar las necesidades de información de cada uno de los procesos que conforman el sistema de información, con el fin de obtener un modelo de datos que contemple todas las entidades, relaciones, atributos y reglas de negocio necesarias para dar respuesta a dichas necesidades. El modelo de datos se elabora siguiendo un enfoque descendente ( top-down ). A partir del modelo conceptual de datos, obtenido en la tarea Determinación del Alcance del Sistema (ASI 1.1), se incorporan a dicho modelo todas las entidades que vayan apareciendo, como resultado de las funcionalidades que se deban cubrir y de las necesidades de información del usuario. Es necesario tener en cuenta el catálogo de requisitos y el modelo de procesos, productos que se están generando en paralelo en las actividades Establecimiento de Requisitos (ASI 2), Identificación de Subsistemas de Análisis (ASI 3) y Elaboración del Modelo de Procesos (ASI 7). Una vez construido el modelo conceptual y definidas sus entidades, se resuelven las relaciones complejas y se completa la información de entidades, relaciones, atributos y ocurrencias de las entidades, generando el modelo lógico de datos. Como última tarea en la definición del modelo, se asegura la normalización hasta la tercera forma normal para obtener el modelo lógico de datos normalizado. Finalmente, si procede, se describen las necesidades de migración y carga inicial de los datos. Esta actividad se realiza en paralelo, y con continuas realimentaciones, con otras tareas realizadas en las actividades Establecimiento de Requisitos (ASI 2), Identificación de Subsistemas de Análisis (ASI 3), Elaboración del Modelo de Procesos (ASI 7) y Definición de Interfaces de Usuario (ASI 8). A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad ASI 7: Elaboración del Modelo de Procesos El objetivo de esta actividad, que se lleva a cabo únicamente en el caso de Análisis Estructurado , es analizar las necesidades del usuario para establecer el conjunto de procesos que conforma el sistema de información. Para ello, se realiza una descomposición de dichos procesos siguiendo un enfoque descendente ( top-down ), en varios niveles de abstracción, donde cada nivel proporciona una visión más detallada del proceso definido en el nivel anterior. Con el fin de facilitar el desarrollo posterior, se debe llegar a un nivel de descomposición en el que los procesos obtenidos sean claros y sencillos, es decir, buscar un punto de equilibrio en el que dichos procesos tengan significado por sí mismos dentro del sistema global y a su vez la máxima independencia y simplicidad. Esta actividad se lleva a cabo para cada uno de los subsistemas identificados en la actividad Identificación de Subsistemas de Análisis (ASI 3). Las tareas de esta actividad se realizan en paralelo y con continuas realimentaciones con otras tareas ejecutadas en las actividades Establecimiento de Requisitos (ASI 2), Elaboración del Modelo de Datos (ASI 6) y Definición de Interfaces de Usuario (ASI 8). A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad ASI 8: Definición de Interfaces de usuario En esta actividad se especifican las interfaces entre el sistema y el usuario: formatos de pantallas, diálogos, e informes, principalmente. El objetivo es realizar un análisis de los procesos del sistema de información en los que se requiere una interacción del usuario, con el fin de crear una interfaz que satisfaga todos los requisitos establecidos, teniendo en cuenta los diferentes perfiles a quiénes va dirigido. Al comienzo de este análisis es necesario seleccionar el entorno en el que es operativa la interfaz, considerando estándares internacionales y de la instalación, y establecer las directrices aplicables en los procesos de diseño y construcción. El propósito es construir una interfaz de usuario acorde a sus necesidades, flexible, coherente, eficiente y sencilla de utilizar, teniendo en cuenta la facilidad de cambio a otras plataformas, si fuera necesario. Se identifican los distintos grupos de usuarios de acuerdo con las funciones que realizan, conocimientos y habilidades que poseen, y características del entorno en el que trabajan. La identificación de los diferentes perfiles permite conocer mejor las necesidades y particularidades de cada uno de ellos. Asimismo, se determina la naturaleza de los procesos que se llevan a cabo (en lotes o en línea). Para cada proceso en línea se especifica qué tipo de información requiere el usuario para completar su ejecución realizando, para ello, una descomposición en diálogos que refleje la secuencia de la interfaz de pantalla tipo carácter o pantalla gráfica. Finalmente, se define el formato y contenido de cada una de las interfaces de pantalla especificando su comportamiento dinámico. Se propone un flujo de trabajo muy similar para desarrollos estructurados y orientados a objetos, coincidiendo en la mayoría de las tareas, si bien es cierto que en orientación a objetos, al identificar y describir cada escenario en la especificación de los casos de uso, se hace un avance muy significativo en la toma de datos para la posterior definición de la interfaz de usuario. Como resultado de esta actividad se genera la especificación de interfaz de usuario, como producto que engloba los siguientes elementos: Principios generales de la interfaz. Catálogo de perfiles de usuario. Descomposición funcional en diálogos. Catálogo de controles y elementos de diseño de interfaz de pantalla. Formatos individuales de interfaz de pantalla. Modelo de navegación de interfaz de pantalla. Formatos de impresión. Prototipo de interfaz interactiva. Prototipo de interfaz de impresión. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad ASI 9: Análisis de Consistencia y Especificación de Requisitos El objetivo de esta actividad es garantizar la calidad de los distintos modelos generados en el proceso de Análisis del Sistema de Información, y asegurar que los usuarios y los Analistas tienen el mismo concepto del sistema. Para cumplir dicho objetivo, se llevan a cabo las siguientes acciones: Verificación de la calidad técnica de cada modelo. Aseguramiento de la coherencia entre los distintos modelos. Validación del cumplimiento de los requisitos. Esta actividad requiere una herramienta de apoyo para realizar el análisis de consistencia. También se elabora en esta actividad la Especificación de Requisitos Software (ERS), como producto para la aprobación formal, por parte del usuario, de las especificaciones del sistema. La Especificación de Requisitos Software se convierte en la línea base para los procesos posteriores del desarrollo del software, de modo que cualquier petición de cambio en los requisitos que pueda surgir posteriormente, debe ser evaluada y aprobada. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad ASI 10: Especificación del Plan de Pruebas En esta actividad se inicia la definición del plan de pruebas, el cual sirve como guía para la realización de las pruebas, y permite verificar que el sistema de información cumple las necesidades establecidas por el usuario, con las debidas garantías de calidad. El plan de pruebas es un producto formal que define los objetivos de la prueba de un sistema, establece y coordina una estrategia de trabajo, y provee del marco adecuado para elaborar una planificación paso a paso de las actividades de prueba. El plan se inicia en el proceso Análisis del Sistema de Información (ASI), definiendo el marco general, y estableciendo los requisitos de prueba de aceptación, relacionados directamente con la especificación de requisitos. Dicho plan se va completando y detallando a medida que se avanza en los restantes procesos del ciclo de vida del software, Diseño del Sistema de Información (DSI), Construcción del Sistema de Información (CSI) e Implantación y Aceptación del Sistema (IAS). Se plantean los siguientes niveles de prueba: Pruebas unitarias. Pruebas de integración. Pruebas del sistema. Pruebas de implantación. Pruebas de aceptación. En esta actividad también se avanza en la definición de las pruebas de aceptación del sistema. Con la información disponible, es posible establecer los criterios de aceptación de las pruebas incluidas en dicho nivel, al poseer la información sobre los requisitos que debe cumplir el sistema, recogidos en el catálogo de requisitos. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad ASI 11: Aprobación del Análisis del Sistema de Información A continuación se incluye una tabla resumen con las tareas de la presente actividad: Diseño del Sistema de Información (DSI)Descripción y Objetivos El objetivo del proceso de Diseño del Sistema de Información (DSI) es la definición de la arquitectura del sistema y del entorno tecnológico que le va a dar soporte, junto con la especificación detallada de los componentes del sistema de información. A partir de dicha información, se generan todas las especificaciones de construcción relativas al propio sistema, así como la descripción técnica del plan de pruebas, la definición de los requisitos de implantación y el diseño de los procedimientos de migración y carga inicial, éstos últimos cuando proceda. Al ser MÉTRICA Versión 3 una metodología que cubre tanto desarrollos estructurados como orientados a objetos, las actividades de ambas aproximaciones están integradas en una estructura común. Las actividades de este proceso se agrupan en dos grandes bloques: En un primer bloque de actividades, que se llevan a cabo en paralelo, se obtiene el diseño de detalle del sistema de información. La realización de estas actividades exige una continua realimentación. En general, el orden real de ejecución de las mismas depende de las particularidades del sistema de información y, por lo tanto, de generación de sus productos.En la actividad Definición de la Arquitectura del Sistema (DSI 1), se establece el particionamiento físico del sistema de información, así como su organización en subsistemas de diseño, la especificación del entorno tecnológico, y sus requisitos de operación, administración, seguridad y control de acceso. Se completan los catálogos de requisitos y normas, en función de la definición del entorno tecnológico, con aquellos aspectos relativos al diseño y construcción que sea necesario contemplar. Asimismo, se crea un catálogo de excepciones del sistema, en el que se registran las situaciones de funcionamiento secundario o anómalo que se estime oportuno considerar y, por lo tanto, diseñar y probar. Este catálogo de excepciones se utiliza como referencia en la especificación técnica de las pruebas del sistema.El particionamiento físico del sistema de información permite organizar un diseño que contemple un sistema de información distribuido, como por ejemplo la arquitectura cliente/servidor, siendo aplicable a arquitecturas multinivel en general. Independientemente de la infraestructura tecnológica, dicho particionamiento representa los distintos niveles funcionales o físicos del sistema de información. La relación entre los elementos del diseño y particionamiento físico, y a su vez, entre el particionamiento físico y el entorno tecnológico, permite una especificación de la distribución de los elementos del sistema de información y, al mismo tiempo, un diseño orientado a la movilidad a otras plataformas o la reubicación de subsistemas.El sistema de información se estructura en subsitemas de diseño. Éstos a su vez se clasifican como de soporte o específicos, al responder a propósitos diferentes. Los subsistemas de soporte contienen los elementos o servicios comunes al sistema y a la instalación, y generalmente están originados por la interacción con la infraestructura técnica o la reutilización de otros sistemas, con un nivel de complejidad técnica mayor. Los subsistemas específicos contienen los elementos propios del sistema de información, generalmente con una continuidad de los subsistemas definidos en el proceso de Análisis del Sistema de Información (ASI). También se especifica en detalle el entorno tecnológico del sistema de información, junto con su planificación de capacidades (capacity planning), y sus requisitos de operación, administración, seguridad y control de acceso.El diseño detallado del sistema de información, siguiendo un enfoque estructurado, comprende un conjunto de actividades que se llevan a cabo en paralelo a la Definición de la Arquitectura del Sistema (DSI 1). El alcance de cada una de estas actividades se resume a continuación: * Diseño de la Arquitectura de Soporte (DSI 2), que incluye el diseño detallado de los subsistemas de soporte, el establecimiento de las normas y requisitos propios del diseño y construcción, así como la identificación y definición de los mecanismos genéricos de diseño y construcción.* Diseño de la Arquitectura de Módulos del Sistema (DSI 5), donde se realiza el diseño de detalle de los subsistemas específicos del sistema de información y la revisión de la interfaz de usuario.* Diseño Físico de Datos (DSI 6), que incluye el diseño y optimización de las estructuras de datos del sistema, así como su localización en los nodos de la arquitectura propuesta. En el caso de Diseño Orientado a Objetos, conviene señalar que el diseño de la persistencia de los objetos se lleva a cabo sobre bases de datos relacionales, y que el diseño detallado del sistema de información se realiza en paralelo con la actividad de Diseño de la Arquitectura de Soporte (DSI 2), y se corresponde con las siguientes actividades: * Diseño de Casos de Uso Reales (DSI 3), con el diseño detallado del comportamiento del sistema de información para los casos de uso, el diseño de la interfaz de usuario y la validación de la división en subsistemas.* Diseño de Clases (DSI 4), con el diseño detallado de cada una de las clases que forman parte del sistema, sus atributos, operaciones, relaciones y métodos, y la estructura jerárquica del mismo. En el caso de que sea necesario, se realiza la definición de un plan de migración y carga inicial de datos. Una vez que se tiene el modelo de clases, se comienza el diseño físico en la actividad Diseño Físico de Datos (DSI 6), común con el enfoque estructurado.Una vez finalizado el diseño de detalle, se realiza su revisión y validación en la actividad Verificación y Aceptación de la Arquitectura del Sistema (DSI 7), con el objeto de analizar la consistencia entre los distintos modelos y conseguir la aceptación del diseño por parte de los responsables de las áreas de Explotación y Sistemas. El segundo bloque de actividades complementa el diseño del sistema de información. En él se generan todas las especificaciones necesarias para la construcción del sistema de información: Generación de Especificaciones de construcción (DSI 8), fijando las directrices para la construcción de los componentes del sistema, así como de las estructuras de datos. Diseño de la Migración y Carga Inicial de Datos (DSI 9), en el que se definen los procedimientos de migración y sus componentes asociados, con las especificaciones de construcción oportunas. Especificación Técnica del Plan de Pruebas (DSI 10), que incluye la definición y revisión del plan de pruebas, y el diseño de las verificaciones de los niveles de prueba establecidos. El catálogo de excepciones permite, de una forma muy ágil, establecer un conjunto de verificaciones relacionadas con el propio diseño o con la arquitectura del sistema. Establecimiento de Requisitos de Implantación (DSI 11), que hace posible concretar las exigencias relacionados con la propia implantación del sistema, tales como formación de usuarios finales, infraestructura, etc. Finalmente, en la actividad de Presentación y Aprobación del Diseño del Sistema de Información (DSI 12), se realiza una presentación formal y aprobación de los distintos productos del diseño. En el siguiente gráfico se muestra la relación de actividades del proceso Diseño del Sistema de Información (DSI), tanto para Desarrollos Estructurados como para Desarrollos Orientados a Objetos. Actividad DSI 1: Definición de la Arquitectura del Sistema En esta actividad se define la arquitectura general del sistema de información, especificando las distintas particiones físicas del mismo, la descomposición lógica en subsistemas de diseño y la ubicación de cada subsistema en cada partición, así como la especificación detallada de la infraestructura tecnológica necesaria para dar soporte al sistema de información. El particionamiento físico del sistema de información se especifica identificando los nodos y las comunicaciones entre los mismos, con cierta independencia de la infraestructura tecnológica que da soporte a cada nodo. Con el fin de organizar y facilitar el diseño, se realiza una división del sistema de información en subsistemas de diseño, como partes lógicas coherentes y con interfaces claramente definidas. Se establece una distinción entre subsistemas específicos del sistema de información (en adelante, subsistemas específicos) y subsistemas de soporte, con la finalidad de independizar, en la medida de los posible, las funcionalidades a cubrir por el sistema de información de la infraestructura que le da soporte. En la mayoría de los casos, los subsistemas específicos provienen directamente de las especificaciones de análisis y de los subsistemas de análisis, mientras que los subsistemas de soporte provienen de la necesidad de interacción del sistema de información con la infraestructura y con el resto de los sistemas, así como de la reutilización de módulos o subsistemas ya existentes en la instalación. Debido a que la definición de los subsistemas de soporte puede exigir la participación de distintos perfiles técnicos, se propone el diseño de ambos tipos de subsistemas en actividades distintas, aunque en paralelo. Una vez identificados y definidos los distintos subsistemas de diseño, se determina su ubicación óptima de acuerdo a la arquitectura propuesta. La asignación de dichos subsistemas a cada nodo permite disponer, en función de la carga de proceso y comunicación existente entre los nodos, de la información necesaria para realizar una estimación de las necesidades de infraestructura tecnológica que da soporte al sistema de información. Este factor es especialmente crítico en arquitecturas multinivel o cliente/servidor, donde las comunicaciones son determinantes en el rendimiento final del sistema. Se propone crear un catálogo de excepciones en el que se especifiquen las situaciones anómalas o secundarias en el funcionamiento y ejecución del sistema de información, y que se irá completando a medida que se avance en el diseño detallado de los subsistemas. En esta actividad también se establecen los requisitos, normas y estándares originados como consecuencia de la adopción de una determinada solución de arquitectura o infraestructura, que serán aplicables tanto en este proceso como en la Construcción del Sistema de Información (CSI). Se detallan a su vez, de acuerdo a las particularidades de la arquitectura del sistema propuesta, los requisitos de operación, seguridad y control, especificando los procedimientos necesarios para su cumplimiento. Como resultado de esta actividad, se actualizan los catálogos de requisitos y normas, y se generan los siguientes productos: Diseño de la Arquitectura del Sistema, como producto que engloba el particionamiento físico del sistema de información y la descripción de subsistemas de diseño. Entorno Tecnológico del Sistema, que a su vez comprende la especificación del entorno tecnológico, las restricciones técnicas y la planificación de capacidades. Catálogo de Excepciones. Procedimientos de Operación y Administración del Sistema. Procedimientos de Seguridad y Control de Acceso. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad DSI 2: Diseño de la Arquitectura de Soporte En esta actividad se lleva a cabo la especificación de la arquitectura de soporte, que comprende el diseño de los subsistemas de soporte identificados en la actividad de Definición de la Arquitectura del Sistema (DSI 1), y la determinación de los mecanismos genéricos de diseño. Estos últimos sirven de guía en la utilización de diferentes estilos de diseño, tanto en el ámbito global del sistema de información, como en el diseño de detalle. El diseño de los subsistemas de soporte, conceptualmente, es similar al diseño de los subsistemas específicos, aunque debe cumplir con unos objetivos claros de reutilización. De esta manera, se consigue simplificar y abstraer el diseño de los subsistemas específicos de la complejidad del entorno tecnológico, dotando al sistema de información de una mayor independencia de la infraestructura que le da soporte. Con este fin, se aconseja la consulta de los datos de otros proyectos existentes, disponible en el Histórico de Proyectos. Si esto no fuera suficiente, se puede contar en esta actividad con la participación de perfiles técnicos, con una visión global de la instalación. Esta actividad se realiza en paralelo al diseño detallado, debido a que existe una constante realimentación, tanto en la especificación de los subsistemas con sus interfaces y dependencias, como en la aplicación de esqueletos o patrones en el diseño. Los productos resultantes de esta actividad son: Diseño Detallado de los Subsistemas de Soporte. Mecanismos Genéricos de Diseño y Construcción. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad DSI 3: Diseño de Casos de Uso Reales Esta actividad, que se realiza solo en el caso de Diseño Orientado a Objetos , tiene como propósito especificar el comportamiento del sistema de información para un caso de uso, mediante objetos o subsistemas de diseño que interactúan, y determinar las operaciones de las clases e interfaces de los distintos subsistemas de diseño. Para ello, una vez identificadas las clases participantes dentro de un caso de uso, es necesario completar los escenarios que se recogen del análisis, incluyendo las clases de diseño que correspondan y teniendo en cuenta las restricciones del entorno tecnológico, esto es, detalles relacionados con la implementación del sistema. Es necesario realizar los comportamientos de excepción para dichos escenarios. Algunos de ellos pueden haber sido identificados en el proceso de análisis, aunque no se resuelven hasta este momento. Dichas excepciones se añadirán al catálogo de excepciones para facilitar las pruebas. Algunos de los escenarios detallados requerirán una nueva interfaz de usuario. Por este motivo es necesario diseñar el formato de cada una de las pantallas o impresos identificados. Es importante validar que los subsistemas definidos en la tarea Identificación de Subsistemas de Diseño (DSI 1.5) tienen la mínima interfaz con otros subsistemas. Por este motivo, se elaboran los escenarios al nivel de subsistemas y, de esta forma, se delimitan las interfaces necesarias para cada uno de ellos, teniendo en cuenta toda la funcionalidad del sistema que recogen los casos de uso. Además, durante esta actividad pueden surgir requisitos de implementación, que se recogen en el catálogo de requisitos. Las tareas de esta actividad se realizan en paralelo con las de Diseño de Clases (DSI 4). A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad DSI 4: Diseño de Clases El propósito de esta actividad, que se realiza sólo en el caso de Diseño Orientado a Objetos , es transformar el modelo de clases lógico, que proviene del análisis, en un modelo de clases de diseño. Dicho modelo recoge la especificación detallada de cada una de las clases, es decir, sus atributos, operaciones, métodos, y el diseño preciso de las relaciones establecidas entre ellas, bien sean de agregación, asociación o jerarquía. Para llevar a cabo todos estos puntos, se tienen en cuenta las decisiones tomadas sobre el entorno tecnológico y el entorno de desarrollo elegido para la implementación. Se identifican las clases de diseño, que denominamos clases adicionales, en función del estudio de los escenarios de los casos de uso, que se está realizando en paralelo en la actividad Diseño de Casos de Uso Reales (DSI 3), y aplicando los mecanismos genéricos de diseño que se consideren convenientes por el tipo de especificaciones tecnológicas y de desarrollo. Entre ellas se encuentran clases abstractas, que integran características comunes con el objetivo de especializarlas en clases derivadas. Se diseñan las clases de interfaz de usuario, que provienen del análisis. Como consecuencia del estudio de los escenarios secundarios que se está realizando, pueden aparecer nuevas clases de interfaz. También hay que considerar que, por el diseño de las asociaciones y agregaciones, pueden aparecer nuevas clases, o desaparecer incluyendo sus atributos y métodos en otras, si se considera conveniente por temas de optimización. La jerarquía entre las clases se va estableciendo a lo largo de esta actividad, a medida que se van identificando comportamientos comunes en las clases, aunque haya una tarea propia de diseño de la jerarquía. Otro de los objetivos del diseño de las clases es identificar para cada clase, los atributos, las operaciones que cubren las responsabilidades que se identificaron en el análisis, y la especificación de los métodos que implementan esas operaciones, analizando los escenarios del Diseño de Casos de Uso Reales (DSI 3). Se determina la visibilidad de los atributos y operaciones de cada clase, con respecto a las otras clases del modelo. Una vez que se ha elaborado el modelo de clases, se define la estructura física de los datos correspondiente a ese modelo, en la actividad Diseño Físico de Datos (DSI 6). Además, en los casos en que sea necesaria una migración de datos de otros sistemas o una carga inicial de información, se realizará su especificación a partir del modelo de clases y las estructuras de datos de los sistemas origen. Como resultado de todo lo anterior se actualiza el modelo de clases del análisis, una vez recogidas las decisiones de diseño. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad DSI 5: Diseño de la Arquitectura de Módulos del Sistema El objetivo de esta actividad, que sólo se realiza en el caso de Diseño Estructurado , es definir los módulos del sistema de información, y la manera en que van a interactuar unos con otros, intentando que cada módulo trate total o parcialmente un proceso específico y tenga una interfaz sencilla. Para cada uno de los subsistemas específicos, identificados en la tarea Identificación de los Subsistemas de Diseño (DSI 1.5), se diseña la estructura modular de los procesos que lo integran, tomando como punto de partida los modelos obtenidos en la tarea Validación de los Modelos (ASI 9.3) del proceso de Análisis del Sistema de Información (ASI) y el catálogo de requisitos. Dicha estructura se irá completando con los módulos que vayan apareciendo como consecuencia del diseño de la interfaz de usuario, así como de la optimización del diseño físico de datos. Durante el diseño de los módulos, se pueden identificar características o comportamientos comunes relacionados con accesos a las bases de datos o ficheros, lógica de tratamiento, llamadas a otros módulos, gestión de errores, etc. que determinen la necesidad de realizar su implementación como subsistemas de soporte. Además, se analizan los comportamientos de excepción asociados a los diferentes módulos y a las interfaces entre los mismos, intentando independizar en la medida de lo posible aquéllos que presenten un tratamiento común. Dichas excepciones se incorporan al catálogo de excepciones. En esta actividad, se consideran los estándares y normas establecidas para el diseño, aplicando, cuando proceda, los mecanismos genéricos de diseño identificados en la tarea Identificación de Mecanismos Genéricos de Diseño (DSI 2.2). Las tareas de esta actividad no se realizan de forma secuencial, sino en paralelo, con continuas realimentaciones entre ellas y con las realizadas en las actividades Definición de la Arquitectura del Sistema (DSI 1), Diseño de la Arquitectura de Soporte (DSI 2) y Diseño Físico de Datos (DSI 6). A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad DSI 6: Diseño Físico de Datos En esta actividad se define la estructura física de datos que utilizará el sistema, a partir del modelo lógico de datos normalizado o modelo de clases, de manera que teniendo presentes las características específicas del sistema de gestión de datos concreto a utilizar, los requisitos establecidos para el sistema de información, y las particularidades del entorno tecnológico, se consiga una mayor eficiencia en el tratamiento de los datos. También se analizan los caminos de acceso a los datos utilizados por cada módulo/clase del sistema en consultas y actualizaciones, con el fin de mejorar los tiempos de respuesta y optimizar los recursos de máquina. Las tareas de esta actividad se realizan de forma iterativa y en paralelo con las realizadas en las actividades Definición de la Arquitectura del Sistema (DSI 1), donde se especifican los detalles de arquitectura e infraestructura y la planificación de capacidades, Diseño de la Arquitectura de Soporte (DSI 2), dónde se determinan y diseñan los servicios comunes que pueden estar relacionados con la gestión de datos (acceso a bases de datos, ficheros, áreas temporales, sincronización de bases de datos, etc.), Diseño de Casos de Uso Reales y de Clases (DSI 3 y 4), para desarrollo orientado a objetos, y Diseño de la Arquitectura de Módulos del Sistema (DSI 5), para desarrollo estructurado, dónde se especifica la lógica de tratamiento y las interfaces utilizadas. En el caso de diseño orientado a objetos, esta actividad también es necesaria. La obtención del modelo físico de datos se realiza aplicando una serie de reglas de transformación a cada elemento del modelo de clases que se está generando en la actividad Diseño de Clases (DSI 4). Asimismo, en esta actividad hay que considerar los estándares y normas establecidos para el diseño aplicando, cuando proceda, los mecanismos genéricos de diseño identificados en la tarea Identificación de Mecanismos Genéricos de Diseño (DSI 2.2). A continuación se incluye una table resumen con las tareas de la presente actividad: Actividad DSI 7: Verificación y Aceptación de la Arquitectura del Sistema El objetivo de esta actividad es garantizar la calidad de las especificaciones del diseño del sistema de información y la viabilidad del mismo, como paso previo a la generación de las especificaciones de construcción. Para cumplir dicho objetivo, se llevan a cabo las siguientes acciones: Verificación de la calidad técnica de cada modelo o especificación. Aseguramiento de la coherencia entre los distintos modelos. Aceptación del diseño de la arquitectura por parte de Explotación y Sistemas. Esta actividad es compleja, por lo que es aconsejable utilizar herramientas de apoyo para la realización de sus tareas. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad DSI 8: Generación de Especificaciones de Construcción En esta actividad se generan las especificaciones para la construcción del sistema de información, a partir del diseño detallado. Estas especificaciones definen la construcción del sistema de información a partir de las unidades básicas de construcción (en adelante, componentes), entendiendo como tales unidades independientes y coherentes de construcción y ejecución, que se corresponden con un empaquetamiento físico de los elementos del diseño de detalle, como pueden ser módulos, clases o especificaciones de interfaz. La división del sistema de información en subsistemas de diseño proporciona, por continuidad, una primera división en subsistemas de construcción, definiendo para cada uno de ellos los componentes que lo integran. Si se considera necesario, un subsistema de diseño se podrá dividir a su vez en sucesivos niveles para mayor claridad de las especificaciones de construcción. Las dependencias entre subsistemas de diseño proporcionan información para establecer las dependencias entre los subsistemas de construcción y, por lo tanto, definir el orden o secuencia que se debe seguir en la construcción y en la realización de las pruebas. También se generan las especificaciones necesarias para la creación de las estructuras de datos en los gestores de bases de datos o sistemas de ficheros. El producto resultante de esta actividad es el conjunto de las especificaciones de construcción del sistema de información, que comprende: Especificación del entorno de construcción. Descripción de subsistemas de construcción y dependencias. Descripción de componentes. Plan de integración del sistema de información. Especificación detallada de componentes. Especificación de la estructura física de datos. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad DSI 9: Diseño de la Migración y Carga Inicial de Datos Esta actividad sólo se lleva a cabo cuando es necesaria una carga inicial de información, o una migración de datos de otros sistemas, cuyo alcance y estrategia a seguir se habrá establecido previamente. Para ello, se toma como referencia el plan de migración y carga inicial de datos, que recoge las estructuras físicas de datos del sistema o sistemas origen implicadas en la conversión, la prioridad en las cargas y secuencia a seguir, las necesidades previas de depuración de la información, así como los requisitos necesarios para garantizar la correcta implementación de los procedimientos de migración sin comprometer el funcionamiento de los sistemas actuales. A partir de dicho plan, y de acuerdo a la estructura física de los datos del nuevo sistema, obtenida en la actividad Diseño Físico de Datos (DSI 6), y a las características de la arquitectura y del entorno tecnológico propuesto en la actividad Definición de la Arquitectura del Sistema (DSI 1), se procede a definir y diseñar en detalle los procedimientos y procesos necesarios para realizar la migración. Se completa el plan de pruebas específico establecido en el plan de migración y carga inicial, detallando las pruebas a realizar, los criterios de aceptación o rechazo de la prueba y los responsables de la organización, realización y evaluación de resultados. Asimismo, se determinan las necesidades adicionales de infraestructura, tanto para la implementación de los procesos como para la realización de las pruebas. Como resultado de esta actividad, se actualiza el plan de migración y carga inicial de datos con la información siguiente: Especificación del entorno de migración. Definición de procedimientos de migración. Diseño detallado de módulos. Especificación técnica de las pruebas. Planificación de la migración y carga inicial. Es importante considerar que una carga inicial de información no tiene el mismo alcance y complejidad que una migración de datos, de modo que las tareas de esta actividad se deben llevar a cabo en mayor o menor medida en función de las características de los datos a cargar. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad DSI 10: Especificación Técnica del Plan de Pruebas En esta actividad se realiza la especificación de detalle del plan de pruebas del sistema de información para cada uno de los niveles de prueba establecidos en el proceso Análisis del Sistema de Información: Pruebas unitarias. Pruebas de integración. Pruebas del sistema. Pruebas de implantación. Pruebas de aceptación. Para ello se toma como referencia el plan de pruebas, que recoge los objetivos de la prueba de un sistema, establece y coordina una estrategia de trabajo, y provee del marco adecuado para planificar paso a paso las actividades de prueba. También puede ser una referencia el plan de integración del sistema de información propuesto, opcionalmente, en la tarea Definición de Componentes y Subsistemas de Construcción (DSI 8.2). El catálogo de requisitos, el catálogo de excepciones y el diseño detallado del sistema de información, permiten la definición de las verificaciones que deben realizarse en cada nivel de prueba para comprobar que el sistema responde a los requisitos planteados. La asociación de las distintas verificaciones a componentes, grupos de componentes y subsistemas, o al sistema de información completo, determina las distintas verificaciones de cada nivel de prueba establecido. Las pruebas unitarias comprenden las verificaciones asociadas a cada componente del sistema de información. Su realización tiene como objetivo verificar la funcionalidad y estructura de cada componente individual. Las pruebas de integración comprenden verificaciones asociadas a grupos de componentes, generalmente reflejados en la definición de subsistemas de construcción o en el plan de integración del sistema de información. Tienen por objetivo verificar el correcto ensamblaje entre los distintos componentes. Las pruebas del sistema, de implantación y de aceptación corresponden a verificaciones asociadas al sistema de información, y reflejan distintos propósitos en cada tipo de prueba: Las pruebas del sistema son pruebas de integración del sistema de información completo. Permiten probar el sistema en su conjunto y con otros sistemas con los que se relaciona para verificar que las especificaciones funcionales y técnicas se cumplen. Las pruebas de implantación incluyen las verificaciones necesarias para asegurar que el sistema funcionará correctamente en el entorno de operación al responder satisfactoriamente a los requisitos de rendimiento, seguridad y operación, y coexistencia con el resto de los sistemas de la instalación, y conseguir la aceptación del sistema por parte del usuario de operación. Las pruebas de aceptación van dirigidas a validar que el sistema cumple los requisitos de funcionamiento esperado, recogidos en el catálogo de requisitos y en los criterios de aceptación del sistema de información, y conseguir la aceptación final del sistema por parte del usuario. Las pruebas unitarias, de integración y del sistema se llevan a cabo en el proceso Construcción del Sistema de Información (CSI), mientras que las pruebas de implantación y aceptación se realizan en el proceso implantación y Aceptación del Sistemas (IAS). Como resultado de esta actividad se actualiza el plan de pruebas con la información siguiente: Especificación del entorno de pruebas. Especificación técnica de niveles de prueba. Planificación de las pruebas. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad DSI 11: Establecimiento de Requisitos de Implantación En esta actividad se completa el catálogo de requisitos con aquéllos relacionados con la documentación que el usuario requiere para operar con el nuevo sistema, y los relativos a la propia implantación del sistema en el entorno de operación. La incorporación de estos requisitos permite ir preparando, en los proceso de Construcción del Sistema de Información (CSI) e Implantación y Aceptación del Sistema (IAS), los medios y recursos necesarios para que los usuarios, tanto finales como de operación, sean capaces de utilizar el nuevo sistema de forma satisfactoria. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad DSI 12: Aprobación del Diseño del Sistema de Información A continuación se incluye una tabla resumen con las tareas de la presente actividad: Construcción del Sistema de InformaciónDescripción y Objetivos En este proceso se genera el código de los componentes del Sistema de Información, se desarrollan todos los procedimientos de operación y seguridad y se elaboran todos los manuales de usuario final y de explotación con el objetivo de asegurar el correcto funcionamiento del Sistema para su posterior implantación. Para conseguir dicho objetivo, en este proceso se realizan las pruebas unitarias, las pruebas de integración de los subsistemas y componentes y las pruebas del sistema, de acuerdo al plan de pruebas establecido. Asimismo, se define la formación de usuario final y, si procede, se construyen los procedimientos de migración y carga inicial de datos. Al ser MÉTRICA Versión 3 una metodología que cubre tanto desarrollos estructurados como orientados a objetos, las actividades de ambas aproximaciones están integradas en una estructura común. El producto Especificaciones de Construcción del Sistema de Información, obtenido en la actividad de Generación de Especificaciones de Construcción (DSI 8), es la base para la construcción del sistema de información. En dicho producto se recoge la información relativa al entorno de construcción del sistema de información, la especificación detallada de los componentes y la descripción de la estructura física de datos, tanto bases de datos como sistemas de ficheros. Opcionalmente, incluye un plan de integración del sistema de información, en el que se especifica la secuencia y organización de la construcción de los distintos componentes. En la actividad Preparación del Entorno de Generación y Construcción (CSI 1), se asegura la disponibilidad de la infraestructura necesaria para la generación del código de los componentes y procedimientos del sistema de información. Una vez configurado el entorno de construcción, se realiza la codificación y las pruebas de los distintos componentes que conforman el sistema de información, en las actividades: Generación del Código de los Componentes y Procedimientos (CSI 2), que se hace según las especificaciones de construcción del sistema de información, y conforme al plan de integración del sistema de información. Ejecución de las Pruebas Unitarias (CSI 3), donde se llevan a cabo las verificaciones definidas en el plan de pruebas para cada uno de los componentes. Ejecución de las Pruebas de Integración (CSI 4), que incluye la ejecución de las verificaciones asociadas a los subsistemas y componentes, a partir de los componentes verificados individualmente, y la evaluación de los resultados. Una vez construido el sistema de información y realizadas las verificaciones correspondientes, se lleva a cabo la integración final del sistema de información en la actividad Ejecución de las Pruebas del Sistema (CSI 5), comprobando tanto las interfaces entre subsistemas y sistemas externos como los requisitos, de acuerdo a las verificaciones establecidas en el plan de pruebas para el nivel de pruebas del sistema. En la actividad Elaboración de los Manuales de Usuario (CSI 6), se genera la documentación de usuario final o explotación, conforme a los requisitos definidos en el proceso Diseño del Sistema de Información. La formación necesaria para que los usuarios finales sean capaces de utilizar el sistema de forma satisfactoria se especifica en la actividad Definición de la Formación de Usuarios Finales (CSI 7). Si se ha establecido la necesidad de realizar una migración de datos, la construcción y pruebas de los componentes y procedimientos relativos a dicha migración y a la carga inicial de datos se realiza en la actividad Construcción de los Componentes y Procedimientos de Migración y Carga Inicial de Datos (CSI 8). Actividad CSI 1: Preparación del Entorno de Generación y Construcción El objetivo de esta actividad es asegurar la disponibilidad de todos los medios y facilidades para que se pueda llevar a cabo la construcción del sistema de información. Entre estos medios, cabe destacar la preparación de los puestos de trabajo, equipos físicos y lógicos, gestores de bases de datos, bibliotecas de programas, herramientas de generación de código, bases de datos o ficheros de prueba, entre otros. Las características del entorno de construcción y sus requisitos de operación y seguridad, así como las especificaciones de construcción de la estructura física de datos, se establecen en la actividad Generación de Especificaciones de Construcción (DSI 8), y constituyen el punto de partida para la realización de esta actividad. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad CSI 2: Generación del Código de los Componentes y Procedimientos El objetivo de esta actividad es la codificación de los componentes del sistema de información, a partir de las especificaciones de construcción obtenidas en el proceso Diseño del Sistema de Información (DSI), así como la construcción de los procedimientos de operación y seguridad establecidos para el mismo. En paralelo a esta actividad, se desarrollan las actividades relacionadas con las pruebas unitarias y de integración del sistema de información. Esto permite una construcción incremental, en el caso de que así se haya especificado en el plan de pruebas y en el plan de integración del sistema de información. A continuación se incluye una tabla resumen con las tareas de la presenta actividad: Actividad CSI 3: Ejecución de las Pruebas Unitarias En esta actividad se realizan las pruebas unitarias de cada uno de los componentes del sistema de información, una vez codificados, con el objeto de comprobar que su estructura es correcta y que se ajustan a la funcionalidad establecida. En el plan de pruebas se ha definido el entorno necesario para la realización de cada nivel de prueba, así como las verificaciones asociadas a las pruebas unitarias, la coordinación y secuencia a seguir en la ejecución de las mismas y los criterios de registro y aceptación de los resultados. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad CSI 4: Ejecución de las Pruebas de Integración El objetivo de las pruebas de integración es verificar si los componentes o subsistemas interactúan correctamente a través de sus interfaces, tanto internas como externas, cubren la funcionalidad establecida, y se ajustan a los requisitos especificados en las verificaciones correspondientes. La estrategia a seguir en las pruebas de integración se establece en el plan de pruebas, donde se habrá tenido en cuenta el plan de integración del sistema de información, siempre y cuando se haya especificado en la tarea Definición de Componentes y Subsistemas de Construcción (DSI 8.2). Esta actividad se realiza en paralelo a las actividades Generación del Código de los Componentes y Procedimientos (CSI 2) y Ejecución de las Pruebas Unitarias (CSI 3). Sin embargo, es necesario que los componentes objeto de las pruebas de integración se hayan verificado de manera unitaria. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad CSI 5: Ejecución de las Pruebas del Sistema El objetivo de las pruebas del sistema es comprobar la integración del sistema de información globalmente, verificando el funcionamiento correcto de las interfaces entre los distintos subsistemas que lo componen y con el resto de sistemas de información con los que se comunica. En la realización de estas pruebas es importante comprobar la cobertura de los requisitos, dado que su incumplimiento puede comprometer la aceptación del sistema por el equipo de operación responsable de realizar las pruebas de implantación del sistema, que se llevarán a cabo en el proceso Implantación y Aceptación del Sistema. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad CSI 6: Elaboración de los Manuales de Usuario A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad CSI 7: Definición de la Formación de Usuarios Finales En esta actividad se establecen las necesidades de formación del usuario final, con el objetivo de conseguir la explotación eficaz del nuevo sistema. Para la definición de la formación hay que tener en cuenta las características funcionales y técnicas propias del sistema de información, así como los requisitos relacionados con la formación del usuario final, establecidos en la tarea Especificación de Requisitos de Implantación (DSI 11.2). El producto resultante de esta actividad es la especificación de la formación de usuarios finales, que consta de los siguientes elementos: Esquema de formación. Materiales y entornos de formación. En el proceso Implantación y Aceptación del Sistema (IAS), se unifican las especificaciones de formación de cada sistema de información implicado en la implantación y se elabora un único plan de formación que esté alineado con el plan de implantación del sistema. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad CSI 8: Construcción de los Componentes y Procedimientos de Migración y Carga Inicial de Datos El objetivo de esta actividad es la codificación y prueba de los componentes y procedimientos de migración y carga inicial de datos, a partir de las especificaciones recogidas en el plan de migración y carga inicial de datos obtenidos en el proceso Diseño del Sistema de Información. Previamente a la generación del código, se prepara la infraestructura tecnológica necesaria para realizar la codificación y las pruebas de los distintos componentes y procedimientos asociados, de acuerdo a las características del entorno de migración especificado en el plan de migración y carga inicial de datos. Finalmente, se llevan a cabo las verificaciones establecidas en la especificación técnica del plan de pruebas propio de la migración. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad CSI 9: Aprobación del Sistema de Información A continuación se incluye una tabla resumen con las tareas de la presente actividad: Implantación y Aceptación del SistemaDescripción y Objetivos Este proceso tiene como objetivo principal la entrega del sistema en su totalidad, y la realización de todas las actividades necesarias para el paso a producción del mismo. En primer lugar, se revisa la estrategia de implantación que ya se determinó en el proceso Estudio de Viabilidad del Sistema (EVS). Se estudia su alcance y, en función de sus características, se define un plan de implantación y se especifica el equipo que lo va a llevar a cabo. Conviene señalar la participación del usuario de operación en las pruebas de implantación, del usuario final en las pruebas de aceptación, y del responsable de mantenimiento. Las actividades previas al inicio de la producción incluyen la preparación de la infraestructura necesaria para configurar el entorno, la instalación de los componentes, la activación de los procedimientos manuales y automáticos asociados y, cuando proceda, la migración o carga inicial de datos. Para ello se toman como punto de partida los productos software probados, obtenidos en el proceso Construcción del Sistema de Información (CSI) y su documentación asociada. Se realizan las pruebas de implantación y de aceptación del sistema en su totalidad. Responden a los siguientes propósitos: Las pruebas de implantación cubren un rango muy amplio, que va desde la comprobación de cualquier detalle de diseño interno hasta aspectos tales como las comunicaciones. Se debe comprobar que el sistema puede gestionar los volúmenes de información requeridos, se ajusta a los tiempos de respuesta deseados y que los procedimientos de respaldo, seguridad e interfaces con otros sistemas funcionan correctamente. Se debe verificar también el comportamiento del sistema bajo las condiciones más extremas. Las pruebas de aceptación se realizan por y para los usuarios. Tienen como objetivo validar formalmente que el sistema se ajusta a sus necesidades. Asimismo, se llevan a cabo las tareas necesarias para la preparación del mantenimiento, siempre y cuando se haya decidido que éste va a efectuarse. En cualquier caso, es necesario que la persona que vaya a asumir el mantenimiento conozca el sistema, antes de su incorporación al entorno de producción. Además hay que determinar los servicios (y niveles para cada uno) que requiere el sistema que se va a implantar, y el acuerdo que se adquiere una vez que se inicie la producción. Hay que distinguir entre servicios de gestión de operaciones (servicios por lotes, seguridad, comunicaciones, etc.) y servicios al cliente (servicio de atención a usuario, mantenimiento, etc.) que se deben negociar en cuanto a recursos, horarios, coste, etc. Se fija el nivel con el que se prestará el servicio como indicador de la calidad del mismo. Conviene señalar que la implantación puede ser un proceso iterativo que se realiza de acuerdo al plan establecido para el comienzo de la producción del sistema en su entorno de operación. Para establecer este plan se tiene en cuenta: El cumplimiento de los requisitos de implantación definidos en la actividad Establecimiento de Requisitos (ASI 2) y especificados en la actividad Establecimiento de Requisitos de Implantación (DSI 11). La estrategia de transición del sistema antiguo al nuevo. Finalmente, se realizan las acciones necesarias para el inicio de la producción. En el siguiente gráfico se muestra la relación de actividades de este proceso. Actividad IAS 1: Establecimiento del Plan de Implantación En esta actividad se revisa la estrategia de implantación para el sistema, establecida inicialmente en el proceso Estudio de Viabilidad del Sistema (EVS). Se identifican los distintos sistemas de información que forman parte del sistema objeto de la implantación. Para cada sistema se analizan las posibles dependencias con otros proyectos, que puedan condicionar el plan de implantación. Una vez estudiado el alcance y los condicionantes de la implantación, se decide si ésta se puede llevar a cabo. Será preciso establecer, en su caso, la estrategia que se concretará de forma definitiva en el plan de implantación. Se constituye el equipo de implantación, determinando los recursos humanos necesarios para la propia instalación del sistema, para las pruebas de implantación y aceptación, y para la preparación del mantenimiento. Se identifican, para cada uno de ellos, sus perfiles y niveles de responsabilidad. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad IAS 2: Formación Necesaria para la Implantación En esta actividad se prepara y se imparte la formación al equipo que participará en la implantación y aceptación del sistema. Se realiza también el seguimiento de la formación de los usuarios finales, cuya impartición queda fuera del ámbito de MÉTRICA Versión 3. De esta forma, se asegura que la implantación se va a llevar a cabo correctamente. Se determina la formación necesaria para el equipo de implantación, en función de los distintos perfiles y niveles de responsabilidad identificados en la actividad anterior. Para ello, se establece un plan de formación que incluye los esquemas de formación correspondientes, los recursos humanos y de infraestructura requeridos para llevarlo a cabo, así como una planificación que queda reflejada en el plan de formación. La formación para que los usuarios finales sean capaces de utilizar el sistema de manera satisfactoria ha sido establecida, previamente, en la actividad Definición de la Formación de Usuarios Finales (CSI 7). En esta actividad, se analizan los esquemas de formación definidos según los diferentes perfiles, y se elabora un plan de formación que esté alineado con el plan de implantación. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad IAS 3: Incorporación del Sistema al Entorno de Operación En esta actividad se realizan todas las tareas necesarias para la incorporación del sistema al entorno de operación en el que se van a llevar a cabo las pruebas de implantación y aceptación del sistema. Mientras que las pruebas unitarias, de integración y del sistema se pueden ejecutar en un entorno distinto de aquel en el que finalmente se implantará, las pruebas de implantación y aceptación del sistema deben ejecutarse en el entorno real de operación. El propósito es comprobar que el sistema satisface todos los requisitos especificados por el usuario en las mismas condiciones que cuando se inicie la producción. Por tanto, como paso previo a la realización de dichas pruebas y de acuerdo al plan de implantación establecido, se verifica que los recursos necesarios están disponibles para que se pueda realizar, adecuadamente, la instalación de todos los componentes que integran el sistema, así como la creación y puesta a punto de las bases de datos en el entorno de operación. Asimismo, se establecen los procedimientos de explotación y uso de las bases de datos de acuerdo a la normativa existente en dicho entorno. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad IAS 4: Carga de Datos al Entorno de Operación Teniendo en cuenta que los sistemas de información que forman parte del sistema a implantar pueden mejorar, ampliar o sustituir a otros ya existentes en la organización, puede ser necesaria una carga inicial y/o una migración de datos cuyo alcance dependerá de las características y cobertura de cada sistema de información implicado. Por tanto, la necesidad de una migración de datos puede venir determinada desde el proceso Estudio de Viabilidad del Sistema (EVS), en la actividad Selección de la Solución (EVS 6). Allí se habrá establecido la estrategia a seguir en la sustitución, evaluando las opciones del enfoque de desarrollo e instalación más apropiados para llevarlo a cabo. En cualquier caso, en la actividad Diseño de la Migración y Carga Inicial de Datos (DSI 9) se habrán definido y planificado los procesos y procedimientos necesarios para llevar a cabo la migración, realizándose su codificación en la actividad Construcción de los Componentes y Procedimientos de Migración y Carga Inicial de Datos (CSI 8). A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad IAS 5: Pruebas de Implantación del Sistema La finalidad de las pruebas de implantación es doble: Comprobar el funcionamiento correcto del mismo en el entorno de operación. Permitir que el usuario determine, desde el punto de vista de operación, la aceptación del sistema instalado en su entorno real, según el cumplimiento de los requisitos especificados. Para ello, el responsable de implantación revisa el plan de pruebas de implantación y los criterios de aceptación del sistema, previamente elaborados. Las pruebas las realizan los técnicos de sistemas y de operación, que forman parte del grupo de usuarios técnicos que ha recibido la formación necesaria para llevarlas a cabo. Una vez ejecutadas estas pruebas, el equipo de usuario técnicos informa de las incidencias detectadas al responsable de implantación, el cual analiza la información y toma las medidas correctoras que considere necesarias para que el sistema dé respuesta a las especificaciones previstas, momento en el que el equipo de operación lo da por probado. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad IAS 6: Pruebas de Aceptación del Sistema Las pruebas de aceptación tienen como fin validar que el sistema cumple los requisitos básicos de funcionamiento esperado y permitir que el usuario determine la aceptación del sistema. Por este motivo, estas pruebas son realizadas por el usuario final que, durante este periodo de tiempo, debe plantear todas las deficiencias o errores que encuentre antes de dar por aprobado el sistema definitivamente. Los Directores de los Usuarios revisan los criterios de aceptación, especificados previamente en el plan de pruebas del sistema, y dirigen las pruebas de aceptación final que llevan a cabo los usuarios expertos. A su vez, éstos últimos deben elaborar un informe que los Directores de los Usuarios analizan y evalúan para determinar la aceptación o rechazo del sistema. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad IAS 7: Preparación del Mantenimiento del Sistema El objetivo de esta actividad es permitir que el equipo que va a asumir el mantenimiento del sistema esté familiarizado con él antes de que el sistema pase a producción. Para conseguir este objetivo, se ha considerado al responsable de mantenimiento como parte integrante del equipo de implantación. Por lo tanto, se habrá tenido en cuenta su perfil al elaborar el esquema de formación correspondiente. Una vez que el responsable de mantenimiento ha recibido la formación necesaria y adquirido una visión global del sistema que se va a implantar, se le entregan los productos que serán objeto del mantenimiento. De esta manera, obtiene de una forma gradual un conocimiento profundo del funcionamiento y facilidades que incorpora el sistema, que van a permitirle acometer los cambios solicitados por los usuarios con mayor facilidad y eficiencia. Se reduce, en consecuencia, el esfuerzo invertido en el mantenimiento. Es importante resaltar que la existencia de una configuración del software permite reducir el esfuerzo requerido y mejora la calidad general del software a mantener, aunque no garantiza un mantenimiento libre de problemas. Una pobre configuración del software puede tener un impacto negativo sobre su facilidad de mantenimiento. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad IAS 8: Establecimiento del Acuerdo de Nivel de Servicio Antes de la aprobación definitiva del sistema por parte del Comité de Dirección es conveniente: Determinar los servicios que requiere el mismo. Especificar los niveles de servicio con los que se va a valorar la calidad de esa prestación. Definir qué compromisos se adquieren con la entrega del sistema. Para ello, en primer lugar, se negocia entre los máximos responsables del usuario y de operación qué servicios y de qué tipo se van a prestar. Una vez acordados, se detallan los niveles de servicio definiendo sus propiedades funcionales y de calidad. Se establece cuáles de ellas son cuantificables y qué indicadores se van a aplicar. Es importante señalar que los niveles de servicio son específicos para cada uno de los subsistemas que componen el sistema de información, y dependen del entorno de operación y de la localización geográfica en que se implante un sistema de información concreto, pudiendo haber servicios básicos para todo el sistema o específicos para un subsistema de información concreto. Por último, se establece formalmente el acuerdo de nivel de servicio, considerando los recursos necesarios, plazos de restablecimiento del servicio, coste y mecanismos de regulación que están asociados a cada servicio especificado anteriormente. Según el ámbito y el alcance de los tipos de servicio que se vayan a prestar, se determinan los productos del ciclo de vida del software necesarios para poder establecer el acuerdo de nivel de servicio. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad IAS 9: Presentación y Aprobación del Sistema Una vez que se han efectuado las pruebas de implantación y de aceptación, y que se ha fijado el acuerdo de nivel de servicio, el Comité de Dirección debe formalizar la aprobación del sistema. Para esto, se lleva a cabo una presentación general del sistema al Comité de Dirección y se espera la confirmación de su aprobación. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad IAS 10: Paso a Producción Esta actividad tiene como objetivo establecer el punto de inicio en que el sistema pasa a producción, se traspasa la responsabilidad al equipo de mantenimiento y se empiezan a dar los servicios establecidos en el acuerdo de nivel de servicio, una vez que el Comité de Dirección ha aprobado el sistema. Para ello es necesario que, después de haber realizado las pruebas de implantación y de aceptación del sistema, se disponga del entorno de producción perfectamente instalado en cuanto a hardware y software de base, componentes del nuevo sistema y procedimientos manuales y automáticos. En función del entorno en el que se hayan llevado a cabo las pruebas de implantación y aceptación del sistema, habrá que instalar los componentes del sistema total o parcialmente. También se tendrá en cuenta la necesidad de migrar todos los datos o una parte de ellos. Una vez que el sistema ya está en producción, se le notifica al responsable de mantenimiento, al responsable de operación y al Comité de Dirección. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Mantenimiento de Sistemas de InformaciónDescripción y Objetivos El objetivo de este proceso es la obtención de una nueva versión de un sistema de información desarrollado con MÉTRICA Versión 3 o Versión 2, a partir de las peticiones de mantenimiento que los usuarios realizan con motivo de un problema detectado en el sistema, o por la necesidad de una mejora del mismo. En este apartado se realiza el registro de las peticiones de mantenimiento recibidas, con el fin de llevar el control de las mismas y de proporcionar, si fuera necesario, datos estadísticos de peticiones recibidas o atendidas en un determinado periodo, sistemas que se han visto afectados por los cambios, en qué medida y el tiempo empleado en la resolución de dichos cambios. Es recomendable, por lo tanto, llevar un catálogo de peticiones de mantenimiento sobre los sistemas de información, en el que se registren una serie de datos que nos permitan disponer de la información antes mencionada. En el momento en el que se registra la petición, se procede a diagnosticar de qué tipo de mantenimiento se trata. Atendiendo a los fines, podemos establecer los siguientes tipos de mantenimiento: Correctivo : son aquellos cambios precisos para corregir errores del producto software. Evolutivo : son las incorporaciones, modificaciones y eliminaciones necesarias en un producto software para cubrir la expansión o cambio en las necesidades del usuario. Adaptativo : son las modificaciones que afectan a los entornos en los que el sistema opera, por ejemplo, cambios de configuración del hardware, software de base, gestores de base de datos, comunicaciones, etc. Perfectivo : son las acciones llevadas a cabo para mejorar la calidad interna de los sistemas en cualquiera de sus aspectos: reestructuración del código, definición más clara del sistema y optimización del rendimiento y eficiencia. Estos dos últimos tipos quedan fuera del ámbito de MÉTRICA Versión 3 ya que requieren actividades y perfiles distintos de los del proceso de desarrollo. Una vez registrada la petición e identificado el tipo de mantenimiento y su origen, se determina de quién es la responsabilidad de atender la petición. En el supuesto de que la petición sea remitida, se registra en el catálogo de peticiones de mantenimiento y continúa el proceso. La petición puede ser denegada. En este caso, se notifica al usuario y acaba el proceso. Posteriormente, según se trate de un mantenimiento correctivo o evolutivo, se verifica y reproduce el problema, o se estudia la viabilidad del cambio propuesto por el usuario. En ambos casos se estudia el alcance de la modificación. Hay que analizar las alternativas de solución identificando, según el tipo de mantenimiento de que se trate, cuál es la más adecuada. El plazo y urgencia de la solución a la petición se establece de acuerdo con el estudio anterior. La definición de la solución incluye el estudio del impacto de la solución propuesta para la petición en los sistemas de información afectados. Mediante el análisis de dicho estudio, la persona encargada del Proceso de Mantenimiento valora el esfuerzo y coste necesario para la implementación de la modificación. Las tareas de los procesos de desarrollo que va a ser necesario realizar son determinadas en función de los componentes del sistema actual afectados por la modificación. Estas tareas pertenecen a actividades de los procesos Análisis, Diseño, Construcción e Implantación. Por último, y antes de la aceptación del usuario, es preciso establecer un plan de pruebas de regresión que asegure la integridad del sistema de información afectado. La mejor forma de mantener el coste de mantenimiento bajo control es una gestión del proceso de Mantenimiento efectiva y comprometida. Por lo tanto, es necesario registrar de forma disciplinada los cambios realizados en los sistemas de información y en su documentación. Esto repercutirá directamente en la mayor calidad de los sistemas actuales. La estructura propuesta para el Proceso de Mantenimiento de MÉTRICA Versión 3 comprende las siguientes actividades: Actividad MSI 1: Registro de la Petición El objetivo de esta actividad es establecer un sistema estandarizado de registro de información para las peticiones de mantenimiento, con el fin de controlar y canalizar los cambios propuestos por un usuario o cliente, mejorando el flujo de trabajo de la organización y proporcionando una gestión efectiva del mantenimiento. Es importante asignar responsabilidades para evitar la realización de cambios que beneficien a un usuario, pero que produzca un impacto negativo sobre otros muchos. Por tanto, es necesario, que todas las peticiones de mantenimiento sean presentadas de una forma estandarizada, que permita su clasificación y facilite la identificación del tipo de mantenimiento requerido. Una vez que la petición ha sido registrada, que ha determinado el tipo de mantenimiento y los sistemas de información a los que inicialmente puede afectar, se comprueba su viabilidad, de acuerdo a las prestaciones de mantenimiento establecidas para dichos sistemas de información. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad MSI 2: Análisis de la Petición En esta actividad se lleva a cabo el diagnóstico y análisis del cambio para dar respuesta a las peticiones de mantenimiento que han sido aceptadas en la actividad anterior. Se analiza el alcance de la petición en lo referente a los sistemas de información afectados, valorando hasta qué punto pueden ser modificados en función del ciclo de vida estimado para los mismos y determinando la necesidad de desviar la petició hacia el proceso Estudio de Viabilidad del Sistema (EVS) o Análisis del Sistema de Información (ASI), en función del impacto sobre los sistemas de información afectados. El enfoque de este estudio varía según el tipo de mantenimiento, teniendo en cuenta que en el caso de un mantenimiento correctivo que implique un error crítico debe abordarse el cambio de forma inmediata sin profundizar en el origen del mismo. No obstante, una vez reanudado el servicio, es imprescindible analizar el problema y determinar cuál es la solución definitiva. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad MSI 3: Preparación de la Implementación de la Modificación Una vez finalizado el estudio previo de la petición y aprobada su implementación, se pasa a identificar de forma detallada cada uno de los elementos afectados por el cambio mediante el análisis de impacto. Este análisis tiene como objetivo determinar qué parte del sistema de información se ve afectada, y en qué medida, dejando claramente definido y documentado qué componentes hay que modificar, tanto de software como de hardware. Con el resultado de este análisis se dispone de los datos cuantitativos sobre los que aplicar los indicadores establecidos. Esto permitirá fijar un plan de acción, valorando la necesidad de realizar un reajuste de dichos indicadores, con el fin de cumplir el plazo máximo de entrega. Una vez aceptado el plan de acción, se activan los correspondientes procesos de desarrollo para llevar a cabo la implementación de la solución. Al mismo tiempo, se especifican las pruebas de regresión con el fin de evitar el efecto onda en el sistema, una vez realizados los cambios. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Actividad MSI 4: Seguimiento y Evaluación de los Cambios hasta la Aceptación Se realiza el seguimiento de los cambios que se están llevando a cabo en los procesos de desarrollo, de acuerdo a los puntos de control del ciclo de vida del cambio establecidos en el plan de acción. Durante este seguimiento, se comprueba que sólo se han modificado los elementos que se ven afectados por el cambio y que se han realizado las pruebas correspondientes, especialmente las pruebas de integración y del sistema. Del resultado obtenido se hace una evaluación del cambio para la posterior aprobación. Una vez finalizado el cambio en desarrollo, se realizan las pruebas de regresión especificadas en la actividad anterior, comprobando que ningún sistema no modificado, pero con posibilidades de verse afectado, ha variado su comportamiento habitual. Se informa si ha habido incidencias con el fin de que se resuelvan del modo más conveniente. Se evalúan las pruebas. La aprobación de la petición se realiza al finalizar las pruebas de regresión, y después de comprobar que todo lo que ha sido modificado o puede verse afectado por el cambio, funciona correctamente. Con el cierre de la petición se podrán incluir en el catálogo, si se considera oportuno, parte de la información obtenida durante el proceso de mantenimiento que pueda facilitar posteriores análisis. A continuación se incluye una tabla resumen con las tareas de la presente actividad: Bibliografia Scribd (Ibiza Ales) Estrategias de determinación de requerimientos: Entrevistas, Derivación de sistemas existentes, Análisis y Prototipos. La especificación de requisitos software.Introducción a la Ingeniería de RequerimientosEn la actualidad, son muchos los procesos de desarrollo de software que existen. Con el paso de los años, la Ingeniería del Software ha introducido y popularizado una serie de estándares para medir y certificar la calidad, tanto del sistema a desarrollar, como del proceso de desarrollo en sí. Se han publicado muchos libros y artículos relacionados con este tema, con el modelado de procesos del negocio y la reingeniería. Un número creciente de herramientas automatizadas han surgido para ayudar a definir y aplicar un proceso de desarrollo de software efectivo. Hoy en día la economía global depende más de sistemas automatizados que en épocas pasadas. Esto ha llevado a los equipos de desarrollo a enfrentarse con una nueva década de procesos y estándares de calidad. Sin embargo, ¿cómo explicamos la alta incidencia de fallos en los proyectos de software? ¿Por qué existen tantos proyectos de software víctimas de retrasos, presupuestos mal dimensionados y con problemas de calidad? ¿Cómo podemos tener una producción o una economía de calidad, cuando nuestras actividades diarias dependen de la calidad de un sistema que no la tiene? Tal vez suene ilógico pero, a pesar de los avances que ha dado la tecnología, aún existen procesos de producción informales, parciales y en algunos casos no confiables. La Ingeniería de Requerimientos cumple un papel primordial en el proceso de producción de software, ya que enfoca un área fundamental: la definición de lo que se desea producir. Su principal tarea consiste en la generación de especificaciones correctas que describan con claridad, sin ambigüedades, en forma consistente y compacta, el comportamiento del sistema. De esta manera, se pretende minimizar los problemas relacionados al desarrollo de sistemas. Existe una gran cantidad de proyectos de software que no llegan a cumplir sus objetivos. En nuestro país somos partícipes de este problema a diario, en donde se ha vuelto común la compra de sistemas extranjeros, para luego “personalizarlos” supuestamente a la medida de las empresas. Tal “personalización” termina retrasando (la mayoría de las veces) el proyecto en meses, o incluso en años. La problemática del año 2000 trajo como consecuencia una serie de cambios apresurados en los sistemas existentes, cambios que no fueron bien planificados. El reemplazo de plataformas y tecnologías obsoletas, la compra de sistemas completamente nuevos, las modificaciones de todos o de casi todos los programas que forman un sistema, entre otras razones, llevan a desarrollar proyectos en calendarios sumamente ajustados y en algunos casos irreales. Esto ocasiona que se omitan muchos pasos importantes en el ciclo de vida de desarrollo, entre estos, la definición de los requerimientos. En 1995 se realizó un estudio (informe CHAOS) sobre el resultado general de los proyectos de software. El estudio fue demoledor y esto a pesar de las herramientas existentes para el desarrollo de software (ver figura). La Ingeniería de Requerimientos¿Qué son Requerimientos?Normalmente, un concepto de la Ingeniería de Software tiene diferentes significados. De las muchas definiciones que existen para requerimiento, a continuación se presenta la definición que aparece en el glosario de la IEEE: Una condición o necesidad de un usuario para resolver un problema o alcanzar un objetivo. Una condición o capacidad que debe estar presente en un sistema o componentes de sistema para satisfacer un contrato, estándar, especificación u otro documento formal. Una representación documentada de una condición o capacidad como en 1 o 2. Los requerimientos pueden dividirse en requerimientos funcionales y requerimientos no funcionales: Los requerimientos funcionales definen las funciones que el sistema será capaz de realizar. Describen las transformaciones que el sistema realiza sobre las entradas para producir salidas. Los requerimientos no funcionales tienen que ver con características que de una u otra forma puedan limitar el sistema, como por ejemplo, el rendimiento (en tiempo y espacio), interfaces de usuario, fiabilidad (robustez del sistema, disponibilidad de equipo), mantenimiento, seguridad, portabilidad, estándares, etc. Características de los RequerimientosLas características de un requerimiento son sus propiedades principales. Un conjunto de requerimientos en estado de madurez deben presentar una serie de características tanto individualmente como en grupo. A continuación se presentan las más importantes: Necesario : Un requerimiento es necesario si su omisión provoca una deficiencia en el sistema a construir, y además su capacidad, características físicas o factor de calidad no pueden ser reemplazados por otras capacidades del producto o del proceso. Conciso : Un requerimiento es conciso si es fácil de leer y entender. Su redacción debe ser simple y clara para aquellos que vayan a consultarlo en un futuro. Completo : Un requerimiento está completo si no necesita ampliar detalles en su redacción, es decir, si se proporciona la información suficiente para su comprensión. Consistente : Un requerimiento es consistente si no es contradictorio con otro requerimiento. No ambiguo : Un requerimiento no es ambiguo cuando tiene una sola interpretación. El lenguaje usado en su definición, no debe causar confusiones al lector. Verificable : Un requerimiento es verificable cuando puede ser cuantificado de manera que permita hacer uso de los siguientes métodos de verificación: inspección, análisis, demostración o pruebas. Dificultades para definir los Requerimientos Los requerimientos no son obvios y vienen de muchas fuentes. Son difíciles de expresar en palabras (el lenguaje es ambiguo). Existen muchos tipos de requerimientos y diferentes niveles de detalle. La cantidad de requerimientos en un proyecto puede ser difícil de manejar. Nunca son iguales. Algunos son más difíciles, más arriesgados, más importantes o más estables que otros. Los requerimientos están relacionados unos con otros, y a su vez se relacionan con otras partes del proceso. Cada requerimiento tiene propiedades únicas y abarcan áreas funcionales específicas. Un requerimiento puede cambiar a lo largo del ciclo de desarrollo. Son difíciles de cuantificar, ya que cada conjunto de requerimientos es particular para cada proyecto. Definiciones par la Ingeniería de Requerimientos Ingeniería de Requerimientos vs Administración de Requerimientos : El proceso de recopilar, analizar y verificar las necesidades del cliente para un sistema es llamado Ingeniería de Requerimientos (IR). La meta de la IR es entregar una especificación de requerimientos de software correcta y completa. Los requerimientos son solicitudes que hace el usuario hacia el equipo de trabajo para la realización de alguna tarea. Los requerimientos que son aceptados, son definidos entre el cliente, los usuarios y el equipo de desarrollo, para identificar claramente los límites del sistema. Los requerimientos de los usuarios que han sido definidos, serán administrados para atenderlos y darles un adecuado seguimiento. La administración de requerimientos permite adicionalmente tener un control económico de los mismos. La determinación tiene que ver con: Alcance, requerimientos funcionales, requerimientos no funcionales, complejidad. La administración tiene que ver con: Actividades a realizar, responsables, productos a entregar, costo, tiempos. Ingeniería de Requerimientos es la disciplina para desarrollar una especificación completa, consistente y no ambigua, la cual servirá como base para acuerdos comunes entre todas las partes involucradas y en dónde se describen las funciones que realizará el sistema. Ingeniería de Requerimientos es el proceso por el cual se transforman los requerimientos declarados por los clientes, ya sean hablados o escritos, a especificaciones precisas, no ambigua, consistentes y completas del comportamiento del sistema, incluyendo funciones, interfaces, rendimiento y limitaciones. Es el proceso mediante el cual se intercambian diferentes puntos de vista para recopilar y modelar lo que el sistema va a realizar. Este proceso utiliza una combinación de métodos, herramientas y actores, cuyo producto es un modelo del cual se genera un documento de requerimientos. Ingeniería de Requerimientos es un enfoque sistémico para recolectar, organizar y documentar los requerimientos del sistema. Es también el proceso que establece y mantiene acuerdos sobre los cambios de requerimientos, entre los clientes y el equipo del proyecto. Una posible visión de la ingeniería de requerimientos es considerarla como un proceso de construcción de una especificación de requerimientos en el que se avanza desde especificaciones iniciales, que no poseen las propiedades idóneas, hasta especificaciones finales completas, formales y acordadas entre todos los participantes. Tres enfoques (dimensiones) desde los que se debe abordar la IR para lograr las especificaciones finales de los requerimientos de un sistema (ver figura): Por un lado están los factores psicológicos y congnitivos que afectan al grado de conocimiento sobre el sistema que se desea desarrollar, es decir, llegar a conocer la totalidad de los requerimientos que debe satisfacer el sistema. Habitualmente, este enfoque se cubre con actividades encaminadas a la adquisición de los requerimientos (elicitación). Por otro lado, está el grado de formalismo de la representación del conocimiento sobre dichos requerimientos, teniendo en cuenta que un mayor grado de formalismo no implica necesariamente un mayor conocimiento. El formalismo se logra en las actividades encaminadas al análisis de requerimientos. Por último, están los aspectos sociales, ya que al ser un proceso en el que participan personas con diferentes puntos de vista, es necesario llegar a un punto de acuerdo, normalmente mediante algún tipo de negociación. Las actividades de validación permiten a la IR resolver los posibles conflictos. Importancia de la Ingeniería de RequerimientosLos principales beneficios que se obtienen de la Ingeniería de Requerimientos son: Permite gestionar las necesidades del proyecto en forma estructurada: Cada actividad de la IR consiste de una serie de pasos organizados y bien definidos. Mejora la capacidad de predecir cronogramas de proyectos, así como sus resultados: La IR proporciona un punto de partida para controles subsecuentes y actividades de mantenimiento, tales como estimación de costos, tiempo y recursos necesarios. Disminuye los costos y retrasos del proyecto: Muchos estudios han demostrado que reparar errores por un mal desarrollo no descubierto a tiempo, es sumamente caro, especialmente aquellas decisiones tomadas durante la IR. Mejora la calidad del software: La calidad en el software tiene que ver con cumplir un conjunto de requerimientos (funcionalidad, facilidad de uso, confiabilidad, desempeño, etc.). Mejora la comunicación entre equipos: La especificación de requerimientos representa una forma de consenso entre clientes y desarrolladores. Si este consenso no ocurre, el proyecto no será exitoso. Evita rechazos de usuarios finales: La ingeniería de requerimientos obliga al cliente a considerar sus requerimientos cuidadosamente y revisarlos dentro del marco del problema, por lo que se le involucra durante todo el desarrollo del proyecto. Personal InvolucradoRealmente, son muchas las personas involucradas en el desarrollo de los requerimientos de un sistema. Es importante saber que cada una de esas personas tienen diversos intereses y juegan roles específicos dentro de la planificación del proyecto. El conocimiento de cada papel desempeñado asegura que se involucren a las personas correctas en las diferentes fases del ciclo de vida, y en las diferentes actividades de la IR. No conocer estos intereses puede ocasionar una comunicación poco efectiva entre clientes y desarrolladores, que a la vez traería impactos negativos tanto en tiempo como en presupuesto. Los roles más importantes pueden clasificarse como sigue: Usuarios finales : Son las personas que usarán el sistema desarrollado. Ellos están relacionados con la usabilidad, la disponibilidad y la fiabilidad del sistema. Están familiarizados con los procesos específicos que debe realizar el software, dentro de los parámetros de su ambiente laboral. Serán quienes utilicen las interfaces y los manuales de usuario. Usuarios líderes : Son los individuos que comprenden el ambiente del sistema o el dominio del problema en donde será empleado el software desarrollado. Ellos proporcionan al equipo técnico los detalles y requerimientos de las interfaces del sistema. Personal de mantenimiento : Para proyectos que requieran un mantenimiento eventual, estas personas son las responsables de la administración de cambios, de la implementación y resolución de anomalías. Su trabajo consiste en revisar y mejorar los procesos del producto ya finalizado. Analistas y programadores : Son los responsables del desarrollo del producto en sí. Ellos interactúan directamente con el cliente. Personal de pruebas : Se encargan de elaborar y ejecutar el plan de pruebas para asegurar que las condiciones presentadas por el sistema son las adecuadas. Son quienes van a validar si los requerimientos satisfacen las necesidades del cliente. Otras personas que pueden estar involucradas, dependiendo de la magnitud del proyecto, pueden ser: administradores de proyecto, documentadores, diseñadores de BD, entre otros. Las Dimensiones de los RequerimientosLos calificativos que se aplican al término requerimiento muestran distintos ortogonales que a menudo son considerados aisladamente. Aquí se ven agrupados en tres dimensiones (ver figura). Ámbito : Esta dimensión indica en qué ámbito se debe entender el requerimiento. En general, un ámbito de sistema indica que el requerimiento debe cumplirse a nivel de sistema, entendiendo por sistema un conjunto de hardware y software. Característica que define : Esta dimensión clasifica los requerimientos en función de la naturaleza de la característica del sistema que se especifica. En [Pohl 1997] aparece una completa clasificación denominada RSM (Requirements Specification Model, Modelo de Especificación de Requerimientos), cuyas principales clases son: requerimientos funcionales, requerimientos de datos y requerimientos no funcionales. Es conveniente separar de los requerimientos funcionales a los requerimientos de datos o de almacenamiento de información, que establecen qué información debe almacenar el sistema por ser relevante para las necesidades y objetivos de clientes y usuarios. Audiencia : Esta dimensión indica la audiencia a la que está dirigido el requisito, es decir, las personas que deben ser capaces de entenderlo. En general, se pueden distinguir dos tipos de audiencia: Los clientes y usuarios, que no tienen porqué tener formación en ingeniería del software (especificación de requerimientos mediante lenguaje natural). Los desarrolladores de software (especificación de requerimientos utilizando técnicas estructuradas, orientadas a objetos o formales). En la literatura se suele referir a los requerimientos orientados a clientes y usuarios como requerimientos-C, requisito de usuario o requisito de cliente; y, a los requerimientos orientados al desarrollador como requerimientos-D o requerimientos software. Puntos a considerar durante la Ingeniería de Requerimientos Objetivos del negocio y ambiente de trabajo: Aunque los objetivos del negocio están definidos frecuentemente en términos generales, son usados para descomponer el trabajo en tareas específicas. En ciertas situaciones la IR se enfoca en la descripción de las tareas y en el análisis de sistemas similares. Esta información proporciona la base para especificar el sistema que será construido, aunque frecuentemente se añadan al sistema tareas que no encajan con el ambiente de trabajo planificado. El nuevo sistema cambiará el ambiente de trabajo, sin embargo, es muy difícil anticipar los efectos actuales sobre la organización. Los cambios no ocurren solamente cuando un nuevo software es implementado y puesto en producción, también ocurren cuando cambia el ambiente que lo rodea (nuevas soluciones a problemas, nuevo equipo para instalar, etc.). La necesidad de cambio es sustentada por el enorme costo de mantenimiento, aunque existen diversas razones que dificultan el mantenimiento del software, la falta de atención a la IR es la principal. Frecuentemente la especificación inicial es también la especificación final, lo que obstaculiza la comunicación y el proceso de aprendizaje de las personas involucradas. Ésta es una de las razones por las cuales existen sistemas inadecuados. Punto de vista de los clientes. Muchos sistemas tienen diferentes tipos de clientes. Cada grupo de clientes tiene necesidades diferentes y, diferentes requerimientos tienen diferentes grados de importancia para ellos. Por otro lado, pocas veces tenemos que los clientes son los usuarios, trayendo como consecuencia que los clientes soliciten procesos que causan conflictos con los solicitados por el usuario. Diferentes puntos de vistas también pueden tener consecuencias negativas, tales como datos redundantes, inconsistentes y ambiguos. El tamaño y complejidad de los requerimientos ocasiona desentendimiento, dificultad para enfocarse en un solo aspecto a la vez y dificultad para visualizar relaciones existentes entre requerimientos. Barreras de comunicación: La ingeniería de requerimientos depende de una intensa comunicación entre clientes y analistas de requerimientos. Sin embargo, existen problemas que no pueden ser resueltos mediante la comunicación. Para remediar esto, se deben abordar nuevas técnicas operacionales que ayuden a superar estas barreras y así ganar experiencia dentro del marco del sistema propuesto. Evolución e integración del sistema: Pocos sistemas son construidos desde cero. En la práctica, los proyectos se derivan de sistemas ya existentes. Por lo tanto, los analistas de requerimientos deben comprender esos sistemas, que por lo general son una integración de componentes de varios proveedores. Para encontrar una solución a problemas de este tipo, es muy importante hacer planeamientos entre los requerimientos y la fase de diseño. Esto minimizará la cantidad de fallos directos en el código. Documentación de requerimientos: Los documentos de ingeniería de requerimientos son largos. La mayoría están compuestos de cientos o miles de páginas, donde cada página contiene muchos detalles que pueden tener efectos profundos en el resto del sistema. Normalmente, las personas se encuentran con dificultades para comprender documentos de este tamaño, sobre todo si lo leen cuidadosamente. Es casi imposible leer un documento de especificación de gran tamaño, pues difícilmente una persona puede memorizar los detalles del documento. Esto causa problemas y errores que no son detectados hasta después de haberse construido el sistema. Metodología de la Ingeniería de RequerimientosObjetivo de la MetodologíaEl objetivo de esta metodología es la definición de las tareas a realizar, los productos a obtener y las técnicas a emplear durante la actividad de elicitación de requerimientos de la fase de ingeniería de requerimientos del desarrollo de software. En esta metodología se distinguen dos tipos de productos: los productos entregables y los productos no entregables o internos. Los productos entregables son aquellos que se entregan oficialmente al cliente como parte del desarrollo en fechas previamente acordadas, mientras que los no entregables son productos internos al desarrollo que no se entregan al cliente. El único producto entregable definido en esta metodología es el Documento de Requerimientos del Sistema (DRS). La estructura de este documento es la siguiente: tareas recomendadas, productos entregables (en este caso el DRS), y por último, se describen algunas de las técnicas recomendadas para obtener productos. También se incluye como apéndice un ejemplo de aplicación de esta metodología. Tareas RecomendadasLas tareas recomendadas para obtener los productos descritos en esta metodología son las siguientes: Tarea 1 : Obtener información sobre el dominio del problema y el sistema actual. Tarea 2 : Preparar y realizar las reuniones de elicitación/negociación. Tarea 3 : Identificar/revisar los objetivos del sistema. Tarea 4 : Identificar/revisar los requerimientos de almacenamiento de información. Tarea 5 : Identificar/revisar los requerimientos funcionales. Tarea 6 : Identificar/revisar los requerimientos no funcionales. Tarea 7 : Priorizar objetivos y requerimientos. El orden recomendado de realización para estas tareas e: 1 … 7, aunque las tareas 4, 5 y 6 pueden realizarse simultáneamente o en cualquier orden que se considere oportuno (ver figura). La tarea 1 es opcional y depende del conocimiento previo que tenga el equipo de desarrollo sobre el dominio del problema y el sistema actual. Tarea 1: Obtener información sobre el dominio del problema y el sistema actualObjetivos Conocer el dominio del problema. Conocer la situación actual. Descripción Antes de mantener las reuniones con los clientes y usuarios e identificar los requerimientos es fundamental conocer el dominio del problema y los contexto organizacional y operacional, es decir, la situación actual. Enfrentarse a un desarrollo sin conocer las características principales ni el vocabulario propio de su dominio suele provocar que el producto final no sea el esperado por clientes ni usuarios. Por otro lado, mantener reuniones con clientes y usuarios sin conocer las características de su actividad hará que probablemente no se entiendan sus necesidades, y que su confianza inicial hacia el desarrollo se vea deteriorada enormemente. Esta tarea es opcional, ya que puede que no sea necesario realizarla si el equipo de desarrollo tiene experiencia en el dominio del problema y el sistema actual es conocido. Productos internos Información recopilada: libros, artículos, folletos comerciales, desarrollos previos sobre el mismo dominio, etc. Modelos del sistema actual. Productos entregables Introducción, participantes en el proyecto (principalmente clientes y desarrolladores), y descripción del sistema actual como parte del DRS. Técnicas recomendadas Obtener información de fuentes externas al negocio del cliente: folletos, informes sobre el sector, publicaciones, consultas con expertos, etc. En el caso de que se trate de un dominio muy específico puede ser necesario recurrir a fuentes internas al propio negocio del cliente, en cuyo caso pueden utilizarse las técnicas auxiliares de elicitación de requerimientos como el estudio de documentación, observación in situ, cuestionarios, inmersión o apredizaje, etc. Modelado del sistema actual. Tarea 2: Preparar y realizar las sesiones de elicitación/negociaciónObjetivos Identificar a los usuarios participantes. Conocer las necesidades de clientes y usuarios. Resolver posibles conflictos. Descripción Teniendo en cuenta la información recopilada en la tarea anterior, en esta tarea se deben preparar y realizar las reuniones con los clientes y usuarios participantes con objeto de obtener sus necesidades y resolver posibles conflictos que se hayan detectado en iteraciones previas al proceso. Esta tarea es especialmente crítica y ha de realizarse con especial cuidado, ya que generalmente el equipo de desarrollo no conoce los detalles específicos de la organización para la que se va a desarrollar el sistema y, por otra parte, los clientes y posibles usuarios no saben qué necesita saber el equipo de desarrollo para llevar a cabo su labor. Productos internos Notas tomadas durante las reuniones, transcripciones o actas de reuniones, formularios, grabaciones en cinta o vídeo de las reuniones o cualquier otra documentación que se considere oportuna. Productos entregables Participantes en el proyecto, en concreto los usuarios participantes, como parte del DRS. Objetivos, requerimientos o conflictos, que se hayan identificado claramente durante las sesiones de elicitación, como parte del DRS. Técnicas recomendadas Técnicas de elicitación de requerimientos. Técnicas de negociación como WinWin. Tarea 3: Identificar/revisar los objetivos del sistemaObjetivos Identificar los objetivos que se esperan alcanzar mediante el sistema software a desarrollar. Revisar, en el caso de que haya conflictos, los objetivos previamente identificados. Descripción A partir de la información obtenida en la tarea anterior, en esta tarea se deben identificar qué objetivos se esperan alcanzar una vez que el sistema software a desarrollar se encuentre en explotación o revisarlos en función de los conflictos identificados. Puede que los objetivos hayan sido proporcionados antes de comenzar el desarrollo. Productos internos No hay productos internos en esta tarea. Productos entregables Objetivos del sistema como parte del DRS. Técnicas recomendadas Análisis de factores críticos de éxito o alguna técnica similar de identificación de objetivos. Especificación de los objetivos del sistema. Tarea 4: Identificar/revisar los requerimientos de almacenamiento de informaciónObjetivos Identificar los requerimientos de almacenamiento de información que deberá cumplir el sistema software a desarrollar. Revisar, en el caso de que haya conflictos, los requerimientos de almacenamiento de información previamente identificados. Descripción A partir de la información obtenida en las tareas 1 y 2, y teniendo en cuenta los objetivos identificados en la tarea 3 y el resto de los requerimientos, en esta tarea se debe identificar (o revisar si existen conflictos) qué información relevante para el cliente deberá gestionar y almacenar el sistema software a desarrollar. Inicialmente se partirán de conceptos generales para posteriormente ir detallándolos hasta obtener todos los datos relevantes. Productos internos No hay productos internos en esta tarea. Productos entregables Requerimientos de almacenamiento de información como parte del DRS. Técnicas recomendadas Definición de requerimientos de almacenamiento de información Tarea 5: Identificar/revisar los requerimientos funcionalesObjetivos Identificar los actores del sistema de software a desarrollar. Identificar los requerimientos funcionales (casos de uso) que deberá cumplir el sistema software a desarrollar. Revisar, en el caso de que haya conflictos, los requerimientos funcionales previamente identificados. Descripción A partir de la información obtenida en las tareas 1 y 2, y teniendo en cuenta los objetivos identificados en la tarea 3 y el resto de los requerimientos, en esta tarea se debe identificar (o revisar si existen conflictos) qué debe hacer el sistema a desarrollar con la información identificada en la tarea anterior. Inicialmente se identificarán los actores que interactuarán con el sistema, es decir aquellas personas u otros sistema que serán los orígenes o destinos de la información que consumirá o producirá el sistema a desarrollar y que forman su entorno. A continuación se identificarán los casos de uso asociados a los actores, los pasos de cada caso de uso, y posteriormente se detallarán los casos de uso con las posibles excepciones hasta definir todas las situaciones posibles. Productos internos No hay productos internos en esta tarea. Productos entregables Requerimientos funcionales como parte del DRS. Técnicas recomendadas Casos de uso. Definición de actores. Definición de los requerimientos funcionales. Tarea 6: Identificar/revisar los requerimientos no funcionalesObjetivos Identificar los requerimientos no funcionales del sistema software a desarrollar. Descripción A partir de la información obtenida en las tareas 1 y 2, y teniendo en cuenta los objetivos identificados en la tarea 3 y el resto de los requerimientos, en esta tarea se deben identificar (o revisar si existen conflictos), los requerimientos no funcionales, normalmente de carácter técnico o legal. Algunos tipos de requerimientos que se suelen incluir en esta sección son los siguientes: Requerimientos de comunicaciones del sistema : Son requerimientos de carácter técnico relativos a las comunicaciones que deberá soportar el sistema software a desarrollar. Por ejemplo: el sistema deberá utilizar el protocolo TCP/IP para las comunicaciones con otros sistemas. Requerimientos de interfaz de usuario : Este tipo de requerimientos especifica las características que deberá tener el sistema en su comunicación con el usuario. Por ejemplo: la interfaz de usuario del sistema deberá ser consistente con los estándares definidos en IBM’s Common User Access. Se debe ser cuidadoso con este tipo de requerimientos, ya que, en esta fase de desarrollo todavía no se conocen bien las dificultades que pueden sufrir ala hora de diseñar e implementar las interfaces, por esto no es conveniente entrar en detalles demasiado específicos. Requerimientos de fiabilidad : Los requerimientos de fiabilidad deben establecer los factores que se requieren para la fiabilidad del software en tiempo de explotación. La fiabilidad mide la probabilidad del sistema de producir una respuesta satisfactoria a las demandas del usuario. Por ejemplo: la tasa de fallos del sistema no podrá ser superior a 2 fallos por semana. Requerimientos de entorno de desarrollo : Este tipo de requerimientos especifican si el sistema debe desarrollarse con un producto específico. Por ejemplo: el sistema deberá desarrollarse con Oracle 7 como servidor y clientes Visual Basic 4. Requerimientos de portabilidad : Los requerimientos de portabilidad definen qué características deberá tener el software para que sea fácil utilizarlo en otra máquina o bajo otro sistema operativo. Por ejemplo: el sistema deberá funcionar en los sistemas operativos Windows 95, Windows 98 y Windows NT 4.0, siendo además posible el acceso al sistema a través de Internet usando cualquier navegador compatible con HTML 4.0. Productos internos No hay productos internos en esta tarea. Productos entregables Requerimientos no funcionales del sistema como parte del DRS. Técnicas recomendadas Definición de requerimientos no funcionales Productos EntregablesEl único producto entregable que se contempla en esta metodología es el Documento de Requerimientos del Sistema (DRS). Documento de Requerimientos del Sistema La estructura del DRS puede verse en la siguiente figura. En las siguientes secciones se describe con detalle cada sección del DRS. Portada La portada del DRS debe tener el formato que puede verse en la siguiente figura: Los elementos que deben aparecer son los siguientes: Nombre del proyecto : el nombre del proyecto al que pertenece el DRS. Versión : la versión del DRS que se entrega al cliente. La versión se compone de dos números X e Y. El primero indica la versión, y se debe incrementar cada vez que se hace una nueva entrega formal al cliente. Cuando se incremente el primer número, el segundo debe volver a comenzar en cero. El segundo número indica cambios dentro de la misma versión aún no entregada,y se debe incrementar cada vez que se publica una versión con cambios respecto a la última que se publicó y que no se vaya a entregar formalmente todavía. Este tipo de versiones pueden ser internas al equipo de desarrollo o ser entregadas al cliente a título orientativo. Fecha : fecha de la publicación de la versión. Equipo de desarrollo : nombre de la empresa o equipo de desarrollo. Cliente : nombre del cliente, normalmente otra empresa. Lista de cambios El documento debe incluir una lista de cambios en la que se especifiquen, para cada versión del documento, los cambios producidos en el mismo con un formato similar al que puede verse en la siguiente figura. Para cada cambio realizado se debe incluir el número de orden, la fecha, una descripción y los autores. Índice El índice del DRS debe indicar la página en la que se comienza cada sección, subsección o apartado del documento. En la medida de los posible, se sangrarán las entradas del índice para ayudar a comprender la estructura del documento. Listas de figuras y tablas El DRS deberá incluir listas de las figuras y tablas que aparezcan en el mismo. Dichas listas serán dos índices que indicarán el número, la descripción y la página en que aparece cada figura o tabla del DRS. Introducción Esta sección debe contener una descripción breve de las principales características del sistema software que se va a desarrollar, la situación actual que genera la necesidad del nuevo desarrollo, la problemática que se acomete, y cualquier otra consideración que sitúe al posible lector en el contexto oportuno para comprender el resto del documento. Participantes en el proyecto Esta sección debe contener una lista con todos los participantes en el proyecto, tanto desarrolladores como clientes y usuarios. Para cada participante se deberá indicar su nombre, el papel que desempeña en el proyecto, la organización a la que pertenece y cualquier otra información adicional que se considere oportuna. Descripción del sistema actual Esta sección debe contener una descripción del sistema actual en el caso de que se haya acometido su estudio. Para describir el sistema actual puede utilizarse cualquier técnica que se considere oportuno. Objetivos del sistema Esta sección debe contener una lista con los objetivos que se esperan alcanzar cuando el sistema software a desarrollar esté en explotación. Catálogo de requerimientos del sistema Esta sección se divide en subsecciones en las que se describen los requerimientos del sistema. Cada uno de los grandes grupos de requerimientos (de almacenamiento de información, funcionales y no funcionales) podrá dividirse para ayudar a la legibilidad del documento, por ejemplo dividiendo cada subsección en requerimientos asociados a un determinado objetivo, requerimientos con características comunes, etc. Requerimientos de almacenamiento de información Esta subsección debe contener la lista de requerimientos de almacenamiento de información que se hayan identificado. Requerimientos funcionales Esta subsección debe contener la lista de requerimientos funcionales que se hayan identificado, dividiéndose en los siguientes apartados que se describen a continuación: Diagrama de casos de uso : Este apartado debe contener los diagramas de casos de uso del sistema que se hayan realizado. Definición de los actores : Este apartado debe contener una lista con los actores que se hayan identificado. Casos de uso del sistema : Este apartado debe contener los casos de uso que se hayan identificado. Requerimientos no funcionales Esta subsección debe contener la lista de los requerimientos no funcionales del sistema que se hayan identificado. Matriz de rastreabilidad objetivos/requerimientos Esta sección debe contener una matriz objetivo-requisito, de forma que para cada objetivo se pueda conocer con qué requerimientos está asociado. El formato de la matriz de rastreabilidad puede verse en la siguiente figura: Conflictos pendientes de resolución Esta sección, que se incluirá en el caso de que no se opte por registrar los conflictos en un documento aparte, deberá contener los conflictos identificados durante el proceso y que aún están pendientes de resolución. Glosario de términos Esta sección, que se incluirá si se considera oportuno, deberá contener una lista ordenada alfabéticamente de los términos específicos del dominio del problema, acrónimos y abreviaturas que aparezcan en el documento y que se considere que su significado deba ser aclarado. Cada término deberá acompañarse de su significado. Apéndices Los apéndices se usarán para proporcionar información adicional a la documentación obligatoria del documento. Sólo deben aparecer si se consideran oportunos y se identificarán con letras ordenadas alfabéticamente: A, B, C, etc. TécnicasA continuación, se describen algunas de las técnicas que se proponen en esta metodología para obtener los productos de las tareas que se han descrito. Las técnicas más habituales en la elicitación de requerimientos son las entrevistas, el Joint Application Development (JAD) o Desarrollo Conjunto de Aplicaciones, el brainstorming o tormenta de ideas y la utilización de escenarios, más conocidos como casos de uso. Estas técnicas, que se describen en los siguientes apartados, se suelen completar con otras técnicas complementarias como la observación in situ, el estudio de documentación, los cuestionarios, la inmersión en el negocio del cliente o haciendo que los ingenieros de requerimientos sean aprendices del cliente. Entrevistas Las entrevistas son la técnica de elicitación más utilizada, y de hecho son practicamente inevitables en cualquier desarrollo ya que son una de las formas de comunicación más naturales entre personas. En las entrevistas se pueden identificar tres fases: preparación, realización y análisis. Preparación de entrevistas : Las entrevistas no deben improvisarse, por lo que conviene realizar las siguientes tareas previas: Estudiar el dominio del problema : Conocer las categorías y conceptos de la comunidad de clientes y usuarios es fundamental para poder entender las necesidades de dicha comunidad y su forma de expresarlas, y para generar en los clientes y usuarios la confianza de que el ingeniero de requerimientos entiende sus problemas. Para conocer el dominio del problema se puede recurrir a técnicas de estudio de documentación, a bibliografía sobre el tema, documentación de proyectos similares realizados anteriormente, la inmersión dentro de la organización para la que se va a desarrollar o a periodos de aprendizaje por parte de los ingenieros de requerimientos. Seleccionar a las personas a las que se va a entrevistar : Se debe minimizar el número de entrevistas a realizar, por lo que es fundamental seleccionar a las personas a entrevistar. Normalmente se comienza por los directivos (que pueden ofrecer una visión global), y se continúa con los futuros usuarios (que pueden aportar información más detallada) y con el personal técnico (que aporta detalles sobre el entorno operacional de la organización). Conviene también estudiar el perfil de los entrevistados, buscando puntos en común con el entrevistador que ayuden a romper el hielo. Determinar el objetivo y contenido de las entrevistas : Para minimizar el tiempo de la entrevista es fundamental fijar el objetivo que se pretende alcanzar y determinar previamente su contenido. Previamente a su realización, se pueden enviar cuestionarios (que los futuros entrevistados deben rellenar y devolver) y un pequeño documento de introducción al proyecto de desarrollo (de forma que el entrevistado conozca los temas que se van a tratar, y el entrevistador recoja información para preparar la entrevista). Es importante que los cuestionarios, si se usan, se preparen cuidadosamente teniendo en cuenta quién los va a responder y no incluir conceptos que se asuman conocidos cuando puedan no serlo. Planificar las entrevistas : La fecha, hora, lugar y duración de la entrevista deben fijarse teniendo en cuenta siempre la agenda del entrevistado. En general, se deben buscar sitios agradables donde no se produzcan interrupciones y que resulten naturales a los entrevistados. Realización de entrevistas : Dentro de la realización de las entrevistas se distinguen tres etapas: Apertura : El entrevistador debe presentarse e informar al entrevistado sobre la razón de la entrevista, qué se espera conseguir, cómo se utilizará la información, la mecánica de las preguntas, etc. Si se va a utilizar algún tipo de notación gráfica o matemática que el entrevistado no conozca debe explicarse antes de utilizarse. Es fundamental causar buena impresión en los primeros minutos. Desarrollo : La entrevista en sí no debería durar más de dos horas, distribuyendo el tiempo en un 20% para el entrevistador y un 80% para el entrevistado. Se deben evitar los monólogos y mantener el control por parte del entrevistador, contemplando la posibilidad de que una tercera persona tome notas durante la entrevista o grabar la entrevista en cinta de vídeo o audio, siempre que el entrevistado esté de acuerdo. Durante esta fase se pueden emplear distintas técnicas: Preguntas abiertas : También denominadas de libre contexto, estas preguntas no pueden responderse con un “sí” o un “no”, permiten una mayor comunicación y evitan la sensación de interrogatorio. Por ejemplo, “¿Qué se hace para registrar un pedido?”, “Dígame qué se debe hacer cuando un cliente pide una factura” o “¿Cómo se rellena un albarán?”. Estas preguntas se suelen utilizar al comienzo de la entrevista, pasando posteriormente a preguntas más concretas. En general, se debe evitar la tendencia de anticipar una respuesta a las preguntas que se formulan. Utilizar palabras apropiadas : Se deben evitar tecnicismos que no conozca el entrevistado y palabras o frases que puedan perturbar emocionalmente la comunicación. Mostrar interés en todo momento : Es fundamental cuidar la comunicación no verbal durante la entrevista: tono de voz, movimiento, expresión facial, etc. Por ejemplo, para animar a alguien a hablar puede asentirse con la cabeza, decir “ya entiendo”, “sí”, repetir algunas respuestas dadas, hacer pausas, poner una postura de atención, etc. Debe evitarse bostezar, reclinarse en el sillón, mirar hacia otro lado, etc. Terminación : Al terminar la entrevista se debe recapitular para confirmar que no ha habido confusiones en la información recogida, agradecer al entrevistado su colaboración y citarle para una nueva entrevista si fuera necesario, dejando siempre abierta la posibilidad de volver a contactar para aclarar dudas que surjan al estudiar la información o al contrastarla con otros entrevistados. Análisis de las entrevistas : Una vez realizada la entrevista es necesario leer las notas tomadas, pasarlas a limpio, reorganizar la información, contrastarla con otras entrevistas o fuentes de información, etc. Una vez elaborada la información, se puede enviar al entrevistado para confirma los contenidos. También es importante evaluar la propia entrevista para determinar los aspectos mejorables. Joint Application Development (JAD) La técnica denominada JAD (Joint Application Development, Desarrollo Conjunto de Aplicaciones), desarrollada por IBM en 1977, es una alternativa a las entrevistas individuales que se desarrolla a lo largo de un conjunto de reuniones en grupo durante un periodo de 2 a 4 días. En estas reuniones se ayuda a los clientes y usuarios a formular problemas y explorar posibles soluciones, involucrándolos y haciéndolos sentirse partícipes del desarrollo. Esta técnica se basa en cuatro principios: dinámica de grupo, el uso de ayudas visuales para mejorar la comunicación (diagramas, transparencias, multimedia, herramientas CASE, etc.), mantener un proceso organizado y racional y una filosofía de documentación WYSIWYG (What You See Is What You Get, lo que se ve es lo que se obtiene), por la que durante las reuniones se trabaja directamente sobre los documentos a generar. El JAD tienen dos grandes pasos, el JAD/Plan (cuyo objetivo es elicitar y especificar requerimientos) y el JAD/Design (en el que se aborda el diseño del software). En este documento sólo se verá con detalle el primero de ellos. Debido a las necesidades de organización que requiere y a que no suele adaptarse bien a los horarios de trabajo de los clientes y usuarios, esta técnica no suele emplearse con frecuencia, aunque cuando se aplica suele tener buenos resultados, especialmente para elicitar requerimientos en el campo de los sistemas de información. En comparación con las entrevistas individuales, presenta las siguientes ventajas: Ahorra tiempo al evitar que las opiniones de los clientes se contrasten por separado. Todo el grupo, incluyendo los clientes y los futuros usuarios, revisa la documentación generada, no sólo los ingenieros de requerimientos. Implica más a los clientes y usuarios en el desarrollo. El JAD queda definida a través de los siguientes elementos: Participantes del JAD : Se pueden distinguir seis clases de participantes o roles en el JAD: Jefe del JAD : Es el responsable de todo el proceso y asume el control durante las reuniones. Debe tener dotes de comunicación y liderazgo. Algunas habilidades importantes que debe tener son: entender y promover la dinámica de grupo, iniciar y centrar discusiones, reconocer cuándo la reunión se está desviando del tema y reconducirla, manejar las distintas personalidades y formas de ser de los participantes, evitar que decaiga la reunión aunque sea larga y difícil, etc. Analista : Es el responsable de la producción de los documentos que se deben generar durante las sesiones JAD. Debe tener la habilidad de organizar bien las ideas y expresarlas claramente por escrito. En el caso de que se utilicen herramientas software durante las sesiones, debe ser capaz de manejarlas eficientemente. Patrocinador ejecutivo : Es el que tiene la decisión final de que se lleve a cabo el desarrollo. Debe proporcionar a los demás participantes información sobre la necesidad del nuevo sistema y los beneficios que se espera obtener de él. Representantes de los usuarios : Durante el JAD/Plan suelen ser directivos con una visión global del sistema. Durante el JAD/Design suelen incorporarse futuros usuarios finales. Representantes de sistemas de información : Son personas expertas en sistemas de información que deben ayudar a los usuarios a comprender qué es o no factible con la tecnología actual y el esfuerzo que implica. Especialistas : Son personas que pueden proporcionar información detallada sobre aspectos muy concretos, tanto desde el punto de vista de los usuarios porque conocen muy bien el funcionamiento de una parte de la organización, como desde el punto de vista de los desarrolladores porque conocen perfectamente ciertos aspectos técnicos de la instalación hardware de la organización. Fases del JAD : Dentro de la técnica del JAD se distinguen tres fases: Adaptación : Es responsabilidad del jefe del JAD, ayudado por uno o dos analistas, adaptar la técnica del JAD para cada proyecto. La adaptación debe comenzar por definir el proyecto a alto nivel, para lo cual pueden ser necesarias entrevistas previas con algunos clientes y usuarios. También suele ser necesario recabar información sobre la organización para familiarizarse con el dominio del problema, por ejemplo utilizando técnicas complementarias como el estudio de documentación o la observación in situ. Una vez obtenida una primera idea de los objetivos del proyecto, es necesario seleccionar a los participantes,citarles para las reuniones y proporcionarles una lista con los temas que se van a tratar en las reuniones para que las puedan preparar. El jefe del JAD debe decidir la duración y el número de sesiones a celebrar, definir el formato de la documentación sobre la que se trabajará y preparar transparencias introductorias y todo el material audiovisual que considere oportuno. Celebración de las sesiones JAD : Durante las sesiones, los participantes exponen sus ideas y se discuten, analizan y refinan hasta alcanzar un acuerdo. Los pasos que se recomienda seguir para este proceso son los siguientes: Presentación : Se presenta y se da la bienvenida a todos los participantes por parte del patrocinador ejecutivo y del jefe del JAD. El patrocinador ejecutivo expone brevemente las necesidades que han llevado al desarrollo y los beneficios que se esperan obtener. El jefe del JAD explica la mecánica de las sesiones y la planificación prevista. Definir objetivos y requerimientos : El jefe del JAD promueve la discusión para elicitar los objetivos o requerimientos de alto nivel mediante preguntas como: “¿Por qué se construye el sistema?”, “¿Qué beneficios se esperan del nuevo sistema?”, “¿Cómo puede beneficiar a la organización en el futuro?”, “¿Qué restricciones de recursos disponibles, normas o leyes afectan al proyecto?”, “¿Es importante la seguridad de los datos?”. A medida que se van elicitando requerimientos, el analista los escribe en transparencias o en algún otro medio que permita que permanezcan visibles durante la discusión. Delimitar el ámbito del sistema : Una vez obtenido un número importante de requerimientos, es necesario organizarlos y llegar a un acuerdo sobre el ámbito del nuevo sistema. En el caso de los sistemas de información, es útil identificar a los usuarios potenciales (actores) y determinar qué tareas les ayudará a realizar (casos de uso). Documentar temas abiertos : Aquellas cuestiones que hayan surgido durante la sesión que no se han podido resolver, deben documentarse para las siguientes sesiones y ser asignadas a una persona responsable de su solución para una fecha determinada. Concluir la sesión : El jefe del JAD concluye la sesión revisando con los demás participantes la información elicitada y las decisiones tomadas. Se da la oportunidad a todos los participantes de expresar cualquier consideración adicional, fomentando por parte del jefe del JAD el sentimiento de propiedad y compromiso de todos los participantes sobre los requerimientos elicitados. Conclusión : Una vez terminadas las sesiones es necesario transformar las transparencias, notas y demás documentación generada en documentos formales. Se distinguen tres pasos: Completar la documentación : Los analistas recopilan la documentación generada durante las sesiones en documentos conformes a las normas o estándares vigentes en la organización para la que se desarrolla el proyecto. Revisar la documentación : La documentación generada se envía a todos los participantes para que la comenten. Si los comentarios son lo suficientemente importantes, se convoca otra reunión para discutirlos. Validar la documentación : Una vez revisados todos los comentarios, el jefe del JAD envía el documento al patrocinador ejecutivo para su aprobación. Una vez aprobado el documento se envían copias definitivas a cada uno de los participantes. Brainstorming El brainstorming o tormenta de ideas es una técnica de reuniones en grupo cuyo objetivo es la generación de ideas en un ambiente libre de críticas o juicios. Las sesiones de brainstorming suelen estar formadas por un número de cuatro a diez participantes, uno de los cuales es el jefe de sesión, encargado más de comenzar la sesión que de controlarla. Como técnica de elicitación de requerimientos, el brainstorming puede ayudar a generar una gran variedad de vistas del problema, y a formularlo de diferentes formas, sobre todo al comienzo del proceso de elicitación cuando los requerimientos son todavía muy difusos. Frente al JAD, el brainstorming tiene la ventaja de que es muy fácil de aprender y requiere poca organización, de hecho, hay propuestas de realización de brainstorming por vídeoconferencia a través de internet. Por otro lado, al ser un proceso poco estructurado, puede no producir resultados con la misma calidad o nivel de detalle que otras técnicas. El brainstorming se puede describir a través de los siguientes elementos: Fases del brainstorming : En elbrainstorming se distinguen las siguientes fases: Preparación : La preparación para una sesión de brainstorming requiere que se seleccione a los participantes y al jefe de la sesión, citarlos y preparar la sala donde se llevará a cabo la sesión. Los participantes en una sesión de brainstorming para elicitación de requerimientos son normalmente clientes, usuarios, ingenieros de requerimientos, desarrolladores y, si es necesario, algún experto en temas relevantes para el proyecto. Generación : El jefe abre la sesión exponiendo un enunciado general del problema a tratar, que hace de semilla para que se vayan generando ideas. Los participantes aportan libremente nuevas ideas sobre el problema semilla, bien por un orden establecido por el jefe de la sesión, bien aleatoriamente. El jefe es siempre el responsable de dar la palabra a un participante. Este proceso continúa hasta que el jefe decide parar, bien porque no se están generando suficientes ideas, en cuyo caso la reunión se pospone, bien porque el número de ideas sea suficiente para pasar a la siguiente fase. Durante esta fase se deben observar las siguientes reglas: Se prohíbe la crítica de ideas, de forma que los participantes se sientan libres de formular cualquier idea. Se fomentan las ideas más avanzadas, que aunque no sean factibles, estimulan a los demás participantes a explorar nuevas soluciones más creativas. Se debe generar un gran número de ideas, ya que cuantas más ideas se presenten más probable será que se generen mejores ideas. Se debe alentar a los participantes a combinar o completar las ideas de otros participantes. Para ello, es necesario, al igual que en la técnica del JAD, que todas las ideas generadas estén visibles para todos los participante en todo momento. Una posibilidad es utilizar como semilla objetivos del sistema e ir identificando requerimientos. Consolidación : En esta fase se deben organizar y evaluar las ideas generadas durante la fase anterior. Se suelen seguir tres pasos: Revisar ideas : Se revisan las ideas generadas para clarificarlas. Es habitual identificar ideas similares, en cuyo caso se unifican en un solo enunciado. Descartar ideas : Aquellas ideas que los participantes consideren excesivamente avanzadas se descartan. Priorizar ideas : Se priorizan las ideas restantes, identificando las absolutamente esenciales, las que estarían bien pero que no son esenciales, y las que podrían ser apropiadas para una próxima versión del sistema a desarrollar. Documentación : Después de la sesión, el jefe produce la documentación oportuna conteniendo las ideas priorizadas y comentarios generados durante la consolidación. Casos de uso Los casos de uso son una técnica para la especificación de requerimientos funcionales propuesta inicialmente en Jacobson y que actualmente forma parte de UML. Un caso de uso es la descripción de una secuencia de interacciones entre el sistema y uno o más actores en la que se considera al sistema como una caja negra y en la que los actores obtienen resultados observables en la siguiente figura: Los actores son personas u otros sistemas que interactúan con el sistema cuyos requerimientos se están describiendo. Los casos de uso presentan ciertas ventajas sobre la descripción meramente textual de los requerimientos funcionales, ya que facilitan la elicitación de requerimientos y son fácilmente comprensibles por los clientes y usuarios. Además, pueden servir de base a las pruebas del sistema y a la documentación para los usuarios. A pesar de ser una técnica ampliamente aceptada, existen múltiples propuestas para su utilización concreta. En esta metodología se propone la utilización de los casos de uso como técnica tanto de elicitación como de especificación de los requerimientos funcionales del sistema. Diagramas de casos de uso Los casos de uso tienen una representación gráfica en los denominados diagramas de casos de uso. En estos diagramas, los actores se representan en forma de pequeños monigotes, y los casos de uso se representan por elipses contenidos dentro de un rectángulo que representa al sistema. La participación de los actores en los casos de uso se indica por una flecha entre el actor y el caso de uso que apunta en la dirección en la que fluye la información. Un ejemplo de este tipo de diagramas se vio en la imagen anterior. Los diagramas de casos de uso sirven para proporcionar una visión global del conjunto de casos de uso de un sistema así como de los actores y los casos de uso en los que éstos intervienen. Las interacciones concretas entre los actores y el sistema no se muestran en este tipo de diagramas. Relaciones entre casos de uso A veces conviene establecer relaciones entre distintos casos de uso para simplificar su descripción. Las dos relaciones posibles y su semántica, cuya representación gráfica puede verse en el siguiente ejemplo son las siguientes: includes : Se dice que un caso de uso A incluye el caso de uso B, cuando B es una parte del caso de uso A, es decir, la secuencia de interacciones de B forma parte de la secuencia de interacciones de A. El caso de uso B se realiza siempre dentro del caso de uso A. Además, siempre que ocurre A ocurre también B, por lo que se dice que B es un caso de uso abstracto. Un caso de uso es abstracto si no puede ser realizado por sí mismo, por lo que sólo tiene significado cuando se utiliza para describir alguna funcionalidad que es común a otros casos de uso. Por otra parte, un caso de uso será concreto si puede ser iniciado por un actor y realizado por sí mismo. Se suele utilizar esta relación cuando se detectan subsecuencias de interacciones comunes a varios casos de uso. Dichas subsecuencias comunes se extren como “factor común” de los casos de uso que las contienen. Posteriormente son incluido por los casos de uso de los que se han “extraído”. De esta forma se evita repetir las mismas subsecuencias de interacciones una y otra vez en varios casos de uso. extends : Un caso de uso A extiende a otro caso de uso B cuando A es una subsecuencia de interacciones de B, que ocurre en una determinada circunstancia. En cierta forma, A completa la funcionalidad de B. El caso de uso A puede realizarse o no cuando se realiza el caso de uso B, según las circunstancias. Por otro lado, el caso de uso A puede ser un caso de uso abstracto o concreto, en cuyo caso puede ocurrir sin necesidad de que ocurra el caso de uso B. Organización de casos de uso En la mayoría de sistemas, el número de casos de uso es lo suficientemente elevado como para que sea oportuno organizarlos de alguna forma en lugar de tener una lista plana por la que no es fácil navegar. Una posible forma de organizar los casos de uso es recurrir a los paquetes. De esta forma, los casos de uso pueden organizarse en niveles, facilitando así su comprensión. Cada paquete contiene a otros paquetes o a varios casos de uso. En el caso de que los casos de uso se agrupen por criterios funcionales, los paquetes que los agrupan pueden estereotiparse como subsistemas, tal como puede verse en la siguiente figura: PrototipadoLos prototipos permiten al desarrollador crear un modelo del software que debe ser construido. Un prototipo es la primera versión de un nuevo tipo de producto, en el que se han incorporado sólo algunas características del sistema final, o no se han realizado completamente. Es un modelo o maqueta del sistema que se construye para comprender mejor el problema y sus posibles soluciones. Esto permite: Evaluar mejor los requerimientos. Probar opciones de diseño. El uso principal es la ayuda a los clientes y los desarrolladores para entender los requerimientos del sistema. El prototipo puede ser usado para entrenamiento antes de que el sistema final sea entregado. El prototipo también puede ser utilizado para pruebas. Las características de los prototipos son: Funcionalidad limitada. Poca fiabilidad. Características de operación pobres. Hay que tener en cuenta que el prototipo representa aproximadamente un 10% del presupuesto del proyecto, y normalmente supone pocos días de desarrollo. Al igual que todos los enfoques orientados al proceso de desarrollo del software, el prototipado comienza con la captura de requerimientos. Desarrolladores y clientes se reúnen y definen los objetivos globales del software, identifican todos los requerimientos que son conocidos, y señalan áreas en las que será necesaria la profundización en las definiciones. Después de esto, tiene lugar un “diseño rápido”. El diseño rápido se centra en una representación de aquellos aspectos del software que serán visibles al usuario (por ejemplo, entradas y formatos de las salidas). El diseño rápido lleva a la construcción de un prototipo. El prototipo es evaluado por el cliente y el usuario, y utilizado para refinar los requerimiento del software a ser desarrollado. Un proceso de iteración tiene lugar a medida que el prototipo es perfeccionado para satisfacer las necesidades del cliente y permitir al mismo tiempo una mejor comprensión del problema por parte del desarrollador. Existen principalmente dos tipos de prototipos: Prototipo rápido (concept prototype) : El prototipado rápido es un mecanismo para lograr la validación inicial. Se utiliza para validar requerimientos en una etapa previa al diseño específico. En este sentido, el prototipo puede ser visto como una aceptación tácita de que los requerimientos no son totalmente conocidos o entendidos antes del diseño y la implementación. El prototipo rápido puede ser usado como un medio para explorar nuevos requerimientos y así ayudar a controlar su constante evolución. Prototipo evolutivo : Desde una perspectiva diferente, todo el ciclo de vida de un producto puede ser visto como una serie incremental de detallados prototipos acumulativos. Tradicionalmente, el ciclo de vida está dividido en dos fases distintas: desarrollo y mantenimiento. La experiencia ha demostrado que esta distinción es arbitraria y va en contra de la realidad, ya que, la mayor parte del costo del software ocurre después de que el producto se ha entregado. El desarrollo evolutivo del ciclo de vida del software considera a la primera entrega como un prototipo inicial en uso. Modificaciones y mejoras subsecuentes resultan en nuevas entregas de prototipos más maduros. Este proceso continúa hasta que se haya desarrollado el producto final (véase la figura). La adopción de esta óptica elimina la distinción arbitraria entre desarrollo y mantenimiento, resultando en un importante cambio de mentalidad que afecta las estrategias para la estimación de costos, enfoques de desarrollo y adquisición de productos. Herramientas para el prototipado Lenguajes dinámicos de alto nivel. Los más usados son: Smalltalk (basado en objetos, sistemas interactivos) Java (basado en objetos, sistemas interactivos) Prolog (lógico, procesamiento simbólico) LISP (basado en listas, procesamiento simbólico) Lenguajes de cuarta generación (4GLs) (programación de BBDD): La mayoría de aplicaciones de gestión son interactivas e implican la manipulación de una BD y la producción de salidas que involucran organizar y dar formato a esos datos. Lenguaje de programación de BBDD (y su entorno de desarrollo), que contiene conocimiento de la BD y operaciones para manipulación de la misma. Lenguaje no procedimental. Reducen claramente los costos del desarrollo. Muy usados en prototipado evolutivo. Muchos 4GLs permiten el desarrollo de interfaces de BBDD basadas en navegadores web. General SQL o código en lenguaje “de bajo nivel” como COBOL. Menos eficientes que los lenguajes de programación convencionales. Por ejemplo, un programa en 4GL reescrito en C++ tiene un 50% menos de requerimientos de memoria y es 10 veces más rápido. Reducen claramente los costos del desarrollo, aunque no sucede lo mismo con el mantenimiento de los mismos, ya que: Son programas no estructurados difíciles de mantener. No están estandarizados ni son uniformes, y por tanto, los usuarios pueden tener que reescribir totalmente los programas debido a que el lenguaje ha quedado obsoleto. Ensamblaje de componentes y aplicaciones. El desarrollo de prototipos con reutilización comprende dos niveles: El nivel de aplicación, en el que una aplicación completa se integra con el prototipo. Por ejemplo: si el prototipo requiere procesamiento de textos, se puede integrar un sistema estándar de procesamiento de textos (MS Office). El nivel de componente, en el que los componentes se integran en un marco de trabajo estándar. Dentro del nivel de componentes se usan diversos tipos de lenguajes de programación, cada uno de ellos adecuado para tareas diversas: Visual Basic, TCL/TK, Python, Perl, … Lenguajes de alto nivel sin tipos, con facilidades gráficas. Desarrollo rápido de aplicaciones pequeñas y relativamente sencillas, construidas por una persona o conjunto de personas. No existe una arquitectura explícita del sistema. CORBA, DCOM, JavaBeans. Junto con un marco arquitectónico, es más apropiado para sistemas grandes. Programación distribuida. Bibliografía Scribd (Alfredo Márquez) Análisis estructurado. Diagramas de flujo de datos. Diagramas de estructura. Diccionario de datos. Flujogramas.Análisis EstructuradoConceptoCuando los analistas comienzan a trabajar sobre un proyecto de sistema de información, tienen que profundizar en un área de la Organización, de la cual tienen poco conocimiento. Del trabajo del analista se espera que se produzca una mejora en el sistema. Así que el analista debe ser capaz de: Aprender los detalles y procedimientos del sistema en uso. Prever necesidades futuras de la Organización en función del crecimiento, cambios futuros en el sector, introducción de nuevas tecnologías, etc. Documentar detalles del sistema actual para su comprensión y discusión por otros profesionales de la organización. Evaluar la efectividad y eficiencia del sistema actual y sus procedimientos. Recomendar modificaciones del sistema actual, o proponer un nuevo sistema completo, justificándolo en cada caso. Documentar las características del nuevo sistema con un nivel de detalle que permita comprender a otros sus componentes. Fomentar la participación de gerentes y empleados en todo el proceso. A todas estas tareas, se les une la de cumplir los plazos establecidos. De modo que una de las claves del éxito será la de estructurar el proceso para el desarrollo del nuevo sistema. Análisis Estructurado ¿Para qué?Por la propia naturaleza los sistemas de información, éstos no están bien estructurados, no siguen leyes como las ciencias, dependen de muchas circunstancias para su funcionamiento (personas, influencias políticas de la organización, restricciones, etc). El analista debe luchar contra estas circunstancias y determinar los requerimientos de los sistemas de información. Ante esta realidad, surgen preguntas como: ¿Deben dos analistas desarrollar una lista idéntica de requerimientos cuando estudian de forma independiente la misma situación? ¿Para una situación dada tenemos un único diseño correcto posible? La respuesta es que dos analistas que examinan de forma independiente una situación, sin herramientas y técnicas preestablecidas, recopilan información diferente que describa el sistema y por lo tanto en determinación de requerimientos diferentes. Esto obliga a normalizar, a estructurar el análisis de sistemas de información. Podemos definir análisis estructurado como: El método para el análisis de sistemas manuales o automatizados, que conduce al desarrollo de especificaciones para sistemas nuevos o para efectuar modificaciones a los ya existentes. El análisis estructurado permite al analista conocer un proceso (actividad) en una forma lógica y manejable al mismo tiempo que proporciona la base para asegurar que no se omite ningún detalle pertinente. Por otra parte una de las claves del éxito de un buen análisis será el que exista una buena comunicación entre usuarios y analistas, esto obliga a disponer de un lenguaje común, sencillo y fiable de modo que permita minimizar costes y errores, y maximizar calidad. ¿Qué debemos estructurar?El objetivo que persigue el análisis estructurado es organizar las tareas asociadas con la determinación de requerimientos para obtener la comprensión completa y exacta para una situación dada. A partir de aquí se determinan los requerimientos que serán la base de un sistema nuevo o modificado. La palabra estructura significa: El método intenta estructurar el proceso de determinación de los requerimientos comenzando con la documentación del sistema existente. El proceso intenta incluir todos los detalles relevantes que describen al sistema en uso. Fácil verificar cuando se han omitido datos relevantes. La identificación de los requerimientos será similar entre varios analistas e incluirá las mejores soluciones y estrategias para las oportunidades de desarrollo de sistemas. Los documentos de trabajo generados para documentar los sistemas existentes y propuestos son dispositivos de comunicación eficientes. Componentes del análisis estructuradoEl análisis estructurado hace uso de los siguientes componentes: Símbolos gráficos. Iconos y convenciones para identificar y describir los componentes de un sistema junto con las relaciones entre esos componentes. Diccionario de datos. Descripción de todos los datos utilizados en el sistema. Descripciones de procesos y procedimientos. Declaraciones formales que emplean técnicas y lenguajes que permiten a los analistas describir actividades importantes que forman parte del sistema. Reglas. Estándares para describir y documentar el sistema en forma correcta y completa. El método de análisis estructurado es sinónimo de análisis de flujo de datos que es una herramienta para documentar el sistema existente o actual y determinar los requerimientos de información de forma estructurada. ¿Qué es análisis de flujo de datos?Los analistas desean conocer las respuestas a cuatros preguntas: ¿Qué procesos integran el sistema? ¿Qué datos emplea cada proceso? ¿Qué datos son almacenados? ¿Qué datos entran y salen del sistema? Como vemos el elemento fundamental en una Organización (sistema de información), van a ser los datos. Los datos son las guías de las actividades de la Organización, inician eventos, son procesados para dar información útil al personal, etc. Seguir el flujo de datos por todos los procesos de la organización, además de ser la finalidad del análisis de flujo de datos, proporciona a los analistas información de cómo se alcanzan los objetivos en la Organización. El análisis de flujo de datos estudia el empleo de los datos en cada actividad. Se basa en los diagramas de flujo de datos que muestra de forma gráfica la relación entre procesos y datos, y en los diccionarios de datos que describen de manera formal los datos del sistema y los sitios donde son utilizados. La estrategia de los flujos de datosEl análisis puede pensarse de tal manera que se estudien actividades del sistema desde el punto de vista de los datos, donde se originan, cómo se utilizan o cambian, hacia dónde van. Los componentes de la estrategia de flujo de datos abarcan tanto la determinación de los requerimientos como el diseño de sistemas. Una notación bien establecida facilita la documentación del sistema actual y su análisis por todos los participantes en el proceso de determinación de requerimientos. Herramientas para el análisis de flujo de datosLas herramientas tienen el objetivo de ayudar a entender las características del sistema. Por lo tanto no deben de ser un fin, sino un medio para el estudio del sistema. Las herramientas utilizadas en el análisis de flujo de datos son: Diagrama de flujo de datos.Una herramienta gráfica empleada para describir y analizar el movimiento de datos a través de un sistema, incluyendo procesos, almacenamiento de datos y retrasos del sistema. Los diagramas de flujo de datos es la herramienta más importante y la base sobre la cual se desarrollan otros componentes.La transformación de datos de entrada en salida por medio de procesos puede describirse en forma lógica e independiente de los componentes físicos. Estos diagramas reciben el nombre de diagramas lógicos de flujo de datos, en contraste de los diagramas físicos del flujo de datos que muestran la implantación y movimiento real de datos entre personas, departamentos y estaciones de trabajo. Diccionario de datos.El diccionario de datos contiene las características lógicas de los sitios donde se almacenan los datos del sistema, incluyendo nombre, descripción, alias, contenidos y organización, así como los procesos donde se emplea los datos y los sitios donde se necesita el acceso inmediato a la información. Servirán para identificar los requerimientos de las bases de datos durante el diseño del sistema. Diagrama entidad-relación.Este diagrama es una descripción de la relación entre entidades (personas, lugares, eventos y objetos) de un sistema y el conjunto de información relacionado con la entidad. No considera el almacenamiento físico de datos. Gráfica de estructura (Especificación de procesos).Herramienta de diseño que muestra con símbolos la relación entre módulos de procesamiento y el software de la computadora. Incluye el análisis de las transformaciones entrada transformación salida y el análisis de transacciones. Ventajas del análisis de flujo de datosLos analistas deben trabajar con los usuarios para hacerles comprender el funcionamiento del sistema actual y el sistema futuro, para ello se hace aconsejable utilizar un lenguaje común, sencillo y fiable, estas son las características de los diagramas de flujo de datos. Los usuarios pueden hacer sugerencias para modificar los diagramas con la finalidad de describir las actividades con mayor exactitud, y permitirá evitar los errores desde el inicio pudiendo prevenir una posible falla del sistema. Diagrama de Flujo de Datos (DFD)El objetivo del diagrama de flujo de datos es la obtención de un modelo lógico de procesos que represente el sistema, con independencia de las restricciones físicas del entorno. Así se facilita su comprensión por los usuarios y los miembros del equipo de desarrollo. El sistema se divide en distintos niveles de detalle, con el objetivo de: Simplificar la complejidad del sistema, representando los diferentes procesos de que consta. Facilitar el mantenimiento del sistema. DescripciónUn diagrama de flujo de datos es una técnica muy apropiada para reflejar de una forma clara y precisa los procesos que conforman el sistema de información. Permite representar gráficamente los límites del sistema y la lógica de los procesos, estableciendo qué funciones hay que desarrollar. Además, muestra el flujo o movimiento de los datos a través del sistema y sus transformaciones como resultado de la ejecución de los procesos. Esta técnica consiste en la descomposición sucesiva de los procesos, desde un nivel general, hasta llegar al nivel de detalle necesario para reflejar toda la semántica que debe soportar el sistema en estudio. El diagrama de flujo de datos se compone de los siguientes elementos: Entidad externa : representa un ente ajeno al sistema que proporciona o recibe información del mismo. Puede hacer referencia a departamentos, personas, máquinas, recursos u otros sistemas. El estudio de las relaciones entre entidades externas no forma parte del modelo.Puede aparecer varias veces en un mismo diagrama, así como en los distintos niveles del DFD para mejorar la claridad del diagrama. Proceso : representa una funcionalidad que tiene que llevar a cabo el sistema para transformar o manipular datos. El proceso debe ser capaz de generar los flujos de datos de salida a partir de los de entrada, más una información constante o variable al proceso.El proceso nunca es el origen ni el final de los datos, puede transformar un flujo de datos de entrada en varios de salida y siempre es necesario como intermediario entre una entidad externa y un almacén de datos. Almacén de datos : representa la información en reposo utilizada por el sistema independientemente del sistema de gestión de datos (por ejemplo un fichero, base de datos, archivador, etc). Contiene la información necesaria para la ejecución del proceso.El almacén no puede crear, transformar o destruir datos, no puede estar comunicado con otro almacén o entidad externa y aparecerá por primera vez en aquel nivel en que dos o más procesos accedan a él. Flujo de datos : representa el movimiento de los datos, y establece la comunicación entre los procesos y los almacenes de datos o las entidades externas.Un flujo de datos entre dos procesos sólo es posible cuando la información es síncrona, es decir, el proceso destino comienza cuando el proceso origen finaliza su función.Los flujos de datos que comunican procesos con almacenes pueden ser de los siguientes tipos:– De consulta : representan la utilización de los valores de uno o más campos de un almacén o la aprobación de que los valores de los campos seleccionados cumplen unos criterios determinados.– De actualización : representan la alteración de los datos de un almacén como consecuencia de la creación de un nuevo elemento, por eliminación o modificación de otros ya existentes.– De diálogo : es un flujo entre un proceso y un almacén que representa una consulta y una actualización. Existen sistemas que precisan de información orientada al control de datos y requieren flujos y procesos de control, así como los mecanismos que desencadenan su ejecución. Para que resulte adecuado el análisis de estos sistemas, se ha ampliado la notación de los diagramas de flujo de datos incorporando los siguientes elementos: Proceso de control : representa procesos que coordinan y sincronizan las actividades de otros procesos del diagrama de flujo de datos. Flujo de control : representa el flujo entre un proceso de control y otro proceso. El flujo de control que sale de un proceso de control activa al proceso que lo recibe y el que entre le informa de la situación de un proceso. A diferencia de los flujos tradicionales, que pueden considerarse como procesadores de datos porque reflejan el movimiento y transformación de los mismos, los flujos de control no representan datos con valores, sino que en cierto modo, se trata de eventos que activan los procesos (señales o interrupciones). Descomposición o explosión por niveles Los diagramas de flujo de datos han de representar el sistema de la forma más clara posible, por ello su construcción se basa en el principio de descomposición o explosión en distintos niveles de detalle. La descomposición por niveles se realiza de arriba abajo (top-down), es decir, se comienza en el nivel más general y se termina en el más detallado, pasando por los niveles intermedios necesarios. De este modo se dispondrá de un conjunto de particiones del sistema que facilitarán su estudio y desarrollo. La explosión de cada proceso de un DFD origina otro DFD y es necesario comprobar que se mantiene la consistencia de información entre ellos, es decir, que la información de entrada y de salida de un proceso cualquiera se corresponde con la información de entrada y de salida del diagrama de flujo de datos en el que se descompone. En cualquiera de las explosiones puede aparecer un proceso que no necesite descomposición. A éste se le denomina Proceso primitivo y sólo se detalla en él su entrada y su salida, además de una descripción de lo que realiza. En la construcción hay que evitar en lo posible la descomposición desigual, es decir, que un nivel contenga un proceso primitivo, y otro que necesite ser particionado en uno o varios niveles más. El modelo de procesos deberá contener: Un diagrama de contexto (Nivel 0). Un diagrama 0 (Nivel 1). Tantos diagramas 1, 2, 3, …, n como funciones haya en el diagrama 0 (Nivel 2). Tantos niveles intermedios como sea necesario. Varios DFD en el último nivel de detalle. El diagrama de contexto tiene como objetivo delimitar el ámbito del sistema con el mundo exterior definiendo sus interfaces. En este diagrama se representa un único proceso que corresponde al sistema en estudio, un conjunto de entidades externas que representan la procedencia y destino de la información y un conjunto de flujos de datos que representan los caminos por los que fluye dicha información. A continuación, este proceso se descompone en otro DFD, en el que se representan los procesos principales o subsistemas. Un subsistema es un conjunto de procesos cuyas funcionalidades tienen algo en común. Éstos deberán ser identificados en base a determinados criterios, como por ejemplo: funciones organizativas o administrativas propias del sistema, funciones homogéneas de los procesos, localización geográfica de los mismos, procesos que actualicen los mismos almacenes de datos, etc. Cada uno de los procesos principales se descompone a su vez en otros que representan funciones más simples y se sigue descomponiendo hasta que los procesos estén suficientemente detallados y tengan una funcionalidad concreta, es decir, sean procesos primitivos. Como resultado se obtiene un modelo de procesos del sistema de información que consta de un conjunto de diagramas de flujo de datos de diferentes niveles de abstracción, de modo que cada uno proporciona una visión más detallada de una parte definida en el nivel anterior. Además de los diagramas de flujo de datos, el modelo de procesos incluye la especificación de los flujos de datos, de los almacenes de datos y la especificación detallada de los procesos que no precisan descomposición, es decir los procesos de último nivel o primitivos. En la especificación de un proceso primitivo se debe describir, de una manera más o menos formal, cómo se obtienen los flujos de datos de salida a partir de los flujos de datos de entrada y características propias del proceso. Dependiendo del tipo de proceso se puede describir el procedimiento asociado utilizando un lenguaje estructurado o un pseudocódigo, apoyándose en tablas de decisión o árboles de decisión. A continuación se muestra un ejemplo gráfico que representa la descomposición jerárquica de los diagramas de flujo de datos. NotaciónEntidad externa Se representa mediante una elipse con un identificador y un nombre significativo en su interior. Si la entidad externa aparece varias veces en un mismo diagrama, se representa con una línea inclinada en el ángulo superior izquierdo. Proceso Se representa por un rectángulo subdividido en tres casillas donde se indica el nombre del proceso, un número identificativo y la localización. Si el proceso es de último nivel, se representa con un asterisco en el ángulo inferior derecho separado con una línea inclinada. El nombre del proceso debe ser lo más representativo posible. Normalmente estará constituido por un verbo más un sustantivo. El número identificativo se representa en la parte superior izquierda e indica el nivel del DFD en que se está. Hay que resaltar que el número no indica orden de ejecución alguno entre los procesos ya que en un DFD no se representa una secuencia en el tratamiento de los datos. El número que identifica el proceso es único en el sistema y debe seguir el siguiente estándar de notación: El proceso del diagrama de contexto se numera como cero. Los procesos del siguiente nivel se enumeran desde 1 y de forma creciente hasta completar el número de procesos del diagrama. En los niveles inferiores se forma con el número del proceso en el que está incluido seguido de un número que lo identifica en ese contexto. La localización expresa el nombre del proceso origen de la descomposición que se esté tratando. Almacén de datos Se representa por dos líneas paralelas cerradas en un extremo y una línea vertical que las une. En la parte derecha se indica el nombre del almacén de datos y en la parte izquierda el identificador de dicho almacén en el DFD. Si un almacén aparece repetido dentro de un DFD se puede representar con dos líneas verticales. Flujo de datos Se representa por una flecha que indica la dirección de los datos, y que se etiqueta con un nombre representativo. La representación de los flujos de datos entre procesos y almacenes es la siguiente: Proceso de control Se representa por un rectángulo, con trazo discontinuo, subdividido en tres casillas donde se indica el nombre del proceso, un número identificativo y la localización. Flujo de control Se representa por una flecha con trazo discontinuo que indica la dirección de flujo y que se etiqueta con un nombre representativo. Ejemplo La figura es un diagrama de flujos de un Sistema Gestor de Pedidos. En él están representados todos los elementos que pueden intervenir en un Diagrama de Flujo de Datos. Consistencia de los diagramas de flujo de datosUna vez construidos los diagramas de flujo de datos que componen el modelo de procesos del sistema de información, es necesario comprobar y asegurar su validez. Para ello, se debe estudiar cada diagrama comprobando que es legible, de poca complejidad y si los nombres asignados a sus elementos ayudan a su comprensión sin ambigüedades. Además, los diagramas deben ser consistentes. En los diagramas hay que comprobar que en un DFD resultado de una explosión: No falten flujos de datos de entrada o salida que acompañaban al proceso del nivel superior. No aparezca algún flujo que no estuviese ya asociado al proceso de nivel superior. Todos los elementos del DFD resultante deben estar conectados directa o indirectamente con los flujos del proceso origen. A continuación se incluyen ejemplos de la consistencia o inconsistencia de los diagramas de flujo de datos. Sea el diagrama de contexto de la figura. Los flujos A, C y D, entran al sistema, y el flujo B sale de él. Ejemplo de consistencia de diagramas de flujo de datos En la explosión del sistema en el diagrama de nivel 1, aparecen todos los flujos, y en su sentido correcto: A y C entran al subsistema o proceso 1, B sale del proceso 2, y D entra en el proceso 3. Se observa que el proceso 3, origina dos flujos de salida: E que va al proceso 1, y F al proceso 2. La descomposición del proceso 1, muestra los flujos A, C y E correctamente, como entradas a las funciones del diagrama. Los demás flujos están enlazados con los almacenes A1 y A2 del mismo modo que en el diagrama anterior. Ejemplo de inconsistencia de diagramas de flujo de datos Partiendo del mismo diagrama de contexto utilizado en el anterior ejemplo, los flujos A, C y D, que entran al sistema, y el flujo B, que sale de él, deben aparecer en la primera descomposición, el diagrama de nivel 1. En la figura se aprecia que falta el flujo D, y hay un flujo G que o bien falta en el nivel anterior, sobra en este. Por otro lado, en el proceso 3 no entra ningún flujo, no es posible por tanto que transforme datos saliendo los flujos E y F y además está desconectado del nivel anterior. En el siguiente paso, la inconsistencia más clara es la falta del flujo C, que entra al proceso 1, y sin embargo no aparece en su explosión. Además, hay otra inconsistencia respecto al almacén A1: en el diagrama del nivel anterior, el proceso 1 se conectaba con un flujo de entrada-salida a este almacén, cosa que no se refleja en el diagrama de este proceso, en el que sólo aparece uno de entrada. Ejemplo de construcción El caso en estudio es un modelo de procesos de un sistema de información de Conocimientos de técnicos. Según estos conocimientos, los técnicos podrán ser asignados a determinados proyectos de la organización. El sistema recogerá la información referente a los técnicos, procedente de la Dirección técnica de la organización y de los proyectos, procedente de cualquier sección o Unidad de Negocio en las que está dividida dicha organización. Las entidades externas son pues Dirección Técnica y Unidad de Negocio, que introducen los datos al sistema y hacen peticiones de consultas e informes sobre los técnicos y sus conocimientos. El diagrama de contexto será el siguiente: Los flujos de entrada son: Datos Técnicos, con datos de los técnicos introducidos por la Dirección Técnica, así como posibles peticiones de información sobre ellos; y Datos Unidad, que proviene de la Unidad de Negocio, conteniendo datos referentes a la unidad, de proyectos y clientes, así como posibles peticiones de consultas sobre los mismos. Los flujos de salida son: Información Técnicos, que contendrá datos de técnicos, de consulta o informes, para uso de la Dirección Técnica y Consultas Unidad, con datos requeridos por la Unidad de Negocio. El sistema de Conocimientos se descompone en el diagrama de nivel 1, conteniendo do subsistemas. El subsistema 1 recogerá las funciones a realizar con los datos de los técnicos de la organización (actualizaciones, consultas, informes, etc), por lo que se denomina Tratar Técnicos. El subsistema 2 contendrá las funciones asociadas al procesamiento de datos de proyectos, por lo que se le da el nombre Tratar Proyectos. En el diagrama se encuentran cuatro almacenes, tres de los cuales son accedidos por funciones de los dos subsistemas: A1 Conocimientos, A2 Proyectos y A3 Técnicos. El cuarto, A4 Clientes, sólo es accedido por el subsistema Tratar Proyectos. Los flujos sin nombre indican que hay entrada y/o salida de todos los datos del almacén. En este diagrama siguen apareciendo las entidades externas para la mayor comprensión del mismo. A partir de ahora, se centrará el ejemplo en la descomposición del subsistema 1 Tratar Técnicos, hasta llegar a su nivel más detallado. En el diagrama resultado de la explosión de Tratar Técnicos, se incluyen cuatro procesos o funciones para el tratamiento completo de éstos. El flujo de entrada Datos Técnicos se compone tanto de los datos profesionales de los técnicos, como de datos de peticiones de información sobre los mismos, por lo cual se ha dividido en dos: Datos Profesionales, que es entrada del proceso 1.1 Validad datos Técnicos y Peticiones Información Técnicos, que entra en la función 1.4 Informar. Para la validación, el proceso 1.1 Validar Datos Técnicos obtiene información del almacén A3 Técnicos y genera una salida, el flujo Datos Técnicos Correctos, que lleva los datos válidos a la función 1.2 Actualizar Almacenes Técnicos. Esta función se encarga de actualizar los almacenes A3 Técnicos y A1 Conocimientos, pero también emite un flujo al proceso 1.3 Asignar a Proyectos. Éste se encarga de hacer asignaciones de técnicos en el almacén A2 Proyectos. La función 1.4 Informar, recibe las peticiones de información sobre técnicos, las procesa utilizando los almacenes necesarios y genera el flujo Información Técnicos que irá a la entidad Dirección Técnica, según muestran los primeros diagramas. Obsérvese que para mayor claridad no se ha incluido ya ninguna entidad externa, y además, se ha repetido el almacén A3 Técnicos, evitando que el cruce de flujos oscurezca la lectura del diagrama. En este momento, todos los procesos se consideran primitivos, excepto el proceso 1.4 Informar, del que se obtiene su descomposición. Sus funciones han de obtener Informes Técnicos y Consultas Técnicos, flujos que componen Información Técnicos que aparecía en el nivel anterior. Por otro lado, también aparece dividido el flujo de entrada Peticiones Información Técnicos, diferenciando la entrada al proceso de consultas o al de emisión de informes. Por último, se puede apreciar que los almacenes son los mismos que se conectaban con el proceso en el nivel anterior y los flujos son de entrada a las funciones. (Nota.- Esta notación es la más habitual, pero MÉTRICA Versión 3 no exige su utilización). Diagrama de EstructuraEl objetivo de este diagrama es representar la estructura modular del sistema o de un componente del mismo y definir los parámetros de entrada y salida de cada uno de los módulos. Para su realización se partirá del modelo de procesos obtenido como resultado de la aplicación de la técnica de diagrama de flujo de datos (DFD). DescripciónUn diagrama de estructura se representa en forma de árbol con los siguientes elementos: Módulo : división del software clara y manejable con interfaces modulares perfectamente definidas. Un módulo puede representar un programa, subprograma o rutina dependiendo del lenguaje a utilizar. Admite parámetros de llamada y retorno. En el diseño de alto nivel hay que ver un módulo como una caja negra , donde se contemplan exclusivamente sus entradas y sus salidas y no los detalles de la lógica interna del módulo. Para que se reduzca la complejidad del cambio ante una determinada modificación, es necesario que los módulos cumplan las siguientes condiciones: Que sean de pequeño tamaño. Que sean independientes entre sí. Que realicen una función clara y sencilla. Conexión : representa una llamada de un módulo a otro. Parámetro : información que se intercambia entre los módulos. Pueden ser de dos tipos en función de la clase de información a procesar: Control : son valores de condición que afectan a la lógica de los módulos llamados. Sincronizan la operativa de los módulos. Datos : información compartida entre módulos y que es procesada en los módulos llamados. Otros componentes que se pueden representar en el diagrama de estructura son: Módulo predefinido : es aquel módulo que está disponible en la biblioteca del sistema o de la propia aplicación, y por tanto no es necesario codificarlo. Almacén de datos : es la representación física del lugar donde están almacenados los datos del sistema. Dispositivo físico : es cualquier dispositivo por el cual se puede recibir o enviar información que necesite el sistema. Estructuras del diagrama Existen ciertas representaciones gráficas que permiten mostrar la secuencia de las llamadas entre módulos. Las posibles estructuras son: Secuencial : un módulo llama a otros módulos una sola vez y, se ejecutan de izquierda a derecha y de arriba a abajo. Repetitiva : cada uno de los módulos inferiores se ejecuta varias veces mientras se cumpla una condición. Alternativa : cuando el módulo superior, en función de una decisión, llama a un módulo u otro nivel de los de nivel inferior. Principios del diseño estructurado El diagrama de estructura se basa en tres principios fundamentales: La descomposición de los módulos, de manera que los módulos que realizan múltiples funciones se descompongan en otros que sólo realicen una. Los objetivos que se persiguen con la descomposición son: Reducir el tamaño del módulo. Hacer el sistema más fácil de entender y modificar y por lo tanto facilitar el mantenimiento del mismo. Minimizar la duplicidad de código. Crear módulos útiles. La jerarquía entre los módulos, de forma que los módulos de niveles superiores coordinen a los de niveles inferiores. Al dividir los módulos jerárquicamente, es posible controlar el número de módulos que interactúan con cualquiera de los otros. La independencia de los módulos, de manera que cada módulo se ve como una caja negra, y únicamente es importante su función y su apariencia externa, y no los detalles de su construcción. Estrategias de diseño Dependiendo de la estructura inicial del diagrama de flujo de datos sobre el que se va a realizar el diseño, existen dos estrategias para obtener el diagrama de estructura. El uso de una de las dos estrategias no implica que la otra no se utilice, eso dependerá de las características de los procesos representados en DFD. Estas estrategias son: Análisis de transformación. Análisis de transacción. Análisis de Transformación El análisis de transformación es un conjunto de pasos que permiten obtener, a partir de un DFD con características de transformación, la estructura del diseño de alto nivel del sistema. Un DFD con características de transformación es aquél en el que se pueden distinguir: Flujo de llegada o entrada. Flujo de transformación o centro de transformación que contiene los procesos esenciales del sistema y es independiente de las características particulares de la entrada y la salida. Flujo de salida. Los datos que necesita el sistema se recogen por los módulos que se encuentren en las ramas de la izquierda, de modo que los datos que se intercambian en esa rama serán ascendentes. En las ramas centrales habrá movimiento de información compartida, tanto ascendente como descendente. En las ramas de la derecha, la información será de salida y, por lo tanto, descendente. Los pasos a realizar en el análisis de transformación son: Identificar el centro de transformación. Para ello será necesario delimitar los flujos de llegada y salida de la parte del DFD que contiene las funciones esenciales del sistema. Realizar el “primer nivel de factorización” o descomposición del diagrama de estructura. Habrá que identificar tres módulos subordinados a un módulo de control del sistema: Módulo controlador del proceso de información de entrada. Módulo controlador del centro de transformación. Módulo controlador del proceso de la información de salida. Elaborar el “segundo nivel de factorización”. Se transforma cada proceso del DFD en un módulo del diagrama de estructura. Revisar la estructura del sistema utilizando medidas y guías de diseño. A continuación se muestra un gráfico explicativo de dicha estrategia de diseño: Análisis de Transacción El análisis de transacción se aplica cuando en un DFD existe un proceso que en función del flujo de llegada, determina la elección de uno o más flujos de información. Se denomina centro de transacción al proceso desde el que parten los posibles caminos de información. Los pasos a realizar en el análisis de transacción son: Identificar el centro de transacción. Se delimita la parte del DFD en la que a partir de un camino de llegada se establecen varios caminos de acción. Transformar el DFD en la estructura adecuada al proceso de transacciones. El flujo de transacciones se convierte en una estructura de programa con una bifurcación de entrada y una de salida. Factorizar la estructura de cada camino de acción. Cada camino se convierte en una estructura que se corresponde con las características específicas del flujo (de transacción o de transformación). Refinar la estructura del sistema utilizando medidas y guías de diseño. A continuación se muestra un gráfico explicativo de dicha estrategia de diseño: Evaluación del diseño Una vez que hayan sido elaborados los diagramas de estructura, habrá que evaluar el diseño estudiando distintos criterios y medidas. Se utilizan dos métricas que miden la calidad estructural de un diseño: Acoplamiento. Cohesión. El acoplamiento se puede definir como el grado de interdependencia existente entre los módulos, por tanto, depende del número de parámetros que se intercambian. El objetivo es que el acoplamiento sea el mínimo posible, es decir, conseguir que los módulos sean lo más independientes entre sí. Es deseable un bajo acoplamiento, debido a que cuantas menos conexiones existan entre dos módulos, menor será la posibilidad de que aparezcan efectos colaterales al modificar uno de ellos. Además, se mejora el mantenimiento, porque al cambiar un módulo por otro, hay menos riesgo de actualizar la lógica interna de los módulos asociados. Los diferentes grados de acoplamiento son: De datos : los módulos se comunican mediante parámetros que constituyen elementos de datos simples. De marca : es un caso particular del acoplamiento de datos, donde la comunicación entre módulos es a través de estructuras de datos. De control : aparece cuando uno o varios de los parámetros de comunicación son de control, es decir variables que controlan las decisiones de los módulos subordinados o superiores. Externo : los módulos están ligados a componentes externos (dispositivos E/S, protocolos de comunicaciones, etc). Común : varios módulos hacen referencia a un área común de datos. Los módulos asociados al área común de datos pueden modificar los valores de los elementos de datos o estructuras de datos que se incluyen en dicha área. De contenido : ocurre cuando un módulo cualquiera accede o hace uso de los datos de una parte de otro módulo. La cohesión es una medida de la relación funcional de los elementos de un módulo, es decir, la sentencia o grupo de sentencias que lo componen, las llamadas a otros módulos o las definiciones de los datos. Un módulo con alta cohesión realiza una tarea concreta y sencilla. El objetivo es intentar obtener módulos con una cohesión alta o media. Los distintos niveles de cohesión, de mayor a menor, son: Funcional : todos los elementos que componen el módulo están relacionados en el desarrollo de una única función. Secuencial : un módulo empaqueta en secuencia varios módulos con cohesión funcional. De comunicación : todos los elementos de procesamiento utilizan los mismos datos de entrada y de salida. Procedimental : todos los elementos de procesamiento de un módulo están relacionados y deben ejecutarse en un orden determinado. En este tipo existe paso de controles. Temporal : un módulo contiene tareas relacionadas por el hecho de que todas deben realizarse en el mismo intervalo de tiempo. Lógica : un módulo realiza tareas relacionadas de forma lógica (por ejemplo un módulo que produce todas las salidas independientemente del tipo). Casual : un modulo realiza un conjunto de tareas que tienen poca o ninguna relación entre sí. Un buen diseño debe ir orientado a conseguir que los módulos realicen una función sencilla e independiente de las demás (máxima cohesión), y que la dependencia con otros módulos sea mínima (acoplamiento mínimo), lo cual facilita el mantenimiento del diseño. NotaciónMódulo Se representa mediante un rectángulo con su nombre en el interior. Un módulo predefinido se representa añadiendo dos líneas verticales y paralelas en el interior del rectángulo. Conexión Se representa mediante una línea terminada en punta de flecha cuya dirección indica el módulo llamado. Para llamadas a módulos estáticos se utiliza trazo continuo y para llamadas a módulos dinámicos trazo discontinuo. Parámetros La representación varía según su tipo: control (flags) o datos. Almacén de datos Dispositivo físico (Nota.- Esta notación es la más habitual, pero MÉTRICA Versión 3 no exige su utilización). Ejemplo En el siguiente ejemplo muestra un proceso de emisión de cheques para el pago de nóminas de los empleados de una empres. En él se diferencian los cálculos relativos a los trabajadores empleados por horas y los que poseen contrato. La lectura del fichero de empleados y la impresión de los cheques son módulos ya disponibles en las librerías del sistema, es decir, módulos predefinidos. Diccionario de DatosDocumentación del sistemaHasta el momento hemos descrito las técnicas utilizadas en el desarrollo de sistemas, pero el desarrollo de modelos no queremos hacerlo sobre hojas sueltas, con el peligro de extraviarlas, y tener dificultad en mantenerlo. Por el contrario, necesitamos organizar el seguimiento de los modelos, principalmente por dos razones: Dar significado a los componentes del modelo, ayudando a gestionar la complejidad del sistema. Soportar el mantenimiento, ya que cualquier trabajo puede pasar de una persona a otra. A esta forma de seguimiento organizado del trabajo producido durante el análisis y diseño del sistema se llama documentación del sistema. La documentación del sistema es tanto una herramienta de comunicación, como de comunicación porque contiene un almacén de todo el trabajo hecho cada día y lo pone a disposición de todas las personas que trabajan en un proyecto grande. También es una herramienta de dirección, porque asegura una alta eficiencia, ya que todas las personas tienen acceso a lo último realizado. Dado que un proyecto se divide en fases, se establece la documentación que se debe aportar en cada fase, lo que ayuda a conocer la situación en cada momento del proyecto. Para que sea útil, la documentación debe ser estructural y fácil de usar. La documentación en un primer momento se divide en informes de proyecto y una descripción del sistema. Informes del proyecto Los informes del proyecto incluyen la información requerida por la dirección del proyecto. Los informes incluyen un resumen de la fase actual, unas recomendaciones para la siguiente fase y un plan con los recursos propuestos. La información específica de la fase depende de la fase del proyecto, por ejemplo el informe de viabilidad incluirá los costes esperados del proyecto, y una recomendación para seguir o abandonarlo. Diccionario del sistema El Diagrama de Flujo de Datos describe el sistema. El diagrama E/R, describe los datos del sistema. El componente descripción del proceso describe los procesos del DFD y el Diccionario de datos que describe los datos del sistema (flujos y almacenes de datos). Los usuarios del sistema y como lo utilizan se incluyen en la descripción del usuario. Descripción de procesos La descripción de procesos incluyen una entrada por cada proceso del diagrama de flujo de datos. Cada entrada del proceso incluye el número de DFD para él, junto con la descripción del proceso. Como ejemplo la descripción de un proceso de alto nivel, incluye el número y nombre del proceso, los nombres de los flujos de datos de entrada y salida y una descripción del proceso. Para la descripción de procesos en los niveles inferiores de DFD, se usará un método de descripción de procesos, no así en los DFD de alto nivel que basta una descripción narrativa del proceso. Diccionario de DatosEl diccionario de datos es una lista organizada de todos los datos pertenecientes al sistema, con una serie de definiciones precisas y rigurosas para que tanto el analista como el usuario comprendan entradas, salidas, elementos de los almacenamientos y cálculos intermedios. En el diccionario de datos incluimos almacenes de datos, flujos de datos, estructuras de datos, elementos de datos y en algunos casos el modelo E/R. El diccionario de datos (DD) define los datos en cuanto que: Describe el significado de los flujos de datos y los almacenes que muestran los DFDs. Describe la composición de la estructura de datos que se mueven a lo largo de los flujos. Describe la composición de la estructura de datos en los almacenes. Describe los detalles de las relaciones entre almacenes que aparecen en un diagrama entidad/relación. Los analistas utilizan los diccionarios de datos por cuatro razones: Para manejar los detalles en sistemas grandes ya que es imposible de recordar todo lo referente a un sistema. Para comunicar un significado común para todos los elementos del sistema. Esto es muy importante cuando trabajan varios analistas y no pueden reunirse todos los días para comunicarse. Para documentar las características del sistema. Localizar errores en el sistema. Contenido de un Diccionario de Datos El DD contiene las siguientes: Definiciones lógicas de datos: Elemento de Dato (Atributos de la Entidad). Estructura de Dato. Flujos de Datos. Almacenes de Datos. Definiciones lógicas de procesos. Definición lógica de entidad externa. Los elementos de dato se agrupan para formar una estructura de dato. Elemento de dato : Ninguna unidad más pequeña tiene significado para los analistas o usuarios. Son los bloques básicos para todos los demás datos del sistema, por sí solo no lleva ningún significado al usuario. Son los atributos de las entidades.Por ejemplo: nº factura, fecha expedición, cantidad adeudada. Estructura de dato : es un grupo de datos elementales que en conjunto describen un componente del sistema.Por ejemplo: Factura. Los flujos de datos, almacenes de datos son estructuras de datos. Notación del Diccionario de Datos Notación del elemento dato : Cada uno está identificado con un nombre, una descripción, un alias, una longitud, un intervalo de valores. Veamos las reglas a seguir para cada elemento. Nombre de los datos: se deben asignar nombres que sean significativos, es decir, que tengan significado en el contexto del desarrollo del sistema.Por ejemplo: Fecha-Factura. Un nombre no debe ser mayor de 30 caracteres y tampoco debe contener espacios en blanco. Descripción de los datos: indica de manera breve lo que éste representa en el sistema, y debe escribirse de forma comprensible para el lector y pensando que quien lo lea no sabe nada con respecto al sistema. Alias: es cuando el mismo dato recibe varios nombres, según quien haga uso del dato.Ejemplo: factura puede tener como alias documento de pago o nota de pago etc.No so alias los siguientes casos: factura autorizada, factura verificada. Longitud: indica la cantidad de espacio necesario para cada dato sin considerar la forma en que serán almacenados. Valores de los datos: si los valores de los datos están restringidos a un intervalo específico, debe reflejarse en la entrada del DD.Por ejemplo: Talla unidad [centímetros], rango [1-200]. Descripción de las estructuras de datos : Las estructuras de datos se construyen sobre cuatro relaciones de componentes (datos o estructuras) que son: Relación secuencial: Define los componentes (datos o estructuras) que siempre se incluyen en una estructura de datos en particular, es decir, también se llama concatenación de dos o más datos. Relación de selección: Define alternativas para datos o estructuras incluidas en una estructura de datos. Relación de iteración: Define la repetición de un componente cero o más veces. Relación opcional: Es un caso especial de la iteración, es decir, una o ninguna relación. Descripción de los flujos de datos : Representamos los flujos de datos siempre y cuando el flujo no sea un único atributo. Está formado por una o más estructuras previamente definidas. Del flujo nos interesa el contenido, fuente, destino, volumen. Nombre del flujo de datos: se deben asignar nombres que sean significativos, es decir, que tengan significado en el contexto del desarrollo del sistema.Por ejemplo: factura. Fuente: indica cual es el proceso fuente de la información. Se indicará el número del proceso. Destino: indica cual es el proceso destino de la información. Se indicará el número del proceso. Definición: explica el contenido del flujo de datos. Contenido: describe cuales son las estructuras de datos incluidas. Descripción de los almacenamientos de datos : Representamos los almacenamientos de datos. Se documenta su contenido, flujos de entrada, flujos de salida. Nombre de almacenamiento de datos: se asignan nombres que sean significativos, es decir, que tengan significado en el contexto del desarrollo del sistema.Ejemplo: histórico facturas. Flujos de entrada: indica cuales son los flujos que alimentan el almacenamiento de datos. Flujos de salida: indica cuales son los flujos que extraen información del almacenamiento de datos. Definición: describe el contenido del almacenamiento de datos. Contenido: especifica el contenido del almacenamiento. Descripción de los procesos : Representamos los procesos del sistema. Se documenta su contenido, flujos de entrada, flujos de salida. Nombre de proceso: se asignan nombres que sean significativos, es decir, que tengan significado en el contexto del desarrollo del sistema.Por ejemplo: verificar_crédito. Entradas: indica cuales son los procesos, almacenamientos de datos que ejercen de fuente de datos. Flujos de salida: indica cuales son los procesos, almacenamientos de datos que ejercen de destino de datos. Definición: indica la misión del proceso. Descripción: describe el proceso. Para ello utilizaremos: Forma narrativa, árboles de decisión, tablas de decisión, lenguaje estructurado. Descripción de las entidades externas : Representamos las entidades externas del sistema. Se documenta a quien representa, flujos de datos relacionados, volumen, etc. Nombre de entidad externa: se asignan nombres que sean significativos, que representen a la entidad.Por ejemplo: clientes. Flujos de datos asociados: indica cuales son los flujos (entrada/salida) asociados. Definición: indica quienes son la entidad. Volumen: número de componentes de la entidad. Sintaxis del Diccionario de Datos Conocida la forma de describir los datos y estructuras de datos, explicados en el apartado anterior, a continuación se va a establecer una sintaxis estandarizada que nos permitirá expresar dichos significados: \\= está compuesto por + y () opcional, puede o no puede estar presente [] selección entre varias alternativas {} iteración, repetir los mismo varias veces ** comentario @ clave principal de un almacenamiento | separador de alternativas en selección Ejemplo: Datos elementales Son datos, que dentro del contexto del usuario, no tiene sentido descomponerlo. Es importante verificar: Valores permitidos y unidad de medida. peso_persona =* * * unidad: kilo ; rango: 1..150 *sexo = * Masculino o Femenino * * valores: [ M | F ] * Datos opcionales Dirección_cliente = (dirección_entrega) + dirección_facturación)Dirección_cliente = [ dirección_entrega | dirección_facturación | dirección entrega + dirección_facturación ]Dirección_cliente = dirección_entrega + (dirección_facturación) Iteración Repetición de uno o más datos elementales o grupo de datos. ‘Cero o más ocurrencias’. pedido = nombre_cliente + dirección_entrega + { producto } Selección ‘Una y solo una de las alternativas’. sexo = [ Masculino | Femenino ] Dominio (No Yourdon) Consiste definir una única vez cada tipo de Dato Elemental y referenciarlo para cada representación del tipo. fecha = * * * unidad: días ; rango : 0..36500 *fec_nacimiento = fechafec_factura = fecha Alias (Sinónimo) No se debe confundir con el dominio. Es un nombre alternativo para un dato elemental. fecha_contable = fechafecha_efectiva = * alias de: fecha_contable *Nombre = Tratamiento + Nombre_pila + Primer_apellido + Segundo_apellidoTratamiento = [ Sr. | Sra. | Srta. | D. | Dr. ]Nombre_pila = {carácter}Primer_apellido = {carácter}Segundo_apellido = {carácter}carácter = [ A-Z | a-z | - ] Definición de un Diccionario de Datos Definición de datos secuenciales Una definición se realiza mediante el símbolo = que significa se define como por lo tanto una expresión como A = B + C, se podría leer igual que de forma matemática, es decir, A está compuesto de B y C , pero para completarla se debería añadir: el significado de dicho dato en el contexto de la aplicación, el rango y tipo de valores que cada dato puede tomar. Por ejemplo: En un sistema informático de un hospital: Datos_del_Paciente = nombre_completo + *nombre completo del paciente * *tipo: array de caracteres* dirección + *dirección completa del paciente* *tipo: array de caracteres* peso + *peso del paciente* *unidad: kilogramos; rango: 1-200* talla + *talla del paciente* *unidad: centímetros; rango: 20-250* fecha_ingreso + *fecha de entrada en el hospital * *tipo: fecha* Definición de datos opcionales Es aquel dato que puede o no formar parte de la composición de un dato compuesto. Ejemplo: La dirección de un cliente puede ser: Única: tanto la dirección comercial como de administración están en el mismo lugar que producción o almacén. Dos direcciones: tiene el almacén y producción separado físicamente de la administración. Esta situación en un DD se trataría así: Cliente = nombre_completo + dni_cliente + dirección_comercial + (dirección_mercancías) Definición de selección Sólo una de entre varias posibilidades será posible. Esta se define mediante []. Ejemplo: Un cliente puede ser una empresa o un particular, por lo tanto los tipos de datos son distintos según sea uno u otro. cliente = [nombre_cliente | nombre_empresa] + [dni_cliente | cif_cliente] + dirección_comercial + (dirección_mercancías) Definición de iteración La iteración se expresa mediante {} y sirve para indicar la repetición de una cierta ocurrencia dentro de una definición. Ejemplo: Factura = fecha_factura + nombre_cliente + numero_factura + {linea_factura} + total_factura El dato línea_factura es un componente de la estructura de datos factura que puede tener una o varias ocurrencias, ya que una factura puede tener muchas líneas de facturación de artículos. Alias (Sinónimos) Son nombres que dentro del Sistema de información tienen el mismo significado, entonces lo que se hace es declarar los sinónimos por medio del símbolo =. Ejemplo: Acreedor = cliente ** definido ya anteriormente. Hemos visto el contenido del Diccionario de Datos, que deberá mostrarse al usuario siempre conjuntamente con las técnicas: Diagrama de Flujo de Datos (DFD). Modelo Entidad/Relación (DER). Especificación de Procesos (EP). Implementación del Diccionario de Datos Varias posibilidades para la implementación de los DD, cada una con sus características y ventajas. Repositorio de datos: Herramientas automáticas integradas dentro de un entorno CASE. Dispone de más posibilidades de las vistas. Diccionario de datos SGBD o SO modernos. Dan soporte automático para definiciones de datos, validar su consistencia, producir algunos informes. Procesador de textos convencional. Totalmente manual. Ejemplo: Dato Elemental Nombre : Estado_CivilDescripción: Código de una letra para indicar el estado civil de cada empleado.Long y tipo: 1 carácter alfabético.Sinónimos : ESTADO (Personal) CIVIL (Nóminas)Valores : S Soltero D Divorciado C Casado S Separado V Viudo O Otros Estructura de dato Nombre : EmpleadoDescripción: Datos necesarios de un empleadoComponentes: Nombre_empleado + Num_empleado + Datos_personales = Fecha_nacimiento + Estado_Civil + Num_hijos [ 0 - ] + (Num_tfno) Dirección = Calle + Número + (Población) + Codigo_Postal + Provincia Flujo de datos Nombre : Pago_aceptadoRef : 11.1 - 11.2Fuente : 11.1 Aceptar pagoDestino : 11.2 Validar pagoDescripción : Pago recibido y sellado pero no validadoEstruct. de datos: Cheque + Recibo_Caja + (Letra_Pago) + Metodo_PagoVolumen : 5000 por díaComentarios : La letra de pago está omitida en el 10% de los casos Almacenamiento de datos Nombre : Historia_PedidosRef : P4Flujo de Entrada: 9 - D4 PedidoFlujo de Salida : D4 - 10 Detalles pedido D4 - 11 Detalle ventas D4 - 9 Demanda anteriorDescripción : Todos los pedidos aceptados en los últimos 6 mesesContenido : Pedido = Id_pedido + Detalle_cliente + Detalle_libro Descripción lógica de un proceso Para el proceso V _erificar_Crédito_ la plantilla correspondiente sería la siguiente: Procesos Nombre : Verificar_créditoRef : 3Definición : Decidir donde van los pedidos sin pago previo, o si debe pedirle el pago al cliente.Entradas : 1 - 3 Pedidos D3 - 3 Historia de pagosSalidas : 3 - C Pedido de pago previo 3 - D3 Nuevo balance de orden 3 - 6 Pedidos con crédito okDescripción: Recuperar historia de pago. Si el cliente es nuevo, enviar pedido de pago previo. Si el cliente corriente (promedio de dos pedidos mensuales) , OK con el pedido, a menos que el balance esté vencido con más de dos meses. Para clientes anteriores (no corrientes) , OK, a menos que tengan cualquier balance vencido. Hemos visto que para describir la lógica de un proceso, utilizaremos varias alternativas como son: narrativa, árboles de decisión, tablas de decisión y lenguaje estructurado. Cuando utilizamos narrativa podemos encontrarnos con: frases oscuras (no solo, pero no obstante, sin embargo, …) rangos con huecos indefinidos (‘ hasta 20 unidades sin descuento, más de 20 unidades al 50%’). Frases con y/o (‘los clientes que nos compran más de 1 millón al año y tienen una buena historia de pago o que han tenido tratos con nosotros por más de 20 años deberán recibir trato preferencial’). Adjetivos indefinidos (‘buena historia de pagos’, ‘trato preferencial’). Estas razones obligan a pensar en otras alternativas: Árbol de decisión : Pueden resultar una técnica no válida en situaciones complejas con gran número de condiciones e implicaciones ya que no asegura que se hayan considerados todas.Se debe utilizar cuando el número de acciones sea pequeño y no sean posibles todas las combinaciones. Tablas de decisión : Son más precisas dado que permiten reflejar todas la combinaciones posibles. Pero son más difíciles de entender para el usuario. Deben simplificarse una vez construidas y se convertirán en árboles de decisión.Se debe utilizar siempre que se dude que el árbol muestra toda la lógica. * Primera orden &gt; 12 días ———- Hacer pedido* Total ordenes &lt; menor que X* Primera orden &lt;= 12 días ———- Esperar* Total ordenes &lt; mayor o igual que X ———- Calcular* Hacer pedido* No descuento ———- Hacer pedido Descripción lógica de una entidad externa Para la entidad Proveedores la plantilla correspondiente sería la siguiente: Entidad Externa Nombre : ProveedoresRef : pDefinición : Proveedores actuales de la empresaFlujos de Datos: 7 - p Pedidos p - 3 Albarán p - 11 FacturasVolumen : Actualmente 25. Se espera llegar a 40. Bibliografia dlsi (Jaime) PAe Modelización conceptual. El modelo Entidad/Relación extendido (E/R): elementos. Reglas de modelización. Validación y construcción de modelos de datos.Modelo Conceptual de DatosA la hora de determinar una BD debemos establecer un proceso partiendo del acotamiento de una parcela del mundo exterior (micromundo o universo del discurso), aquél que nos interesa representar en los datos. En este proceso se debe aprender, comprender y conceptualizar dicho mundo exterior transformándolo en un conjunto de ideas y definiciones que supongan una imagen fiel del comportamiento del mundo real. A esta imagen del mundo exterior la llamaremos Modelo Conceptual . Una vez definido el modelo conceptual, éste se ha de transformar en una descripción de datos, atributos y relaciones que denominaremos Esquema Conceptual de datos. Por último, este esquema conceptual habrá que traducirlo a estructuras almacenables en soportes físicos. Por tanto es necesario distinguir entre Bases de Datos, que será el banco, el almacén de los valores (ocurrencias) de los datos. Los modelos de Datos, que son las herramientas para diseñar los datos y sus relaciones de forma que puedan soportar los valores correspondientes. Y finalmente los Sistemas Gestores de Bases de Datos (SGBD), que serán los encargados de las acciones que llevemos a cabo con las bases de datos, permitiendo también cumplimentar a los usuarios, mostrándoles los datos de acuerdo a sus necesidades. Con todo ello, se puede definir un Modelo de Datos como: Un grupo de herramientas conceptuales para describir los datos, sus relaciones, su semántica y sus limitaciones; de tal forma, que facilita la interpretación de nuestro mundo real y su representación en forma de datos, en nuestro sistema informático. Definido el modelo de datos, pasamos a analizarlo. Para ello, partiremos de las propiedades que podemos diferenciar en dos tipos: Estáticas : Son las propiedades invariantes en el tiempo. Quedan especificadas en el Modelo de Datos por ESTRUCTURAS. Esta se define mediante el ESQUEMA, con el lenguaje de definición de datos (DDL). El esquema, a su vez está constituido por Estructura y Restricciones. La Estructura queda definida por los Objetos del Modelo y las Restricciones inherentes, conformando un conjunto de reglas de definición de dichas Estructuras. Los objetos y Restricciones de la Estructura dependen de cada Modelo, pero en general son: Entidades. Atributos. Dominio. Relaciones. Representación. Restricciones: Hay tres tipos de restricciones: Restricciones inherentes : vienen impuestas por la propia naturaleza del Modelo introduciendo rigideces en la modelización. Restricciones opcionales o de usuario : restricciones propiamente dichas en el Esquema, son definidas por el usuario, pero el Modelo de Datos las reconoce y suministra herramientas para manejarlas. Restricciones libres de usuarios : son responsabilidad del usuario y el Modelo de Datos ni las reconoce, ni las maneja. Dinámicas : Son las propiedades que varían con el tiempo. En el modelo de datos son las OPERACIONES. Se define como un conjunto de Operaciones con el Lenguaje de Manipulación de Datos (DML). Las operaciones sobre un Modelo de Datos pueden ser de: Selección. Localización de los datos deseados. Acción. Realización de una acción sobre los datos seleccionados. Dicha acción puede ser: Recuperación (obtención de los datos seleccionados). Actualización, que a su vez pueden ser: Modificación. Inserción. Borrado. Generalmente, toda operación de Actualización va precedida de una Recuperación, aunque no necesariamente. Modelo Entidad/Relación ExtendidoSe trata de una técnica cuyo objetivo es la representación y definición de todos los datos que se introducen, almacenan, transforman y producen dentro de un sistema de información, sin tener en cuenta las necesidades de la tecnología existente, ni otras restricciones. Dado que el modelo de datos es un medio para comunicar el significado de los datos, las relaciones entre ellos y las reglas de negocio de un sistema de información, una organización puede obtener numerosos beneficios de la aplicación de esta técnica, pues la definición de los datos y la manera en que éstos operan son compartidos por todos los usuarios. Las ventajas de realizar un modelo de datos son, entre otras: Comprensión de los datos de una organización y del funcionamiento de la organización. Obtención de estructuras de datos independientes del entorno físico. Control de los posibles errores desde el principio, o al menos, darse cuenta de las deficiencias lo antes posible. Mejora del mantenimiento. Aunque la estructura de datos puede ser cambiante y dinámica, normalmente es mucho más estable que la estructura de procesos. Como resultado, una estructura de datos estable e integrada proporciona datos consistentes que puedan ser fácilmente accesibles según las necesidades de los usuarios, de manera que, aunque se produzcan cambios organizativos, los datos permanecerán estables. Este diagrama se centra en los datos, independientemente del procesamiento que los transforma y sin entrar en consideraciones de eficiencia. Por ello, es independiente del entorno físico y debe ser una fiel representación del sistema de información objeto del estudio, proporcionando a los usuarios toda la información que necesiten y en la forma en que la necesiten. DescripciónEl modelo entidad/relación extendido describe con un alto nivel de abstracción la distribución de datos almacenados en un sistema. Existen dos elementos principales: las entidades y las relaciones. Las extensiones al modelo básico añaden además los atributos de las entidades y la jerarquía entre éstas. Estas extensiones tienen como finalidad aportar al modelo una mayor capacidad expresiva. Los elementos fundamentales del modelo son los siguientes: Entidad Es aquel objeto, real o abstracto, acerca del cual se desea almacenar información en la base de datos. La estructura genérica de un conjunto de entidades con las mismas características se denomina tipo de entidad. Existen dos clases de entidades: Regulares: tienen existencia por sí mismas. Débiles: cuya existencia depende de otra entidad. Las entidades deben cumplir las siguientes tres reglas: Tienen que tener existencia propia. Cada ocurrencia de un tipo de entidad debe poder distinguirse de las demás. Todas las ocurrencias de un tipo de entidad deben tener los mismos atributos. Relación Es una asociación o correspondencia existente entre una o varias entidades. La relación puede ser regular, si asocia tipos de entidad regulares, o débil, si asocia un tipo de entidad débil con un tipo de entidad regular. Dentro de las relaciones débiles se distinguen la dependencia en existencia y la dependencia en identificación . Se dice que la dependencia es en existencia cuando las ocurrencias de un tipo de entidad débil no pueden existir sin la ocurrencia de la entidad regular de la que dependen. Se dice que la dependencia es en identificación cuando, además de lo anterior, las ocurrencias del tipo de entidad débil no se pueden identificar sólo mediante sus propios atributos, sino que se les tiene que añadir el identificador de la ocurrencia de la entidad regular de la cual dependen. Además, se dice que una relación es exclusiva cuando la existencia de una relación entre dos tipos de entidades implica la no existencia de las otras relaciones. Una relación se caracteriza por: Nombre : que lo distingue unívocamente del resto de relaciones del modelo. Tipo de correspondencia : es el número máximo de ocurrencias de cada tipo de entidad que pueden intervenir en una ocurrencia de la relación que se está tratando. Conceptualmente se pueden identificar tres clases de relaciones: Relaciones 1:1 : Cada ocurrencia de una entidad se relaciona con una y sólo una ocurrencia de la otra entidad. Relaciones 1:N : Cada ocurrencia de una entidad puede estar relacionada con cero, una o varias ocurrencias de la otra entidad. Relaciones M:N : Cada ocurrencia de una entidad puede estar relacionada con cero, una o varias ocurrencias de la otra entidad y cada ocurrencia de la otra entidad puede corresponder a cero, una o varias ocurrencias de la primera. Cardinalidad : representa la participación en la relación de cada una de las entidades afectadas, es decir, el número máximo y mínimo de ocurrencias de un tipo de entidad que pueden estar interrelacionadas con una ocurrencia de otro tipo de entidad. La cardinalidad máxima coincide con el tipo de correspondencia.Según la cardinalidad, una relación es obligatoria, cuando para toda ocurrencia de un tipo de entidad existe al menos una ocurrencia del tipo de entidad asociado, y es opcional cuando, para toda ocurrencia de un tipo de entidad, puede existir o no una o varias ocurrencias del tipo de entidad asociado. Dominio Es un conjunto nominado de valores homogéneos. El dominio tiene existencia propia con independencia de cualquier entidad, relación o atributo. Atributo Es una propiedad o característica de un tipo de entidad. Se trata de la unidad básica de información que sirve para identificar o describir la entidad. Un atributo se define sobre un dominio. Cada tipo de entidad ha de tener un conjunto mínimo de atributos que identifiquen unívocamente cada ocurrencia del tipo de entidad. Este atributo o atributos se denomina identificador principal. Se pueden definir restricciones sobre os atributos, según las cuales un atributo puede ser: Univaluado, atributo que sólo puede tomar el valor para todas y cada una de las ocurrencias del tipo de entidad al que pertenece. Obligatorio, atributo que tiene que tomar al menos un valor para todas y cada una de las ocurrencias del tipo de entidad al que pertenece. Además de estos elementos, existen extensiones del modelo entidad/relación que incorporan determinados conceptos o mecanismos de abstracción para facilitar la representación de ciertas estructuras del mundo real: La generalización , permite abstraer un tipo de entidad de nivel superior (supertipo) a partir de varios tipos de entidad (subtipos); en estos casos los atributos comunes y relaciones de los subtipos se asignan al supertipo. Se pueden generalizar por ejemplo los tipos profesor y estudiante obteniendo el supertipo persona. La especialización es la operación inversa a la generalización, en ella un supertipo se descompone en uno o varios subtipos, los cuales heredan todos los atributos y relaciones del supertipo, además de tener los suyos propios. Un ejemplo es el caso del tipo empleado , del que se pueden obtener los subtipos secretaria , técnico e ingeniero . Categorías . Se denomina categoría al subtipo que aparece como resultado de la unión de varios tipos de entidad. En este caso, hay varios supertipos y un sólo subtipo. Si por ejemplo se tienen los tipos persona y compañía y es necesario establecer una relación con vehículo , se puede crear propietario como un subtipo unión de los dos primeros. La agregación , consiste en construir un nuevo tipo de entidad como composición de otros y su tipo de relación y así poder manejarlo en un nivel de abstracción mayor. Por ejemplo, se tienen los tipos de entidad empresa y solicitante de empleo relacionados mediante el tipo de relación entrevista ; pero es necesario que cada entrevista se corresponda con una determinada oferta de empleo . Como no se permite la relación entre tipos de relación, se puede crear un tipo de entidad compuesto por empresa , entrevista y solicitante de empleo y relacionarla con el tipo de entidad oferta de empleo . El proceso inverso se denomina desagregación. La asociación , consiste en relacionar dos tipos de entidades que normalmente son de dominios independientes, pero coyunturalmente se asocian. La existencia de supertipos y subtipos, en uno o varios niveles, da lugar a una jerarquía , que permitirá representar una restricción del mundo real. Una vez construido el modelo entidad/relación, hay que analizar si se presentan redundancias. Para poder asegurar su existencia se deben estudiar con mucho detenimiento las cardinalidades mínimas de las entidades, así como la semántica de las relaciones. Los atributos redundantes, los que se derivan de otros elementos mediante algún cálculo, deben ser eliminados del modelo entidad/relación o marcarse como redundantes. Igualmente, las relaciones redundantes deben eliminarse del modelo, comprobando que al eliminarlas sigue siendo posible el paso, tanto en un sentido como en el inverso, entre las dos entidades que unían. NotaciónEntidad La representación gráfica de un tipo de entidad regular es un rectángulo con el nombre del tipo de entidad. Un tipo de entidad débil se representa con dos rectángulos concéntricos con su nombre en el interior. Relación Se representa por un rombo unido a las entidades relacionadas por dos líneas retas a los lados. El tipo de correspondencia se representa gráficamente con una etiqueta 1:1 , 1:N o M:N , cerca de alguno de los vértices del rombo, o bien situando cada número o letra cerca de la entidad correspondiente, para mayor claridad. La representación gráfica de las cardinalidades se realiza mediante una etiqueta del tipo (0,1) , ( 1,1) , (0,n) o (1,n) , que se coloca en el extremo de la entidad que corresponda. Si se representan las cardinalidades, la representación del tipo de correspondencia es redundante. Relaciones del tipo M:N (Muchos a Muchos): Si existe un concepto que puede sustituir la relación, tiene sentido como entidad y aporta una mejor comprensión al modelo (para usuarios y analistas) es conveniente deshacerlas mediante esta entidad y las relaciones uno a muchos adecuadas. Relaciones entre Tres o Más Entidades Las relaciones entre tres o más entidades se reclasificarán mediante una entidad relacionada con cada una de ellas, si existe un concepto que puede ser representado como una entidad, y aporta mayor comprensión al problema. Relaciones potencialmente redundantes Pueden serlo o no, depende del significado de las relaciones y de las cardinalidades. Deben ser eliminadas. Relaciones Recursivas o Autorrelaciones Atributo Un atributo se representa mediante una elipse, con su nombre dentro, conectada por una línea al tipo de entidad o relación. En lugar de una elipse puede utilizarse un círculo con el nombre dentro, o un círculo más pequeño con el nombre del atributo a un lado. También pueden representarse en una lista asociada a la entidad. El identificador aparece con el nombre marcada o subrayado, o bien con su círculo en negro. Exclusividad En la representación de las relaciones exclusivas se incluye un arco sobre las líneas que conectan el tipo de entidad a los dos o más tipos de relación. Jerarquía (tipos y subtipos) La representación de las jerarquías se realiza mediante un triángulo invertido, con la base paralela al rectángulo que representa el supertipo y conectando a éste y a los subtipos. Si la división en subtipos viene determinada en función de los valores de un atributo discriminante, éste se representará asociado al triángulo que representa la relación. En el triángulo se representará: con una letra d el hecho de que los subtipos sean disjuntos, con un círculo o una O si los subtipos pueden solaparse y con un U el caso de uniones por categorías. La presencia de una jerarquía total se representa con una doble línea entre el supertipo y el triángulo. Ejemplo Modelo entidad-relación extendido para un sistema de gestión de técnicos y su asignación a proyectos dentro de una empresa u organización. Como se aprecia en el diagrama, TÉCNICO es un subtipo de EMPLEADO, generado por especialización, pues era necesario para establecer la relación Trabaja en con PROYECTO, ya que no todos los empleados de la empresas, como los administrativos, son susceptibles de trabajar en un proyecto. La entidad TÉCNICO tendrá los atributos de EMPLEADO más el atributo nivel . Los tipos de correspondencia son 1:N entre DEPARTAMENTO y EMPLEADO, pues un departamento tienen 1 o varios empleados. Entre TÉCNICO y PROYECTO es M:N , pues un técnico puede trabajar en 1 o varios proyectos, y en un proyecto trabajan 1 o varios técnicos. Por otro lado, se han incluido atributos que caracterizan la relación Trabaja en , como son fecha de asignación y fecha de cese , ya que un técnico no siempre estará trabajando en un proyecto, sino en determinado periodo. (Nota.- Esta notación es la más habitual, pero MÉTRICA Versión 3 no exige su utilización). NormalizaciónLa teoría de la normalización tiene por objetivo la eliminación de dependencias entre atributos que originen anomalías en la actualización de los datos, y proporcionar una estructura más regular para la representación de las tablas, constituyendo el soporte para el diseño de bases de datos relacionales. Como resultado de la aplicación de esta técnica se obtiene un modelo lógico de datos normalizado. DescripciónLa teoría de la normalización, como técnica formal para organizar los datos, ayuda a encontrar fallos y a corregirlos, evitando así introducir anomalías en las operaciones de manipulación de datos. Se dice que una relación está en una determinada forma normal si satisface un cierto conjunto de restricciones sobre los atributos. Cuanto más restricciones existan, menor será el número de relaciones que las satisfagan, así, por ejemplo, una relación en tercera forma normal estará también en segunda y en primera forma normal. Antes de definir las distintas formas normales se explican, muy brevemente, algunos conceptos necesarios para su comprensión. Dependencia funcional Un atributo _Y_ se dice que depende funcionalmente de otro _X_ si, y sólo si, a cada valor de _X_ le corresponde un único valor de _Y_ , lo que se expresa de la siguiente forma: X → Y (también se dice que _X_ determina o implica a _Y_ ). _X_ se denomina implicante o determinante e _Y_ es el implicado. Dependencia funcional completa Un atributo _Y_ tiene dependencia funcional completa respecto de otro _X_ , si depende funcionalmente de él en su totalidad, es decir, no depende de ninguno de los posibles atributos que formen parte de _X_ . Dependencia transitiva Un atributo depende transitivamente de otro si, y sólo si, depende de él a través de otro atributo. Así, _Z_ depende transitivamente de _X_ , si: X → YY —/→ XY → Z Se dice que _X_ implica a _Z_ a través de _Y_ . Una vez definidas las anteriores dependencias, se pueden enunciar las siguientes formas normales: Primera Forma Normal (1FN) Una entidad está en 1FN si no tiene grupos repetitivos, es decir, un atributo sólo puede tomar un único valor de un dominio simple. Una vez identificados los atributos que no dependen funcionalmente de la clave principal, se formará con ellos una nueva entidad y se eliminarán de la antigua. La clave principal de la nueva entidad estará formada por la concatenación de uno o varios de sus atributos más la clave principal de la antigua entidad. Segunda Forma Normal (2FN) Una entidad está en 2FN si está en 1FN y todos los atributos que no forman parte de las claves candidatas (atributos no principales) tienen dependencia funcional completa respecto de éstas, es decir, no hay dependencias funcionales de atributos no principales respecto de una parte de las claves. Cada uno de los atributos de una entidad depende de toda la clave. Una vez identificados los atributos que no dependen funcionalmente de toda la clave, sino sólo de parte de la misma, se formará con ellos una nueva entidad y se eliminarán de la antigua. La clave principal de la nueva entidad estará formada por la parte de la antigua de la que dependen funcionalmente. Tercera Forma Normal (3FN) Una entidad está en 3FN si está en 2FN y todos sus atributos no principales dependen directamente de la clave primaria, es decir, no hay dependencias funcionales transitivas de atributos no principales respecto de las claves. Una vez identificados los atributos que dependen de otro atributo distinto de la clave, se formará con ellos una nueva entidad y se eliminarán de la antigua. La clave principal de la nueva entidad será el atributo del cual dependen. Este atributo en la entidad antigua, pasará a ser una clave ajena. NotaciónUna herramienta muy útil para visualizar las dependencias funcionales es el grafo o diagrama de dependencias funcionales, mediante el cual se representa un conjunto de atributos y las dependencias funcionales existentes entre ellos. En el grafo aparecen los nombre de los atributos unidos por flechas, las cuales indican las dependencias funcionales completas que existen entre ellos, partiendo del implicante hacia el implicado. Cuando el implicante de una dependencia no es un único atributo, es decir, se trata de un implicante compuesto, los atributos que lo componen se encierran en un recuadro y la flecha parte de éste, no de cada atributo. En la figura se presenta un ejemplo de cómo se visualizan las dependencias. Se puede observar que COD_LIBRO determina funcionalmente el TITULO del libro y la EDITORIAL, como indica la correspondiente flecha; de forma análoga, NUM_SOCIO determina NOMBRE, DOMICILIO y TEL. del socio (suponiendo que sólo se proporciona un teléfono); mientras que ambos atributos en conjunto COD_LIBRO y NUM_SOCIO (lo que se indica mediante el recuadro que los incluye) determinan FECHA_PRESTAMO y FECHA_DEV. EjemploSea una entidad TÉCNICOS de un grupo de empresas, con los siguientes atributos: cod_empresa cod_técnico nombre_técnico cod_categoría categoría nombre_empresa fecha_alta fecha_baja cod_conoc titulo_conoc área_conoc grado cod_proyecto nombre_proyecto f_inicio f_fin f_asignación f_cese cod_cliente nombre_cliente La entidad TÉCNICOS tiene la clave principal compuesta por _cod_empresa_ y _cod_técnico_ , ya que, al ser varias empresas, el código de técnico no será único, sino que puede coincidir con otros de otras empresas. Primera Forma Normal (1FN) Evidentemente no se cumple la primera forma normal, ya que un técnico tendrá más o de un conocimiento (lenguajes, sistemas, operativos, bases de datos, etc), es decir habrá varios valores de _cod_conoc_ , por lo que este atributo y los asociados a conocimientos no dependen funcionalmente de la clave principal. Los atributos _cod_conoc_ , _título_conoc_ , _área_conoc_ y grado identificados como no dependientes, formarán la nueva entidad CONOCIMIENTOS y desaparecerán de la entidad TÉCNICOS. La clave de la nueva entidad será _cod_conoc_ concatenada con _cod_empresa_ y _cod_técnico_ . Por otro lado, en este sistema un técnico puede trabajar en más de un proyecto a tiempo parcial, por lo que _cod_proyecto_ tampoco depende funcionalmente de la clave principal de TÉCNICOS. Se obtiene entonces la entidad PROYECTOS con los atributos de los proyectos, y su clave compuesta de _cod_proyecto_ concatenada con _cod_empresa_ y _cod_técnico_ de la antigua entidad. Esta situación se completará con dos tipos de relaciones: Poseen , cuyo tipo de correspondencia es 1:N entre TÉCNICOS y CONOCIMIENTOS y Están asignados , también del tipo M:N entre TÉCNICOS y PROYECTOS, tal y como muestra el diagrama siguiente: Como se aprecia en la figura, se ha trasladado el atributo grado de la entidad CONOCIMIENTOS a la relación Poseen , pues es un atributo que determina la relación entre las dos entidades. También han sido trasladado los atributos que caracterizan la relación Están asignados , como son _f_asignación_ y _f_cese_ , ya que un técnico no siempre estará trabajando en un proyecto, sino en determinado periodo. Segunda Forma Normal (2FN) En la entidad TÉCNICOS se observa que el atributo _nombre_empresa_ no tiene una dependencia funcional completa con la clave, sino que la tiene sólo de una parte de la misma: _cod_empresa_ . El atributo identificado formará parte de una nueva entidad, EMPRESAS, siendo eliminado de la antigua. La clave principal de la nueva entidad será _cod_empresa_ . Para representar la segunda forma normal en el modelo de datos, deberá añadirse un tipo de relación, Se componen , y el tipo de correspondencia 1:N . Tercera Forma Normal (3FN) En la entidad TÉCNICOS de la figura se puede observar que para un _cod_técnico_ hay un único _cod_categoría_ , es decir, el segundo depende funcionalmente del primero; para un _cod_categoría_ hay una única categoría , es decir, que este atributo depende funcionalmente del _cod_categoría_ ; y por último, para un _cod_categoría_ hay varios valores de _cod_técnico_ . Así pues, la categoría depende transitivamente del _cod_técnico_ , por lo que la entidad TÉCNICOS no está en 3FN. Una vez identificado el atributo categoría que depende de otro atributo distinto de la clave, _cod_categoría_ , se formará con él una nueva entidad y se quitará de la antigua. La clave principal de la nueva entidad será el atributo del cual depende _cod_categoría_ y en la entidad antigua pasará a ser una clave ajena. Del mismo modo, puede observarse que la entidad PROYECTOS tampoco está en 3FN, pues el _nombre_cliente_ depende de _cod_cliente_ , que no forma parte de la clave de la entidad. Así pues, aparecen dos entidades nuevas en el modelo: CATEGORÍAS y CLIENTES, y sus respectivas relaciones y tipos de correspondencias: Están clasificados 1:N y Tiene contratados 1:N . OptimizaciónEl objetivo de esta técnica es reestructurar el modelo físico de datos con el fin de asegurar que satisface los requisitos de rendimiento establecidos y conseguir una adecuada eficiencia del sistema. DescripciónLa optimización consiste en una desnormalización controlada del modelo físico de datos que se aplica para reducir o simplificar el número de accesos a la base de datos. Para ello, se seguirán alguna de las recomendaciones que a continuación se indican: Introducir elementos redundantes. Dividir entidades. Combinar entidades si los accesos son frecuentes dentro de la misma transacción. Redefinir o añadir relaciones entre entidades para hacer más directo el acceso entre entidades. Definir claves secundarias o índices para permitir caminos de acceso alternativos. Con el fin de analizar la conveniencia o no de la desnormalización, se han de considerar, entre otros, los siguientes aspectos: Los tiempos de respuesta requeridos. La tasa de actualizaciones respecto a la de recuperaciones. Las veces que se accede conjuntamente a los atributos. La longitud de los mismos. El tipo de aplicaciones (en línea / por lotes). La frecuencia y tipo de acceso. La prioridad de los accesos. El tamaño de las tablas. Los requisitos de seguridad: accesibilidad, confidencialidad, integridad y disponibilidad que se consideren relevantes. Reglas de Obtención del Modelo Físico a partir del LógicoEl objetivo de esta técnica es obtener un modelo físico de datos a paritr del modelo lógico de datos normalizado. Para ello es necesario aplicar un conjunto de reglas que conserven la semántica del modelo lógico. DescripciónCada uno de los elementos del modelo lógico se tiene que transformar en un elemento del modelo físico. En algunos casos la transformación es directa porque el concepto se soporta igual en ambos modelos, pero otras veces no existe esta correspondencia, por lo que es necesario buscar una transformación que conserve lo mejor posible la semántica, teniendo en cuenta los aspectos de eficiencia que sean necesarios en cada caso. Transformación de entidades Una entidad se transforma en una tabla. Transformación de atributos de entidades Cada atributo se transforma en una columna de la tabla en la que se transformó la entidad a la que pertenece. El identificador único se convierte en clave primaria. Si existen restricciones asociadas a los atributos, éstas pueden recogerse con algunas cláusulas del lenguaje lógico, que se convertirán en disparadores cuando éstos sean soportados por el sistema gestor de base de datos. Transformación de relaciones Según el tipo de correspondencia: Relaciones 1:N : se propaga el identificador de la entidad de cardinalidad máxima _1_ a la que es _N_ , teniendo en cuenta que: Si la relación es de asociación, la clave propagada es clave ajena en la tabla a la que se ha propagado. Si la relación es de dependencia, la clave primaria de la tabla correspondiente a la entidad débil está formada por la concatenación de los identificadores de ambas entidades. Relaciones 1:1 : es un caso particular de las 1:N y por tanto se propaga la clave en las dos direcciones. Se debe analizar la situación, intentando recoger la mayor semántica posible, y evitar valores nulos. Las relaciones de agregación se transforman del mismo modo que las 1:N . Transformación de relaciones exclusivas Después de haber realizado la transformación según las relaciones 1:N , se debe tener en cuenta que si los identificadores propagados se han convertido en claves ajenas de la tabla originada por la entidad común a las relaciones, hay que comprobar que una y sólo una de esas claves es nula en cada ocurrencia. En otro caso, estas comprobaciones se deben hacer en las tablas resultantes de transformar las relaciones. Transformación de la jerarquía Existen varias posibilidades que deben ser evaluadas por el diseñador a fin de elegir la que mejor se ajuste a los requisitos. Las opciones para tratar la transformación de la jerarquía son: Opción a : Consiste en crear una tabla para el supertipo que tenga de clave primaria el identificador y una tabla para cada uno de los subtipos que tengan el identificador del supertipo como clave ajena.Esta solución es apropiada cuando los subtipos tienen muchos atributos distintos y se quieren conservar los atributos comunes en una tabla. También se deben implantar las restricciones y aserciones adecuadas. Es la solución que mejor conserva la semántica. Opción b : Se crea una tabla para cada subtipo, los atributos comunes aparecen en todos los subtipos y la clave primaria para cada tabla es el identificador del supertipo.Esta opción mejora la eficiencia en los accesos a todos los atributos de un subtipo, sean los comunes al supertipo o los específicos. Opción c : Agrupar en una tabla todos los atributos de la entidad supertipo y de los subtipos. La clave primaria de esta tabla es el identificador de la entidad. Se añade un atributo que indique a qué subtipo pertenece cada ocurrencia (el atributo discriminante de la jerarquía). Esta solución puede aplicarse cuando los subtipos se diferencien en pocos atributos y las relaciones entre los subtipos y otras entidades sean las mismas. Para el caso de que la jerarquía sea total, el atributo discriminante no podrá tomar valor nulo (ya que toda ocurrencia pertenece a alguna de las entidades subtipo). NotaciónTabla La representación gráfica de una tabla es un rectángulo con una línea horizontal que lo divide en dos. La parte superior, de ancho menor, se etiqueta con el nombre de la tabla. Relación La relación entre tablas se representa gráficamente mediante una línea que las une. En ella pueden aparecer en sus extremos diversos símbolos para indicar la cardinalidad de la relación, como se muestra a continuación: Ejemplo. Sea el diagrama entidad-relación del ejemplo realizado para la Normalización sobre conocimientos de técnicos informáticos y su asignación a proyectos. El modelo físico de la figura muestra que cada una de las entidades se ha convertido en una tabla, cuyo contenido coincide con los atributos de la entidad. Pero hay dos tablas más: POSEEN, que surge de la relación del mismo nombre y ASIGNACIONES, que se origina a partir de la relación Están asignados . La tabla POSEEN está formada por su atributo grado , más _cod_empresa_ , _cod_tecnico_ y _cod_conoc_ . La tabla ASIGNACIONES se forma con los atributos clave _cod_empres_ a, _cod_tecnico_ y _cod_proyecto_ y los propios _f_asignación_ y _f_cese_ . La relación entre EMPRESAS y TÉCNICOS era 1:N , y la cardinalidad de la figura así lo muestra, pues la empresa siempre estará compuesta de uno o varios técnicos. Lo mismo sucede entre CLIENTES y PROYECTOS: un cliente siempre tendrá 1 o varios proyectos contratados. El caso de CATEGORÍAS y TÉCNICOS es (0,n) . Cada técnico es de una categoría y una categoría corresponde, por regla general, a varios técnicos, pero puede existir alguna en la que no encaje ningún técnico (contable, secretaria de dirección, etc.). La situación del subconjunto TÉCNICOS-POSEEN-CONOCIMIENTOS tienen algo más de complejidad. Un técnico posee normalmente varios conocimientos, pero debe poseer al menos uno para que tenga sentido su situación. La cardinalidad es pues (1,n) entre TÉCNICOS y POSEEN. En el otro lado, lo natural es que un conocimiento sea poseído por varios técnicos, sin embargo puede existir algún conocimiento que no sea poseído por ningún técnico, por lo que la cardinalidad es (0,n) y dibujada desde la tabla CONOCIMIENTOS a POSEEN. Por último, en el subconjunto TÉCNICOS-ASIGNACIONES-PROYECTOS, se dispone de: una cardinalidad (0,n) , pues a un proyecto estarán asignados uno o más técnicos, pero puede haber algún técnico que, en un momento dado, no esté asignado aún a ningún proyecto y una cardinalidad (1,n) , pues un proyecto siempre tendrá asignado al menos a un técnico, o varios. (Nota.- La notación utilizada para el ejemplo es la más habitual, pero MÉTRICA Versión 3 no exige su utilización). Reglas de TransformaciónEl objetivo de esta técnica es obtener un modelo físico de datos a partir del modelo de clases. Para ello es necesario aplicar un conjunto de reglas de transformación que conserven la semántica del modelo de clases. DescripciónCada uno de los elementos del modelo de clases se tiene que transformar en un elemento del modelo físico. En algunos casos la transformación es directa porque el concepto se soporta igual en ambos modelos, pero otras veces no existe esta correspondencia, por lo que es necesario buscar una transformación que conserve lo mejor posible la semántica, teniendo en cuenta los aspectos de eficiencia que sean necesarios en cada caso. Transformación de clases Una clase se transforma en una tabla. Lo habitual es que en los modelos con herencia pueden surgir excepciones cuando se apliquen las reglas de transformación propias de la herencia. Además, es posible que dos clases se transformen en una sola tabla cuando el comportamiento de una de ellas sea irrelevante en la base de datos. Transformación de atributos de clases Cada atributo se transforma en una columna de la tabla en la que se transformó la clase a la que pertenece. El identificador único se convierte en clave primaria. Además, se deben tener en cuenta las reglas de transformación que se aplican a la herencia de clases. Si existen restricciones asociadas a los atributos, éstas pueden recogerse con algunas cláusulas del lenguaje lógico, que se convertirán en disparadores cuando éstos sean soportados por el sistema gestor de base de datos. Transformación de relaciones Según el tipo de correspondencia: Relaciones M:N : se transforman en una tabla, cuya clave primaria es la concatenación de los identificadores de las clases asociadas, siendo cada uno de ellos clave ajena de la propia tabla. Si la relación tienen atributos, éstos se transforman en columnas de la tabla. Relaciones 1:N : existen varias posibilidades: Propagar el identificador de la clase de cardinalidad máxima _1_ a la que es _N_ , teniendo en cuenta que: Si la relación es de asociación, la clave propagada es clave ajena en la tabla a la que se ha propagado. Si la relación es de dependencia, la clave primaria de la tabla correspondiente a la clase débil está formada por la concatenación de los identificadores de ambas clases. La relación se transforma en una tabla de clave primaria sólo el identificador de la clase de cardinalidad máxima _N_ si: La relación tiene atributos propios y se desea que aparezcan como tales. Se piensa que en un futuro la relación puede convertirse en M:N . El número de ocurrencias relacionadas de la clase que propaga su clave es muy pequeño (y por tanto pueden existir muchos valores nulos). Al igual que en el caso de relaciones M:N , las claves propagadas son claves ajenas de la nueva tabla creada. Relaciones 1:1 : es un caso particular de las 1:N y se puede tanto crear una tabla o propagar la clave, si bien, en este último caso, la clave se propaga en las dos direcciones. Para decidir qué solución adoptar, se debe analizar la situación, intentando recoger la mayor semántica posible, y evitar valores nulos.Las relaciones de agregación se transforman del mismo modo que las 1:N . Transformación de relaciones exclusivas Después de haber realizado la transformación según las relaciones 1:N , se debe tener en cuenta que si se han propagado los atributos de las clases, convirtiéndose en claves ajenas de la tabla que provenía de la clase común a las relaciones, hay que comprobar que una y sólo una de esas claves es nula en cada ocurrencia. En caso de no propagarse las claves, estas comprobaciones se deben hacer en las tablas resultantes de transformar las relaciones. Transformación de la herencia Existen varias posibilidades que deben ser evaluadas por el diseñador a fin de elegir la que mejor se ajuste a los requisitos. Las opciones para tratar la transformación de la herencia son: Opción a : Consiste en crear una tabla para la superclase que tenga de clave primaria el identificador y una tabla para cada una de las subclases que tengan el identificador de la superclase como clave ajena.Esta solución es apropiada cuando las subclases tienen muchos atributos distintos, y se quieren conservar los atributos comunes en una tabla. También se deben implantar las restricciones y/o aserciones adecuadas. Es la solución que mejor conserva la semántica. Opción b : Se crea una tabla para cada subclase, los atributos comunes aparecen en todas las subclases y la clave primaria para cada tabla es el identificador de la superclase.Esta opción mejora la eficiencia en los accesos a todos los atributos de una subclase (los heredados y los específicos). Opción c : Agrupar en una tabla todos los atributos de la clase y sus subclases. La clave primaria de esta tabla es el identificador de la clase. Se añade un atributo que indique a qué subclase pertenece cada ocurrencia (el atributo discriminante de la jerarquía).Esta solución puede aplicarse cuando las subclases se diferencien en pocos atributos y las relaciones que asocian a las subclases con otras clases, sean las mismas. Para el caso de que la jerarquía sea total, el atributo discriminante no podrá tomar valor nulo (ya que toda ocurrencia pertenece a alguna subclase). Técnicas MatricialesLas técnicas matriciales tienen como objetivo representar las relaciones existentes entre distintos tipos de entidades, objetos o cualquier otro elemento del sistema. Se utilizan, principalmente, para analizar la consistencia entre los modelos generados durante el desarrollo, comprobar la trazabilidad con los requisitos especificados por el usuario, etc. DescripciónLas técnicas matriciales son útiles para representar las relaciones entre elementos comunes de los distintos modelos, tales como entidades/procesos, procesos/diálogos, datos/localización geográfica, y asegurar que los modelos son coherentes entre sí. Las siguientes son algunas de las matrices empleadas en MÉTRICA Versión 3: Procesos/localización geográfica: permite representar la localización geográfica de los procesos de una organización. Almacenes de datos/entidades del modelo lógico de datos normalizado: establece las relaciones existentes entre los almacenes de datos y las entidades, y permite verificar que cada almacén de datos definido en el modelo de procesos se corresponde con una o varias entidades del modelo lógico de datos normalizado. Atributos de interfaz/atributos de entidades del modelo lógico de datos normalizado: permite verificar que los atributos que aparecen en cada diálogo de la interfaz de usuario forman parte del modelo lógico de datos normalizado. Entidades/procesos: permite representar el tratamiento lógico de los procesos sobre los datos del sistema y verificar que cada entidad del modelo lógico de datos normalizado es accedida por algún proceso primitivo representado en el DFD. Diálogos/procesos: permite representar los diálogos asociados a un proceso interactivo y verificar que cada proceso interactivo tiene asociado al menos un diálogo. Objetos Diagrama de interacción / clases, atributos al modelo de clases: permite verificar que cada mensaje entre objetos se corresponde con un método de una clase. Mensajes Diagrama de interacción / métodos, atributos del modelo de clases: permite verificar que una clase tiene capacidad para proporcionar los datos que se soliciten en los mensajes que recibe. Evento, acción, actividad de clases / métodos de clases: permite verificar que todo evento, actividad o acción de una clase se corresponde con un método de esa clase. Clases/elementos del modelo físico de datos: permite verificar que cada elemento del modelo físico de datos se corresponde con un elemento del modelo de clases. Dependencias entre subsistemas/subsistemas: permite representar para cada subsistema, los subsistemas que dependen de él. Esquemas físicos de datos / nodos: permite representar la localización física de los datos en los nodos de la arquitectura del sistema, así como verificar que cada esquema del modelo físico de datos está asociado con un nodo del particionamiento físico del sistema de información. NotaciónDados dos tipos de elementos A y B, su representación será una matriz bidimensional NxM, siendo N el número de elementos de A, y M el número de elementos de B. En el cruce de una fila y una columna (C), se tendrá el modo en que se relacionan un elemento concreto de A y uno de B. Bibliografia Junta de Castilla y León (SOP_INF_T09_FINAL) PAe Diseño de bases de datos.La arquitectura ANSI/SPARC. El modelo lógico relacional. Normalización. Diseño lógico. Diseño físico. Problemas de concurrencia de acceso. Mecanismos de resolución de conflictos.IntroducciónEl diseño lógico es la etapa del proceso de diseño de una BD en la que se obtiene la representación de la estructura de la BD en términos de almacenamiento (tablas). La obtención de esta estructura implica la aplicación de unas reglas de transformación de los elementos previamente existentes en el modelo conceptual de la BD, reglas que se van a describir en este tema. Por su parte, el diseño físico es la etapa que incluye las acciones de configuración y ajuste del almacenamiento físico y de la seguridad de la BD. El diseño físico es una tarea compleja y dependiente del SGBD utilizados y del uso concreto que se pretenda hacer de la BD diseñada, por lo que en este tema se van a describir la problemática general que se aborda en esta etapa y los criterios de toma de decisiones en esas etapas para resolver un problema. Finalmente, el tema describirá la necesidad del uso concurrente de las BD y los conflictos que ese uso plantea, así como los mecanismos que los SGBD utilizan para resolverlos. Diseño LógicoEl diseño lógico es la etapa de creación de la BD en la que se va a traducir el modelo conceptual obtenido en la etapa de diseño conceptual en modelo lógico y un esquema lógico expresado de un modo comprensible para un SGBD. Si consideramos que el SGBD es relacional, es esquema lógico estará expresado en tablas y columnas (o relaciones y atributos). El diseño lógico se puede dividir en dos etapas: Diseño lógico estándar : En esta etapa se obtiene un modelo lógico estándar y un esquema lógico estándar, independientes del SGBD comercial en el que se vaya a implementar la BD. El modelo lógico estándar puede expresarse empleando varias técnicas, entre las que cabe citar el Diagrama de Estructura de Datos (DED) y el modelo relacional. El esquema lógico estándar se obtiene utilizando un Lenguaje de Definición de Datos (DDL) estándar, habitualmente el SQL-92 (que es el estándar ISO). Diseño lógico específico : Utilizando el esquema lógico estándar que se ha obtenido, se estudia su implementación en un SGBD comercial (Oracle, DB2, Sybase, etc). Para ello, habrá de analizarse la compatibilidad del modelo lógico estándr con el modelo lógico específico del SGBD elegido, y proponer un modo de solucionar aquellos aspectos del modelo lógico estándar que no recoge el modelo lógico específico. Una vez realizada esa tarea, se obtiene un esquema lógico específico usando el Lenguaje de Definición de Datos propio del SGBD (normalmente, será un lenguaje SQL con algunas peculiaridades propias de cada SGBD). A la etapa del diseño lógico específico también se la conoce como etapa de implementación de la BD. Diseño lógico estándarTal y como se ha comentado en el punto anterior, el diseño lógico estándar consiste en convertir el modelo conceptual de BD en un esquema lógico estándar, independiente del SGBD que se vaya a utilizar. El esquema lógico estándar será la expresión del modelo lógico estándar utilizando un Lenguaje de Definición de Datos independiente del SGBD (SQL habitualmente). Las técnicas de modelado conceptual son diferentes de las técnicas de modelado lógico, por lo que habrá que convertir cada elemento presente en el modelo conceptual en elementos expresables en la técnica de modelado lógico que hayamos seleccionado. En las BD relacionales, es habitual usar el modelo E/R para el modelado conceptual y el modelo relacional para el modelado lógico. En este apartado vamos a estudiar la conversión de los elementos del E/R al modelo relacional. Transformación de dominios Un dominio del modelo E/R se transforma en un dominio en el modelo relacional utilizando la sentencia SQL: CREATE DOMAIN Transformación de entidades Cada entidad del modelo E/R se transformará en una tabla (relación) en el modelo relacional. La tabla tendrá el mismo nombre que la entidad de la que proviene. Vemos un ejemplo a continuación y una entidad de un modelo E/R (conceptual). Transformando la entidad al modelo relacional, obtendremos lo siguiente: EMPRESA (CIF, Nombre, Dirección) Para obtener el esquema lógico estándar, la tabla se definirá usando la sentencia SQL CREATE TABLE. CREATE TABLE EMPRESA ( CIF Código_CIF, Nombre Nombres, Dirección Direccines, PRIMARY KEY (CIF)) Transformación de atributos Cada atributo de una entidad se transformará en una columna de la tabla (relación) del modelo relacional. Los atributos de las entidades pueden ser de cuatro tipos: Identificadores : Se transforman en una columna que es la clave primaria de la tabla. En SQL, la condición de clave primaria se representará colocando la cláusula PRIMARY KEY al lado del nombre de la columna en la que se ha convertido el atributo (dentro de la sentencia CREATE TABLE con la que se crea la tabla en la que está incluida la columna). Identificadores alternativos : Se transforman en una columna a la que se le añade la restricción UNIQUE, lo cual significa que no puede haber valores repetidos en esa columna. Atributos no identificadores : Se transforman en una columna de la tabla. Atributos multivaluados : En el modelo relacional una instancia de una relación sólo toma un valor para cada atributo. Por ello, será obligatorio crear una nueva tabla que contenga a la clave primaria de la tabla anterior y al atributo multivaluado, siendo la clave primaria de la nueva tabla la concatenación de los dos atributos y marcándose la clave primaria de la tabla anterior como clave foránea en la nueva tabla. Transformación de interrelaciones En el modelo relacional, la única unidad de modelado son las tablas, por lo que las interrelaciones del modelo E/R deben transformarse en tablas del modelo relacional (se produce una cierta pérdida de semántica). El modo de transformar las interrelaciones en tablas depende del tipo de interrelación considerada. Los tipos de transformaciones existentes son: Transformación de interrelaciones N:M Una interrelación N:M se va a convertir en una tabla del modelo relacional. La nueva tabla tendrá como columnas a la concatenación de los atributos identificadores de las entidades que estaban unidas por la interrelación. Una interrelación en el modelo E/R: EMPRESA (CIF, Dirección, …)VENDE (CIF, Codigo_producto)PRODUCTO (Codigo_producto, …) Si la interrelación tiene atributos en el modelo E/R, cada atributo de la interrelación pasará a ser una columna de la nueva tabla. Los atributos identificadores de las entidades unidas mediante la interrelación serán claves primarias en las tablas del modelo relacional que representen a sus entidades. Por tanto, esos mismos atributos serán considerados claves foráneas en la tabla que representa a la interrelación. La condición de calve ajena de una columna se expresará mediante la cláusula FOREIGN KEY en SQL. CREATE TABLE VENDE ( CIF Codigo_CIF, Producto Codigo_Producto, …, PRIMARY KEY (Codigo_CIF, Codigo_Producto), FOREIGN KEY (Codigo_CIF) REFERENCES Empresa, FOREIGN KEY (Codigo_Producto) REFERENCES Producto) Las cardinalidades mínimas de cada entidad participante en la interrelación se van a expresas en el esquema lógico estándar usando la sentencia de SQL. CREATE ASSERTION Transformación de interrelaciones 1:N Las relaciones 1:N se pueden transformar de dos maneras: No crear ninguna tabla que represente a la interrelación y añadir a la tabla que representa a la entidad con cardinalidad n el conjunto de atributos que son clave primaria de la entidad con cardinalidad 1. Este es el modo habitual de realizar la transformación. Convirtiendo la interrelación en una tabla, siendo la clave primaria de la tabla el conjunto de atributos identificadores del lado n de la relación. Esta opción se utiliza en los siguientes casos: Cuando se cree que en un futuro la relación se va a transformar en una de tipo N:M (y por tanto, será necesario tener una tabla que represente a esa relación). Cuando la interrelación tiene atributos en el modelo relacional. Cuando la interrelación es optativa para las ocurrencias de las entidades situadas en el lado 1 de la relación (cardinalidad 0:1) y el porcentaje de ocurrencias interrelacionadas es bajo, lo cual va a significar que en las columnas absorbidas en la tabla n van a existir muchos valores nulos. Transformación de interrelaciones 1:1 Se realizan del mismo modo que las interrelaciones 1:N, pero teniendo en cuenta que si se decide no crear una tabla que represente a la interrelación, la elección de la tabla a la que se le añaden los atributos del otro extremo es optativa, si bien se suele seguir el siguiente criterio: Si una de las dos entidades de la interrelación tiene cardinalidad (0,1) y la otra entidad tiene cardinalidad (1,1), entonces se propagan los atributos identificadores de la tabla (1,1) a la tabla (0,1), evitándose los valores nulos. Si las dos tablas tienen cardinalidad (1,1), se puede escoger cualquiera de los dos extremos para propagar la clave. En este caso, la elección puede depender de criterios como las frecuencias de acceso a las tablas. Si los dos extremos participan con una cardinalidad (0,1), crear una tabla que represente a la interrelación. El identificador de la tabla podrá ser el identificador de cualquiera de los dos extremos, y los atributos que sean clave primaria en uno de los dos extremos, serán clave foránea en la nueva tabla. Transformación de interrelaciones con un grado superior a 2 El mecanismo de transformación es igual al de las tablas N:M, se crea una tabla que representa a la relación, y su clave primaria será la concatenación de los atributos identificadores de todas las entidades a las que interrelaciona (lógicamente, tres o más). Transformación de interrelaciones de dependencia y existencia El modelo relacional no distingue tipos de relaciones, por lo que las interrelaciones de dependencia y existencia se han de convertir en relaciones del mismo modo que las interrelaciones 1:N. Habitualmente, se propaga la clave de la tabla que representa a la entidad débil a la tabla que representa al entidad fuerte. Transformación de restricciones de entidades o atributos En el modelo E/R pueden estar expresadas restricciones de usuario. Estas restricciones se recogen en el esquema lógico estándar del siguiente modo: Si la restricción indica un rango de valores, se usa la cláusula BETWEEN. Si la restricción implica que un determinado atributo o conjunto de atributos sólo puede tomar un valor de entre los pertenecientes a una lista, se emplea la cláusula IN. Si la restricción es de otro tipo, se puede utilizar la sentencia CHECK para comprobar el cumplimiento de la condición fijada, o la sentencia CREATE ASSERTION si la restricción afecta a más de una tabla. CREATE TABLE Decreto( Num_Decreto Numeros_Decreto, Fecha_Aprobacion Fecha, Fecha_Publicacion Fecha, …, CHECK (Fecha_Aprobacion &lt; Fecha_Publicacion)) Transformación de dependencias de identificación y existencia La transformación de estas dependencias se realizarán del mismo modo que el de las relaciones 1:M, es decir, no escribiendo ninguna relación que las represente y propagando la clave de la tabla que representa a la entidad fuerte de la tabla que representa a la entidad débil, en la cual jugará el papel de clave foránea. Para dicha clave foránea, no se admitirán los valores nulos, y se añadirá la condición de borrado y modificación en cascada (la eliminación o modificación de una ocurrencia de la entidad fuerte obligará a la eliminación o modificación de las ocurrencias de las entidades débiles que tiene asociadas). Si la dependencia es en identificación, entonces la clave primaria de la tabla que representa a la entidad débil se formará mediante la concatenación de la clave primaria de la entidad débil y de la clave propagada que proviene de la entidad fuerte. Transformación de restricciones en las interrelaciones Se utilizarán los mismos mecanismos que se han comentado para la transformación de restricciones de las entidades o de sus atributos (usando las condiciones CHECK o CREATE ASSERTION si la restricción afecta una interrelación o a varias). Transformación de generalizaciones (Relaciones ISA) Hay tres estrategias para llevar a cabo esta transformación: Transformar la entidad y sus subtipos en una sola tabla, la cual tendrá como atributos la concatenación de los atributos de la entidad y de los subtipos. En el ejemplo: EMPLEADO_PUBLICO (DNI, Nombre, Relacion_laboral, Inicio_contrato, Toma_posesion) Crear una tabla para la entidad generalizadora y una tabla por cada subtipo. Cada tabla tendrá como atributos los de su entidad o subtipo correspondiente. Ésta es la opción que mejor mantiene la semántica del modelo E/R. En el ejemplo: EMPLEADO_PUBLICO (DNI, Nombre)INTERINO (DNI, Inicio_Contrato)FUNCIONARIO (DNI, Toma_Posesion) Crear una tabla para cada subtipo. Cada tabla tendrá como columnas los atributos del subtipo al que representa y los atributos comunes (los que posee la entidad generalizadora). En el ejemplo: INTERINO (DNI, Nombre, Inicio_Contrato)FUNCIONARIO (DNI, Nombre, Toma_Posesion) Mostramos la figura de una relación ISA en el modelo E/R: Transformación de la dimensión temporal Se distinguen dos casos: Si la dimensión temporal aparece en el modelo E/R como una entidad, se transformará del mismo modo que el resto de las entidades. Si la dimensión temporal aparece en forma de atributos de una interrelación, estos atributos se ubicarán en la tabla que les corresponda al transformar la interrelación (bien en la tabla de la interrelación o bien en la tabla hacia la que se hayan propagado claves). Ahora bien, debe considerarse que estos atributos de tipo fecha pueden tener que formar parte de la clave primaria de la tabla en la que se ubiquen en función de la semántica de la situación que se representa. Transformación de atributos derivados Los atributos derivados se transformarán en columnas de la entidad a la que pertenezcan (como el resto de los atributos). Además, se establecerá un disparador o un procedimiento almacenado que calcule el valor del atributo cada vez que se inserta una nueva fila en la tabla o cada vez que se modifique en una fila el valor de alguno de los atributos a partir de los cuales se calcula el atributo derivado. Normalización del esquema obtenido Finalizada la transformación del esquema conceptual en un esquema lógico, debe aplicarse a dicho esquema lógico un proceso de normalización, evitándose así las anomalías de inserción, modificación y borrado que provoca la redundancia. Diseño lógico específicoPartiendo del esquema lógico obtenido en el apartado anterior, se elabora un esquema adaptado al sistema gestor de BD que se va a utilizar, creándose las tablas del esquema utilizando el Lenguaje de Definición de Datos propio del cada sistema. En las BDR, el Lenguaje de Definición de Datos (LMD) es el SQL, si bien existen pequeñas variaciones entre el SQL usado en cada sistema, que normalmente incluye pequeñas modificaciones o extensiones del lenguaje SQL (que es el estándar ISO). En el paso del modelo lógico estándar al modelo lógico específico de cada SGBD puede encontrarse que el modelo lógico específico soporta todos los conceptos del modelo lógico estándar (del modelo relacional) o, por el contrario, que existen determinados aspectos del modelo lógico estándar que el modelo lógico específico no soporta. En este último caso, habrá que realizar un trabajo complementario de adaptación (bien añadiendo programación complementaria en el diccionario de datos del SGBD o bien haciendo que la puesta en práctica de esas restricciones no soportadas por el modelo lógico del SGBD, la lleve a cabo el código de los programas que utilicen los datos de la BD). Algunos de los aspectos del modelo lógico estándar que pueden tener que adaptarse son los siguientes: Dominios : El DDL del SGBD puede no incluir ninguna sentencia que nos permita crear dominios (los únicos dominios que reconoce automáticamente son los asociados a los tipos de datos predefinidos en el propio SGBD). En este caso, habrá que elegir una de las dos opciones siguientes: Cuando se especifique la columna (dentro de la sentencia CREATE TABLE), escoger el tipo de datos predefinido que mejor se ajuste, fijar la longitud y añadir alguna restricción CHECK. Crear una tabla de dominio, que contendrá una sola columna, y en la que cada fila será uno de los valores posibles del dominio. Una vez creada esta tabla, crear un procedimiento almacenado que comprueba que los valores que se intentan insertar en la columna son compatibles con el dominio que queremos establecer. La tabla de dominio será estática, es decir, sólo podrá ser modificada por el administrador de la BD. Lógicamente, la opción de construir una tabla de dominio sólo será validad si el dominio a construir es finito. Clave Primaria : Si el SGBD no incluye una cláusula PRIMARY KEY, debe conservarse la semántica dando los siguientes pasos: Añadir la restricción NOT NULL en los atributos que formen parte de la clave primaria (debe recordarse que una clave primaria no admite valores nulos). Añadir la restricción UNIQUE al conjunto de atributos de la clave primaria (ya que una clave primaria no admite valores repetidos). Añadir a la tabla un índice construido sobre las columnas que forman parte de la clave primaria. Este índice se debe crear al crear la tabla y se debe destruir cuando la tabla sea eliminada. Documentar el esquema con un comentario que indique cuál es la clave primaria. Clave ajena : Si el SGBD no incluye una cláusula FOREIGN KEY, debe conservarse la semántica dando los siguientes pasos: Añadir la restricción NOT NULL en los atributos de la clave ajena que no admitan nulos (cuando la cardinalidad mínima de la interrelación original fuera de al menos uno). Hacer que los programas que trabajen con la BD implementen las restricciones de clave ajena (integridad referencial). Documentar el esquema con un comentario que indique que una columna o conjunto de columnas son clave ajena. El resto de los aspectos del modelo lógico estándar no recogidos por el modelo lógico específico del SGBD suelen modelarse empleando procedimientos almacenados o triggers. Finalizada la etapa del diseño lógico específico, habremos creado un esquema lógico específico en el SGBD. Esto quiere decir que ya tendremos una BD operativa en la que se podrán insertar, eliminar o modificar datos y sobre la cual podremos realizar consultas. Diseño FísicoEl diseño físico es la última etapa del proceso de creación de una BD. El objetivo de esta fase es obtener un esquema interno de la BD que cumpla lo mejor posible los objetivos de funcionamiento de la BD que los usuarios esperan. Más concretamente, se trata de: Disminuir el tiempo de respuesta de la BD (tanto el tiempo medio como la respuesta ante los picos de carga) Disminuir el espacio de almacenamiento utilizado Incrementar la Seguridad de la BD Estos objetivos del diseño físico no siempre son compatibles entre sí. Por ejemplo, para reducir el tiempo de respuesta de las consultas a una BD, puede ser necesario incrementar la redundancia de los datos (tener los mismos datos almacenados en varias tablas a la vez). Obviamente, esto incrementará el espacio de almacenamiento utilizado. Un buen diseño físico debe tener en cuenta para cada BD las necesidades de uso, establecer unos objetivos concretos, y, cuando estos objetivos sean contradictorios, priorizarlos y alcanzar un nivel de compromiso aceptable entre ellos. Para llevar a cabo esta etapa, es preciso contar con información precisa sobre muchos aspectos de la BD que se va a crear y de la plataforma en la que se va a trabajar. El diseño físico comienza a realizarse cuando se ha recopilado información suficiente sobre: Los recursos software de los que se dispone Los recursos hardware de los que se dispone El esquema lógico específico de la BD Políticas de seguridad de los datos Estudio detallado de las aplicaciones que va a utilizar la BD y de las transacciones que van a generar. El nivel físico de las BD no está estandarizado, por lo que la realización del diseño físico es dependiente del SGBD que se esté utilizando. Cada SGBD relacional definirá su propia estructura de archivos, índices, buffers de memoria, roles de seguridad y objetos de gestión del nivel físico, manipulándose este nivel a través de una extensión del lenguaje SQL estándar específica de cada sistema gestor. De los dicho en los párrafos anteriores se pueden extraer las siguientes conclusiones: Al contrario que en el diseño lógico general, el enfoque del diseño físico no puede ser formal, sino casuístico, adaptado a cada SGBD y a cada BD utilizados. No hay recetas universalmente válidas, sino “buenas ideas” (heurísticas) asentadas en conceptos de almacenamiento, arquitectura de computadores, redes o algoritmia. Al no haber una estandarización del nivel físico, esas ideas deben ser puestas en práctica en cada caso concreto, probadas, evaluadas y, si es necesario, refinadas hasta alcanzar la situación final deseada. A este proceso de mejora del diseño físico se le conoce como ajuste de la BD o tunning. Por último, es preciso comentar que no todos los SGBD tienen el mismo grado de flexibilidad en su nivel físico. En función del grado de manejo que permitan para el diseño físico, podemos distinguir tres tipos de SGBD: Rígidos : El SGBD fija una estructura interna que apenas admite configuración. Esto asegura la independencia físico/lógica de la BD, pero es poco adaptable a cada situación concreta, lo que puede suponer una pérdida de eficiencia. Flexibles : El SGBD permite que sea el Administrador de BD el que diseñe toda la estructura interna. El diseño de toda la estructura interna es un trabajo extenso y complejo, y la toma de decisiones del administrador puede afectar a la independencia físico/lógica de los datos. Sin embargo, también es el enfoque más adaptable a cada necesidad concreta, con lo que es la alternativa con la que se podría obtener un mayor grado de eficiencia en el uso de la BD. Semiflexibles : El SGBD proporciona una estructura inicial configurable a través de un conjunto de parámetros. La modificación de estos parámetros por parte del Administrador de BD permite ir mejorando esa estructura interna, y por ende, el rendimiento de la BD. Ésta opción ofrece un buen compromiso entre eficiencia e independencia físico/lógica, siendo habitual en los SGBD. Metodología de trabajo para la obtención del diseño físicoPodemos dividir la etapa del diseño físico en tres fases: Diseño de la representación física. Análisis de las transacciones. Selección del modo de almacenamiento en memoria secundaria. Creación de índices secundarios. Realización de Agrupamientos de tablas. Realización de procesos de desnormalización. Estimación de la necesidad de espacio en disco. Diseñar los mecanismos de seguridad. Diseñar las vistas de los usuarios. Diseñar las reglas de acceso. Monitorizar y ajustar el sistema. Análisis de las transacciones Para realizar un buen diseño físico es necesario conocer las consultas y las transacciones que se van a ejecutar sobre la BD. Esto incluye tanto información cualitativa, como cuantitativa. Para cada transacción, hay que especificar: La frecuencia con que se va a ejecutar. Las relaciones y los atributos a los que accede la transacción, y el tipo de acceso: consulta, inserción, modificación o eliminación. Los atributos que se modifican no son buenos candidatos para construir estructuras de acero. Los atributos que se utilizan en los predicados WHERE de las sentencias SQL. Estos atributos pueden ser candidatos para construir estructuras de acceso dependiendo del tipo de predicado que se utilice. Si es una consulta, los atributos involucrados en el join de dos o más relaciones. Estos atributos pueden ser candidatos para construir estructuras de acceso. Las restricciones temporales impuestas sobre la transacción. Los atributos utilizados en los predicados de la transacción pueden ser candidatos para construir estructuras de acceso. Selección de la organización del almacenamiento en memoria secundaria Las BD van a almacenar la información en dispositivos de almacenamiento secundarios (discos o cintas), los cuales se caracterizan por tener mayor capacidad que la memoria principal y por la no volatilidad de los datos. Sin embargo, son mucho más lentos que la memoria principal a la hora de recuperar información, por lo que es preciso realizar un estudio detallado sobre el modo de organizar la información en ellos, de modo que consigamos un rendimiento en tiempo de acceso ajustado a cada necesidad de uso de la BD. Las alternativas de organización consisten básicamente en la elección del tipo de fichero o estructura de datos más adecuado para cada caso. En este apartado nos limitaremos a estudiar las ventajas y las desventajas de las más habituales, pero sin entrar en una descripción detallada de esas estructuras. Ficheros secuenciales Organizados de tal manera que cada registro es adyacente al siguiente registro. Esta relación de adyacencia puede ser física (direcciones físicas consecutivas) o lógica (haciendo que cada registro contenga un puntero al siguiente registro). Los ficheros secuenciales no permiten el acceso directo a los datos, por lo que el acceso a los registros se realiza en el mismo orden en el que fueron introducidos en el fichero. Ficheros secuenciales indexados ISAM Es una estructura de fichero indexado en el que los registros se agrupan en bloques, y en el interior de dichos bloques están organizados secuencialmente. El índice que se crea sobre el fichero contiene apuntadores a las direcciones de inicio de cada bloque, y el acceso a datos a través del índice se realiza de la siguiente manera: Se localiza el índice de la clave que cumpla la condición de búsqueda. Se accede al bloque apuntado por el nodo que contenía a la clave anterior. Una vez dentro del bloque, el registro deseado se busca secuencialmente. La organización de ficheros ISAM mantiene el equilibrio entre el tamaño de los índices y el tiempo de acceso a los registros. Como el tamaño de los bloques está limitado, en estos ficheros hay una zona de desbordamiento en que se van a almacenar los registros que no se pueden guardar en el bloque que les corresponde cuando éste ya está lleno. El uso de la zona de desbordamiento disminuye el rendimiento del sistema, puesto que se accede a ella tras buscar al registro en el bloque en el que le correspondería estar, y porque la búsqueda en la zona de desbordamiento es secuencial. Cuando el área de desbordamiento es muy grande, se reorganiza el fichero, realizándose una nueva división de bloques, ubicando a todos los registros en los bloques y reorganizando el índice de apuntadores a bloques. Árboles-B Estructura de indexación en forma de árbol equilibrado. El hecho de ser equilibrados (misma altura en todas sus ramas) permite minimizar el número de accesos a disco cuando se realiza una búsqueda: las búsquedas rápidas son una característica que distingue a los árboles-B. En cuanto al almacenamiento, los árboles-B consiguen una gestión del espacio razonablemente buena, ya que si el árbol es de orden n, cada nodo debe tener al menos n/2 claves (es decir, como mucho se desaprovecha la mitad del espacio de almacenamiento del índice, lo cual es fácilmente asumible con los recursos de almacenamiento de que se dispone hoy en día). Ficheros de acceso aleatorio empleando técnicas de Hashing En estos ficheros se accede directamente a los registros mediante el valor de su clave (siendo la clave uno o más de los campos del registro). Para ello, se dispone de una función “hash” o de mapeado que permite calcular la dirección del registro a partir del valor de la clave. Este sistema es el que más rápido permite realizar una búsqueda de un registro concreto, pero sólo funciona para resolver consultas exactas (con todo el valor de la clave). Si la búsqueda es por rango o por patrón (por ejemplo, LIKE ‘%a’), no se puede aplicar la función hash. Criterios de elección entre las estructuras La elección de una estructura de organización dependerá del uso que se realice de los datos almacenados. La siguiente table recoge las situaciones en las que se suele preferir cada estructura: La elección entre un fichero ISAM y un árbol-B se realiza considerando los siguientes factores: Frecuencia de las actualizaciones de los datos: Si la frecuencia de las actualizaciones es alta, debe elegirse un árbol-B, ya que los ficheros ISAM se irán degradando al irse añadiendo registros a la zona de desbordamiento. Elevado número de consultas recurrentes: Si se realizan muchas consultas simultáneas sobre los datos indexados, el fichero ISAM debe ser la estructura elegida, ya que al ser su índice estático, no se bloquea (facilita el acceso concurrente). Si se deben tener en cuenta los dos factores o no se conoce bien el entorno de explotación de la BD (existen dudas sobre el número de usuarios, la frecuencia de acceso a datos, etc), la estructura elegida será el árbol-B, ya que es la más adaptable de las dos y sus aspectos desfavorables respecto a los ficheros ISAM tienen poca repercusión en el funcionamiento del sistema. Creación de los índices secundarios Las BD relacionales indexan a cada tabla por su clave primaria, creándose automáticamente el índice de clave primaria en el mismo momento en el que se crea la tabla. Sin embargo, es frecuente añadir índices adicionales a las tablas, para que hagan más rápidas las consultas que se realizan sobre determinados campos de esas tablas. La introducción de un índice secundario en una tabla repercute negativamente en los tiempos de ejecución de la inserción o eliminación de registros, y supone un incremento del espacio de almacenamiento necesario para la tabla. El administrador de BD debe ponderar en cada caso las pérdidas y ganancias de introducción de un nuevo índice, para así determinar si su creación es interesante. Algunas situaciones que hacen interesante la indexación son: Atributos de la relación a los que se accede con mucha frecuencia. Claves foráneas de la relación sobre las que es habitual realizar joins. Algunas situaciones que desaconsejan la introducción de índices secundarios son: Tablas con pocas filas: el recorrido secuencial de las tablas sería muy breve, y la reducción del tiempo de acceso obtenida de la introducción del índice es muy escasa. Atributos cuyo valor se modifica muy a menudo: Los índices utilizan como clave de búsqueda los valores de los atributos indexados. Si la modificación de esos valores es muy frecuente, habrá que reconstruir el índice cada poco tiempo, lo cual puede suponer un coste elevado si la tabla tiene muchas filas. Atributos con valores poco selectivos: Los atributos en los que es muy habitual la repetición de su valor en distintas filas de la tabla no son buenos candidatos para la indexación. Después de recorrer el índice y localizar su clave de búsqueda, tendríamos todavía muchos registros con el mismo valor de clave, y habría que recorrer secuencialmente esos registros para encontrar el que se busca; en este caso, la introducción del índice no supondría ningún beneficio importante desde el punto de vista del tiempo de acceso. Por ejemplo, si tenemos una tabla de personas y deseamos buscar una persona en concreto, no tiene sentido indexar por un campo ‘Sexo’, ya que aproximadamente la mitad de los registros de la tabla tendrían el valor ‘hombre’ y la otra mitad el valor ‘mujer’. Realización de agrupamientos de tablas (clustering) El clustering o agrupación de tablas es una técnica consistente en almacenar un grupo de tablas en una misma área de memoria secundaria. De este modo, los accesos simultáneos a las tablas agrupadas no obligan al SGBD a la búsqueda de los datos en zonas lejanas del almacenamiento secundario, lo que reduce el tiempo de resolución de esos accesos. El clustering será una técnica a considerar si esos accesos simultáneos son frecuentes. Realización de procesos de desnormalización El proceso de desnormalización consiste en introducir redundancias en un esquema lógico previamente normalizado (típicamente en 3FN o en FN de Boyce-Codd). Aunque ésta es una decisión de nivel lógico, se suele tomar por motivos de eficiencia de la BD (repetir datos en varias tablas evita navegar entre ellas para encontrarlos), es decir, la decisión viene motivada por cuestiones de implementación. La desnormalización ralentiza las actualizaciones de datos, hace perder flexibilidad al esquema lógico (puede afectarnos si decidimos añadir nuevas tablas, por ejemplo) y complica la implementación de la BD y su documentación. Por tanto, es una alternativa que sólo se debe utilizar cuando el resto de las técnicas de diseño físico no nos proporcionan un rendimiento de las consultas de la BD que sea aceptable. Entre las acciones de desnormalización que se pueden realizar, se pueden destacar: Introducir atributos derivados: Los atributos derivados son aquellos cuyo valor se puede obtener realizando un conjunto de operaciones sobre atributos ya existentes en la BD. La introducción de un atributo derivado ayuda a obtener ese valor rápidamente en una consulta, puesto que no hay que realizar las acciones de cálculo del mismo, pero incrementa los costes de almacenamiento y de actualización de las tuplas de la BD (cada vez que se modifique el valor de un atributo que interviene en su cálculo, habrá que recalcular el valor del atributo derivado). Fusionar tablas involucradas en relaciones uno a uno: Si el acceso conjunto a ambas tablas es muy habitual, puede ser interesante crear una nueva tabla cuyos atributos sean la concatenación de los atributos de las dos tablas, lo cual puede reducir su tiempo de acceso. Sin embargo, este cambio perjudicará a las operaciones join que se realicen entre cualquiera de esas dos tablas y una tercera, ya que las filas de la nueva tabla que se han de leer ocuparán más en memoria (la tabla fusionada tiene más campos), por lo que se recuperarán menos registros en cada lectura, lo cual obligará a un número de accesos a disco superior. Introducir atributos duplicados en relaciones 1:N: Consiste en añadir atributos no clave de la tabla con cardinalidad 1 en la tabla con cardinalidad N. Así, cuando se realiza una operación de join entre las dos tablas que sólo involucra a los atributos duplicados, no será necesario recorrer la tabla con cardinalidad 1. Determinación del espacio de almacenamiento necesario Una vez obtenido el esquema de implementación definitivo, el administrador de la BD debe determinar el espacio de almacenamiento necesario para la BD. La cantidad de espacio que se determine será una estimación basada en el estudio de la cantidad de datos ya existente y que hay que cargar en la BD y en las estimaciones de crecimiento futuro de la BD. La estimación del espacio puede verse afectada también por el nivel de seguridad que se desee para el almacenamiento de datos y por el grado de disponibilidad de la BD. Por ejemplo, si uno de los requisitos de la BD es que su disponibilidad sea 24×7 (24 horas al día, 7 días a la semana), es posible que se necesite usar una estructura de almacenamiento secundario en forma de RAID 1+0 o 0+1, lo que duplicaría el espacio de almacenamiento necesario para la BD. Diseño de los mecanismos de seguridad de los datosCreación de las vistas de los usuarios Las vistas son un elemento de la BD que permiten a los usuarios ver los datos de una determinada forma (corresponden al nivel externo en la arquitectura ANSI/SPARC). El uso de las vistas permite: Reducir la complejidad del esquema lógico global, manteniendo un esquema único para todos los usuarios y adaptando ese esquema a las necesidades que tenga cada tipo de usuario. Mejorar el nivel de seguridad de la BD, ya que los usuarios no verán más datos de las tablas que aquellos que están incluidos en la vista. Fijar las reglas de acceso de los usuarios En esta fase, se establecen los perfiles de usuario, donde cada perfil es el conjunto de acciones que cada tipo de usuario va a poder hacer sobre cada elemento o conjunto de elementos de la BD (tablas, filas, unidades lógicas de almacenamiento, etc.) Monitorización y ajuste del sistemaEl diseño físico no se debe concebir como un proceso secuencial que finaliza cuando se ha encontrado una configuración válida. Las BD pueden sufrir variaciones a lo largo del tiempo, tanto en su tamaño, como en su esquema lógico global o en el uso que se desee hacer de ellas, lo que obligará a una monitorización continua de su rendimiento (para comprobar si se va degradando) y a la introducción de ajustes que permitan adaptarse a los cambios sufridos por la BD o solventar las pérdidas de eficiencia que se hayan producido. Cada proceso de ajuste supone en cierta medida una reconstrucción del diseño físico que se haya realizado, por lo cual esta etapa de diseño volvería a comenzar de nuevo. La gestión de la concurrenciaVeamos la gestión de la concurrencia en los SGBD. Para ello, empezaremos introduciendo la necesidad de dicha gestión y el concepto de transacción, pasando posteriormente a abordar los problemas que plantea la concurrencia (Lectura Fantasma, Lectura Sucia, etc.) y los mecanismos de resolución existentes. Necesidad de gestión de la concurrenciaLos SGBD deben poder mantener la integridad de los datos que almacenan. Durante la vida de una BD, existen secuencias de acciones de escritura y lectura que pueden originar que la BD quede en un estado inconsistente si no se ejecutan todas las acciones de esa secuencia. Para evitar este tipo de problemas, los SGBD utilizan las transacciones, que son unidades lógicas de proceso que se componen de una secuencia de acciones cuya ejecución es atómica, o se ejecutan todas o no se ejecuta ninguna. En un punto intermedio de una transacción, el estado del BD puede ser inconsistente, siendo siempre consistente al principio y al final de la transacción. El elemento del SGBD encargado de conseguir este objetivo es el Gestor de Transacciones (transaction manager), el cual gestiona las peticiones de los usuarios en forma de transacciones. Además, los SGBD que existen en la actualidad ofrecen la posibilidad de ser usados en modo multiusuario, lo que implica que múltiples usuarios pueden trabajar a la vez con el sistema como si fuera un recurso dedicado, sin apercibirse de la presencia de otros usuarios. Para lograr este efecto, los tiempos de respuesta del SGBD a todos los usuarios deben ser reducidos, lo que obliga a realizar una ejecución concurrente (simultánea) de las transacciones de cada usuario. La ejecución atómica que caracteriza a las transacciones no garantiza que las pertenecientes a un programa de un usuario no puedan interferir en las transacciones que pertenecen a otros programas y que se están ejecutando al mismo tiempo. Conseguir que las transacciones no interfieran con el funcionamiento de otras transacciones se conoce como aislamiento de la transacción (isolation), y el conjunto de problemas que plantea conseguir ese aislamiento en un entorno multiusuario es conocido como el problema de la gestión de la concurrencia. Además, durante el transcurso de una transacción, pueden producirse problemas en el SGBD que no permitan que la transacción concluya con éxito, lo que, en virtud del principio de “todo o nada” que acompaña a las transacciones, obliga al SGBD a estar preparado para deshacer las acciones que una transacción no concluida ha realizado sobre una BD. Es el problema de la recuperación y se gestiona añadiendo una nueva propiedad a las transacciones, que es la persistencia, y que consiste en que las modificaciones de datos que se realizan durante una transacción no se almacenan en la BD hasta que la transacción ha finalizado con éxito. Resumiendo, una transacción ha de tener las siguientes propiedades: Atomicidad : Las acciones de una transacción se ejecutan todas o ninguna. Consistencia : La BD se encuentra en un estado consistente antes de la ejecución de la transacción y debe estar en un estado consistente cuando la transacción termine. Aislamiento : La ejecución de una transacción no debe interferir en la ejecución de otras transacciones, la transacción debe ejecutarse como si estuviera aislada. Persistencia : Los efectos de una transacción no son permanentes en la BD hasta que la transacción ha finalizado con éxito. Manejo de transacciones en el SGBDEl gestor de transacciones es el componente funcional del SGBD que planifica y controla la ejecución de transacciones concurrentes en un BD. Un gestor de transacciones se puede dividir conceptualmente en los siguientes elementos: Un contenedor de entrada: Al que llegan las transacciones que se deben ejecutar. Un planificador (scheduler): Que determina el orden en el que las transacciones de la cola van a ser ejecutadas. Cuando el planificador da paso a una transacción, ésta es enviada al gestor de datos, desde el cual se realizarán las operaciones de la transacción sobre la BD. Un contenedor de salida: Al que llega la transacción cuando ha terminado de ejecutarse. Si la transacción ha finalizado correctamente, el gestor de transacciones realizará una operación commit, haciendo las modificaciones de la transacción persistentes en la BD. Si la transacción no ha finalizado correctamente debido a algún problema, el gestor de transacciones ejecuta una operación abort y envía la transacción a un gestor de recuperación, el cual deshace las operaciones de la transacción y devuelve los datos al estado en que estaban antes de iniciarse la transacción. La transacción abortada será devuelta al contenedor de salida para volver a ser ejecutada. Con el fin de incrementar el rendimiento de las BD en un entorno multiusuario, las transacciones no se ejecutan secuencialmente una detrás de otra. El modelo de ejecución secuencial de las transacciones podría originar una situación en la que un programa corto se quedase bloqueado a la espera de la finalización de un programa largo. En este caso, el usuario del programa debería sufrir un gran tiempo de espera para la realización de un conjunto reducido de operaciones, con lo que no percibiría a la BD como un recurso dedicado. Para evitar estas situaciones, las transacciones se despachan concurrentemente, es decir, sus acciones se van a llevar a cabo de forma entrelazada. Sin embargo, el entrelazado de acciones de las transacciones puede originar conflictos, algunos de los cuales van a ser comentados en el punto siguiente. La siguiente figura representa a un gestor de transacciones: Problemas de la concurrenciaEl problema de la actualización perdida Problema de concurrencia que ocurre cuando: Dos transacciones T1 y T2 trabajan en paralelo e intentan modificar el valor del mismo objeto de la BD. Ambas transacciones leen el valor del objeto antes de que la otra lo actualice. En este caso, cada una de las transacciones modificará el valor del objeto en memoria y tratará posteriormente de escribirlo en la BD. El valor que se almacenará en la BD será el que tiene el objeto en la transacción que escriba más tarde, ya que el valor que ha escrito la primera transacción se sobrescribirá. Así, la actualización realizada por la transacción que ha escrito primero quedará sin efecto. El problema de la lectura sucia Se produce cuando: Una transacción T1 lee un valor de un objeto que ha sido modificado por otra transacción T2 y que todavía no se ha hecho persistente en la BD. La transacción T2, que había modificado los datos no termina correctamente. En este caso, la transacción T1 está trabajando con un valor que es inconsistente. Debe recordarse que los datos pueden encontrarse en un estado inconsistente en el transcurso de una transacción, por lo que la transacción primera puede haber leído dichos datos inconsistentes, utilizándolos para completar su secuencia de acciones. El problema de la lectura fantasma o lectura no repetible Puede aparecer cuando: Una transacción T1 está leyendo un conjunto de datos al tiempo que una transacción T2 los está modificando. La transacción T1 lee algunos de los datos antes de que sean modificados por T2 y otros después de ser modificados por T2. El resultado de la operación que realiza T1 se ve afectado por el hecho de combinar datos no actualizados por T2 con datos actualizados por T2. En este caso, el valor ‘s’ es la agregación de los valores x, y, z. La acción de la transacción T2 no modifica el valor de esa agregación, sin embargo, la lectura de valores con diferentes versiones ha provocado que T1 ofrezca un valor de s que es equivalente a x+y+x-10. Mecanismos de resolución de conflictosComo ya hemos comentado en el punto anterior, la labor fundamental de un gestor de transacciones es manejar las transacciones que generan los programas de los usuarios de tal modo que se mantenga la consistencia de la BD. En un entorno multiusuario, el gestor de transacciones deberá conseguir mantener la consistencia cuando se ejecutan entrelazadamente las acciones que componen las transacciones de los distintos programas. Este orden de ejecución lo establece el planificador (scheduler) del gestor de transacciones, llamándose plan de ejecución a cada posible secuencia de ejecución de un conjunto de acciones. Lógicamente, existen múltiples planes de ejecución posibles, siendo interesantes los planes que cumplen la condición de serializabilidad; un plan de ejecución A es serializable para un conjunto de transacciones T si existe un plan de ejecución secuencial A’ que es equivalente a A. O dicho de un modo menos formal, un plan es serializable si el resultado de la ejecución entrelazada de las acciones de esas transacciones es equivalente al resultado de ejecutar las transacciones en serie. Sin embargo, la comprobación “a priori” de la serializabilidad de un plan es muy costosa (problema NP-completo), por lo que el gestor de transacción no trata de determinar si un plan de ejecución es serializable; en lugar de eso, el gestor de transacciones emplea diversos mecanismos de ejecución de las acciones que o eliminan los conflictos debidos a la concurrencia o permiten recuperar el estado consistente de la BD cuando se producen. Podemos considerar los métodos de resolución de conflictos divididos en dos tipos: Métodos pesimistas : Basados en la suposición de que los conflictos entre transacciones concurrentes ocurren con alta frecuencia, lo que obliga a realizar una serie de acciones para manejar esos conflictos. Métodos optimistas : Basados en la suposición de que los conflictos entre transacciones se producen con escasa frecuencia. Métodos pesimistas Concepto de bloqueo y técnicas basadas en bloqueos Este mecanismo de resolución de conflictos se basa en el concepto de bloqueo, que consiste en que cuando una transacción T1 necesita realizar alguna acción sobre un objeto de la BD, debe solicitar al SGBD una reserva de ese objeto, no pudiendo ejecutar la acción hasta que la reserva no se ha producido. El SGBD sólo concederá esa reserva si otra transacción T2 no mantiene el mismo objeto reservado (en otras palabras, mientras una transacción tenga a un objeto de la BD reservado, el acceso al resto de las transacciones puede considerarse bloqueado). En un momento dado, bien durante su transcurso o bien a su finalización, la transacción terminará las acciones que necesite hacer sobre el objeto de la BD, deshaciendo la reserva y quedando el objeto disponible para otras transacciones. Las acciones de bloqueo o desbloqueo no tienen porqué ejecutarse inmediatamente antes y después de la acción durante la cual queremos que el objeto esté bloqueado. En el transcurso de una transacción se hacen múltiples lecturas y escrituras, pudiendo realizarse varias de ellas sobre el mismo objeto. Si se bloquease y desbloquease antes y después de cada acción, se incurrirá en un coste de gestión de los bloqueos muy alto, lo que repercutirá en el rendimiento de la BD (en sus tiempos de respuesta). Por tanto, una política de gestión de bloqueos debe llegar a un buen compromiso entre dos aspectos de signo contrario: Tiempo que el objeto (los datos) están bloqueados: Cuanto mayor sea, más largo es el tiempo durante el cual otras transacciones no pueden realizar sus acciones sobre el conjunto de datos, lo que supone que han de estar a la espera del desbloqueo, retardándose su finalización. Tiempo empleado en gestionar los bloqueos: Realizar muchas operaciones de bloqueo/desbloqueo repercute en el rendimiento de la BD, ya que hay que almacenar esos bloqueos, comprobar su compatibilidad, etc. Así pues, la solución consisten en encontrar la manera de ejecutar un número elevado de acciones entre dos bloqueos, consiguiendo a la vez que la distancia entre bloqueos sea razonablemente pequeña. Considerando el tipo de acción que se impide, podemos distinguir dos clases de bloqueo: Bloqueo de escritura (write-lock o wlock): Impide que otras transacciones modifiquen un dato. Bloqueo de lectura (read-lock o rlock): Impide que otras transacciones lean un dato. La distinción entre bloqueos de escritura y bloqueos de lectura se establece porque interesa que el gestor de transacciones pueda hacer un manejo diferenciado de cada tipo: Un bloqueo de lectura realizado por una transacción es compatible con un bloqueo de lectura de otra transacción. Dicho de otro modo, no existe ningún problema porque dos transacciones lean un objeto a la vez (y evitar esperas innecesarias supone incrementar el rendimiento de la BD). Un bloqueo de escritura es incompatible con cualquier otro tipo de bloqueo, ya que no se puede permitir que una transacción lea un dato cuando otra transacción lo está modificando (como ya vimos en los problemas de la concurrencia) y, desde luego, un objeto no puede ser sobrescrito a la vez. Así pues, se puede establecer la siguiente tabla de compatibilidades entre bloqueos de lectura y escritura: Los SGBD también permiten manejar diferentes granularidades de los bloqueos: Un bloqueo sobre distintas clases de objetos de la BD, como por ejemplo, una fila, un conjunto de filas o una tabla. De este modo las transacciones pueden efectuar reservas del tamaño estrictamente necesario, no bloqueando innecesariamente partes de los objetos que pueden ser utilizadas simultáneamente por otras transacciones (ejemplo, una transacción puede necesitar hacer un bloqueo de escritura sobre las primeras n filas de una tabla, lo cual no debe impedir que otras transacciones realicen acciones de lectura o escritura sobre las n+1 restantes). Cuanto menor sea el tamaño de los objetos bloqueados (granularidad más fina), menor será el número de esperas que los bloqueos originarán. Sin embargo, una granularidad fina genera más situaciones de interbloqueo que una granularidad gruesa, por lo que será necesario hacer una selección cuidados del nivel de granularidad empleado. EL PROBLEMA DEL INTERBLOQUEO El uso de técnicas basada en bloqueo para la gestión de transacciones concurrentes pueden conducir al problema denominado interbloqueo (o deadlock o abrazo mortal). El interbloqueo es una situación que se produce cuando dos transacciones se quedan esperando indefinidamente por un recurso que la otra transacción tiene bloqueada, de tal manera que ninguna de las dos accede nunca al recurso que la otra ha reservado. Por tanto, el gestor de transacciones deberá implantar mecanismos que le permitan detectar esas situaciones de interbloqueo y resolverlas, existiendo tres tipos de estrategias de resolución: La predicción : Consiste en la no generación de transacciones que puedan producir interbloqueo. Esto obligaría a analizar todas las transacciones a priori, lo cual produciría una gran sobrecarga en el sistema (todos los algoritmos de predicción son exponenciales, y por tanto no utilizables en un sistema con altas exigencias de rendimiento como es una BD). La prevención : Basada en que las transacciones puedan renunciar a un bloqueo que hayan realizado previamente. Para ello, se detectan los casos en los que una transacción T1 tenga reservado un ítem y una transacción T2 intenta hacer un tipo de bloqueo sobre ese ítem que es incompatible con el bloqueo de T1, aplicando técnicas de prevención, de entre las que se pueden destacar: Wait-Die: Si T2 es más antigua, se le hace esperar. En caso contrario, T2 se cancela. Kill-Wait: Si T2 es más antigua, se cancela. En caso contrario, se hace esperar a T2. La eliminación : Es el modo más sencillo de los tres. Consiste en eliminar una de las transacciones bloqueadas, continuando la otra transacción su camino. El gestor de transacciones volvería entonces a generar la transacción eliminada y la reintroduciría en la cola de transacciones, siendo lo más probable que la otra transacción haya terminado o haya avanzado hasta un punto en el que no se repita la situación de interbloqueo. EL PROTOCOLO DE BLOQUEO EN DOS FASES Es una implementación muy extendida de uno de los principios de la resolución de conflictos basada en bloqueos (técnica pesimista). El protocolo de bloque en dos fases define un conjunto de reglas sobre la aplicación de bloqueos que pueden ser utilizadas por el gestor de transacciones para gestionar las transacciones concurrentes. Estas reglas son: En una transacción sólo se va a escribir sobre un objeto una vez. Si una transacción necesita leer y escribir, realizará en primer lugar el bloqueo de escritura. De esta manera, basta con realizar una única operación de desbloqueo en la transacción (un desbloqueo genérico que termina con el bloqueo de lectura y escritura). Antes de realizarse las acciones de lectura o escritura, deben hacerse los bloqueos correspondientes, los cuales deben permanecer hasta que las acciones hayan finalizado. Los bloqueos del mismo tipo sólo se realizan una vez por transacción. En una transacción, una vez que se hace el primer desbloqueo, no se vuelve a hacer ningún bloqueo sobre ningún objeto. Esta norma caracteriza al bloqueo en dos fases, en la que se distingue una fase de crecimiento, donde van apareciendo nuevos bloqueos, y una fase de decrecimiento, que se inicia a partir del primer desbloqueo y dura hasta el final de la transacción, y en cuyo transcurso el número de bloqueos siempre decrece. Gráficamente, el comportamiento de este protocolo puede representarse así (evolución del número de bloqueos de una transacción con el algoritmo 2PL): Entre las ventajas del protocolo de bloqueo en dos fases o 2PL, podemos citar do: Es un protocolo seguro, es decir, nunca se van a producir las situaciones anómalas de concurrencia estudiadas en el punto anterior. Es un protocolo sencillo, fácil de implantar. Entre sus desventajas, podemos citar: Los bloqueos tienden a ser grandes, lo que va a provocar incompatibilidades habituales con otras transacciones, derivándose de ello una pérdida de rendimiento. Este protocolo permite la aparición de interbloqueo. Time-Stamping El time-stamping trata de conseguir la serializabilidad de los planes de ejecución haciendo que, cuando varias transacciones tengan que acceder a objetos comunes, lo hagan en distintos momentos. Este objetivo se consigue asignando una marca temporal distinta (time-stamp) a cada transacción. Cuando una transacción intenta acceder a un objeto para leerlo o modificarlo, se comprueba previamente si ya ha sido accedido por otra transacción más joven. Existen tres modalidades de time-stamping: TIME-STAMPING BÁSICO Se almacena en cada objeto el time-stamp de la última transacción que lo ha leído (TSR) y el de la última que lo ha grabado (TSW). Cuando una transacción T intenta leer un objeto, si su time-stamp es mayor o igual que el TSR del objeto, podrá leerlo, debiendo ser cancelada en caso contrario. Si una transacción T intenta escribir un objeto, se pueden producir las siguientes situaciones: TS (T) &gt;= TSW(Objeto): Si TS(T) &gt;= TSR(Objeto), podrá escribir. Si no, T tendrá que se canceladaSi TS(T) &lt;= TSW(Objeto): Si TS(T) &gt;= TSR(Objeto), T puede seguir adelante saltándose la grabación, ya que la versión que habría grabado no sería la última y no habrá sido leída por ninguna transacción (regla de escritura de Thomas). En caso contrario, T deberá ser cancelada. El time-stamping básico no asegura que se eviten las anomalías de concurrencia, pero si que impide los efectos de las anomalías de la Actualización Perdida y los de la Lectura Sucia (ya que si la transacción no puede hacer un commit, tampoco podrán hacerlo las que hayan leído o escrito los objetos que ella ha actualizado). TIME-STAMPING DINÁMICO Consiste en asignar un time-stamp a las transacciones cuando tratan de acceder a un objeto que ha sido leído o actualizado por otra transacción que no ha terminado todavía. De este modo, se evitan algunas de las cancelaciones de transacciones que se producen en el time-stamp básico. Su funcionamiento es el siguiente: Cuanto T1 intenta leer o actualizar un objeto actualizado por T2 o intenta actualizar un objeto que T2 ya ha leído: Si T1 y T2 no tienen todavía time-stamp, se les asigna a cada una un time-stamp, cumpliéndose que TS(T1) &gt; TS(T2). Si una de las dos no tiene time-stamp, se le asigna, respetándose de nuevo que TS(T1) &gt; TS(T2). Si T1 y T2 tiene ya time-stamp y TS(T1) &gt; TS(T2), entonces continúa normalmente la ejecución de las dos transacciones. En caso contrario, se cancelará la transacción T1. Al igual que el time-stamping básico, el time-stamping dinámico no asegura que se eviten las anomalías de concurrencia, pero si que impide los efectos de las anomalías de la Actualización Perdida y la Lectura Sucia. Además, es más eficiente que el time-stamping estático, ya que el número de cancelaciones que produce es menor. TIME-STAMPING MULTIVERSIÓN Basada en almacenar versiones de los ítems de la BD, permite el acceso simultáneo a un ítem por parte de transacciones diferentes, de forma que los accesos de cada transacción a los ítems que necesita se hace siempre usando versiones consistentes de esos ítems. Métodos optimistas Basados en la suposición de que los conflictos entre transacciones se producen con escasa frecuencia. Por tanto, se acepta como válido cualquier plan de ejecución de transacciones, y es tras finalizar la ejecución del plan y antes de realizar el commit (los datos todavía no son persistentes en la BD) cuando se comprueba si se ha producido algún conflicto entre transacciones o se ha incumplido alguna condición de consistencia. Si estos problemas han aparecido, las transacciones afectadas se deshacen (abort) y vuelven a situarse en la entrada del gestor de transacciones para su ejecución. En los métodos optimistas se puede considerar la ejecución de una transacción dividida en tres fases: La fase de lectura : En ella se produce la ejecución de las acciones de la transacción. Todos los objetos de la BD que se necesitan son leídos y escritos de un buffer de memoria local al programa, y por tanto las acciones no afectan al resto de las transacciones en ejecución. La fase de validación : Se comprueba si las modificaciones introducidas por la transacción pueden ser hechas persistentes en la BD sin incumplir la condición de serializabilidad de la transacción. La fase de escritura : Si la comprobación realizada durante la fase de validación es positiva, las acciones de la transacción se ejecutan sobre la BD. Si la comprobación es negativa, la transacción es abortada. TÉCNICA OPTIMISTA BÁSICA La técnica optimista básica determina el conjunto de transacciones que se han validado utilizando una técnica de time-stamping. Durante la fase de validación se analiza si hay algún ítem común y del mismo nivel de granularidad entre el conjunto de ítems leídos por una transacción T1 y el conjunto de ítems que han escrito las transacciones cuyo time-stamp se ha asignado entre el instante de inicio y el instante de finalización de la fase de lectura de la transacción. Si no existe esa coincidencia, se añade un time-stamp a la transacción T1. Esta técnica exige que haya como mucho una transacción en la fase de validación en cada instante de tiempo, por lo que la fase de validación actúa como cuello de botella del sistema de gestión de transacciones. Métodos de control de concurrencia basados en la semántica Son métodos que, además de la sintaxis, utilizan la semántica de las acciones a realizar para determinar si un plan de ejecución es o no serializable. Es decir, no se limitan a comprobar si los accesos de las transacciones son de lectura o escritura, sino que analizan que van a escribir concretamente las acciones de las transacciones y determinan si esas acciones pueden generar un conflicto o no. Bibliografía Scribd (Ibiza Ales) Tipos abstractos de datos y estructuras de datos. Grafos. Tipos de algoritmos: ordenación y búsqueda. Estrategias de diseño de algoritmos. Organizaciones de ficheros.Tablas, listas y árbolesTipos abstractos de datosAl escribir un programa para resolver un problema, el enfoque tradicional consistía en pasar de la realidad a una implantación en el lenguaje de programación utilizado, lo que conducía, inevitablemente, a que la forma de “ver” los datos estuviera muy influida por la máquina donde se ejecutaba el programa. Con el tiempo y la experiencia acumulada surgió otro enfoque mejor para afrontar esta situación, que consistía en establecer un nivel intermedio, donde se modela lo esencial de la realidad, mediante técnicas de abstracción, sin comprometerse con detalles de implantación. Estas técnicas de abstracción han evolucionado en el sentido de alejar los elementos que aparecen en los sistemas de software de las nociones propias de las máquinas sobre las que se implantan dichos sistemas, aproximándolos a las nociones propias de los dominios en que se presentan las situaciones modeladas. La principal de estas técnicas está basada en los Tipos Abstractos de Datos (TAD) . Esta técnica está centrada en las abstracciones de datos, consiste en: Identificar los distintos tipos de datos que intervienen en el sistema y la función principal de dicho sistema. Caracterizar cada tipo de datos en función de las operaciones que se puedan realizar con los objetos de los distintos tipos (haciendo abstracción de sus representaciones concreta). Componer el sistema utilizando objetos de los tipos definidos junto con sus operaciones características. Implantar cada uno de los tipos utilizados. Estas características son la base conceptual de los TAD. De forma sintetizada, puede decirse que: Un TAD es una estructura algebraica, a saber, un conjunto de objetos estructurados de alguna forma y con ciertas operaciones definidas sobre ellos. Piénsese, por ejemplo, en una calculadora; los elementos que maneja son cantidades numéricas y las operaciones definidas son operaciones aritméticas. Otro ejemplo posible es el TAD matriz matemática; los elementos que maneja son las matrices y las operaciones son las que nos permiten crearlas, sumarlas, invertirlas, etc. Otro tipo de TAD es cualquier tipo de cola de espera: en el autobús, cine, la compra, etc. Los elementos de las colas son las personas y las operaciones más usuales son entrar o salir de la cola, pero no son las únicas. Desde un punto de vista abstracto podemos pensar en operaciones como crear un elemento de la cola, comprobar si la cola está vacía, si está llena, etc. Es conveniente observar que las operaciones de un TAD son de diferentes clases. Algunos nos deben permitir crear objetos nuevos, otras determinar su estado, construir otros objetos a partir de algunos ya existentes, etc. Puesto que los TAD deben ser lo más independientes posibles de los detalles de implantación, es obvio que no deben tener ninguna dependencia con respecto al lenguaje de programación elegido para implantarlos. Aun así, ciertos lenguajes no incorporan ciertas estructuras de datos necesarias para implantar algunos tipos de TAD. El concepto de algoritmo Un algoritmo es un conjunto de pasos o instrucciones que se deben seguir para realizar una determinada tarea . Estas instrucciones deben cumplir las siguientes características: FINITUD : Debe ser un conjunto finito de instrucciones y que se realicen en un tiempo finito . PRECISIÓN : Debe indicar el orden de realización de cada instrucción o paso de forma inequívoca . DEFINICIÓN : Debe tener un número finito (0 … N) de datos de entrada y un número finito (0 … M) de datos de salida (resultados) . Frente a un mismo conjunto de datos de partida se debe llegar siempre a un mismo conjunto de resultados. Otra cualidad deseable en un buen algoritmo, aunque no imprescindible para ser considerado como tal, es que sea óptimo , es decir, que sea la forma más fácil y rápida de hacer una determinada tarea, si bien es cierto que en muchas ocasiones facilidad y rapidez son dos cualidades contrapuestas. De forma gráfica, la siguiente figura engloba las principales características de un algoritmo. Coste de un algoritmo Normalmente, si se escribe un programa para resolver un problema, se hace para que éste sea utilizado muchas veces, por lo que resulta conveniente caracterizarlos según su tiempo de ejecución y la calidad de la respuesta. Cuando estudiamos algoritmos es muy importante caracterizar la solución obtenida de cada algoritmo antes de estar seguros de que dos algoritmos son equivalentes (para un mismo valor de entrada dan exactamente el mismo resultado) o son similares (pueden dar resultados diferentes, pero desde el punto de vista del problema que estamos resolviendo somos indiferentes a cualquiera de los resultados). Por esta razón, uno de los criterios más importantes para seleccionar un algoritmo es evaluar el tiempo que tarda en ejecutarse, que llamaremos coste del algoritmo o tiempo de ejecución . Para analizar el coste de un algoritmo adoptamos un modelo que nos dice que recursos usados por la implantación del algoritmo son importantes para su desempeño. La complejidad de un algoritmo bajo un modelo computacional es la cantidad de recursos, en tiempo o espacio, que el algoritmo usa para resolver el problema. Desafortunadamente, por lo general es imposible predecir el comportamiento exacto de un algoritmo, ya que existe la influencia de muchos factores, de aquí que se trate de extraer las características principales de un algoritmo. Así, se definen ciertos parámetros y ciertas medidas que son las más importantes en el análisis y se ignoran detalles relativos a la implantación. Así, el análisis es una aproximación, no es perfecto, pero lo importante es que se pueden comparar diferentes algoritmos para determinar cuál es mejor para un propósito determinado. La metodología que generalmente se utiliza para predecir el tiempo de ejecución de algoritmos se basa en el comportamiento asintótico , en este método se ignoran los factores constantes y se concentra en el comportamiento del algoritmo cuando el tamaño de la entrada tiende a infinito. Es totalmente análogo al estudio de los límites de una función cuando su variable independiente tienen a infinito, por ejemplo: De hecho, es muy usual evaluar el coste de un algoritmo como una función matemática del número de entradas del algoritmo (N), siendo un algoritmo de mayor coste que otro cuando para un mismo N la función que expresa el coste da un resultado superior. Así pues, los costes o tiempos de ejecución más usuales de los algoritmos, de menor a mayor, son: Por otra parte, conviene considerar dos casos límite, el mejor caso y el peor caso . Puede ocurrir que al realizar una operación se parta de unos datos iniciales del TAD que hagan que al aplicar el algoritmo éste se ejecute de la manera más rápida posible, y puede ocurrir también lo contrario, encontrarnos con el caso en que se ejecute de la forma más lenta. Por ejemplo, si queremos ordenar unos datos, parece lógico pensar que el mejor caso será cuando los datos ya estén ordenados, y el peor caso cuando ningún dato inicial esté en el orden que tendrá cuando se haya ejecutado la ordenación. De forma estadística se supone que se parte de un caso medio. Por otra parte, hay algoritmos cuya diferencia de coste entre los casos mejor y peor es muy grande, y otros en los que, debido a la naturaleza del algoritmo, la diferencia es muy pequeña o incluso inexistente. Si un algoritmo crece de manera proporcional a N, se dice que es de orden N. En general, el tiempo de ejecución es proporcional, esto es, multiplica por una constante a alguno de los costos anteriormente propuestos, además de la suma de algunos términos más pequeños. Implantación tradicional frente a los TAD Según Nicklaus Wirth, un programa responde a la ecuación: (Ec. 1) Programa = Datos + Algoritmos El enfoque tradicional se ciñe bastante bien a esta concepción. En la ecuación de Wirth la parte Algoritmo la podemos expresar como: (Ec. 2) Algoritmo = Algoritmo de datos + Algoritmo de control Se entiende como Algoritmo de datos a la parte del algoritmo encargada de manipular las estructuras de datos del problema, y Algoritmo de control a la parte restante (la que representa en sí el método de solución del problema, también llamada lógica de negocio , independiente hasta cierto punto de las estructuras de datos seleccionadas). Con los TAD se identifican ciertas operaciones o partes del algoritmo que manipulan los datos. Además de proporcionar una estructura de datos, por lo que podemos sustituir el sumando “ Datos ” de la ecuación anterior por el sumando “ Estructura de Datos “. De esta forma, podemos entonces escribir: (Ec. 3) Programa = Estructura de Datos + Algoritmo de Datos + Algoritmos de Control Definiendo: (Ec. 4) Implantación del TAD = Estructura de Datos + Algoritmos de Datos Se establece la ecuación fundamental que describe el enfoque de desarrollo con Tipos Abstractos de Datos: (Ec. 5) Programa = Implantación del TAD + Algoritmo de Control A la hora de crear un Tipo Abstracto de Datos, la ecuación a seguir es (Ec. 4), es decir, determinar como es la estructura de datos y cuales son los algoritmos de control para manera los datos, en otras palabras, las operaciones sobre los datos que se pueden hacer, o interesa hacer, sobre la mencionada estructura de datos. Operaciones de los TAD Las operaciones en los TAD pueden servir para crear nuevos objetos abstractos, para obtener información acerca de ellos, y algunas para construir nuevos elementos a partir de otros ya existentes. De esta forma las operaciones las podemos clasificar de esta manera: Operaciones de CREACIÓN: Se utilizan para definir y/o crear el TAD y sus elementos. Operaciones de TRANSFORMACIÓN: Se utilizan para realizar algún tipo de transformación en los componentes del TAD. Por ejempo: asignación de valor, ordenaciones, permutaciones, balanceados, etc. Operaciones de ANÁLISIS: Se utilizan para obtener información concerniente a cualquiera de los componentes del TAD o de su estructura. Por ejemplo: búsquedas, recorridos, obtención de algún dato del TAD (valor de un componente, cantidad de elementos, profundidad, número de niveles, etc.) Implantación y tipos de TAD Cuando ya se tiene bien diseñado un Tipo Abstracto de Dato, el siguiente paso es decidir una implantación. Esto supone elegir algunas de las estructuras de datos que nos proporcione el lenguaje de programación utilizado para representar cada uno de los objetos abstractos y, por otro lado, escribir una rutina (Procedimiento o función) en tal lenguaje que simule el funcionamiento de cada una de las operaciones especificadas para el TAD. La selección de las estructuras de datos determina la complejidad del algoritmo que implanta una operación y su elección es, por esta razón, de gran importancia. Existen estructuras de datos muy dependientes de un lenguaje de programación y debido a esto deben tratar de evitarse cuando el TAD se quiere que sea portable. Existen muchos tipos de TAD, pero los más utilizados son: Tablas Listas Árboles TablasUna tabla o matriz , es un TAD que representa una estructura homogénea de datos donde se cumple: Todos sus c omponentes son del mismo tipo . Tiene un número predefinido de componentes que no puede variarse en tiempo de ejecución. Los elementos de la tabla contienen una clave que los identifica de forma unívoca. El conjunto de claves forman un conjunto de índices para localizar los elementos. Se permite el acceso directo a cualquiera de los elementos de la tabla a través de los índices. La operación fundamental en el uso de una tabla es localizar la posición de sus elementos con una clave conocida “a priori”. Dicho de otra forma, lo más usual es que se quiera conocer el contenido del i-esimo (índice i) elemento de una tabla. Es menos frecuente la operación inversa, dado un valor saber los índices de los elementos cuyo contenido coincide con el valor dado; aunque también resulta útil en ocasiones esta operación. Tipos Recordando la (Ec. 4) de Wirth, desde el punto de vista de la Estructura de Datos de una tabla, la principal característica de su estructura es la dimensión . Se habla entonces de: Tabla Monodimensional : Se refiere a tablas de una dimensión con un determinado número (N) de elementos. La declaración de estas tablas responde a la sintaxis genérica: Nombre_Tabla = matriz[1..N] de Tipo_Elemento; Tabla Multidimensional : Se refiere a tablas de más de una dimensión (d), con un determinado número de elementos para cada dimensión (N1, N2, …, Nd). Su declaración es: Nombre_Tabla = matriz[1..N1, … 2..Nd] de Tipo_Elemento; Aunque cada dimensión puede tener diferente número de elementos, todas las dimensiones tienen el mismo tipo de elemento, no pudiéndose declarar diferentes tipos de elementos según las diferentes dimensiones de la matriz, pues supondría violar la primera característica de las tablas. Operaciones Desde el punto de las operaciones (Algoritmo de Datos), las operaciones básicas en una tabla son: Definir/Crear la tabla: Se refiere a la forma que cada lenguaje debe tener para definir la estructura de una tabla y crear una variable del tipo de la tabla definida. Insertar/Eliminar elementos de la tabla: Aunque cuando se define una tabla se indica “a priori” el número de elementos que tiene, eso no quiere decir que desde el principio esos elementos tengan un valor significativo para el uso que se les piensa dar. Es muy común al crear una tabla asignar a todos sus elementos un valor especial (nulo) que indique que en caso de que el elemento tenga ese valor, a todos los efectos, desde un punto de vista abstracto, es como si ese elemento no existiese. Buscar elementos de la tabla: Esta operación es fundamental en el uso de tablas. Existen distintas formas de hacer búsquedas en una tabla, y según los valores de los componentes y el tipo de búsqueda ésta será más o menos eficiente (rápida). Ordenar los elementos de la tabla: Esta operación resulta muy útil a la hora de realizar búsqueda. Veremos más adelante que resulta mucho más efectivo realizar búsquedas sobre tablas donde se ha establecido algún tipo de orden sobre otras donde no lo hay. Contar los elementos de la tabla: Calcular el número de elementos que hay en la tabla en un momento dado. La acción de preguntar si una tabla está llena o vacía (de elementos significativos) son casos particulares de la operación de contar. También pueden considerarse recuentos más especializados, como contar el número de elementos con un determinado valor, o con valor superior a uno dado, etc. Hay varios algoritmos que implantan las operaciones de búsqueda y ordenación en una tabla (las más complejas y utilizadas). Las operaciones de definición, creación y recuento resultan muy sencillas de implantar. Representación La forma más simple de representación abstracta de una tabla es: Para índices que van desde 0 a (N-1) y siendo ei el contenido del elemento i de la tabla. Es muy común la siguiente representación, más intuitiva, de una tabla: En la figura anterior se representa una tabla de una dimensión con N elementos, cuyos índices van desde 0 hasta (N-1), y cuyo contenido se refiere a nombres de persona. Para representar tablas multidimensionales, basta con utilizar una representación como la indicada para cada dimensión de la tabla. Implantación Puesto que una de las características de las tablas es la invariabilidad del número de componentes, prácticamente todos los lenguajes de programación implantan las tablas usando memoria estática (en oposición a la memoria dinámica). Una vez que el programa ha sido preparado para ser ejecutado (compilado y enlazado), la posición y el espacio de memoria que utilizará la tabla en la ejecución del programa ya está fijado y no puede de ninguna forma cambiarse o utilizarse para otros fines que no sea almacenar los elementos de la tabla. Hay otros tipos de TAD cuya implantación se realiza frecuentemente con memoria dinámica, cuyo significado fundamental es que, en oposición a la memoria estática, el tamaño y la posición del espacio de memoria utilizado si puede cambiar en tiempo de ejecución, pudiéndose utilizar para otros fines. ListasUna lista es una estructura de datos que cumple: Todos sus componente son del mismo tipo . Cada elemento o nodo va seguido de otro del mismo tipo o de ninguno. Sus componentes se almacenan según cierto orden . Nótese las diferencias con respecto a las tablas: Los elementos de las tablas no se almacenan según un orden. En las listas siempre hay un orden. No existe unos conjuntos de índices asociados a cada posición, tal como ocurre con las tablas. Por tanto, en las listas será precio ir recorriendo los elementos hasta encontrar el buscado. Tipos Una manera de clasificar las listas es por la forma de acceder al siguiente elemento: Lista densa : La propia estructura determina cuál es el siguiente elemento de la lista. Por ejemplo, aplicando ciertos “trucos” se puede usar una tabla para implementar una lista, de forma que el siguiente elemento de la lista fuera dado por el orden del índice (no confundir con el orden de los elementos del array). Nótese sin embargo que, puesto que un array es estático, la memoria reservada para implementar esta “lista” también será estática. Lista enlazada : Cada elemento contiene la información necesaria para llegar al siguiente. La posición del siguiente elemento de la estructura la determina el elemento actual. Es necesario almacenar al menos la posición de memoria del primer elemento. Además es dinámica, es decir, su tamaño cambia durante la ejecución del programa. Dentro de las listas enlazadas podemos distinguir, según la cantidad de enlaces por nodo, entre: Lista simplemente enlazada : Cada elemento conoce qué elemento es el que le sucede en el orden, pero no cual le precede. Lista doblemente enlazada : Cada elemento conoce qué elemento le precede y cual le sucede en el orden. Lista con enlaces múltiples : Son aquellas listas en donde cada elemento, aparte de conocer los elementos que le preceden o suceden, también tiene uno o varios enlaces al primer elemento de otra lista, siendo éstas sublistas de la anterior. También podemos clasificar las listas en función de que los elementos se coloquen en la lista por orden de llegada y se acceda a ellos por su posición. Los ejemplos más usuales de este tipo de listas son: PILAS (Estructuras LIFO. Last In First Out): El último elemento en entrar es el primero en salir. COLAS (Estructuras FIFO. First In First Out): El primer elemento en entrar es el primero en salir. Operaciones Las operaciones básicas a realizar en una lista son las mismas que en las tablas, si bien, la forma en que se realizan en las listas difiere notablemente entre ambos TAD, sobre todo en lo concerniente a las operaciones de Inserción, Eliminación, Búsqueda y Ordenación. A la hora de insertar o eliminar elementos de una lista, puede distinguirse entre insertar o eliminar el primer elemento de la lista, un elemento intermedio o el último elemento. En todos los casos resulta de capital importancia reordenar los diferentes enlaces entre los elementos de forma correcta, pues realizar mal un enlace o perderlo supone la imposibilidad definitiva de acceder a parte de la lista, o incluso a toda ella. Representación La representación más habitual de una lista se hace colocando sus elementos entre paréntesis, separados por comas, según muestra el siguiente ejemplo: Siendo e1, e2, …, en los elementos de la lista. Puesto que los componentes de una lista se almacenan según cierto orden, es muy importante notar que: L1 = (e1, e2, …, en) es una lista distinta de L2 = (en, e1, …, e2) De forma gráfica presentamos las siguientes listas: Simplemente enlazada: Doblemente enlazada: En estas representaciones las flechas indican los enlaces entre los componentes de las listas. El punto negro de la flecha indica el elemento origen y la punta de flecha el elemento destino, de tal forma que el origen conoce el destino, pero el destino no conoce su elemento origen. Por esta razón es por lo que las listas doblemente enlazadas precisan “dos” enlaces, uno en cada sentido. En la lista L1 el elemento de contenido “Juan” sabe que le sigue el elemento “Pedro”, y éste sabe que le sigue “Inés”, pero “Pedro” no sabe que le precede “Juan”. El símbolo X o aspa que aparece en algunos elementos indican que no tienen enlace a ningún sitio, se entiende que es un enlace nulo. Obsérvese que todas las listas tienen que tener un “punto de entrada”, es decir, una referencia al primer elemento de la lista. Estas referencias, en nuestro ejemplo, son L1, L2 y L3 . También conviene fijarse de forma especial en las listas con enlaces múltiples. Mientras que en las listas simples y dobles únicamente hay una forma de “recorrer” la lista, desde el primer elemento hasta el último”, en las listas con múltiples enlaces surgen varias formas de recorrerlas, según nos decidamos a avanzar por uno de los múltiples enlaces a otras sublistas que puede tener cada elemento. Implantación Las listas son un tipo de TAD cuya implantación puede realizarse de múltiples maneras. Según el tamaño, manejo o rendimiento que queramos que tengan ciertas operaciones, como búsquedas, ordenaciones, inserciones, etc, las listas pueden implementarse con arrays, ficheros secuenciales o punteros. Estructuras de datos todas ellas muy comunes en la mayoría de los lenguajes de programación. También conviene observar que para implantar listas puede usarse tanto memoria estática (arrays) como memoria dinámica (punteros). Si bien es poco frecuente usar los arrays, pues en esencia son tablas. Para implementar una lista simple usando tablas hace falta recurrir a algún tipo de “truco”, por ejemplo, almacenar en el contenido de los elementos del array dos datos, el contenido de los elementos de la lista y el “puntero” al siguiente elemento. Veamos un ejemplo de lista implementada con array: Nótese como junto al nombre de las personas, usamos el carácter especial (#) para separar los nombres de los valores numéricos que hacen de puntero, indicando qué índice de la tabla contiene el siguiente elemento de la lista. Para indicar el valor nulo usamos un valor de índice imposible, en este caso el negativo (-1). Hay que sobreentender que el puntero de comienzo de la lista es siempre el índice cero del array. Si se eliminasen algunos elementos de la lsita, por ejemplo a Inés y a Pedro, supondría reorganizar los apuntadores, de forma que tendríamos: Juan#3, Inés#(-1); Pedro#(-1); Ana#(-1). Desde un punto de vista abstracto la lista sólo tiene ahora dos elementos Juan y Ana, sin embargo esto no quiere decir que el array haya disminuido su tamaño al dejar de apuntar a los elementos Inés y Pedro; simplemente estamos diciendo que hemos dejado de apuntarlos, pero el array sigue teniendo el mismo espacio reservado en memoria, y este espacio no está disponible para otro uso más que para almacenar los elementos del array. Esto es debido a que un array se implementa con memoria estática. Usando memoria dinámica (punteros) no se tendría esta desventaja. Bajo esta circunstancia, si se elimina de la lista un elemento, el espacio de memoria que ocupaba queda disponible para ser usado para otras finalidades. ÁrbolesComo dice David Harel en su libro “The Spirit of Computing”: “Una de las estructuras de datos más importantes y prominentes que existen es el árbol . No es un árbol en el sentido botánico de la palabra, sino uno de naturaleza más abstracta. Todos hemos visto usar tales árboles para describir conexiones familiares. Los dos tipos más comunes de árboles familiares son el – árbol de antecesores – , que empieza en un individuo y va hacia atrás a través de padres, abuelos, etc, y el – árbol de descendientes – , que va hacia adelante a través de hijos, nietos, etc”. Un árbol es un TAD cuya estructura corresponde con el siguiente gráfico: En donde podemos ya notar las principales características de los árboles. Cada uno de las diferentes “circunferencias” representa un elemento o nodo del TAD tipo árbol. El nodo 1 suele ser llamada nodo de nivel 0 (también nodo raíz por su similitud con un árbol real). Es importante percatarse de que sólo puede haber un nodo de nivel 0 en cada árbol. Los nodos 2, 3 y 4 son llamados nodos de nivel 1, por estar todos ellos al mismo “nivel” al ser todos “hijos” del nodo 1. Este razonamiento se puede ir aplicando a los sucesivos niveles, hablándose de nodos nietos (o hijos si nos referimos al nivel inmediatamente superior). Por la similitud con un árbol en el sentido botánico, también se dice que los nodos que no tienen hijos son nodos hoja y los nodos que sí tienen hijos, salvo la raíz, son llamados nodos rama o nodos internos. Con el ejemplo del árbol anterior ( A1 ), vamos a mostrar los principales conceptos usados al hablar de árboles: Antecesor Directo o Padre : El nodo 4 es el Antecesor Directo o Padre de los nodos 7, 8 y 9. Antecesor o Ancestro : El nodo 1 es el Antecesor de todos los otros nodos. Sucesor Directo o Hijo : El nodo 5 es el Sucesor Directo o Hijo del nodo 2. Sucesor o Descendiente : Todos los nodos son Sucesores o Descendientes del nodo 1. Nodo de Nivel Cero o Raíz : En nuestro ejemplo es el nodo 1. Nodo Interno o Rama: En nuestro ejemplo son los nodos 2, 3 y 4. Nodo Hoja : En nuestro ejemplo son los nodos 5, 6, 7, 8 y 9. Nivel de un Nodo : El nodo 1 tiene nivel 0. Los nodos 2, 3 y 4 nivel 1. Los nodos 5, 6, 7, 8 y 9 nivel 2. Grado de un Nodo : Es el número de descendientes directos que tiene un nodo. El grado del nodo 2 es 1. Grado del Árbol : Es el mayor de los grados de los nodos que los componen. En nuestro caso es 3. Altura del Árbol : Es el mayor de los niveles del árbol. En nuestro caso es 2. Longitud de Camino de un Nodo : Número de enlaces o arcos que hay que atravesar para ir de la raíz a un nodo. La longitud de camino del nodo raíz es la unidad. Por ejemplo, el nodo 5 tiene longitud de camino 3. Tipos En función de la estructura que define un árbol, se pueden establecer distintos tipos de árboles atendiendo a ciertos aspectos en la forma de árbol. Según ésto es muy corriente hablar de los siguientes tipos de árboles: Árbol BINARIO o Árbol B : Es el árbol cuyo máximo número de nodos hijos que tiene cualquier nodo es dos. Árbol MULTIRAMA : Son aquellos en los que no hoy límite para el número de nodos hijo. El número de niveles no crece tanto como en los binarios. Es el caso de nuestro árbol de ejemplo A1 . Árbol BALANCEADO : Son aquellos árboles en los cuales se cumple que entre todos sus nodos hoja no hay una diferencia de nivel superior a la unidad . Veamos algunos ejemplos de árboles binarios balanceados (también denominados árboles AVL) y no-balanceados. Balanceados: No Balanceados: Es de observar que una lista realmente es un árbol en el que cada nodo únicamente tiene un hijo, y se considera al primer nodo de la lista el nodo raíz del árbol (tal como se muestra en el árbol (3) de la figura. Una lista de más de dos elementos se corresponde a la estructura de un árbol con un grado de no-balanceo máximo (sólo hay un nodo hoja). Se suele decir en estos casos que la lista es un árbol totalmente degenerado . Operaciones Las operaciones básicas que se pueden realizar sobre los árboles son: Definir/Crear el árbol: Se refiere a la forma que cada lenguaje debe tener para definir la estructura de tipo árbol y crear una variable de este tipo. Recorrer los diferentes caminos del árbol: Esta es una operación que resulta muy importante en los árboles, pues es sobre la que se apoyan las demás operaciones. Vamos a ver que existen dos formas de recorrer un árbol, en ambas, por convenio, se asume que siempre nos desplazaremos de izquierda a derecha en horizontal, al igual que la lectura de un libro. Respecto a la dimensión vertical, nos moveremos de arriba hacia abajo o viceversa. Existen dos forma de recorrer un árbol: Recorrido en AMPLITUD : Se trata de recorrer consecutivamente los nodos que se encuentran en el mismo nivel (siempre de izquierda a derecha). La dimensión que prima en el recorrido es la horizontal. Tomando como ejemplo el árbol A1 de nuestro ejemplo, el recorrido en amplitud nos daría la siguiente secuencia de nodos: (1, 2,3,4,5,6,7,8,9). Recorrido en PROFUNDIDAD : La dimensión que prima al recorrer el árbol es la vertical . Siempre se tenderá a alejarse del nodo raíz mientras se pueda. Sin embargo, a diferencia del recorrido en amplitud, existen diferentes formas de recorrido en profundidad, estas formas son: Profundidad en PREORDEN : Se empieza por el nodo raíz y se tiende a alejarse lo máximo posible de él. Moviéndose en vertical de arriba hacia abajo. En nuestro ejemplo obtendríamos la secuencia: (1,2,5,3,6,4,7,8,9) . Profundidad en POSTORDEN : Se comienza por el nodo hoja más a la izquierda y se mantiene la máxima distancia que se pueda con el nodo raíz. En nuestro ejemplo se obtiene la secuencia: (5,2,6,3,7,8,9,4,1) . Profundidad en INORDEN : Como el recorrido postorden pero cuidando que no aparezcan, siempre que sea posible, dos nodos del mismo padre (nodos hermanos) de forma consecutiva en la secuencia de recorrido. En nuestro ejemplo se tendría: (5,2,1,6,3,7,4,8,9) . Para los árboles binarios, en este tipo de recorrido, jamás pueden aparecer consecutivos dos nodos hermanos. Insertar/Eliminar elementos en el árbol: Al igual que en las listas, hay que distinguir entre insertar el nodo raíz, un nodo interno o un nodo hoja. También resulta muy importante nunca cambiar de forma incorrecta o perder un enlace, pues supondría la imposibilidad de acceder a parte o a todo el árbol. Ordenar los elementos del árbol: Ordenar un árbol no implica cambiar su estructura, sino modificar el contenido de los nodos para que sigan algún tipo de orden deseado. Las posibles ordenaciones de un árbol son aquellas que hacen que al hacer el recorrido de un árbol se obtenga una secuencia ordenada según algún criterio. Por ejemplo, nuestro árbol de ejemplo está ordenado en amplitud, pues al hacer un recorrido de este tipo se obtiene la secuencia ordenada (1,2,3,4,5,6,7,8,9) . El mismo árbol ordenado en profundidad preorden sería: (1,2,3,4,5,6,7,8,9) . Buscar elementos en el árbol: Consiste en localizar el/los nodo/s que contiene un dato igual al dato dado como referencia. Es muy importante darse cuenta de que según esté el árbol ordenado o no, si está balanceado o no, y dependiendo del recorrido que se haga, las búsquedas pueden ser más o menos efectivas (rápidas). Contar los elementos del árbol: Básicamente consiste en recorrer el árbol en cualquiera de sus formas e ir contando los elementos. Existen sin embargo variantes de esta contabilidad. Por ejemplo, podría solicitarse el número de nodos que tiene el nivel N de un árbol, lo cual exigiría un recorrido en amplitud teniendo en cuenta siempre el nivel donde se está en cada momento. Otra posible cuenta sería conocer el número de nodos que hay desde la raíz hasta el nodo hoja de menor nivel, etc. Balancear el árbol: Consiste en hacer que un árbol no-balanceado pase a ser balanceado. Esta operación sí que supone un cambio en la estructura del árbol, suponiendo siempre el cambio de nodo raíz y la variación de niveles de ciertos nodos. Veamos un ejemplo de balanceo de un árbol binario: Representación Ya hemos visto una primera forma de representar un árbol muy intuitiva y clarificadora, usando circunferencias para representar los nodos o elementos del árbol y líneas para representar los enlaces. No obstante, sin tocar en absoluto la estructura del árbol, conviene considerar la forma de enlazar los nodos del árbol, pues al igual que con las listas, los nodos del árbol pueden estar simplemente enlazados o doblemente enlazados. Esta diferencia afecta en la facilidad o dificultad de implementar ciertas operaciones sobre los árboles, sobre todo las diferentes operaciones de recorrido del árbol. Cuando se quiere especificar en un gráfico el tipo de enlaces del árbol, se utiliza una forma parecida a las listas, veamos un ejemplo de un mismo árbol simplemente enlazado y doblemente enlazado. Implantación Al igual que las listas, con los árboles, según diversas consideraciones sobre el tamaño, manejo o rendimiento que queramos que tengan ciertas operaciones, tales como recorridos, ordenaciones, inserciones, etc. los árboles pueden implantarse con arrays, ficheros secuenciales o punteros. Estructuras de datos todas ellas muy comunes en la mayoría de los lenguajes de programación. Lo más habitual es que se utilice memoria dinámica, es decir, punteros, para la implantación de todo tipo de árboles, aunque en ciertas ocasiones pueden considerarse la implantación con memoria estática mediante arrays. Algoritmos: Ordenación, Búsqueda, Recursión, GrafosLa búsqueda de un elemento es, con diferencia, la operación más utilizada en los TAD, y la más importante. Por otra parte, la ordenación de los elementos del TAD puede resultar de gran ayuda a la hora de realizar búsquedas, así pues, la ordenación de los elementos del TAD es la segunda operación más útil. Se pueden distinguir dos tipos dentro de estos algoritmos: Algoritmos Recursivos : Los algoritmos recursivos son aquellos que se basan en el uso de rutinas de programación que se llaman a sí mismas. Algoritmos Iterativos : Se consideran algoritmos iterativos a todos aquellos que no son recursivos. OrdenaciónLa finalidad de los algoritmos de ordenación es organizar ciertos datos (estos datos pueden estar contenidos en diferentes estructuras, ya sean TAD o en ficheros) en un orden creciente o decreciente mediante una regla prefijada (numérica, alfabética, …). Atendiendo al tipo de elemento que se quiera ordenar puede ser: Ordenación interna : Los datos se encuentran en memoria (ya sean tablas, listas, árboles, etc). Ordenación externa : Los datos están en un dispositivo de almacenamiento externo (ficheros), y su ordenación es más lenta que la interna. En este apartado vamos a estudiar los métodos de ordenación interna para los TAD vistos en apartados anteriores, esto es, para ordenar tablas, listas y árboles. Inicialmente nos centraremos en los TAD tabla y lista (simple o doble) y más adelante comentaremos el caso de listas múltiples y árboles. Además, aquí consideraremos métodos de ordenación iterativa, es decir, no recursivos. Cuando estudiemos la recursividad veremos ejemplos de ordenaciones de tipo recursivo. Los principales algoritmos de ordenación interna de tipo iterativo son: Selección Burbuja Inserción Directa Inserción Binaria Shell Intercalación Selección Este método consiste en buscar el elemento más pequeño del TAD y ponerlo en la primera posición; luego, entre los restantes, se busca el elemento más pequeño y se coloca en segundo lugar, y así sucesivamente hasta colocar el último elemento. La idea de esta ordenación es independiente del tipo de TAD a la que se aplique, lo único que puede variar son ciertos aspectos o detalles en la forma explícita de hacer la ordenación según se implemente el TAD con memoria estática (arrays) o memoria dinámica (punteros). Por ejemplo, si tenemos el array{40,21,4,9,10,35}, o la lista (40,21,4,9,10,35), los pasos a seguir son: Burbuja Consiste en comparar pares de elementos adyacentes e intercambiarlos entre sí hasta que estén todos ordenados. Utilizando como ejemplo la tabla anterior {40,21,4,9,10,35}: Primera pasada: Se comienza por el primer elemento. Segunda pasada: Se comienza por el segundo elemento. Inserción directa En este método lo que se hace es tener una sublista ordenada de elementos de la lista, o array, e ir insertando el resto en el lugar adecuado para que la sublista no pierda el orden. La sublista ordenada se va haciendo cada vez mayor, de modo que al final la lista entera queda ordenada. Utilizando como ejemplo la tabla anterior {40,21,4,9,10,35}: Inserción binaria Es el mismo método que la inserción directa, excepto que la búsqueda del orden de un elemento en la sublista ordenada se realiza mediante una búsqueda binaria, lo que en principio supone un ahorro de tiempo. No obstante, dado que para la inserción sigue siendo necesario un desplazamiento de los elementos, el ahorro, en la mayoría de los casos, no se produce, si bien hay compiladores que realizan optimizaciones que lo hacen ligeramente más rápido. Shell Es una mejora del método de inserción directa, utilizado cuando la tabla tiene un gran número de elementos. En este método no se compara a cada elemento con el de su izquierda, como en el de inserción, sino con el que está a un cierto número de lugares (llamado salto) a su izquierda. Este salto es constante, y su valor inicial es N/2 (siendo N el número de elementos, y siendo división entera). Se van dando pasadas hasta que en una pasada no se intercambie ningún elemento de sitio. Entonces el salto se reduce a la mitad, y se vuelven a dar pasadas hasta que no se intercambie ningún elemento, y así sucesivamente hasta que el salto vale 1. Utilizando como ejemplo {40,21,4,9,10,35}: Con sólo 6 intercambios se ha ordenado el array, cuando por inserción se necesitaban muchos más. Intercalación No es propiamente un método de ordenación, consiste en la unión de dos tablas o listas ordenadas de modo que la unión esté también ordenada. Para ello, basta con recorrer los TAD de izquierda a derecha e ir cogiendo el menor de los dos elementos, de forma que sólo aumenta el contador del array del que sale el elemento siguiente para el array-suma. Si quisiéramos sumar las tablas {1,2,4} y {3,5,6}, los pasos serían: BúsquedaComo se ha mencionado anteriormente la búsqueda es la operación más importante en el procesamiento de la información, y permite la recuperación de datos previamente almacenados. El tipo de búsqueda se puede clasificar como interna o externa, según el lugar en el que esté almacenada la información (en memoria o en dispositivos externos). Todos los algoritmos de búsqueda tienen dos finalidades: Determinar si el elemento buscado se encuentra en el conjunto en el que se busca. Si el elemento está en el conjunto, hallar la posición en la que se encuentra. En este apartado nos centraremos en la búsqueda interna iterativa. Como principales algoritmos en tablas y listas tenemos las búsquedas: Secuencial Binario o Dicotómica Utilizando tablas Hash Búsqueda Secuencial Esta búsqueda consiste en recorrer y examinar cada uno de los elementos hasta encontrar el o los elementos buscados, o hasta que se han mirado todos los elementos. Por ejemplo, para la lista (10,12,4,10,9) donde queremos encontrar el elemento cuyo contenido es 10, la idea del algoritmo sería: Utilizando como ejemplo la tabla anterior {40,21,4,9,10,35}: Puesto que queremos encontrar todas las ocurrencias del valor 10, es necesario recorrer siempre toda la lista o tabla, así pues habrá que recorrer los N elementos, lo cual hace que el coste del algoritmo sea N, y su orden O(N). Si sólo queremos encontrar la primera ocurrencia que se produzca, si tenemos la certeza de que no puede haber elementos repetidos, se puede parar la búsqueda en cuanto se produzca la primera ocurrencia, con lo cual el algoritmo es más eficiente. En este caso, el número medio de comparaciones que hay que hacer antes de encontrar el elemento buscado es de (N+1)/2. Aún así, el orden sigue siendo O(N). Búsqueda binaria o dicotómica Para utilizar este algoritmo, se precisa que la tabla o lista considerada esté ordenada. La búsqueda binaria consiste en dividir la tabla por su elemento medio en dos subtablas más pequeñas, y comparar el elemento con el del centro. Si coinciden, la búsqueda se termina. Si el elemento es menor, debe estar (si está) en la primera subtabla, y si es mayor está en la segunda. Por ejemplo, para buscar el elemento 3 en {1,2,3,4,5,6,7,8,9} se realizarían los siguientes pasos: Si al final de la búsqueda todavía no lo hemos encontrado, y la subtabla a dividir está vacía {}, quiere decir que el elemento no se encuentra en la tabla. En general, este método realiza {log2 (N+1)} comparaciones antes de encontrar el elemento, o antes de descubrir que no está. Este número es muy inferior que el necesario para la búsqueda lineal para casos grandes. Este método también se puede implementar de forma recursiva, siendo la función recursiva la que divide la tabla o lista. Búsqueda utilizando Tablas Hash Este método no es realmente un método de búsqueda, sino una forma de mejorar la velocidad de búsqueda al utilizar algún otro método. Consiste en asignar a cada elemento un índice mediante una transformación del elemento. Esta correspondencia se realiza mediante una función de conversión, llamada función hash. La correspondencia más sencilla es la identidad, esto es, al número 0 se le asigna el índice 0, al elemento 1 el índice 1, y así sucesivamente. Pero si los números a almacenar son demasiado grandes esta función es inservible. Por ejemplo, se quiere guardar en un array la información de 1000 usuarios de una empresa, y se elige el número de DNI como elemento identificativo. Es inviable hacer un array de 100.000.000 elementos, sobre todo porque se desaprovecha demasiado espacio. Por eso, se realiza una transformación al número de DNI para que nos de un número menor, de 3 cifras pues hay 1000 usuarios, y utilizar este resultado de la función hash como índice de un array de 1000 elementos para guardar a los empleados. Así, dado un DNI a buscar, bastaría con realizar la transformación según la función hash a un número de 3 cifras; usar este número como índice de búsqueda del array de 1000 elementos, con cualquiera de los métodos anteriores, y obtener el contenido del array dado por la posición cuyo índice es el resultado de la función hash. La función de hash ideal debería ser biyectiva, esto es, que a cada elemento le corresponda un índice, y que a cada índice le corresponda un elemento, pero no siempre es fácil encontrar esa función, e incluso a veces es inútil, ya que puede no saberse “a priori” el número de elementos a almacenar. La función de hash depende de cada problema y de cada finalidad, y se pueden utilizar con números o cadenas, pero las más utilizadas son: Restas sucesivas : Esta función se emplea con claves numéricas entre las que existen huecos de tamaño conocido, obteniéndose direcciones consecutivas. Aritmética modular : El índice de un número es resto de la división de ese número entre un número N prefijado, preferentemente primo. Los número se guardarán en las direcciones de memoria de 0 a N-1. Este método tiene el problema de que cuando hay (N+1) elementos, al menos un índice es señalado por dos elementos (teorema del palomar). A este fenómeno se le llama colisión. Mitad del cuadrado : Consiste en elevar al cuadrado la clave y coger las cifras centrales. Este método también presenta problemas de colisión. Truncamiento : Consiste en ignorar parte del número y utilizar los elementos restantes como índice. También se produce colisión. Plegamiento : Consiste en dividir el número en diferentes partes, y operar con ellas (normalmente con suma o multiplicación). También se produce colisión. Ahora se nos presenta el problema de qué hacer con las colisiones, es decir, el Tratamiento de Colisiones . ¿Qué pasa cuando a dos elementos diferentes les corresponde el mismo índice?. Pues bien, hay tres posibles soluciones: Cuando el índice correspondiente a un elemento ya está ocupado, se le asigna el primer índice libre a partir de esa posición. Este método es poco eficaz, porque al nuevo elemento se le asigna un índice que podrá estar ocupado por un elemento posterior a él, y la búsqueda se ralentiza, ya que no se sabe la posición exacta del elemento. También se pueden reservar unos cuantos lugares al final del array para alojar a las colisiones. Este método también tiene un problema: ¿Cuánto espacio se debe reservar? Además, sigue la lentitud de búsqueda si el elemento a buscar es una colisión. Lo mejor es en vez de crear un array de números, crear un array de punteros, donde cada puntero señala el principio de una lista enlazada. Así, cada elemento que llega a un determinado índice se pone en el último lugar de la lista de ese índice. El tiempo de búsqueda se reduce considerablemente, y no hace falta poner restricciones al tamaño del array, ya que se pueden añadir nodos dinámicamente a la lista. RecursividadDefinición de recursividad La recursividad es una característica que permite que una determinada acción se pueda realizar en función de invocar la misma acción pero en un caso más sencillo, hasta llegar a un punto (caso base) donde la realización de la acción sea muy sencilla. Esta forma de ver las cosas es útil para resolver problemas definibles en sus propios términos. En cierta medida, es análogo al principio de inducción. Para desarrollar algoritmos recursivos hay que partir del supuesto de que ya hay un algoritmo que resuelve una versión más sencilla del problema. A partir de esta suposición debe hacerse lo siguiente: Identificar subproblemas atómicos de resolución inmediate. Los denominados casos base . Descomponer el problema en subproblemas resolubles mediante el algoritmo preexistente; la solución de estos subproblemas debe aproximarnos a los casos base. No existen problemas intrínsecamente recursivos o iterativos; cualquier proceso iterativo puede expresarse de forma recursiva y viceversa. Si bien ciertos problemas se prestan mucho mejor que otros al uso de la recursividad. En el mundo de las matemáticas, un clásico ejemplo de recursividad lo tenemos en el cálculo del factorial de un número, a saber: Factorial del número natural n (incluido el 0) = n! (1) si n = 0 entonces: 0! = 1 (2) si n &gt; 0 entonces: n! = n · (n-1)! Aunque en este estudio de los TAD hemos evitado el uso de lenguajes de programación específicos, por ser una de las características de los tipos abstractos de datos la independencia de su implantación; en esta ocasión recurriremos al lenguaje C para poner un ejemplo muy claro e ilustrativo de cómo implementar un algoritmo recursivo. El siguiente programa es un ejemplo del cálculo del factorial de un número n: int factorial(int n){ if (n == 0) return 1; // Caso base return (n * factorial(n-1)); // Llamada a sí mismo} La función factorial es llamada pasándole un determinado entero y devuelve otro numero entero. Como se observa, en cada llamada recursiva se reduce el valor de n, llegando el caso en el que n es 0 y no efectúa más llamadas recursivas. Aunque la función factorial se preste muy bien al uso de la recursividad, no quiere decir esto, tal como hemos mencionado anteriormente, que no pueda ser implementado de forma iterativa. De hecho el factorial puede obtenerse con facilidad sin necesidad de emplear funciones recursivas, es más, el uso del programa anterior es muy ineficiente (con un número n grande, al ejecutarse en una computadora, consumiría mucha más memoria y tiempo que si se usase un algoritmo iterativo), pero es un ejemplo muy claro. Uso de la recursión La pregunta que surge de manera natural es: ¿Cuándo utilizar la recursividad? . No se debe utilizar si la solución iterativa es sencilla y clara. En otros casos, obtener una solución iterativa es mucho más complicado que una solución recursiva, y entonces se planteará si merece la pena transformar la solución recursiva en una iterativa. Por otra parte, hay TAD que, debido a sus características, sus operaciones se adaptan muy bien al uso de la recursión. Casi todos los algoritmos basados en los esquemas de vuelta atrás y divide y vencerás son recursivos. Otras estructuras que se adaptan muy bien a la recursividad son los grafos, y cuando se habla de grafos se está incluyendo a las listas y a los árboles. Si nos centramos en el mundo de la programación, algunos lenguajes de programación no admiten el uso de recursividad. Es obvio que en este caso se requerirá una solución no recursiva (iterativa). Aunque parezca mentira, es en general mucho más sencillo escribir un programa recursivo que su equivalente iterativo. Y desde luego siempre resulta más económico en cantidad de líneas de código. Ordenaciones y búsquedas recursivas Ya hemos visto en apartados anteriores como se realizan ordenaciones y búsquedas iterativas en TAD tipo tabla y lista. Veamos ahora algoritmos de naturaleza recursiva para realizar estas operaciones. La Ordenación Rápida (Quicksort) es un algoritmo de ordenación ilustrativo del uso de la recursividad en ordenaciones. Este método se basa en la táctica “divide y vencerás”, que consiste en ir subdividiendo el TAD considerado, en nuestro caso una tabla o lista en partes más pequeñas, y ordenar éstas. Para hacer esta división, se toma el valor como pivote, y se mueven todos los elementos menores que este pivote a su izquierda, y los mayores a su derecha. A continuación se aplica el mismo método a cada una de las dos partes en las que queda dividido el array. Normalmente se toma como pivote el primer elemento del array, y se realizan dos búsquedas: una de izquierda a derecha, buscando un elemento mayor que el pivote, y otra de derecha a izquierda, buscando un elemento menor que el pivote. Cuando se han encontrado los dos, se intercambian, y se sigue realizando la búsqueda hasta que las dos búsquedas se encuentran. Por ejemplo, para dividir el array {21,40,4,9,10,35}, los pasos serían: Respecto a los árboles, la idea fundamental que hace posible el uso de la recursividad es que realmente un árbol puede considerarse como un nodo raíz del que cuelgan otros árboles, llamados subárboles. Véase la siguiente figura como ejemplo de esta idea de un árbol binario: Para realizar cualquier exploración, recorrido o búsqueda en un árbol binario (o cualquier árbol genérico) bastaría con aplicar la misma operación a cada uno de sus subárboles, hasta llegar al caso base, que sería la búsqueda o recorrido en un subárbol formado por el nodo raíz y nodos hojas, pues en este caso la búsqueda o recorrido es muy fácil, basta seguir los enlaces del nodo raíz para visitar los nodos hojas. Una vez hecho esto el algoritmo recursivo consiste en hacer lo mismo, esto es, seguir los enlaces del nodo raíz, pero tomando como nodo raíz el nodo padre de los nodos caso base. Se repite el proceso hasta llegar al nodo raíz del árbol completo. En nuestro ejemplo, los casos bases se darían al llegar a los nodos B y C. Ambos nodos son casos base pues únicamente tienen nodos hojas. Una vez visto sus enlaces y sus nodos hojas se realizaría la misma operación pero tomando el nodo padre de B y C, el nodo A, y tratando a los nodos B y C como si fuesen hojas. Veremos en el siguiente apartado sobre grafos que un TAD tipo árbol no es más que un tipo particular de grafo. Y con respecto a estos, comprobaremos que sus operaciones, entre las que está la búsqueda y la ordenación, son de índole profundamente recursiva. GrafosUn grafo es un objeto matemático que se utiliza para representar circuitos, redes, caminos, etc. Los grafos son muy utilizados en computación, ya que permiten resolver problemas muy complejos. Supongamos el siguiente ejemplo. Disponemos de una serie de ciudades y de carreteras que las unen. De cada ciudad saldrán varias carreteras, por lo que para ir de una ciudad a otra se podrán tomar diversos caminos. Cada carretera tendrá un coste asociado (por ejemplo, la longitud de la misma). Gracias a la representación por grafos podremos elegir el camino más corto que conecta dos ciudades, determinar si es posible llegar de una ciudad a otra, si desde cualquier ciudad existe un camino que llegue a cualquier otra, etc. Para tener una idea visual de lo expuesto, supongamos que representamos las ciudades como circunferencias y los caminos por líneas que unen las distintas circunferencias (ciudades): Una primera cosa que salta a la vista es que tanto una lista como un árbol no son más que un caso particular de grafo, en donde hemos puesto ciertas restricciones. Veamos los grafos correspondientes a una lista y un árbol binario: Componentes de un grafo Así pues, un grafo consta de: Vértices (o nodos): Los vértices son objetos que contienen información. Para representarlos se suelen utilizar puntos o circunferencias con el contenido del nodo escrito dentro. En nuestro ejemplo serían las circunferencias que representan las ciudades. Aristas : Son las conexiones entre vértices. Para representarlos se utilizan líneas. Es frecuente añadir junto a la línea el nombre de los nodos origen y destino y el peso de la arista en algún tipo de unidad. Definición formal de grafo Conviene decir que la definición de un grafo no depende de su representación. Desde un punto de vista matemático puramente formal, la definición de grafo es la siguiente: Un grafo G es un par (V(G), A(G)), donde V(G) es un conjunto no vacío de elementos llamados vértices, y A(G) es una familia finita de pares no ordenados de elementos de V(G) llamados aristas. Al ser una familia de aristas se permite la posibilidad de aristas múltiples en el grafo, es decir, la existencia de más de una arista con el mismo par de vértices como origen y destino. También se permite la existencia de aristas bucles, con inicio y destino el mismo vértice. Por ejemplo, según lo dicho y utilizando la forma de representación indicada en el apartado anterior, se puede observar que el conjunto de vértices V(G){a, b, c, d, e, f} y el de aristas A(G) formado por los pares {a, b}, {a, b}, {b, c}, {a, d}, {d, e}, {d, e}, {b, f}, {b, f}, {c, e}, {a, a}, {b, b} y {c, c} determinan el grafo de la figura siguiente: Conceptos fundamentales sobre grafos Algunos conceptos fundamentales al hablar de grafos son: Un CAMINO o CIRCUITO entre dos vértices es una lista de vértices en la que dos elementos sucesivos están conectados por una arista del grafo. Desde un punto de vista formal, cualquier secuencia finita de aristas de G de la forma (v0v1), (v1v2), (v2v3), …, (vm-1vm) será denominada camino o circuito de G. Es frecuente que cuando entre dos vértices, por ejemplo los vértices “v” y “w”, existe uno o más caminos, sehable de cualquiera de estos caminos como un camino “vw”. Se habla de GRAFO CONEXO si existe un camino desde cualquier nodo del grafo hasta cualquier otro. Si no es conexo constará de varias componentes conexas . Formalmente, un grafo G es conexo si para cualquier par de vértices “v”, “w” de G existe un camino de “v” a “w” (“vw”). Un PUENTE o ARISTA PUENTE será cualquier arista de un grafo conexo que mediante su eliminación deje al grafo dividido en dos componentes conexas. En el grafo de la figura siguiente la arista {c, f} será una arista puente que dejará el grafo G dividido en las componentes conexas formada por los vértices {f} y {a, b, c, d, e}. Un CAMINO SIMPLE es un camino desde un nodo a otro en que ningún nodo se repite (no se pasa dos veces). Si el camino simple tiene como primer y último elemento al mismo nodo se denomina ciclo . Cuando el grafo no tiene ciclos tenemos un árbol. Varios árboles independientes forman un bosque . Un ÁRBOL DE EXPANSIÓN de un grafo es una reducción del grafo en el que solo entran a formar parte el número mínimo de aristas que forman un árbol y conectan a todos los nodos. Por ejemplo: Según el número de aristas que contiene, se habla de GRAFO COMPLETO si cuenta con todas las aristas posibles (es decir, todos los nodos están conectados con todos), GRAFO DISPERSO si tiene relativamente pocas aristas y GRAFO DENSO si le faltan pocas para ser completo. Las aristas son la mayor parte de la veces bidireccionales, es decir, si una arista conecta dos nodos A y B se puede recorrer tanto en sentido hacia B como en sentido hacia A. Estos son llamados GRAFOS NO DIRIGIDOS . Sin embargo, en ocasiones tenemos que las uniones son unidireccionales. Estas uniones se suelen dibujar con una flecha y definen un GRAFO DIRIGIDO . Se habla de GRAFO PONDERADO cuando las aristas llevan un coste asociado (un entero denominado peso ). Una RED es un grafo dirigido y ponderado. Exploración de grafos Cuando se habla de la exploración de un grafo nos referimos a la exploración de todos los vértices (con algún tipo de fin, por ejemplo, un recuento) o hasta que se encuentra uno determinado, es decir, una búsqueda. El orden en que los vértices éstos son “visitados” decide radicalmente el tiempo de ejecución del algoritmo. Supongamos el siguiente grafo de ejemplo: A la hora de explorar el grafo de la figura anterior, nos encontramos con dos métodos distintos: Exploración o búsqueda en Anchura o Amplitud : Lo que prima en la exploración es la dimensión horizontal, de manera que por cada vértice del nodo visitamos primeramente todos los nodos enlazados directamente con él (vecinos), y una vez hecho esto realizamos la misma operación con estos nodos vecinos visitados. Exploración o búsqueda en Profundidad : Lo que prima en la exploración es la dimensión vertical. Una vez visitado uno de los nodos vecinos de un nodo, antes de visitar a cualquiera de los demás vecinos del nodo, se vuelve a realizar la misma operación con el nodo vecino recién visitado. Suponiendo que el orden en que están almacenados los nodos en la estructura de datos correspondiente es A-B-C-D-E-F-G-H-I-J-K (en orden alfabético), tenemos que: En un recorrido en anchura el orden sería: A-B-C-D-E-G-H-I-J-K-F El orden que seguiría el recorrido en profundidad será: A-B-E-I-F-C-G-J-K-H-D Implantación de la exploración de grafos En el ejemplo anterior, es destacable que el nodo D es el último en explorarse en la búsqueda en profundidad pese a ser adyacente al nodo de origen (A). Esto es debido a que primero se explora la rama del nodo C, que también conduce al nodo D. Es decir, hay que tener en cuenta que es fundamental el orden en que los nodos están almacenados en las estructuras de datos. Si, por ejemplo, el nodo D estuviera antes que el C, en la búsqueda en profundidad se tomaría primero la rama del D (con lo que el último en visitarse sería el C), y en la búsqueda en anchura se exploraría antes H que el G. Otro punto a observar es que, cuando hemos hablado en el apartado anterior de exploraciones en anchura y en profundidad, aparecen las siguientes frases “una vez hecho esto realizamos la misma operación” y “se vuelve a realizar la misma operación”. Esto está sugiriendo de forma muy clara que las exploraciones en grafos, tanto en anchura como en profundidad tienen una fuerte naturaleza recursiva. De hecho, es frecuente que se implanten ambos algoritmos usando recursividad. Esto no quiere decir que no puedan implementarse exploraciones de grafos de forma iterativa. Usando algoritmos iterativos, la diferencia principal entre implementar una exploración en anchura frente a una en profundidad está en la estructura de datos usada, de forma que: Las exploraciones o búsquedas en anchura iterativas usa una estructura de datos tipo cola , pues las características de este tipo de estructura FIFO (primero en entrar, primero en salir) se adaptan muy bien a este tipo de recorrido. Las exploraciones o búsquedas en profundidad iterativas usa una estructura de datos tipo pila , pues las características de este tipo de estructura LIFO (último en entrar, primero en salir) se adaptan muy bien a este tipo de recorridos. Organizaciones de FicherosTodas las aplicaciones necesitan almacenar y recuperar información. En una computadora, cuando se ejecuta una aplicación (un proceso), la información almacenada en la memoria principal electrónica del computador; este es un tipo de memoria volátil, de forma que cuando la aplicación termina la información se pierde. Esto es inaceptable para muchas aplicaciones, que pueden requerir que la información permanezca disponible durante largos periodos de tiempo. Con respecto a la memoria principal de las computadoras, se trata de un tipo de memoria electrónicas cuyas principales características son: La memoria principal tiene poca capacidad de almacenamiento. No se pueden manipular grandes cantidades de datos, ya que puede haber casos en los que no quepan en la memoria principal. La memoria principal es volátil. Acceso rápido a la información. Otro problema es que varios procesos pueden necesitar acceder a una misma información de forma concurrente. Como los espacios de memoria de los procesos son privados, un proceso no puede acceder a los datos en el espacio de memoria de otro. La solución es hacer que la información sea independiente de los procesos. Por tanto, hay tres requisitos esenciales para almacenar la información en discos magnéticos u otros dispositivos en una unidades llamadas ficheros o archivos . Un fichero es una abstracción de un mecanismo que permite almacenar información en un dispositivo y leerla posteriormente. Podemos definir un fichero como una colección de información que tiene un nombre. Los ficheros pueden ser leídos y escritos por cualquier proceso: son una forma de almacenamiento denominada memoria secundaria . Sus principales cualidades son: Capacidad de almacenamiento sólo limitada por el soporte físico de que se disponga. La información está almacenada permanentemente. Acceso lento a la información, ya que tiene que ser transportada desde el dispositivo externo hasta la memoria principal para su tratamiento. Existe un área de memoria principal destinada a recibir esta información procedente del dispositivo secundario. Esta área se denomina Buffer . Gráfico de Memoria Principal y Memoria Secundaria: En la figura anterior representamos de manera muy esquemática la forma de operar de un procesador, siendo: U.C. U nit C ontrol (Unidad de Control). Circuito principal de control del procesador. A.L.U. A ritmetic L ogic U nit (Unidad Aritmético Lógica). Circuito especializado en operaciones aritméticas. E : Datos de E ntrada. S : Datos de S alida. La información almacenada en ficheros debe ser persistente , es decir, no debe verse afectada por la creación y finalización de los procesos. La gestión de ficheros es tarea del SO, y la parte del mismo que realiza dicha gestión se conoce como sistema de ficheros . Estructura de un FicheroDe la definición vista de fichero, se deduce que existen diferentes tipos de ficheros en función de: La información contenida El método de organización de la información Una primera clasificación de los ficheros se puede hacer según el método usado para codificar la información: Ficheros de texto : Se guarda la información en caracteres, tal y como se mostraría en pantalla. Ficheros binarios : Se guarda la información en binario, tal y como está en memoria. Otra clasificación de los ficheros es según la forma que tiene su estructura. Las formas más usuales son: Organizar un fichero como una secuencia de bytes . De esta forma, el sistema operativo no conoce el significado del contenido de los ficheros, lo que simplifica la gestión de los mismos. Serán los programas de aplicación los que deberán de conocer la estructura de los ficheros que utilizan. Este enfoque es el empleado por MS-DOS y UNIX. Un esquema más estructurado es considerar un fichero como una secuencia de registros de longitud fija , cada uno de los cuales presenta una estructura determinada. La idea es que las operaciones de lectura devuelvan un registro y las escrituras modifiquen o añadan un registro. El SO CP/M usa registros de 128 bytes. Una tercera forma es organizar el fichero en forma de árbol de registros , que no tienen por qué tener la misma longitud. Cada registro tiene un campo clave por el que está ordenado el árbol de forma que las operaciones de búsqueda por clave se realizan rápidamente. Este esquema se emplea en grandes computadores (mainframes) orientados al proceso de grandes cantidades de información. Conceptos básicos sobre FicherosVeamos ahora una serie de conceptos básicos: Registro lógico : Un registro es una colección de información relativa a una entidad particular. Por tanto, el registro va a contener a todos aquellos campos lógicamente relacionados, referentes a una determinada entidad, y que pueden ser tratados globalmente por un programa. Por ejemplo la información de un determinado alumno, que contiene los campos DNI, nombre, apellidos, fecha de nacimiento, etc. Clave de un registro lógico : Una clave es un campo o conjunto de campos de datos que identifica al registro lógico y lo diferencia del resto de registros lógicos del fichero. Por tanto, esta clave debe ser distinta para cada registro. Registro activo : El registro lógico que va a procesarse en la siguiente operación del fichero. Apuntador : Marca interna que siempre apunta al registro lógico activo. Se incrementa automáticamente cada vez que se procesa un registro (se lee o se escribe). Marca de fin de fichero : Una marca situada al final de cada fichero, para no acceder más allá del último registro lógico existente, ya que el tamaño del fichero no está limitado y no se conoce a priori. Existe una función lógica, eof (end of file) , que toma el valor verdadero cuando llegamos al final del fichero y falso en caso contrario. Factor de bloqueo : Factor de bloqueo es el número de registros lógicos que puede contener un registro físico. Registro físico o bloque : Un registro físico o bloque es la cantidad más pequeña de datos que pueden transferirse en una operación de E/S entre la memoria principal del ordenador y los dispositivos periféricos o viceversa. El tamaño del bloque o registro físico dependerá de las características del ordenador. En la mayoría de los casos el tamaño del bloque suele ser mayor que el de registro lógico. La adaptación consiste en empaquetar en cada bloque tantos registros lógicos como se pueda. El empaquetamiento puede ser de tipo fuerte o débil , según que se permita o no aprovechar el sobrante de un bloque, situando registros a caballo entre dos bloques contiguos. La siguiente figura ilustra ambas formas de empaquetamiento. Una vez visto lo que es un registro lógico y teniendo ya en mente la idea de fichero, podemos dar una definición más precisa de lo que es un archivo o fichero. Un fichero es una colección de registros lógicos relacionados entre sí con aspectos en común y organizados para un propósito específico. Los datos en los archivos deben estar organizados de tal forma que puedan ser recuperados fácilmente, actualizados o borrados y almacenados en el archivo con todos los cambios realizados. Dese un punto de vista puramente estructural: Un fichero es una estructura de datos compuesta que agrupa una secuencia de cero o más tuplas, denominadas registros, y que a su vez se pueden componer de otras estructuras de datos a las que se les suele llamar campos. Operaciones sobre FicherosUna vez visto lo que es un fichero y los principales conceptos al hablar de ellos, pasemos ahora a estudiarlos desde un punto de vista operativo. Básicamente se trata de responder: ¿qué operaciones se pueden realizar sobre un fichero? La respuesta es: Creación : Para poder realizar cualquier operación sobre un fichero es necesario que haya sido creado previamente, almacenando sobre el soporte seleccionado la información requerida para su posterior tratamiento, como por ejemplo el nombre del dispositivo, el nombre del fichero, etc. Con anterioridad a la creación de un archivo se requiere diseñar la estructura del mismo mediante los campos del registro, longitud y tipo de los mismos. Apertura : Para poder trabajar con la información almacenada en un fichero, éste debe estar abierto, permitiendo así el acceso a los datos, dando la posibilidad de realizar sobre ellos las operaciones de lectura y escritura necesarias. Cierre : Una vez finalizadas las operaciones efectuadas sobre el fichero, éste debe permanecer cerrado para limitar el acceso a los datos y evitar así un posible deterioro o pérdida de información. Para cerrar un fichero previamente debe estar abierto. Actualización : Esta operación permite la puesta al día de los datos del fichero mediante la escritura de nuevos registros (alta) y la eliminación (baja) o modificación de los ya existentes. La actualización puede afectar a parte o la totalidad de los registros del fichero. Cuando se escribe un nuevo registro en el fichero se debe comprobar que no existe previamente. La baja de un registro puede ser lógica o física. Una baja lógica supone el no borrado del registro en el archivo. Esta baja lógica se manifiesta en un determinado campo del registro con una bandera, indicador o “flag”, o bien con la escritura o rellenado de espacios en blanco en el registro específico. Una baja física implica el borrado y desaparición del registro, de modo que se crea un nuevo archivo que no incluye al registro dado de baja. Consulta : Tiene como fin visualizar la información contenida en el fichero, bien de un modo completo, bien de modo parcial. Borrado o destrucción : Es la operación inversa a la creación de un fichero. Consiste en la supresión de un fichero del soporte o dispositivo de almacenamiento. El espacio utilizado por el archivo borrado puede ser utilizado por otros archivos. Para borrar un fichero tiene que estar cerrado. Ordenación o clasificación : Consiste en lograr una nueva disposición sobre el soporte de los registros de un archivo, con una secuencia de ubicación determinada por el valor de uno o varios campos. Compactación o empaquetamiento : Esta operación permite la reorganización de los registros de un fichero eliminando los huecos libres intermedios existentes entre ellos normalmente ocasionados por la eliminación de registros. Organización de un Sistema de FicherosLos discos magnéticos son la base sobre la que se sustentan los sistemas de ficheros. Para mejorar la eficiencia, la transferencia de información entre memoria y los discos se realiza en unidades denominadas bloques. Cada bloque está formado por uno o varios sectores de disco. El tamaño de sector de un disco suele ser de 512 bytes. El diseño de un sistema de ficheros plantea dos problemas diferentes: Definir cómo el sistema de ficheros aparece al usuario. Diseñar los algoritmos y estructuras de datos necesarias para implementar este sistema de ficheros lógico en los dispositivos físicos de almacenamiento secundario. Un sistema de ficheros se puede estructurar en diferentes capas o niveles, se puede ver en la siguiente figura: El nivel de control de E/S se compone de los manejadores de dispositivo ( device drivers ) y los manejadores de interrupciones ( interrupt handlers ), que son necesarios para transmitir la información entre la memoria y los discos. Los manejadores de dispositivos reciben instrucciones de bajo nivel, del tipo “escribir o leer el bloque nº x”, y generan el conjunto de instrucciones dependientes del hardware que son enviadas al controlador de disco. El sistema de ficheros básico transmite las instrucciones de bajo nivel al manejador de dispositivo adecuado para leer y escribir bloques físicos en disco. Cada bloque físico se identifica por su dirección numérica en el disco, que viene dado por el dispositivo, cilindro, superficie y sector. El módulo de organización de ficheros tiene conocimiento sobre los ficheros, los bloques lógicos que lo componen y los bloques físicos. Mediante el tipo de esquema de asignación de bloques y la localización del fichero, el módulo de organización de ficheros traslada las direcciones de disco lógicas en direcciones de disco físicas. Cada bloque de disco lógico tiene un número (de 0 a N) que no suele coincidir con la dirección de los bloques físicos, por lo que es necesario un mecanismo de traducción. Este módulo también incluye el gestor de espacio libre, que controla los bloques libres para que puedan ser usados posteriormente. Por último,el sistema de ficheros lógicos proporciona la estructura de directorio conocida por los programas de usuario. También es responsable de proporcionar seguridad y protección al sistema de ficheros. Por otra parte, para crear un nuevo fichero, un programa de aplicación realiza una llamada al sistema sobre el sistema de ficheros lógico. Este lee el directorio correspondiente en memoria, le añade una nueva entrada y lo escribe en disco. En este proceso, el sistema de ficheros lógico ha solicitado al módulo de organización de ficheros la dirección física del directorio, que se envía al sistema de ficheros básico y al control de E/S. Cuando el directorio ha sido actualizado, el sistema de ficheros lógico lo puede usar para realizar operaciones de E/S. Para optimizar los accesos, el SO tiene en memoria una tabla de ficheros abiertos que contiene información sobre todos los ficheros abiertos en un momento dado, como nombre, permisos, atributos, direcciones de disco, etc. La primera referencia a un fichero suele ser una operación de apertura, que hace que se realice una búsqueda en la estructura de directorio y se localice la entrada para el fichero que se quiere abrir. Esta entrada se copia en la tabla de ficheros abiertos y el índice de dicha entrada (denominado descriptor de fichero) se devuelve al programa de aplicación, que lo usa para realizar las operaciones sobre el fichero. De esta forma, todas las operaciones relacionadas con el entrada de directorio del fichero se realizan en la tabla de ficheros abiertos en memoria. Cuando un fichero se cierra, la entrada modificada se copia a disco. Organización y Acceso a FicherosSe entiende por Organización de un fichero a la manera en la que los datos son estructurados y almacenados internamente en el fichero y sobre el soporte de almacenamiento. El tipo de organización de un fichero se establece durante la fase de creación del mismo. Los requisitos que determinan la organización de un fichero son del tipo: tamaño del fichero, frecuencia de utilización o uso, etc. La organización de un fichero es muy dependiente del soporte físico en que se almacene. Hay dos tipos de soportes: Soportes Secuenciales : Los registros están dispuestos físicamente uno a continuación de otro. Para acceder a un determinado registro se necesita pasar por todos los anteriores a él. Soportes Direccionables : Permiten localizar a un registro directamente por su información (clave) sin tener que pasar por todos los anteriores. Los tipos de organizaciones de ficheros fundamentales son: Organización Secuencial . Organización Directa o Aleatoria . Organización Indexada . En cuanto al acceso, se entiende por tal al procedimiento necesario que debemos seguir para situarnos sobre un registro concreto con la intención de realizar una operación sobre él. Según las características del soporte empleado y la organización se consideran dos tipos de acceso: El acceso secuencial implica el acceso a un archivo según el orden de almacenamiento de sus registros, uno tras otro. Se puede dar en dispositivos secuenciales y direccionables. El acceso directo implica el acceso a un registro determinado, sin que ello implique la consulta de los registros precedentes. Obviamente, sólo puede darse en soportes direccionables. Organización Secuencial Son aquellos ficheros caracterizados porque los registros se escriben o graban sobre el soporte de almacenamiento en posiciones de memoria físicamente contiguas, en la misma secuencia u orden en que han sido introducidos, sin dejar huecos o espacios libres entre ellos. Todos los dispositivos de memoria auxiliar soportan la organización secuencial. El acceso a los datos almacenados en estos ficheros siempre es secuencial independientemente del soporte utilizado. Los registros organizados secuencialmente tienen un registro especial, el último, que tiene una marca de fin de archivo. Sus ventajas son: Rapidez en el acceso a un bloque de registros que se encuentran almacenados en posiciones de memoria físicamente contiguas. No deja espacios vacíos entre registro y registro, optimizado al máximo la memoria ocupada. Sus inconvenientes son: El acceso a registros individuales es muy lento. Se tiene que procesar todo el fichero para operaciones de inserción y borrado de registros. Organización Directa o Aleatoria Estos ficheros se caracterizan porque los registros se sitúan en el fichero y se accede a ellos a través de una clave, que indica la posición del registro dentro del fichero y la posición de memoria donde está ubicado. Estos ficheros se almacenan en soportes direccionables . Además, los registros han de tener un identificativo o clave, el cual indica la posición de cada registro en el fichero. Como principales ventajas genéricas de este tipo de organización, tenemos que: Cada posición solamente puede ser ocupada por un registro, pues no podemos tener en el fichero más de un registro con el mismo valor de clave. El acceso a cualquier registro se hace de una forma directa e inmediata mediante su clave. La actualización de un registro es inmediata, sin que se deban utilizar archivos auxiliares para copia. Se puede utilizar el acceso secuencial, aunque suponga generalmente una pérdida de tiempo. Dentro de la organización directa, según el algoritmo utilizado en la gestión de la clave, se pueden distinguir entre: Organización directa con Clave Directa : La dirección de almacenamiento del registro está indicado por la propia clave. Sus ventajas son: Cada posición solamente puede ser ocupada por un registro, pues no podemos tener en el fichero más de un registro con el mismo valor de clave. Es muy rápido el acceso a los registros individuales. Sus inconvenientes son: Deja gran cantidad de huecos dentro del fichero, con el consecuente desaprovechamiento del soporte de almacenamiento. Esto es debido a que este sistema precisa que el soporte donde se almacena la información tenga una mínima unidad de asignación (denominado cluster ) a la cual acceder directamente siguiendo unas coordenadas de localización. Estas unidades no pueden ser usadas para almacenar información de distintos ficheros. Si los cluster en que divide el disco son grandes, y los ficheros a almacenar pequeños, habrá muchos cluster que se quedarán a medio llenar, con el consecuente desaprovechamiento de espacio. Una consulta total del fichero puede suponer un gran inconveniente, pues hay que analizar todas las posiciones de memoria, aunque algunas posiciones estén vacías. Para comprender este hecho hagamos el siguiente símil. Supongamos que tenemos un libro con todos sus capítulos correctamente ordenados pero sin índice que nos identifique la página de inicio de cada capítulo, para leer el libro entero lo único que hay que hacer es comenzar por la primera página, sin embargo, si queremos leer un capítulo en concreto, tendríamos también que empezar desde el principio hasta encontrarlo. Esto sería el análogo a un fichero secuencial. Supongamos ahora un libro con todos los capítulos desordenados, pero con un índice al comienzo del libro que nos indica la página de comienzo de cada capítulo; es decir, el análogo al de un fichero de acceso directo. Leer cada capítulo por separado es ahora muy fácil, basta con buscar el comienzo en el índice, sin embargo, si necesitamos leer el libro entero, de principio a fin, necesitamos constantemente mirar el índice para ir viendo la secuencia de los capítulos, lo cual es muy ineficiente (por no decir pesado). Esto sería el análogo a leer completamente un fichero de acceso directo. Organización directa con Clave Indirecta : La dirección de almacenamiento se obtiene a partir de la clave, después de realizar algún tipo de transformación. Este tipo de transformación se denomina algoritmo Hashing y suele ser de tipo matemático. En este tipo de algoritmo se pueden dar dos situaciones no deseadas (denominadas colisiones ) que son: Hay direcciones que no corresponden a ninguna clave y, por tanto, zonas de disco sin utilizar. Hay direcciones que corresponden a más de una clave. En este caso se dice que las claves son sinónimas para esa transformación. Hay dos formas de resolver el problema de los sinónimos o colisiones: Buscar secuencialmente en el archivo hasta encontrar una posición libre donde escribir el registro o aplicando a la dirección obtenida un segundo método de direccionamiento. Estos procedimientos son lentos y degradan el archivo. Reservar una zona de desbordamiento o de sinónimos en donde se escribirán los registros que no se pueden escribir en la dirección que le corresponde según la transformación. Organización Indexada Al fichero le acompaña un fichero de índice que tiene la función de permitir el acceso directo a los registros del fichero de datos. El índice se puede organizar de diversas formas, las más típicas son: Secuencial Multinivel Árbol A través del índice podremos procesar un fichero de forma secuencial o de forma directa según la clave de indexación, y esto independientemente de como esté organizado el fichero por sí mismo. El índice debe estar organizado en función de alguno de los campos de los registros de datos. Se pueden tener tantos índices como se quiera variando la clave (o campo) que se emplee. El índice está formado por registros (entradas) que contienen: Clave de organización. Puntero(s) al fichero de datos, en concreto al registro que corresponda. Los índices se pueden clasificar en dos tipos, según cada entrada señale a la dirección de un registro del fichero de datos ( índice total o denso ), o bien apunte a un grupo de registros del fichero de datos que debe estar ordenado ( índice escaso o no denso ). En el caso de índices totales, el fichero puede estar desordenado. Véase la siguiente figura como ejemplo: Con el segundo tipo se podría procesar directamente el fichero de datos de forma secuencial. Los índices totales o densos no suelen utilizarse de forma simple, sino combinados con índices escasos o más cortos, de esta manera pueden almacenarse en memoria principal obteniendo así una mayor rapidez de acceso. Implantación de FicherosEl aspecto básico de la implantación de ficheros consiste en determinar qué bloques de disco están asociados a cada fichero. El problema que se plantea es cómo asignar el espacio libre en disco a los ficheros de forma que ese espacio sea usado de forma eficiente y los ficheros puedan ser accedidos rápidamente. Hay que tener en cuenta que el tamaño de los ficheros es variable, por lo que habrá que diseñar algún mecanismo de almacenamiento dinámico tanto para los ficheros como para el espacio libre. Existen tres métodos básicos de asignación: Asignación Contigua . Asignación Enlazada . Asignación Indexada . Asignación Contigua El esquema de asignación más simple consiste en almacenar cada fichero como una secuencia adyacente de bloques en disco. La asignación contigua de un fichero se define por la dirección del primer bloque y el tamaño del fichero, es decir, simplemente responder a las preguntas ¿dónde empezar? y ¿hasta cuando seguir?. Las ventajas de este método son: Fácil implementación, ya que únicamente hay que conocer la dirección en disco del primer bloque del fichero y el número de bloques que ocupa. Eficiencia, ya que la lectura de un fichero se puede realizar en una sola operación. El acceso a un fichero almacenado de forma contigua es sencillo y rápido, siempre y cuando pretendamos leer todo el fichero y no tengamos que hacer búsquedas de parte de su contenido. En este tipo de operaciones se pierde toda la ventaja que proporciona esta forma de asignación. Para este tipo de operaciones veremos que es mucho más ventajosa la asignación. Para el acceso secuencial, el sistema de ficheros no tiene más que recordar la dirección del último bloque referenciado. El acceso directo del bloque _i_ de un fichero que comienza en el bloque _b_ se realiza, simplemente, accediendo al bloque b+i . Un inconveniente de este esquema de asignación es que no se puede implantar a no ser que se conozca el tamaño de los ficheros en su momento de creación. Si esta información no está disponible, el SO no sabe cuánto espacio en disco debe reservar. Otra desventaja es la fragmentación externa que se produce en el disco, que se origina debido a que entre los ficheros quedan porciones de disco libre que no tienen el tamaño suficiente para ser asignadas a un nuevo fichero. La gravedad de este problema depende del espacio de almacenamiento disponible y del tamaño medio de los ficheros. La compactación de disco es una solución, pero es costos y de poder hacerse sería cuando el sistema estuviese parado. El problema de conocer el tamaño de los ficheros en tiempo de creación se resuelve haciendo que los ficheros sean inmutables. Esto significa que cuando un fichero se crea no se puede modificar. Una modificación implica borrar el fichero original y crear uno nuevo. Por otro lado, los ficheros se han de leer de una sola vez en memoria, por lo que la eficiencia es muy alta, pero a cambio se exigen unos requisitos mínimos de memoria muy elevados. Por último, la fragmentación de disco se soluciona teniendo discos de gran capacidad de almacenamiento. Asignación Enlazada Otro esquema consiste en mantener una lista enlazada con los bloques que pertenecen a un fichero, de forma que una palabra del bloque sería un índice al bloque siguiente. Con este método se elimina el problema de la fragmentación, y, al igual que en la asignación contigua, cada entrada de directorio únicamente contiene la dirección del primer bloque del fichero. Los fichero pueden crecer de forma dinámica mientras que exista espacio libre en disco y no es necesario compactar los discos. Un inconveniente es que, mientras que el acceso secuencial es fácil de realizar, el acceso aleatorio es muy lento. Otro inconveniente viene dado por el espacio requerido por los punteros. Si un puntero requiere 4 bytes y los bloques son de 512 bytes, significa que: (4 / 512) = 0,0078 –&gt; se pierde un 0,78% (casi un 1%) de espacio útil por bloque. Una solución a este problema consiste en no asignar bloques individuales, sino conjuntos de bloques denominados clusters . De esta forma el porcentaje de espacio perdido por los punteros se reduce. Además, se mejora el rendimiento de los discos porque se lee más información en una misma operación de lectura, y se reduce la lista de bloques libres. El coste de este método es que se produce un incremento de la fragmentación interna (espacio no ocupado en un bloque o cluster). Otro problema de este método es la fiabilidad, ya que si un bloque se daña, se puede perder el resto del fichero. Peor aún, sise altera el contenido del puntero, se puede acceder a la lista de bloques libres o a bloques de otros ficheros como si fuesen bloques propios. Un problema más sutil viene dado por el hecho de que la cantidad de información útil para el usuario que contiene cada bloque no es potencia de dos, ya que el puntero al bloque siguiente, que estará constituido por varias palabras, ocupará parte del bloque, es decir, no se puede utilizar todo el bloque para información útil, siempre habrá que dejar mínimo espacio para el puntero. Esto puede originar una pérdida de eficiencia porque los programas normalmente leen y escriben en bloques cuyo tamaño es potencia de dos. Esta desventaja del esquema de lista encadenada desaparece si el puntero de cada bloque de disco se almacena en una tabla o índice de memoria. Así cada bloque únicamente contiene datos útiles. Además, aunque el acceso aleatorio implica seguir una cadena, ésta está toda en memoria, por lo que la búsqueda es mucho más eficiente que en el esquema anterior. De igual modo, es suficiente que cada entrada de directorio contenga únicamente la dirección del primer bloque para localizar todo el fichero. Este esquema es empleado por el SO MS-DOS. Para que este esquema funcione, la tabla debe permanecer entera en memoria todo el tiempo. Si el disco es grande, el gasto de memoria principal puede ser considerable. Asignación Indexada El último método para determinar qué bloques pertenecen a un fichero consiste en asociar a cada fichero un nodo índice ( index-node o i-node ), que incluye información sobre los atributos del fichero y las direcciones de los bloques de disco que pertenecen al mismo. Las direcciones de los primeros bloques se almacenan en el propio nodo índice, de forma que, para ficheros pequeños, toda la información necesaria para su localización siempre está en memoria cuando son abiertos. Para ficheros de mayor tamaño, una de las direcciones del nodo índice es la dirección de un bloque simple indirecto, que contiene a su vez direcciones de otros bloques del fichero. Si no es suficiente, existen dos direcciones, una para un bloque doble indirecto, que contiene direcciones de bloques simples directos, y otra para un bloque triple indirecto, que contiene direcciones de bloques dobles indirectos. Una ventaja es que permite el acceso directo a cualquier bloque del fichero. Además, los accesos a un fichero, una vez abierto, no implican accesos adicionales a disco a no ser que sean necesarios bloques indirectos. Un inconveniente es que añadir o borrar bloques en medio del fichero implica tener que reorganizar los punteros en el bloque de índices, lo cual constituye una operación costosa. DirectoriosLos sistemas de ficheros pueden contener grandes volúmenes de información, por lo que los SO proporcionan mecanismos para estructurarlos. Esta estructuración se suele realizar a dos niveles. En primer lugar, el sistema de ficheros se divide en particiones, que se pueden considerar como discos virtuales. Un disco puede contener una o varias particiones, y una partición puede estar repartida. En segundo lugar, cada partición contiene una tabla de contenidos de la información que contiene. Esta tabla se denomina directorio, y su función principal es hacer corresponder un nombre de un fichero con una entrada en la tabla. Cada entrada contiene, como mínimo, el nombre del fichero. A continuación se pueden almacenar los atributos y direcciones del fichero en disco, o un puntero a otra estructura de datos que contiene dicha información. Esta estructura de datos se suele conocer con el nombre nodo índice . Mostramos los atributos y direcciones a una estructura de datos: Cuando se abre un fichero el SO busca en el directorio correspondiente hasta que encuentra el nombre del fichero, toma la información que necesita y la introduce en una tabla residente en la memoria principal. Las siguientes referencias al fichero utilizarán la información que ya está en memoria. Implantación de DirectoriosPara acceder a un fichero, ya sea para lectura o escritura, previamente hay que abrir el fichero. Esta operación implica que el SO, a partir de la ruta de acceso, debe localizar la entrada de directorio correspondiente. Esta entrada proporciona la información necesaria para localizar los bloques de disco del fichero. Esta información, según el tipo de sistema, puede ser la dirección en disco del fichero completo (asignación contigua), el número del primer bloque (los dos métodos basados en listas encadenadas) o el número de nodo índice. En todo caso, la principal función del sistema de directorios es asociar el nombre en ASCII de un fichero con la información precisa para localizar los datos. Los atributos de los ficheros se pueden almacenar en la misma entrada de directorio. Si usan nodos índice, es posible almacenar los atributos en el nodo índice. A continuación se exponen brevemente la implantación de directorios en varios SO: CP/M, MS-DOS y UNIX. No vamos a mencionar nada de las características de cada SO, simplemente mencionamos estos SO para ver distintas filosofías de implementar directorios. Directorios en CP/M En CP/M solamente existe un directorio, por lo que el sistema de ficheros únicamente tiene que buscar el nombre de un fichero en dicho directorio. Si un fichero ocupa más de 16 bloques se le asignan más entradas de directorio. Los campos de una entrada de directorio en CP/M son los siguientes: El código de usuario (1 byte) permite conocer el propietario del fichero. El nombre y la extensión del fichero (8 y 3 bytes, respectivamente). Si un fichero ocupa más de 16 bloques, el campo magnitud (1 byte) indica el orden que ocupa esa entrada. El contador de bloques (1 byte) indica cuántos bloques están en uso de los 16 posibles. Los últimos 16 campos contienen las direcciones de bloques de disco del fichero. Como el último bloque puede no estar lleno, es imposible conocer con exactitud el tamaño en bytes de un fichero. Directorios en MS-DOS MS-DOS tiene un sistema de ficheros jerárquico. Cada entrada de directorio tiene 32 bytes, y contiene, entre otros datos, el nombre del fichero, la extensión, los atributos y el número del primer bloque en disco del fichero. Este número es un índice a una tabla de asignación denominada FAT ( File Allocation Table ). Un directorio puede contener otros directorios, lo que origina la estructura jerárquica del sistema de ficheros. Directorios en UNIX La estructura de una entrada de directorio en un sistema UNIX tradicional es muy simple, y contiene únicamente el nombre del fichero y el número de su nodo índice. Toda la información sobre el fichero reside en el propio nodo índice. Formatos de Información y FicherosIndependientemente de la plataforma con que se trabaje, la información obtenida se genera con un ordenador y se suele almacenar en un fichero, con la intención de recuperarla más tarde cuando sea necesaria, o compartirla con los demás a través de algún medio de transmisión de datos. En consecuencia, es conveniente conocer los formatos de archivo más indicados para almacenar los distintos tipos de información que deben contener. En este apartado vamos a estudiar los ficheros desde un punto de vista operativo, es decir, según el uso o función que se le da al fichero. Podría decirse que desde el punto de vista del usuario, al que le interesa guardar datos en un archivo y tenerlo localizado de alguna forma en su ordenador independientemente de cómo esté organizado el fichero o cual sea su estructura. Formatos de InformaciónLa mayoría de aplicaciones suelen guardar la información que producen en formatos de fichero propios, de modo que podemos editarlos posteriormente con la garantía que se respetarán todas las peculiaridades de los datos y el nivel de edición que poseían en el momento de guardarlos, ahora, cuando compartimos información debemos ser muy cuidadosos con la elección del tipo de fichero ya que no debemos asumir que todo el mundo posee nuestras mismas herramientas ni nuestro mismo sistema. No se trata de analizar en profundidad las características de todos los tipos de archivo, sino indicar algunas referencias generales sobre los más usados que nos ayuden a decidir el formato adecuado en cada ocasión. Ficheros con Información de Texto Si queremos almacenar o compartir un fichero de texto tenemos dos formatos básicos independientes de la plataforma, es decir, son legibles con un editor de texto sobre cualquier SO: TXT , para ficheros de texto plano. RTF , Rich Text Format (Formato de texto enriquecido) cuando sea necesario incluir en el texto algunos elementos de realce como cursivas o negrita. Ficheros con Información de Imagen Hay gran variedad de formatos de archivos gráficos, la mayoría compatibles con cualquier plataforma. Entre los más habituales se encuentran: JPG para imágenes de tono continuo en mapa de bits. Es un formato comprimido pues prescinde de los datos de color de la imagen que no están en el espectro visible. GIF usado especialmente con animaciones y gráficos con regiones transparentes. Tiene algunos problemas legales con los términos de su licencia. PNG tiene similares características al GIF aunque se trata de un formato más evolucionado y de mayor calidad, con muy buenos ratios de compresión y soporte para multitransparencia. Posee una licencia libre. TIFF se utiliza para almacenar imágenes sin pérdida de calidad, por lo que genera tamaños de archivo mayores que el resto pese a que incorpora un algoritmo de compresión. SVG para ilustraciones vectoriales. Ficheros con Información Compuesta Cuando se trata de compartir documentos que integran texto con imágenes o gráficos, o la composición y aspecto son fundamentales por tratarse de formularios estandarizados o similares, tenemos dos alternativas: PS es un documento PostScript o formato de impresión capaz de ser visualizado con alguna aplicación auxiliar e impreso sin problemas, directamente. Mantienen la misma calidad de resolución que el documento original. PDF es una versión del anterior, desarrollada por la compañía Adobe que se usa frecuentemente para compartir documentación en Internet. Ficheros con Información Comprimida Para aliviar las dificultades de transmitir archivos de gran tamaño a través de las redes o ahorrar espacio en disco, se desarrollaron distintos algoritmos de compresión capaces de reducir la cantidad de memoria ocupada por un fichero. Los formatos más usados son ZIP , RAR . Tipos de Fichero según su usoSe pueden clasificar en: Ficheros Permanentes : Contienen los datos relevantes para una aplicación. Su vida es larga y no pueden generarse de forma inmediata. Entre estos se puede distinguir entre: Ficheros maestros : contienen el estado actual de los datos suceptibles de ser modificados en la aplicación. Ejemplo: el fichero de clientes de un banco. Ficheros constantes : contienen datos fijos para la aplicación. Ejemplo: el fichero de códigos postales. Ficheros históricos : contienen datos que fueron actuales en tiempos anteriores. Ejemplo: el fichero de clientes que se han dado de baja. Ficheros Temporales : Contienen datos relevantes para un proceso o programa. Su vida es corta y se utilizan para actualizar los ficheros permanentes. Ficheros de movimientos : almacenan resultados de un programa que han de ser utilizados por otro, dentro de la misma tarea. Ejemplo: el fichero de movimientos de una cuenta bancaria. Ficheros de maniobras : almacenan datos que un programa no puede conservar en memoria principal por falta de espacio. Ejemplo: editores, compiladores y programas de cálculo numérico. Ficheros de resultados : se utilizan para almacenar datos elaborados que van a ser transferidos a un dispositivo de salida. Ejemplo: un fichero de impresión. Denominación de FicherosLos ficheros tienen asignados un nombre a través del cual los usuarios se refieren a ellos. Sin embargo, las reglas de denominación de ficheros difieren dependiendo del SO. Muchos SO dividen el nombre de los ficheros en dos partes separadas por un punto. La primera parte sería el nombre propiamente dicho. La segunda parte se suele denominar extensión y suele aportar información sobre el contenido del fichero. Las reglas básicas de denominación de ficheros en los SO MS-DOS, UNIX y Windows NT son: MS-DOS : El nombre de un fichero se compone de un máximo de 8 caracteres, seguidos, opcionalmente, por un punto y una extensión de tres caracteres como máximo. Las mayúsculas y minúsculas son consideradas iguales. Por ejemplo: archivo12.doc . UNIX : El nombre de un fichero se compone de un máximo de 256 caracteres. Se distinguen las mayúsculas de las minúsculas. Un fichero puede tener más de una extensión. Por ejemplo: image.tar.z . Windows NT : El nombre de un fichero se compone de un máximo de 256 caracteres. Las mayúsculas y minúsculas no son distinguibles y los ficheros pueden tener más de una extensión. En muchos casos, las extensiones son meras convenciones y no relacionadas con el contenido de los ficheros. Por otro lado, muchas aplicaciones requieren que los ficheros que utilizan tengan unas extensiones concretas. Bibliografía Scribd (Ibiza Ales) Diseño de programas. Diseño estructurado. Análisis de transformación y de transacción. Cohesión y acoplamiento.Introducción al Diseño EstructuradoConceptos Generales Sobre el DiseñoDefinición: “Diseño es el proceso de aplicar distintas técnicas y principios con el propósito de definir un dispositivo, proceso, o sistema, con los suficientes detalles como para permitir su realización física”. El objetivo del diseñador es producir un modelo de una entidad que se construirá más adelante. El proceso por el cual se desarrolla el modelo combina: La intuición y los criterios en base a la experiencia de construir entidades similares. Un conjunto de principios y/o heurísticas que guían la forma en la que se desarrolla el modelo. Un conjunto de criterios que permiten discernir sobre la calidad. Un proceso de iteración que conduce finalmente a una representación del diseño final. La actividad de Diseño se dedica a asignar porciones de la especificación estructurada (también conocida como modelo esencial) a procesadores adecuados (sean máquinas o humanos) y a labores apropiadas (o tareas, particiones, etc) dentro de cada procesador. Dentro de cada labor, la actividad de diseño se dedica a la creación de una jerarquía apropiada de módulos de programas y de interfaces entre ellos para implantar la especificación creada en la actividad de análisis. Además, la actividad de diseño se ocupa de la transformación de modelos de datos de entidad/relación en un diseño de base de datos. ¿Qué es Diseño Estructurado?Definición: “Diseño estructurado es el proceso de decidir qué componentes, y la interconexión entre los mismos, para solucionar un problema bien especificado”. El diseño es una actividad que comienza cuando el analista de sistemas ha producido un conjunto de requerimientos funcionales lógicos para un sistema, y finaliza cuando el diseñador ha especificado los componentes del sistema y las relaciones entre los mismos. Frecuentemente analista y diseñador son la misma persona, sin embargo es necesario que se realice un cambio de enfoque mental al pasar de una etapa a la otra. Al abordar la etapa de diseño, la persona debe quitarse el sombrero de analista y colocarse el sombrero de diseñador . Una vez que se han establecido los requisitos del software (en el análisis), el diseño del software es la primera de tres actividades técnicas: diseño , codificación y prueba . Cada actividad transforma la información de forma que finalmente se obtiene un software para computadora válido. En la figura se muestra el flujo de información durante la fase de desarrollo. Los requisitos del sistema, establecidos mediante los modelos de información , funcional y de comportamiento , alimentan el proceso del diseño. Mediante alguna metodología (en nuestro caso, estructurada basada en el flujo de información) se realiza el diseño estructural, procedimiental y de datos. El diseño de datos transforma el modelo del campo de información, creado durante el análisis, en las estructuras de datos que se van a requerir para implementar el software. El diseño estructural define las relaciones entre los principales elementos estructurales del programa. El objetivo principal del diseño estructural es desarrollar una estructura de programa modular y representar las relaciones de control entre los módulos. El diseño procedimental transforma los elementos estructurales en una descripción procedimental del software. El diseño procedimental se realiza después de que se ha establecido la estructura del programa y de los datos. Define los algoritmos de procesamiento necesarios. Concluido el diseño se genera el código fuente y para integrar y validar el software, se llevan a cabo pruebas de testeo. Las fases del diseño, codificación y prueba absorben el 75% o más del coste de la ingeniería del software (excluyendo el mantenimiento). Es aquí donde se toman decisiones que afectarán finalmente al éxito de la implementación del programa y, con igual importancia, a la facilidad de mantenimiento que tendrá el software. Estas decisiones se llevan a cabo durante el diseño del software, haciendo que sea un paso fundamental de la fase de desarrollo. La importancia del diseño del software se pueden sentar con una única palabra: calidad . El diseño es el proceso en el que se asienta la calidad del desarrollo del software. El diseño produce las representaciones del software de las que puede evaluarse su calidad. El diseño sirve como base para todas las posteriores etapas del desarrollo y de la fase de mantenimiento. Sin diseño nos arriesgamos a construir un sistema inestable, un sistema que falle cuando se realicen pequeños cambios, un sistema que pueda ser difícil de probar, un sistema cuya calidad no pueda ser evaluada hasta más adelante en el proceso de ingeniería de software, cuando quede poco tiempo y se haya gastado ya mucho dinero. Objetivos del Diseño EstructuradoEl diseño estructurado, tiende a transformar el desarrollo de software de una práctica artesanal a una disciplina de ingeniería. Eficiencia Mantenibilidad Modificabilidad Flexibilidad Generalidad Utilidad “Diseño” significa planear la forma y método de una solución . Es el proceso que determina las características principales del sistema final, establece los límites en performance y calidad que la mejor implementación puede alcanzar, y puede determinar a qué costos se alcanzará. El diseño se caracteriza usualmente por un gran número de decisiones técnicas individuales. En orden de transformar el desarrollo de software en una disciplina de ingeniería, se debe sistematizar tales decisiones, hacerlas más explícitas y técnicas, y menos implícitas y artesanales. Un ingeniero no busca simplemente una solución, busca la mejor solución, dentro de las limitaciones reconocidas, y realizando compromisos requeridos en el trabajo del mundo real. En orden de convertir el diseño de sistemas de computadoras en una disciplina de ingeniería, previo a todo, debemos definir objetivos técnicos claros para los programas de computadora. Es esencial además comprender las restricciones primarias que condicionan las soluciones posibles. Para realizar decisiones concisas y deliberadas, debemos identificar los puntos de decisión . Finalmente necesitamos una metodología que nos asista en la toma de decisiones . Dadas estas cosas: objetivos, restricciones, decisiones reconocidas, y una metodología efectiva, podemos obtener soluciones de ingeniería, y no artesanales. Diseño estructurado y calidad del software Un concepto importante a clarificar es el de calidad . Desafortunadamente, muchos diseñadores se conforman con un sistema que “funcione” sin reparar en un buen sistema. Una corriente de pensamiento estima que un programa es bueno si sus algoritmos son astutos y no obvios a otro programador; esto refleja la “inteligencia” del programador. Otra escuela de pensamiento asocia calidad con incremento de la velocidad de ejecución y disminución de los requerimientos de memoria central. Estos son aspectos de un concepto más amplio: eficiencia . En general, se busca diseños que hagan un uso inteligente de los recursos . Estos recursos no incluyen solamente procesador y memoria, también incluyen almacenamiento secundario, tiempo de periféricos de entrada/salida, tiempo de líneas de teleproceso, tiempo de personal y más. Otra medida de calidad es la confiabilidad . Es importante notar que si bien la confiabilidad del software puede ser vista como un problema de depuración de errores en los programas, es también un problema de diseño. La confiabilidad se expresa en como MTBF (mean time between fairules: tiempo medio entre fallas). Un concepto muy relacionado a la confiabilidad y de suma importancia es el de mantenibilidad . Podemos definir la mantenibilidad como: donde: MBTF: tiempo medio entre fallas. MTTR: tiempo medio de reparación (mean time to repair). Diremos que un sistema es mantenible si permite la detección, análisis, rediseño y corrección de errores fácilmente. En tanto la mantenibilidad afecta la viabilidad del sistema en un entorno relativamente constante, la modificabilidad influye en los costos de mantener un sistema viable en condiciones de cambio de requerimientos. La modificabilidad es la posibilidad de realizar modificaciones y extensiones a partes del sistema, o agregar nuevas partes con facilidad (no corrección de errores). En estudios realizados se determinó que las organizaciones abocadas al procesamiento de datos invierten aproximadamente un 50% del presupuesto en mantenimiento de los sistemas, involucrando esto corrección de errores y modificaciones, razón por la cual la mantenibilidad y la modificabilidad son dos objetivos primarios en el diseño de software. La flexibilidad representa la facilidad de que el mismo sistema pueda realizar variaciones sobre una misma temática, sin necesidad de modificaciones. La generalidad expresa el alcance sobre un determinado tema. Flexibilidad y generalidad son dos objetivos importantes en el diseño de sistemas del tipo de propósitos generales. La utilidad o facilidad de uso es un factor importante que influye en el éxito del sistema y su aceptación por parte del usuario. Un sistema bien diseñado pero con interfaces muy “duras” tiende a ser resistido por los usuarios. Finalmente diremos que eficiencia, mantenibilidad, modificabilidad, flexibilidad, generalidad y utilidad, con componentes de la calidad objetiva de un sistema. En términos simples también diremos que nuestro objetivo primario es obtener sistemas de costo mínimo . Es decir, es nuestro interés obtener sistemas económicos para desarrollar, operar, mantener y modificar. Restricciones, compromisos y decisiones del DiseñoPodemos ver los objetivos técnicos del diseño como constituyendo una “función objetivo” de un problema de optimización, la cual se desea maximizar, sujeta a ciertas restricciones. Como regla, las restricciones sobre un proceso de diseño de un sistema, caen en dos categorías: restricciones de desarrollo y restricciones operacionales . Las restricciones de desarrollo son limitaciones al consumo de recursos durante el período de desarrollo, y pueden ser expresadas en términos generales o descomponerla en sus partes como ser tiempo de máquina y horas/hombre. Dentro de las restricciones de desarrollo, entran también las restricciones de planificación . Estas se refieren a metas y plazos a ser cumplidos (“el módulo X debe terminarse para Febrero”). Las restricciones operacionales pueden ser expresadas en términos técnicos, como ser máximo tamaño de memoria disponible, máximo tiempo de respuesta aceptable, etc. El carácter de muchas decisiones de diseño no fija límites rígidos, si no un intervalo de tolerancia, dentro del cual el diseñador puede moverse a costa de variaciones en otros aspectos del sistema. Por ejemplo se puede priorizar eficiencia, en detrimento de facilidad de mantenimiento, o velocidad de ejecución contra tamaño de memoria utilizada. La esencia del diseño en el mundo real y las decisiones inherentes al mismo es obtener una solución de compromiso . El diseño total es el resultado acumulativo de un gran número de decisiones técnicas incrementales. Principios utilizados por el Diseño EstructuradoAbstracción La noción psicológica de abstracción permite concentrarse en un problema al mismo nivel de generalización, independientemente de los detalles irrelevantes de bajo nivel. El uso de la abstracción también permite trabajar con conceptos y términos que son familiares al entorno del problema, sin tener que transformarlos a una estructura no familiar. Cada paso de un proceso de ingeniería de software es un refinamiento del nivel de abstracción de la solución de software. Conforme nos movemos por diferentes niveles de abstracción, trabajamos para crear abstracciones de datos y de procedimientos. Una abstracción procedural es una determinada secuencia de instrucciones que tienen una función limitada y específica. Una abstracción de datos es una determina colección de datos que describen un objeto. Rumbaugh: O.O. Modeling and DesignLa abstracción es el exámen selectivo de ciertos aspectos de un problema . El objetivo de la abstracción es aislar aquellos aspectos que son importantes para algún propósito y suprimir aquellos aspectos que no son importantes. La abstracción debe realizarse siempre con un propósito, ya que el propósito determina que es y que no es relevante. Muchas abstracciones son posibles sobre una misma cosa, dependiendo de cual sea su propósito. Alan Cameron Will (Object Expert Jan/Feb 1996)La abstracción, para mí, es cercana a palabras como “teórico”, “esotérico”, “académico”, e “impráctico”. Pero en un sentido en particular, significa la separación de los aspectos más importantes de un determinado problema, del detalle. Este el el único camino que tengo para abordar con mi mente finita cualquier tema complejo . Refinamiento sucesivo El refinamiento sucesivo es una primera estrategia de diseño descendente propuesta por Niclaus Wirth. La arquitectura de un programa se desarrolla en niveles sucesivos de refinamiento de los detalles procedimentales. Se desarrolla una jerarquía descomponiendo una declaración macroscópica de una función de una forma sucesiva, hasta que se llega a las sentencias del lenguaje de programación. Modularidad La arquitectura implica modularidad, el software se divide en componentes con nombres y ubicaciones determinados, que se denominan módulos , y que se integran para satisfacer los requisitos del problema. Arquitectura del software La arquitectura del software se refiere a dos características importantes del software de computadoras: la estructura jerárquica de los componentes procedimientales (módulos) la estructura de datos Jerarquía de control La jerarquía de control , también denominada estructura de programa , representa la organización (frecuentemente jerárquica) de los componentes del programa (módulos) e implica una jerarquía de control. No representa aspectos procedimentales del software, tales como secuencias de procesos, o la repetición de operaciones. Estructura de datos La estructura de datos es una representación de la relación lógica existente entre los elementos individuales de datos. Debido a que la estructura de la información afectará invariablemente al diseño procedimental final, la estructura de datos es tan importante como la estructura del programa en la representación de la arquitectura del software. Procedimientos del software La estructura del programa define la jerarquía de control, independientemente de las decisiones y secuencias de procesamiento. El procedimiento del software se centra sobre los detalles de procesamiento de cada módulo individual. El procedimiento debe proporcionar una especificación precisa del procesamiento, incluyendo la secuencia de sucesos, los puntos concretos de decisiones, la repetición de operaciones, e incluso la organización/estructura de los datos. Ocultamiento de la información El principio de ocultamiento de la información sugiere que los módulos se han de caracterizar por decisiones de diseño que los oculten unos a otros. Los módulos deben especificarse y diseñarse de forma que la información (procedimientos y datos) contenida dentro de un módulo sea accesible a otros módulos únicamente a través de las interfaces formales establecidas para cada módulo. Conceptos Básicos de Diseño EstructuradoEstrategia del Diseño EstructuradoCuando se trata con un problema de diseño de reducida envergadura, por ejemplo un sistema que pueda ser desarrollado en un par de semanas,no se tienen mayores problemas, y el desarrollador puede tener todos los elementos del problema “en mente” a la vez. Sin embargo cuando se trabaja en proyectos de gran escala, es difícil que una sola persona sea capaz de llevar todas las tareas y tener en mente todos los elementos a la vez. El diseño exitoso se basa en un viejo principio conocido desde los días de Julio César: Divide y conquistarás . Específicamente, diremos que el costo de implementación de un sistema de computadora podrá minimizarse cuando pueda separarse en partes: manejablemente pequeñas solucionables separadamente Por supuesto la interpretación de manejablemente pequeña varía de persona en persona. Por otro lado muchos intentos de particionar sistemas en pequeñas partes arribaron incrementos en los tiempos de implementación. Esto se debe fundamentalmente al segundo punto: solucionables separadamente. En muchos sistemas para implementar la parte A, debemos conocer algo sobre la B, y para implementar algo de B, debemos conocer algo de C. De manera similar, podemos decir que el costo de mantenimiento puede ser minimizado cuando las partes de un sistema son: fácilmente relacionables con la aplicación manejablemente pequeñas corregibles separadamente Muchas veces la persona que realiza la modificación no es quien diseñó el sistema. Es importante que las partes de un sistema sean manejablemente pequeñas en orden de simplificar el mantenimiento. Un trabajo de encontrar y corregir un error en una “pieza” de código de 1000 líneas es muy superior a hacerlo con piezas de 20 líneas. No solo disminuye el tiempo de localizar la falla sino que si la modificación es muy engorrosa, puede reescribirse la pieza completamente. Este concepto de “módulos descartables” ha sido utilizado con éxito muchas veces. Por otra parte, para minimizar los costos de mantenimiento debemos lograr que cada pieza sea independiente de otra. En otras palabras debemos ser capaces de realizar modificaciones al módulo A sin introducir efectos indeseados en el módulo B. Finalmente, diremos que el costo de modificación de un sistema puede minimizarse si sus partes son: fácilmente relacionables con la aplicación modificables separadamente En resumen, podemos afirmar lo siguiente: los costos de implementación, mantenimiento y modificación, generalmente serán minimizados cuando cada pieza del sistema corresponda a exactamente una pequeña, bien definida pieza del dominio del problema, y cada relación entre las piezas del sistema corresponde a relaciones entre piezas del dominio del problema . En la siguiente figura apreciamos este concepto. Particionamiento y OrganizaciónUn buen diseño estructurado es un ejercicio de particionamiento y organización de los componentes de un sistema. Entenderemos por particionamiento, la subdivisión de un problema en subproblemas más pequeños, de tal forma que cada subproblema corresponda a una pieza del sistema. La cuestión es: ¿Dónde y cómo debe dividirse el problema? ¿Qué aspectos del problema deben pertenecer a la misma pieza del sistema, y cuales a distintas piezas? El diseño estructurado responde estas preguntas con dos principios básicos: Partes del problema altamente interrelacionadas deberán pertenecer a la misma pieza del sistema . Partes sin relación entre ellas, deben pertenecer a diferentes piezas del sistema sin relación directa . Otro aspecto importante del diseño estructurado es la organización del sistema. Debemos decidir como se interrelacionan las partes, y que parte está en relación con cual. El objetivo es organizar el sistema de tal forma que no existan piezas más grandes de lo estrictamente necesario para resolver los aspectos del problema que ella abarca. Igualmente importante, es el evitar la introducción de relaciones en el sistema, que no existe en el dominio del problema. El concepto de Cajas NegrasUna caja negra es un sistema (o un componente) con entradas conocidas, salidas conocidas, y generalmente transformaciones conocidas, pero del cual no se conoce el contenido en su interior. En la vida diaria existe innumerable cantidad de ejemplos de uso cotidiano: una radio, un televisor, un automóvil, son cajas negras que usamos a diario sin conocer (en general) como funciona en su interior. Solo conocemos como controlarlos (entradas) y las respuestas que podemos obtener de los artefactos (salidas). El concepto de caja negra utiliza el principio de abstracción . Este concepto es de suma utilidad e importancia en la ingeniería en general, y por ende en el desarrollo de software. Lamentablemente muchas veces para poder hacer un uso efectivo de determinado módulo, el diseñador debe revisar su contenido ante posibles contingencias como ser comportamientos no deseados ante determinados valores. Por ejemplo es posible que una rutina haya sido desarrollada para aceptar un determinado rango de valores y falla si se la utiliza con valores fuera de dicho rango, o produce resultados inesperados. Una buena documentación en tales casos, es de utilidad pero no transforma al módulo en una verdadera caja negra. Podríamos hablar en todo caso de “cajas blancas”. Los módulos de programas de computadoras pueden variar en un amplio rango de aproximación al ideal de caja negra. En la mayoría de los casos podemos hablar de “cajas grises”. La Estructura de los Programas de ComputadoraDiagramas de Flujo y Diagramas de EstructuraNormalmente los procedimientos se representan con diagramas de flujo (no confundir con diagramas de flujo de datos) los cuales modelan la secuencia de operaciones que realiza el programa a través del tiempo. Un diagrama de estructura en cambio no modela la secuencia de ejecución sino la jerarquía de control existente entre los módulos que conforman el programa, independientemente del factor tiempo. Existe un módulo raíz de máximo nivel, del cual dependen los demás, en una estructura arborescente. Notación de los Diagramas de Flujo de Control Notación de los Diagramas de Estructura Ejemplo comparativo entre Diagramas de Procesamiento y de Estructura AcoplamientoIntroducciónMuchos aspectos de la modularización pueden ser comprendidos solo si se examinan módulos en relación con otros. En principio veremos el concepto de independencia : diremos que dos módulos son totalmente independientes si ambos pueden funcionar completamente sin la presencia del otro. Esto implica que no existen interconexiones entre los módulos, y que se tiene un valor cero en la escala de “dependencia”. En general veremos que a mayor número de interconexiones entre dos módulos, se tiene una menor independencia. El concepto de independencia funcional es una derivación directa del de modularidad y de los conceptos de abstracción y ocultamiento de la información. La cuestión aquí es: ¿cuánto debe conocerse acerca de un módulo para poder comprender otro módulos? Cuanto más debamos conocer acerca del módulo B para poder comprender el módulo A, menos independientes serán A y B. La simple cantidad de conexiones entre módulos, no es una medida completa de la independencia funcional. La dependencia funcional se mide con dos criterios cualitativos: acoplamiento y cohesión . Estudiaremos en principio el primero de ellos. Módulos altamente “acoplados” estarán unidos por fuertes interconexiones, módulos débilmente acoplados tendrán pocas y débiles interconexiones, en tanto que los módulos “desacoplados” no tendrán interconexiones entre ellos y serán independientes. El acoplamiento es un concepto abstracto que nos indica el grado de interdependencia entre módulos. En la práctica podemos materializarlo como la probabilidad de que en la codificación, depuración, o modificación de un determinado módulo, el programador necesite tomar conocimiento acerca de partes de otro módulo. Si dos módulos están fuertemente acoplados, existe una alta probabilidad de que el programador necesite conocer uno de ellos en orden de intentar realizar modificaciones al otro. Claramente, el costo total del sistema se verá fuertemente influenciado por el grado de acoplamiento entre módulos. Factores que influencian el AcoplamientoLos cuatro factores principales que influyen en el acoplamiento entre módulos son: Tipo de conexión entre módulos : los sistemas normalmente conectados, tienen menor acoplamiento que aquellos que tienen conexiones patológicas. Complejidad de la interface : está determinada por la cantidad, accesibilidad y estructura de la información que define la interface. Tipo de flujo de información en la conexión : los sistemas con acoplamiento de datos tienen menor acoplamiento que los sistema con acoplamiento de control y estos a su vez menos que los que tienen acoplamiento híbrido. Momento en que se produce el ligado de la Conexión : conexiones ligadas a referentes fijos en tiempo de ejecución, resultan con menor acoplamiento que cuando el ligado tiene lugar en tiempo de carga, el cual tiene a su vez menor acoplamiento que cuando el ligado se realiza en tiempo de ligado (link-edición), el cual tiene menos acoplamiento que el que se realiza en tiempo de compilación, todos los que a su vez tiene menos acoplamiento que cuando el ligado se realiza en tiempo de codificación. Acoplamiento en Entorno Común (common-environment coupling)Siempre que dos o más módulos interactúan con un entorno de datos común, se dice que dichos módulos están en acoplamiento por entorno común . Ejemplos de entorno común pueden ser áreas de datos globales como la DATA DIVISION de COBOL, un archivo en disco. El acoplamiento de entorno común es una forma de acoplamiento de segundo orden, distinto de los tratados anteriormente. La severidad del acoplamiento dependerá de la cantidad de módulos que acceden simultáneamente al entorno común. En el caso extremo de solo dos módulos donde uno utiliza como entrada los datos generados por el otro hablaremos de un acoplamiento de entrada/salida . El punto es que el acoplamiento por entorno común no es necesariamente malo y deba ser evitado a toda costa. Por el contrario existen ciertas circunstancias en que es una opción válida. DesacoplamientoEl concepto de acoplamiento invita a un concepto recíproco: desacoplamiento . Desacoplamiento es cualquier método sistemático o técnica para hacer más independientes a los módulos de un programa. Cada tipo de acoplamiento generalmente sugiere un método de desacoplamiento. Por ejemplo, el acoplamiento causado por ligado, puede desacoplarse cambiando los parámetros apropiados. El desacoplamiento desde el punto de vista funcional, rara vez puede realizarse, excepto en los comienzos de la fase del diseño. Como regla general, una disciplina de diseño que favorezca el acoplamiento de entrada/salida y el acoplamiento de control por sobre el acoplamiento por contenido y el acoplamiento híbrido, y que busque limitar el alcance del acoplamiento por entorno común es el enfoque más efectivo. Otras técnicas para reducir el acoplamiento son: Convertir las referencias implícitas en explícitas. Lo que puede verse con mayor facilidad es más fácil de comprender. Estandarización de las conexiones. Uso de “buffers” para los elementos comunicados en una conexión. Si un módulo puede ser diseñado desde el comienzo asumiendo que un buffer mediará cada corriente de comunicación, las cuestiones temporización, velocidad, frecuencia, etc, dentro de un módulo no afectarán al diseño de otros. Localización. Utilizado para reducir el acoplamiento por entorno común. Consiste en dividir el área común en regiones para que los módulos solo tengan acceso a aquellos datos que les son de su estricta incumbencia. CohesiónIntroducción: Relación FuncionalHemos visto que la determinación de módulos en un sistema no es arbitraria. La manera en la cual dividimos físicamente un sistema en piezas (particularmente en relación con la estructura del problema) puede afectar significativamente la complejidad estructural del sistema resultante, así como el número total de referencias intermodulares. Adaptar el diseño del sistema a la estructura del problema (o estructura de la aplicación, o dominio del problema) es una filosofía de diseño sumamente importante. A menudo encontramos que elementos de procesamiento del dominio del problema altamente relacionados, son trasladados en código altamente interconectado. Las estructuras que agrupan elementos del problema altamente interrelacionados, tienden a ser modularmente efectivas. Imaginemos que tengamos una magnitud para medir el grado de relación funcional existente entre pares de módulos. En términos de tal medida, diremos que el sistema más modularmente efectivo será aquel cuya suma de relación funcional entre pares de elementos que pertenezcan a diferentes módulos sea mínima. Entre otras cosas, esto tiende a minimizar el número de conexiones intermodulares requeridas y el acoplamiento intermodular. Esta relación funcional intramodular se conoce como cohesión . La cohesión es la medida cualitativa de cuan estrechamente relacionados están los elementos internos de un módulo. Otros términos utilizados frecuentemente son “fuerza modular”, “ligazón” y “funcionalidad”. En la práctica un elemento de procesamiento simple aislado, puede estar funcionalmente relacionado en diferentes grados a otros elementos. Como consecuencia, diferentes diseñadores, con diferentes “visiones” o interpretaciones de un mismo problema, pueden obtener diferentes estructuras modulares con diferentes niveles de cohesión y acoplamiento. A esto se suma el inconveniente de que muchas veces es difícil evaluar el grado de relación funcional de un elemento respecto de otro. La cohesión modular puede verse como el cemento que amalgama junto a los elementos de procesamiento dentro de un mismo módulo. Es el factor más crucial en el diseño estructurado, y el de mayor importancia en un diseño modular efectivo. Este concepto representa la técnica principal que posee un diseñador para mantener su diseño lo más semánticamente próximo al problema real, o dominio del problema. Claramente los conceptos de cohesión y acoplamiento están íntimamente relacionados. Un mayor grado de cohesión implica uno menor de acoplamiento. Maximizar el nivel de cohesión intramodular en todo el sistema resulta en una minimización del acoplamiento intermodular. Niveles de CohesiónDiferentes principios asociativos fueron desenvolviéndose a través de los años por medio de la experimentación, argumentos teóricos, y la experiencia práctica de muchos diseñadores. Existen siete niveles de cohesión distinguibles por siete principios asociativos. Estos se listan a continuación en orden creciente del grado de cohesión, de menor a mayor relación funcional: Cohesión Casual (la peor) Cohesión Lógica (sigue a la peor) Cohesión Temporal (de moderada a pobre) Cohesión de Procedimiento (moderada) Cohesión de Comunicación (moderada a buena) Cohesión Secuencial Cohesión Funcional (la mejor) Podemos visualizar el grado de cohesión como un espectro que va desde un máximo a un mínimo. Cohesión Casual (la peor) La cohesión casual ocurre cuando existe poca o ninguna relación entre los elementos de un módulo. La cohesión casual establece un punto cero en la escala de cohesión. Es muy difícil encontrar módulos puramente casuales. Puede aparecer como resultado de la modularización de un programa ya escrito, en el cual el programador encuentra una determinada secuencia de instruccciones que se repiten de forma aleatoria, y decide por lo tanto agruparlas en una rutina. Otro factor que influenció muchas veces la confección de módulos casualmente cohesivos, fue la mala práctica de la programación estructurada, cuando los programadores mal entendían que modularizar consistía en cambiar las sentencias GOTO por llamadas a subrutinas. Finalmente diremos que si bien en la práctica es difícil encontrar módulos casualmente cohesivos en su totalidad, es común que tengan elementos casualmente cohesivos. Tal es el caso de operaciones de inicialización y terminación que son puestas juntas en un módulo superior. Debemos notar que si bien la cohesión casual no es necesariamente perjudicial (de hecho es preferible un casualmente cohesivo a uno lineal), dificulta las modificaciones y mantenimiento del código. Cohesión Lógica (sigue a la peor) Los elementos de un módulo están lógicamente asociados si puede pensarse en ellos como pertenecientes a la misma clase lógica de funciones, es decir, aquellas que pueden pensarse como juntas lógicamente. Por ejemplo, se puede combinar en un módulo simple todos los elementos de procesamiento que caen en la clase de “entradas”, que abarca todas las operaciones de entrada. Podemos tener un módulo que lea desde consola una tarjeta con parámetros de control, registros con transacciones erróneas de un archivo en cinta, registros con transacciones válidas de otro archivo en cinta, y los registros maestros anterior de un archivo en disco. Este módulo que podría llamarse “Lecturas”, y que agrupa todas las operaciones de entrada, es lógicamente cohesivo. La cohesión lógica es más fuerte que la casual, debido a que representa un mínimo de asociación entre el problema y los elementos del módulo. Sin embargo podemos ver que un módulo lógicamente cohesivo no realiza una función específica, sino que abarca una serie de funciones. Cohesión Temporal (de moderada a pobre) Temporal cohesión significa que todos los elementos de procesamiento de una colección ocurren en el mismo periodo de tiempo durante la ejecución del sistema. Debido a que dicho procesamiento debe o puede realizarse en el mismo periodo de tiempo, los elementos asociados temporalmente pueden combinarse en un único módulo que los ejecute a la misma vez. Existe una relación entre cohesión lógica y la temporal, sin embargo, la primera no implica una relación de tiempo entre los elementos de procesamiento. La cohesión temporal es más fuerte que la cohesión lógica, ya que implica un nivel de relación más: el factor tiempo. Sin embargo la cohesión temporal aún es pobre en nivel de cohesión y acarrea inconvenientes en el mantenimiento y modificación del sistema. Un ejemplo común de cohesión temporal son las rutinas de inicialización (start-up) comúnmente encontradas en la mayoría de los programas, donde se leen parámetros de control, se abren archivos, se inicializan variables contadores y acumuladores, etc. Cohesión de Procedimiento (moderada) Elementos de procesamiento relacionados proceduralmente son elementos de una unidad procedural común. Estos se combinan en un módulo de cohesión procedural. Una unidad procedural común puede ser un proceso de iteración (loop) y de decisión, o una secuencia lineal de pasos. En este último caso la cohesión es baja y es similar a la cohesión temporal, con la diferencia que la cohesión temporal no implica una determinada secuencia de ejecución de los pasos. Al igual que en los casos anteriores, para decir que un módulo tiene solo cohesión procedural, los elementos de procesamiento deben ser elementos de alguna iteración, decisión, o secuencia, pero no deben estar vinculados con ningún principio asociativo de orden superior. La cohesión procedural asocia elementos de procesamiento sobre la base de sus relaciones algorítmicas o procedurales. Este nivel de cohesión comúnmente se tiene como resultado de derivar una estructura modular a partir de modelos de procedimiento como ser diagramas de flujo, o diagramas Nassi-Shneiderman. Cohesión de Comunicación (moderada a buena) Ninguno de los niveles de cohesión discutidos previamente están fuertemente vinculados a una estructura de problema en particular. Cohesión de Comunicación es el menor nivel en el cual encontramos una relación entre los elementos de procesamiento que es intrínsecamente dependiente del problema. Decir que un conjunto de elementos de procesamiento están vinculados por comunicación significa que todo los elementos operan sobre el mismo conjunto de datos de entrada o de salida . En el diagrama de la figura podemos observar que los elementos de procesamiento 1, 2 y 3 están asociados por comunicación sobre la corriente de datos de entrada, en tanto que 2, 3 y 4 se vinculan por los datos de salida. Los diagramas de flujo de datos (DFD) son un medio objetivo para determinar si los elementos en un módulo están asociados por comunicación. Las relaciones por comunicación presentan un grado de cohesión aceptable. La cohesión por comunicación es común en aplicaciones comerciales. Ejemplos típicos pueden ser: un módulo que imprima o grabe un archivo de transacciones un módulo que reciba datos de diferentes fuentes y los transforme y ensamble en una línea de impresión Cohesión Secuencial El siguiente nivel de cohesión en la escala es la asociación secuencial . En ella, los datos de salida (resultados) de un elemento de procesamiento sirven como datos de entrada al siguiente elemento de procesamiento. En términos de un diagrama de flujo de datos de un problema, la cohesión secuencial combina una cadena lineal de sucesivas transformaciones de datos. Este es claramente un principio asociativo relacionado con el dominio del problema. Cohesión Funcional (la mejor) En el límite superior del espectro de relación funcional encontramos la cohesión funcional. En un módulo completamente funcional, cada elemento de procesamiento, es parte integral de, y esencial para, la realización de una función simple. En términos prácticos podemos decir que cohesión funcional es aquella que no es secuencial, por comunicación, por procedimiento, temporal, lógica o casual. Los ejemplos más claros y comprensibles provienen del campo de las matemáticas. Un módulo para realizar el cálculo de raíz cuadrada ciertamente será altamente cohesivo, y probablemente, completamente funcional. Es improbable que haya elementos superfluos más allá de los absolutamente esenciales para realizar la función matemática, y es improbable que elementos de procesamiento puedan ser agregados sin alterar el cálculo de alguna forma. En contraste, un módulo que calcule la raíz cuadrada y coseno, es improbable que sea enteramente funcional (deben realizarse dos funciones ambiguas). En adición a estos ejemplos matemáticos obvios, usualmente podemos reconocer módulos funcionales que son elementales en naturaleza. Un módulo llamado LEER-REGISTRO-MAESTRO o TRATAR-TRANS-TIPO3, presumiblemente serán funcionalmente cohesivos, en cambio TRATAR-TODAS-TRANS presumiblemente realizará más de una función y será lógicamente cohesivo. Criterios para establecer el grado de cohesiónUna técnica útil para determinar si un módulo está acotado funcionalmente, es escribir una frase que describa la función (propósito) del módulo y luego examinar dicha frase. Puede hacerse la siguiente prueba: Si la frase resulta ser una sentencia compuesta, contiene una coma, o contiene más de un verbo, probablemente el módulo realiza más de una función: por tanto, probablemente tienen vinculación secuencial o de comunicación. Si la frase contiene palabras relativas al tiempo, tales como “primero”, “a continuación”, “entonces”, “después”, “cuando”, “al comienzo”, etc, entonces probablemente el módulo tiene una vinculación secuencial o temporal. Si el predicado de la frase no contiene un objeto específico sencillo a continuación del verbo, probablemente el módulo esté acotado lógicamente. Por ejemplo editar todos los datos tiene una vinculación lógica; editar sentencia fuerte puede tener vinculación funcional. Palabras tales como “inicializar”, “limpiar”, etc, implican vinculación temporal. Los módulos acotados funcionalmente siempre se pueden describir en función de sus elementos usando una sentencia compuesta. Pero si no se puede evitar el lenguaje anterior, siendo aún una descripción completa de la función del módulo, entonces probablemente el módulo no esté acotado funcionalmente. Es importante notar que no es necesario determinar el nivel preciso de cohesión. En su lugar, lo importante es intentar conseguir una cohesión alta y saber reconocer la cohesión baja, de forma que se pueda modificar el diseño del software para que disponga de una mayor independencia funcional. Árbol de valuación Análisis de TransformaciónIntroducción El Análisis de Transformación, o diseño centrado en la transformación , es una estrategia para derivar diseños estructurados que son bastante buenos (con respecto a su modularidad) y que necesitan solo una modesta reestructuración para llegar al diseño final. Es una forma particular de la estrategia descendente (top-down), que toma ventaja de la perspectiva global. Aplicado rigurosamente, el análisis de transacción conduce a estructuras que son altamente factorizadas. Produce un número variable de módulos en los niveles intermedios de la jerarquía, los cuales representan composición de funciones básicas. Siempre se trata de evitar que los módulos intermedios realicen cualquier “trabajo” excepto el de control y coordinación de sus subordinados. El propósito de la estrategia es el de identificar las funciones de procesamiento primarias del sistema, las entradas de alto nivel de dichas funciones, y las salidas de alto nivel. Se crean módulos de alto nivel dentro de la jerarquía que realizan cada una de estas tareas: creación de entradas de alto nivel, transformación de entradas en salidas de alto nivel, y procesamiento de dichas salidas. El análisis de transformación es un modelo de flujo de información más que un modelo procedural. La estrategia de análisis de transformación consiste de cuatro pasos principales: Plantear el problema como un diagrama de flujo de datos. Identificar los elementos de datos aferentes y eferentes. Datos Aferentes .Son aquellos elementos de datos de alto nivel que habiendo sido removidos de sus entradas físicas, todavía pueden considerarse entradas al sistema. Datos Eferentes .Son elementos de datos que desde sus salidas física a través de los flujos, hasta que no puedan seguir siendo considerados como datos de salida del sistema. Factorización del primer nivel. Factorización de las ramas aferente, eferente y de transformación. Análisis de TransacciónIntroducciónEn el anterior capítulo exploramos la estrategia del análisis de transformación como la estrategia principal para el diseño de programas y sistemas bien estructurados. En verdad, el análisis de transformación, servirá de guía en el diseño de la mayoría de los sistemas. Sin embargo hay numerosas situaciones en las cuales estrategias adicionales pueden utilizarse para suplementar, y aún reemplazar, el enfoque básico del análisis de transformación. Una de estas estrategias suplementarias principales se conoce como Análisis de Transacción . El análisis de transacción es sugerido por un DFD del siguiente tipo: En este DFD existe una transformación que bifurca la corriente de datos de entrada en varias corrientes de salida discretas. En muchos sistemas tal transformación puede ocurrir dentro de la transformación central . En otros, podremos encontrarla tanto en las ramas aferentes como eferentes del diagrama de estructura. La frase análisis de transacción sugiere que construiremos un sistema alrededor del concepto de “transacción”, y para muchos la palabra transacción está asociada con sistemas administrativos. Esto si bien es cierto, es común encontrar centros de transacción en los sistemas administrativos, también pueden encontrarse en otro tipo de sistemas como los de tiempo real, aplicaciones de ingeniería, etc. Un factor importante es como definimos el término transacción. En el sentido más general podemos decir: Una transacción es cualquier elemento de datos, control, señal, evento, o cambio de estado, que causa, dispara o inicia alguna acción o secuencia de acciones. Acorde a esta definición, un gran número de situaciones encontradas en aplicaciones de procesamiento de datos comunes pueden ser consideradas transacciones. Por ejemplo cualquiera de los siguientes casos pueden considerarse transacciones: El operador presiona el botón de inicio de un dispositivo de entrada. Algún tipo de datos de entrada que designe un ingreso en el inventario. Un carácter de escape desde una terminal, indicando la necesidad e un procesamiento especial. Una interrupción de hardware ante un índice fuera de los rangos definidos dentro de un programa de aplicación. Un cuelgue o descuelgue de teléfono para un sistema de control de llamadas telefónicas. Bibliografía Universidad Tecnológica Nacional – F.R.R. Pruebas. Planificación y documentación. Utilización de datos de prueba. Pruebas de software, hardware, procedimientos y datos.Documentación y Pruebas en el Desarrollo Tradicional del SoftwareDocumentación y Desarrollo de SoftwareEn general se habla mucho de la documentación, pero no se hace, no se le asigna presupuesto, no se la mantiene y casi nunca está al día en los proyectos de desarrollo de software. Lo importante es la disponibilidad de la documentación que se necesita en el momento en que se la necesita. Muchas veces se hace porque hay que hacerla y se escribe, con pocas ganas, largos textos, a la vez que se está convencido de estar haciendo un trabajo inútil. A veces se peca por exceso y otras por defecto. Ocurre mucho en la Web y con productos RAD. En ocasiones se olvida que el mantenimiento también debe llegar a la documentación. La documentación se suele clasificar en función de las personas o grupos a los cuales está dirigida: Documentación para los desarrolladores. Documentación para los usuarios. Documentación para los administradores o soporte técnico. La documentación para desarrolladores es aquélla que se utiliza para el propio desarrollo del producto y, sobre todo, para su mantenimiento futuro. Se documenta para comunicar estructura y comportamiento del sistema o de sus partes, para visualizar y controlar la arquitectura del sistema, para comprender mejor el mismo y para controlar el riesgo, entre otras cosas. Obviamente, cuanto más complejo es el sistema, más importante es la documentación. En este sentido, todas las fases de un desarrollo deben documentarse: requerimientos, análisis, diseño, programación, pruebas, etc. Una herramienta muy útil en este sentido es una notación estándar de modelado, de modo que mediante ciertos diagramas se puedan comunicar ideas entre grupos de trabajo. Hay decenas de notaciones, tanto estructuradas como orientadas a objetos. Un caso particular es el de UML. De todas maneras, los diagramas son muy útiles, pero siempre y cuando se mantengan actualizados, por lo que más vale calidad que cantidad. La documentación para desarrolladores a menudo es llamada modelo , pues es una simplificación de la realidad para comprender mejor el sistema como un todo. Otro aspecto a tener en cuenta cuando se documenta o modela, es el del nivel de detalle. Así como cuando construimos planos de un edificio podemos hacer planos generales, de arquitectura, de instalaciones y demás, también al documentar el software debemos cuidar el nivel de detalle y hacer diagramas diferentes en función del usuario de la documentación, concentrándonos en un aspecto a la vez. De toda la documentación para los desarrolladores, nos interesa especialmente en esta obra aquella que se utiliza para documentar la programación, y en particular hemos analizado la que se usa para documentar desarrollos orientados a objetos. La documentación para usuarios es todo aquello que necesita el usuario para la instalación, aprendizaje y uso del producto. Puede consistir en guías de instalación, guía del usuario, manuales de referencia y guía de mensajes. En el caso de los usuarios que son programadores, esta documentación se debe acompañar con ejemplos de uso recomendados o de muestra y una reseña de efectos no evidentes de las bibliotecas. Más allá de todo esto, debemos tener en cuenta que la estadística demuestra que los usuarios no leen los manuales a menos que nos les quede otra opción. Las razones pueden ser varias, pero un análisis de la realidad muestra que se recurre a los manuales solamente cuando se produce un error o se desconoce cómo lograr algo muy puntual, y recién cuando la ayuda en línea no satisface las necesidades del usuario. Por lo tanto, si bien es cierto que debemos realizar manuales, la existencia de un buen manual nunca nos libera de hacer un producto amigable para el usuario, que incluso contenga ayuda en línea. Es incluso deseable proveer un software tutorial que guíe al usuario en el uso del sistema, con apoyo multimedia, y que puede llegar a ser un curso on-line. Buena parte de la documentación para los usuarios puede empezar a generarse desde que comienza el estudio de requisitos del sistema. Esto está bastante en consonancia con las ideas de extreme programming y con metodologías basadas en casos de uso. La documentación para administradores o soporte técnico, a veces llamada manual de operaciones, contiene toda la información sobre el sistema terminado que no hace al uso por un usuario final. Es necesario que tenga una descripción de los errores posibles del sistema, así como los procedimientos de recuperación. Como esto no es algo estático, pues la aparición de nuevos errores, problemas de compatibilidad y demás nunca se puede descartar, en general el manual de operaciones es un documento que va engrosándose con el tiempo. Las Pruebas en el Desarrollo de SoftwareCalidad, errores y pruebas La calidad no es algo que se pueda agregar al software después de desarrollado si no se hizo todo el desarrollo con la cantidad en mente. Muchas veces parece que el software de calidad es aquel que brinda lo que se necesita con adecuada velocidad de procesamiento. En realidad, es mucho más que eso. Tiene que ver con la corrección, pero también con usabilidad, costo, consistencia, confiabilidad, compatibilidad, utilidad, eficiencia y apego a los estándares. Todos estos aspectos de la calidad pueden ser objeto de tests o pruebas que determinen el grado de calidad. Incluso la documentación para el usuario debe ser probada. Como en todo proyecto de cualquier índole,siempre se debe tratar que las fallas sean mínimas y poco costosas, durante todo el desarrollo. Y además, es sabido que cuanto más tarde se encuentra una falla, más caro resulta eliminarla. Es claro que si un error es descubierto en la mitad del desarrollo de un sistema, el costo de su corrección será mucho menor al que se debería enfrentar en caso de descubrirlo con el sistema instalado y en funcionamiento. Desde el punto de vista de la programación,nos interesa la ausencia de errores (corrección), la confiabilidad y la eficiencia. Dejando de lado las dos últimas, nos concentraremos en este capítulo en las pruebas que determinan que un programa está libre de errores. Un error es un comportamiento distinto del que espera un usuario razonable. Puede haber errores aunque se hayan seguido todos los pasos indicados en el análisis y en el diseño, y hasta en los requisitos aprobados por el usuario. Por lo tanto,no necesariamente un apego a los requisitos y un perfecto seguimiento de las etapas nos lleva a un producto sin errores, porque aún en la mente de un usuario pudo haber estado mal concebida la idea desde el comienzo. De allí la importancia del desarrollo incremental, que permite ir viendo versiones incompletas del sistema. Por lo tanto, una primera fuente de errores ocurre antes de los requerimientos o en el propio proceso de análisis. Pero también hay errores que se introducen durante el proceso de desarrollo posterior. Así, puede haber errores de diseño y errores de implementación. Finalmente, puede haber incluso errores en la propia etapa de pruebas y depuración. Categorías de pruebas Según la naturaleza de lo que se esté controlando, las pruebas se pueden dividir en dos categorías: Pruebas centradas en la verificación. Pruebas centradas en la validación. Las primeras sirven para determinar la consistencia entre los requerimientos y el programa terminado. Soporta metodologías formales de testeo, de mucho componente matemático. De todas maneras, hay que ser cuidadoso, porque no suele ser fácil encontrar qué es lo que hay que demostrar. La verificación consisten en determinar si estamos construyendo el sistema correctamente, a partir de los requisitos. En general a los informáticos no les gustan las pruebas formales, en parte porque no las entienden y en parte porque requieren un proceso matemático relativamente complejo. La validación consiste en saber si estamos construyendo el sistema correcto. Las tareas de validación son más informales. Las pruebas suelen mostrar la presencia de errores, pero nunca demuestran su ausencia. Las pruebas y el desarrollo de software La etapa de pruebas es una de las fases del ciclo de vida de los proyectos. Se la podría ubicar después del análisis, el diseño y la programación, pero dependiendo del proyecto en cuestión y del modelo de proceso elegido, su realización podría ser en forma paralela a las fases citadas o inclusive repertirse varias veces durante la duración del proyecto. La importancia de esta fase será mayor o menor según las características del sistema desarrollado, llegando a ser vital en sistemas de tiempo real u otros en los que los errores sean irrecuperables. Las pruebas no tienen el objeto de prevenir errores sino de detectarlos. Se efectúan sobre el trabajo realizado y se deben encarar con la intención de descubrir la mayor cantidad de errores posible. Para realizar las pruebas se requiere gente que disfrute encontrando errores, por eso no es bueno que sea el mismo equipo de desarrollo el que lleve a cabo este trabajo. Además, es un principio fundamental de las auditorías. Por otro lado, es bastante común que a quien le guste programar no le guste probar y viceversa. A veces se dan por terminadas las pruebas antes de tiempo. En las pruebas de caja blanca no es mala idea probar un 85% de las ramas y dar por terminado luego de esto. Otra posibilidad es la siembra de errores y seguir las pruebas hasta que se encuentren un 85% de los errores sembrados, lo que presumiblemente implica que se encontró un 86% de los no sembrados. Otros métodos se basan en la estadística y las comparaciones, ya sea por similitud con otro sistema en cantidad de errores o por el tiempo de prueba usado en otro sistema. En un proyecto ideal, podríamos generar casos de prueba para cubrir todas las posibles entradas y todas las posibles situaciones por las que podría atravesar el sistema. Examinaríamos así exhaustivamente el sistema para asegurar que su comportamiento sea perfecto. Pero hay un problema con esto: el número de casos de prueba para un sistema complejo es tan grande que no alcanzaría una vida para terminar con las pruebas. Como consecuencia, nadie realiza una prueba exhaustiva de nada salvo en sistemas triviales. En un sistema real, los casos de prueba se deben hacer sobre las partes del sistema en los cuales una buena prueba brinde un mayor retorno de la inversión o en las cuales un error represente un riesgo mayor. Las pruebas cuestan mucho dinero. Pero para ello existe una máxima: “pague por la prueba ahora o pague el doble por el mantenimiento después”. Todo esto lleva a que se deban planificar bien las pruebas, con suficiente anticipación, y determinar desde el comienzo los resultados que se deben obtener. La idea de extreme programming es más radical: propone primero escribir los programas de prueba y después la aplicación, obligando a correr las pruebas siempre antes de una integración. Se basa en la idea bastante acertada de que los programas de prueba son la mejor descripción de los requerimientos. Las pruebas son prácticas a realizar en diversos momentos de la vida del sistema de información para verificar: El correcto funcionamiento de los componentes del sistema. El correcto ensamblaje entre los distintos componentes. El funcionamiento correcto de las interfaces entre los distintos subsistemas que lo componen y con el resto de sistemas de información con los que se comunica. El funcionamiento correcto del sistema integrado de hardware y software en el entorno de operación. Que el sistema cumple con el funcionamiento esperado y permite al usuario de dicho sistema que determine su aceptación, desde el punto de vista de su funcionalidad y rendimiento. Que los cambios sobre un componente de un sistema de información, no introducen un comportamiento no deseado o errores adicionales en otros componentes no modificados. Las diversas pruebas a que debe ser sometido un sistema deben ser realizadas tanto por el equipo de desarrolladores, como por los usuarios, equipos de operación y mantenimiento en la implantación, aceptación y mantenimiento del sistema de información. Tipos de pruebas Analizaremos 7 tipos de pruebas: Revisiones de código. Pruebas unitarias. Pruebas de integración. Pruebas de sistema. Pruebas de implantación. Pruebas de aceptación. Pruebas de regresión. No son tipos de pruebas intercambiables, ya que testean cosas distintas. Otra posible clasificación de las pruebas es: De caja blanca o de código De caja negra o de especificación En las primeras se evalúa el contenido de los módulos, mientras en las segundas se trata al módulo como una caja cerrada y se lo prueba con valores de entrada, evaluando los valores de salida. Vistas de este modo, las pruebas de caja negra sirven para verificar especificaciones. Las pruebas unitarias suelen ser de caja blanca o de caja negra, mientras que las de integración, sistema y aceptación son de caja negra. Las tareas de depuración luego de encontrar errores son más bien técnicas de caja blanca, así como las revisiones de código. En todos los casos, uno de los mayores desafíos es encontrar los datos de prueba: hay que encontrar un subconjunto de todas las entradas que tengan alta probabilidad de detectar el mayor número de errores. Revisiones de código Las revisiones de código son las únicas que se podrían omitir de todos los tipos de pruebas, pero tal vez sea buen idea por lo menos hacer alguna de ellas: Pruebas de escritorio. Recorridos de código. Inspecciones de código. La prueba de escritorio rinde muy poco, tal vez menos de lo que cuesta, pero es una costumbre difícil de desterrar. Es bueno centrarse en buscar anomalías típicas, como variables u objetos no inicializados o que no se usan, ciclos infinitos y demás. Los recorridos rinden mucho más. Son exposiciones del código escrito frente a pares. El programador, exponiendo su código, encuentra muchos errores. Además da ideas avanzadas a programadores nuevos que se lleva a recorrer. Las llamadas inspecciones de código consisten en reuniones en conjunto entre los responsables de la programación y los responsables de la revisión.Tienen como objetivo revisar el código escrito por los programadores para chequear que cumpla con las normas que se hayan fijado y para verificar la eficiencia del mismo. Se realizan siguiendo el código de un pequeño porcentaje de módulos seleccionados al azar o según su grado de complejidad. Las inspecciones se pueden usar en sistemas grandes, pero con cuidado para no dar idea de estar evaluando al programador. Suelen servir porque los revisores están más acostumbrados a ver determinados tipos de errores comunes a todos los programadores. Además, después de una inspección a un programador, de la que surge un tipo de error, pueden volver a inspeccionar a otro para ver si no cayó en el mismo error. El concepto de extreme programming propone programar de a dos, de modo que uno escribe y el otro observa el trabajo. Si el que está programando no puede avanzar en algún momento, sigue el que miraba. Y si ambos se traban pueden pedir ayuda a otro par. Esta no sólo es una forma más social de programación, sino que aprovecha las mismas ventajas de los recorridos e inspecciones de código, y puede prescindir de ellos. Pruebas unitarias Las pruebas unitarias se realizan para controlar el funcionamiento de pequeñas porciones de código como ser subprogramas (en la programación estructurada) o métodos (en POO). Generalmente son realizadas por los mismos programadores puesto que al conocer con mayor detalle el código, se les simplifica la tarea de elaborar conjuntos de datos de prueba para testearlo. Si bien una prueba exhaustiva sería impensada teniendo en cuenta recursos, plazos, etc, es posible y necesario elegir cuidadosamente los casos de prueba para recorrer tantos caminos lógicos como sea posible. Inclusive procediendo de esta manera, deberemos estar preparados para manejar un gran volumen de datos de prueba. Los métodos de cobertura de caja blanca tratan de recorrer todos los caminos posibles por lo menos una vez, lo que no garantiza que no haya errores pero pretende encontrar la mayor parte. El tipo de prueba a la cual se someterá a cada uno de los módulos dependerá de su complejidad. Recordemos que nuestro objetivo aquí es encontrar la mayor cantidad de errores posible. Si se pretende realizar una prueba estructurada, se puede confeccionar un grafo de flujo con la lógica del código a probar. De esta manera se podrán determinar todos los caminos por los que el hilo de ejecución pueda llegar a pasar, y por consecuente elaborar los juegos de valores de pruebas para aplicar al módulo, con mayor facilidad y seguridad. Un grafo de flujo se compone de: Nodos (círculos), que representan una o más acciones del módulo. Aristas (flechas), que representan el flujo de control entre los distintos nodos. Los nodos predicados son aquellos que contienen una condición, por lo que de ellos emergen dos o más aristas. El paso de un diseño detallado o un pseudocódigo que representa una porción de programa a un grafo de flujo, requiere de las siguientes etapas: Señalar cada condición, tanto en sentencias _if_ y case como en bucles while y repeat . Agrupar todas las secuencias siguiendo los esquemas de representación de construcciones. Numerar cada uno de los nodos resultantes de manera que consten de un identificador único. Las ramas de cada bifurcación pueden identificarse por el mismo número seguido de distintas letras. Dibujar en forma ordenada los nodos y sus aristas. En el siguiente ejemplo, se muestra la manera de traducir un pequeño tramo de programa escrito en pseudocódigo a forma de grafo de flujo: Las pruebas unitarias tienen como objetivo verificar la funcionalidad y estructura de cada componente individualmente una vez que ha sido codificado. Las pruebas unitarias constituyen la prueba inicial de un sistema y las demás pruebas deben apoyarse sobre ellas. Existen dos enfoques principales para el diseño de casos de prueba: Enfoque estructural o de caja blanca . Se verifica la estructura interna del componente con independencia de la funcionalidad establecida para el mismo. Por tanto, no se comprueba la corrección de los resultados si éstos se producen. Ejemplos de este tipo de pruebas pueden ser ejecutar todas las instrucciones del programa, localizar código no usado, comprobar los caminos lógicos del programa, etc. Enfoque funcional o de caja negra . Se comprueba el correcto funcionamiento de los componentes del sistema de información, analizando las entradas y salidas y verificando que el resultado es el esperado. Se consideran exclusivamente las entradas y salidas del sistema sin preocuparse por la estructura interna del mismo. El enfoque que suele adoptarse para una prueba unitaria está claramente orientado al diseño de casos de caja blanca, aunque se complemente con caja negra. El hecho de incorporar casos de caja blanca se debe, por una parte, a que el tamaño del componente es apropiado para poder examinar toda la lógica y por otra, a que el tipo de defectos que se busca, coincide con los propios de la lógica detallada en los componentes. Los pasos necesarios para llevar a cabo las pruebas unitarias son los siguientes: Ejecutar todos los casos de prueba asociados a cada verificación establecida en el plan de pruebas, registrando su resultado. Los casos de prueba deben contemplar tanto las condiciones válidas y esperadas como las inválidas e inesperadas. Corregir los errores o defectos encontrados y repetir las pruebas que lo detectaron. Si se considera necesario, debido a su implicación o importancia, se repetirán otros casos de prueba ya realizados con anterioridad. La prueba unitaria se da por finalizada cuando se hayan realizado todas las verificaciones establecidas y no se encuentre ningún defecto, o bien se determine su suspensión. Pruebas de integración En el caso de las pruebas de integración y de sistema,dado que ya se han realizado las pruebas unitarias, se tomará a cada uno de los módulos unitarios como una caja negra. Las pruebas de integración tienen como base las pruebas unitarias y consisten en una progresión ordenada de testeos para los cuales los distintos módulos van siendo ensamblados y probados hasta haber integrado el sistema completo. Si bien se realizan sobre módulos ya probados en forma individual, no es necesario que se terminen todas las pruebas unitarias para comenzar con las de integración. Dependiendo de la forma en que se organicen, se pueden realizar en paralelo a las unitarias. El orden de integración de los módulos influye en: La forma de preparar los casos de prueba. Las herramientas a utilizar (módulos ficticios, muñones, impulsores o “stubs”). El orden para codificar y probar los módulos. El costo de preparar los casos. El costo de la depuración. Tanto es así que se le debe prestar especial atención al proceso de elección del orden de integración que se emplee. Existen principalmente dos tipos de integración: La integración incremental La integración no incremental . La integración incremental consiste en combinar el conjunto de módulos ya probados (al principio será un conjunto vacío) con los siguientes módulos a probar. Luego se va incrementando progresivamente el número de módulos unidos hasta que se forma el sistema completo. En la integración no incremental o Big Bang se combinan todos los módulos de una vez. Para ambos tipos de integración se deberán preparar los datos de prueba junto con los resultados esperados. Esta tarea debe ser realizada por personas ajenas a la programación de los módulos. No es necesario que la lleven a cabo una vez codificados los módulos puesto que basta con conocer qué módulos compondrán el sistema y cuál será la interfaz entre ellos. Si en algún momento de la prueba se detectara uno o más errores, se dejará constancia del hecho y se reenviarán los módulos afectados al responsable de la programación para que identifique la causa del problema y lo solucione. Luego se volverán a efectuar las pruebas programadas y así sucesivamente hasta que el sistema entero esté integrado y sin errores. Por el hecho de poder ser llevada a cabo por distintos caminos, la integración incremental brinda una mayor flexibilidad en el uso de recursos. Se puede integrar la estructura de módulos desde un extremo a otro y continuar hacia el extremo opuesto según distintas prioridades. La forma de llevar a cabo esta tarea dependerá de la naturaleza del sistema en cuestión, pero sea cual fuere el camino elegido, será de suma importancia una correcta planificación. En la integración incremental ascendente (De abajo arriba – bottom-up) se comienza integrando primero los módulos de más bajo nivel. El proceso deberá seguir los siguientes pasos: Elegir los módulos de bajo nivel que se van a probar. Escribir un módulo impulsor para la entrada de datos de prueba a los módulos y para la visualización de los resultados. Probar la integración de los módulos. Eliminar los módulos impulsores y juntar los módulos ya probados con los módulos de niveles superiores, para continuar con las pruebas. Estas tareas se pueden realizar en paralelo si es que se dispone de tres equipos de trabajo, o en serie de lo contrario. Para la prueba de cada uno de los módulos mencionados, es necesaria la preparación de un módulo impulsor. El objeto de los módulos impulsores es transmitir o impulsar los casos de prueba a los módulos testeados y recibir los resultados que estos produzcan en los casos en que sea necesario. Es decir, que deben simular las operaciones de llamada de los módulos jerárquicos superiores correspondientes. Estos módulos tienen que estar bien diseñados, para que no arrojen ni más ni menos errores que los que realmente pueden producirse. Al fin y al cabo, deben simular todas las situaciones que se van a producir en el sistema real. La integración incremental descendente (De arriba abajo – top-down) parte del módulo de control principal (de mayor nivel) para luego ir incorporando los módulos subordinados progresivamente. No hay un procedimiento considerado óptimo para seleccionar el siguiente módulo a incorporar. La única regla es que el módulo incorporado tenga todos los módulos que lo invocan previamente probados. En general no hay una secuencia óptima de integración. Debe estudiarse el problema concreto y de acuerdo a este, buscar el orden de integración más adecuado para la organización de las pruebas. No obstante, pueden considerarse las siguientes pautas: Si hay secciones críticas como ser un módulo complicado, se debe proyectar la secuencia de integración para incorporarlas lo antes posible. El orden de integración debe incorporar cuanto antes los módulos de entrada y salida. Existen principalmente dos métodos para la incorporación de módulos: Primero en profundidad: primero se mueve verticalmente en la estructura de módulos. Primero en anchura: Primero se mueve horizontalmente en la estructura de módulos. Etapas de la integración descendente: El módulo de mayor nivel hace de impulsor y se escriben módulos ficticios simulando a los subordinados, que serán llamados por el módulo de control superior. Probar cada vez que se incorpora un módulo nuevo al conjunto ya engarzado. Al terminar cada prueba, sustituir un módulo ficticio subordinado por el real que reemplazaba, según el orden elegido. Escribir los módulos ficticios subordinados que se necesiten para la prueba del nuevo módulo incorporado. Los módulos ficticios subordinados se crean para permitir la prueba de los demás módulos. Pueden llevar a cabo una variedad de funciones, como por ejemplo: Mostrar un mensaje que demuestre que ese módulo fue alcanzado (“Módulo XX alcanzado”). Establecer una conversación con una terminal. De esta forma se puede permitir que la misma persona que realiza la prueba actúe de módulo subordinado. Devolver un valor constante, tabulado o elegido al azar. Ser una versión simplificada del módulo representado. Mostrar los datos recibidos. Ser un loop sin nada que hacer más que dejar pasar el tiempo. Ventajas de la integración descendente: Las fallas que pudieran existir en los módulos superiores se detectan en una etapa temprana. Permite ver la estructura del sistema desde un principio, facilitando la elaboración de demostraciones de su funcionamiento. Concuerda con la necesidad de definir primero las interfaces de los distintos subsistemas para después seguir con las funciones específicas de cada uno por separado. Desventajas de la integración descendente: Requiere mucho trabajo de desarrollo adicional ya que se deben escribir un gran número de módulos ficticios subordinados que no siempre son fáciles de realizar. Suelen ser más complicados de lo que aparentan. Antes de incorporar los módulos de entrada y salida resulta difícil introducir los casos de prueba y obtener los resultados. Los juegos de datos de prueba pueden resulta difíciles o imposibles de generar puesto que generalmente son los módulos de nivel inferior los que proporcionan los detalles. Induce a diferir la terminación de la prueba de ciertos módulos. Ventajas de la integración incremental ascendente: Las entradas para las pruebas son más fáciles de crear ya que los módulos inferiores suelen tener funciones más específicas. Es más fácil la observación de los resultados de las pruebas puesto que es en los módulos inferiores donde se elaboran. Resuelve primero los errores de los módulos inferiores que son los que acostumbran tener el procesamiento más complejo, para luego nutrir de datos al resto del sistema. Desventajas de la integración incremental ascendente: Se requieren módulos impulsores, que deben escribirse especialmente y que no son necesariamente sencillos de codificar. El sistema como entidad no existe hasta que se agrega el último módulo. El método de integración incremental sándwich (estrategia combinada) combina facetas de los métodos ascendente y descendente. Consiste en integrar una parte del sistema en forma ascendente y la restante en forma descendente, provocando la unión de ambas partes en algún punto intermedio. La principal ventaja es que nos da mayor libertad para elegir el orden de integración de los módulos según las características específicas del sistema en cuestión. De esta manera, podremos incluir y probar antes los módulos que consideremos críticos: Módulos dirigidos a múltiples propósitos. Módulos con mayor control (en general, los módulos de mayor nivel controlan a muchos otros módulos). Módulos con alto grado de complejidad. Módulos con requisitos de rendimiento muy definidos. La integración no incremental puede ser beneficiosa par la prueba de sistema de pequeñísima envergadura cuya cantidad de módulos sea muy limitada y la interfaz entre los mismos clara y sencilla. Consiste en integrar todos los módulos del sistema a la vez e ingresar los valores de prueba para testear todas las interfaces. La única ventaja es que no se necesita ningún tipo de trabajo adicional: ni planificar el orden de integración, ni crear módulos impulsores, ni crear módulos ficticios subordinados. Por otro lado, la lista de desventajas incluye: No se tiene noción de la comunicación de los módulos hasta el final. En ningún momento se dispone de un producto -siquiera parcial- para mostrar o presentar. El hecho de realizar todas las pruebas de una vez hace que las sesiones de prueba sean largas y tediosas. La cantidad de errores que arroje puede ser atemorizante. La tarea de encontrar la causa de los errores resulta mucho más compleja que con los métodos incrementales. Se corre el riesgo de que a poco tiempo de que se cumpla el plazo de entrega, haya que volver sobre el diseño y la codificación del sistema. Pruebas de sistema Son pruebas de integración del sistema de información completo, y permiten probar el sistema en su conjunto y con otros sistemas con los que se relaciona para verificar que las especificaciones funcionales y técnicas se cumplen. Dan una visión muy similar a su comportamiento en el entorno de producción. Las pruebas de sistema se realizan una vez integrados todos los componentes. Su objetivo es ver la respuesta del sistema en su conjunto, frente a distintas situaciones. Se simulan varias alternativas que podrían darse con el sistema implantado y en base a ellas se prueba la eficacia y eficiencia de la respuesta que se obtiene. Se pueden distinguir varios tipos de pruebas distintos, por ejemplo: Pruebas negativas : se trata de que el sistema falle para ver sus debilidades. Pruebas funcionales : dirigidas a asegurar que el sistema de información realiza correctamente todas las funciones que se han detallado en las especificaciones dadas por el usuario del sistema. Pruebas de comunicaciones : determinan que las interfaces entre los componentes del sistema funcionan adecuadamente, tanto a través de dispositivos remotos, como locales. Asimismo, se han de probar las interfaces hombre/máquina. Pruebas de volumen : consisten en examinar el funcionamiento del sistema cuando está trabajando con grandes volúmenes de datos, simulando las cargas de trabajo esperadas. Pruebas de sobrecarga : consisten en comprobar el funcionamiento del sistema en el umbral límite de los recursos, sometiéndole a cargas masivas. El objetivo es establecer los puntos extremos en los cuales el sistema empieza a operar por debajo de los requisitos establecidos. Pruebas de disponibilidad de datos : consisten en demostrar que el sistema puede recuperarse ante fallos, tanto de equipo físico como lógico, sin comprometer la integridad de los datos. Pruebas de facilidad de uso : consisten en comprobar la adaptabilidad del sistema a las necesidades de los usuarios, tanto para asegurar que se acomoda a su modo habitual de trabajo, como para determinar las facilidades que aporta al introducir datos en el sistema y obtener los resultados. Pruebas de operación : consisten en comprobar la correcta implementación de los procedimientos de operación, incluyendo la planificación y control de trabajos, arranque y rearranque del sistema, etc. Pruebas de entorno : consisten en verificar las interacciones del sistema con otros sistemas dentro del mismo entorno. Pruebas de recuperación : se simulan fallas de software y/o hardware para verificar la eficacia del proceso de recuperación. Pruebas de rendimiento : tiene como objeto evaluar el rendimiento del sistema integrado en condiciones de uso habitual. Consisten en determinar que los tiempos de respuesta están dentro de los intervalos establecidos en las especificaciones del sistema. Pruebas de resistencia o de estrés : comprueban el comportamiento del sistema ante situaciones donde se demanden cantidades extremas de recursos (número de transacciones simultáneas anormal, excesivo uso de las memorias, etc). Pruebas de seguridad : se utilizan para testear el esquema de seguridad intentando vulnerar los métodos utilizados para el control de accesos no autorizados al sistema. Consisten en verificar los mecanismos de control de acceso al sistema para evitar alteraciones indebidas en los datos. Pruebas de instalación : verifican que el sistema puede ser instalado satisfactoriamente en el equipo del cliente, incluyendo todas las plataformas y configuraciones de hardware necesarias. Pruebas de compatibilidad : se prueba al sistema en las diferentes configuraciones de hardware o de red y de plataformas de software que debe soportar. Pruebas de implantación El objetivo de las pruebas de implantación es comprobar el funcionamiento correcto del sistema integrado de hardware y software en el entorno de operación, y permitir al usuario que, desde el punto de vista de operación, realice la aceptación del sistema una vez instalado en su entorno real y en base al cumplimiento de los requisitos no funcionales especificados. Una vez que hayan sido realizadas las pruebas del sistema en el entorno de desarrollo, se llevan a cabo las verificaciones necesarias para asegurar que el sistema funcionará correctamente en el entorno de operación. Debe comprobarse que responde satisfactoriamente a los requisitos de rendimiento, seguridad, operación y coexistencia con el resto de los sistemas de la instalación para conseguir la aceptación del usuario de operación. Las pruebas de seguridad van dirigidas a verificar que los mecanismos de protección incorporados al sistema cumplen su objetivo; las de rendimiento a asegurar que el sistema responde satisfactoriamente en los márgenes establecidos en cuanto a tiempos de respuesta, de ejecución y de utilización de recursos, así como los volúmenes de espacio en disco y capacidad; por último con las pruebas de operación se comprueba que la planificación y control de trabajos del sistema se realiza de acuerdo a los procedimientos establecidos, considerando la gestión y control de las comunicaciones y asegurando la disponibilidad de los distintos recursos. Asimismo, también son llevadas a cabo las pruebas de gestión de copias de seguridad y recuperación, con el objetivo de verificar que el sistema no ve comprometido su funcionamiento al existir un control y seguimiento de los procedimientos de salvaguarda y de recuperación de la información, en caso de caídas en los servicios o en algunos de sus componentes. Para comprobar estos últimos, se provoca el fallo del sistema, verificando si la recuperación se lleva a cabo de forma apropiada. En el caso de realizarse de forma automática, se evalúa la inicialización, los mecanismos de recuperación del estado del sistema, los datos y todos aquellos recursos que se vean implicados. Las verificaciones de las pruebas de implantación y las pruebas del sistema tienen muchos puntos en común al compartir algunas de las fuentes para su diseño como pueden ser los casos para probar el rendimiento (pruebas de sobrecarga o stress ). El responsable de implantación junto al equipo de desarrollo determina las verificaciones necesarias para realizar las pruebas así como los criterios de aceptación del sistema. Estas pruebas las realiza el equipo de operación, integrado por los técnicos de sistemas y de operación que han recibido previamente la formación necesaria para llevarlas a cabo. Pruebas de aceptación El objetivo de las pruebas de aceptación es validar que un sistema cumple con el funcionamiento esperado y permitir al usuario de dicho sistema que determine su aceptación, desde el punto de vista de su funcionalidad y rendimiento. Las pruebas de aceptación, al igual que las de sistema, se realizan sobre el producto terminado e integrado; pero a diferencia de aquellas, están concebidas para que sea un usuario final quien detecte los posibles errores. Se clasifican en dos tipos: Pruebas Alfa. Pruebas Beta. Las pruebas Alfa se realizan por un cliente en un entorno controlado por el equipo de desarrollo. Para que tengan validez, se debe primero crear un ambiente con las mismas condiciones que se encontrarán en las instalaciones del cliente. Una vez logrado esto, se procede a realizar las pruebas y a documentar los resultados. Cuando el software sea la adaptación de una versión previa, deberán probarse también los procesos de transformación de datos y actualización de archivos de todo tipo. Las pruebas Beta se realizan en las instalaciones propias de los clientes. Para que tengan lugar, en primer término se deben distribuir copias del sistema para que cada cliente lo instale en sus oficinas, dependencias y/o sucursales, según sea el caso. Si se tratase de un número reducido de clientes el tema de la distribución de las copias no representa grandes dificultades, pero en el caso de productos de venta masiva, la elección de los beta testers debe realizarse con sumo cuidado. En el caso de las pruebas Beta ,cada usuario realizará sus propias pruebas y documentará los errores que encuentre, así como las sugerencias que crea conveniente realizar, para que el equipo de desarrollo tenga en cuenta al momento de analizar las posibles modificaciones. Cuando el sistema tenga un cliente individual, las pruebas de aceptación se hacen de común acuerdo con éste, y los usuarios se determinan en forma programada, así como también se definen los aspectos a probar y la forma de informar resultados. Cuando, en cambio, se está desarrollando un producto masivo, los usuarios para pruebas de determinan de formas menos estrictas, y hay que ser muy cuidado en la evaluación del feedback que proveen. Por lo tanto, en este segundo caso hay que dedicar un esfuerzo considerable a la planificación de las pruebas de aceptación. Pruebas de regresión El objetivo de las pruebas de regresión es eliminar el efecto onda, es decir, comprobar que los cambios sobre un componente de un sistema de información, no introducen un comportamiento no deseado o errores adicionales en otros componentes no modificados. Las pruebas de regresión se deben llevar a cabo cada vez que se hace un cambio en el sistema, tanto para corregir un error como para realizar una mejora. No es suficiente probar sólo los componentes modificados o añadidos, o las funciones que en ellos se realizan, sino que también es necesario controlar que las modificaciones no produzcan efectos negativos sobre el mismo u otros componentes. Normalmente, este tipo de pruebas implica la repetición de las pruebas que ya se han realizado previamente, con el fin de asegurar que no se introducen errores que puedan comprometer el funcionamiento de otros componentes que no han sido modificados y confirmar que el sistema funciona correctamente una vez realizados los cambios. Las pruebas de regresión pueden incluir: La repetición de los casos de pruebas que se han realizado anteriormente y están directamente relacionados con la parte del sistema modificada. La revisión de los procedimientos manuales preparados antes del cambio, para asegurar que permanecen correctamente. La obtención impresa del diccionario de datos de forma que se comprueba que los elementos de datos que han sufrido algún cambio son correctos. El responsable de realizar las pruebas de regresión será el equipo de desarrollo junto al técnico de mantenimiento, quien a su vez, será responsable de especificar el plan de pruebas de regresión y de evaluar los resultados de dichas pruebas. Relación ante los resultados de las pruebasLas pruebas nos llevan a descubrir errores, que en la mayoría de los casos son de tipo funcional, es decir, del tipo: “el sistema debería hacer tal cosa y hace tal otra”. En este apartado analizaremos nada más que los pasos a seguir cuando el error sólo es atribuible a la codificación. Depuración La depuración es la corrección de errores que sólo afectan a la programación, porque no provienen de errores previos en el análisis o en el diseño. A veces la depuración se hace luego de la entrega del sistema al cliente y es parte del mantenimiento. En realidad, en las revisiones de código y las pruebas unitarias, encontrar un error es considerablemente más sencillo, ya que se hace con el código a mano. Aun cuando se hubiera optado por una prueba unitaria de caja negra,es sencillo recorrer el módulo que revela un comportamiento erróneo por dentro para determinar el lugar exacto del error. Existen incluso herramientas de depuración ( debugging ) de los propios ambientes de desarrollo que facilitan esta tarea, que incluso proveen recorrido paso a paso y examen de valores de datos. Y el lenguaje C traía una macro assert portable, que sencillamente abortaba un programa si una condición no se cumplía. De todas maneras, es importante analizar correctamente si el error está donde parece estar o proviene de una falla oculta más atrás en el código. Para encontrar estos casos más complejos son útiles las herramientas de recorrida hacia atrás, que permiten partir del lugar donde se genera el error y recorrer paso a paso el programa en sentido inverso. Las pruebas de integración, de sistema y de aceptación también pueden llevar a que sea necesaria una depuración, aunque aquí es más difícil encontrar el lugar exacto del error. Por eso a menudo se utilizan bitácoras ( logs ), que nos permiten evaluar las condiciones que se fueron dando antes de un error mediante el análisis de un historial de uso del sistema que queda registrado en medios de almacenamiento permanente. La depuración se hace en cuatro pasos: Reproducir el error. Diagnosticar la causa. Corregirla. Verificar la corrección. Si el error no se repite al intentar reproducirlo es muy difícil hacer el diagnóstico. Como en casi todas las ciencias, se buscan causas y efectos, condiciones necesarias y suficientes para que se produzca el error. Luego hay que buscar el sector del código donde se produce el error, lo que nos lleva a las consideraciones hechas recientemente. La corrección del error entraña mucho riesgo, porque a menudo se introducen nuevos errores (hay quienes hablan de tasas de 0,2 a 0,5 nuevos errores por corrección). Y nunca hay que olvidarse de realizar una nueva verificación después de la corrección. Reacción ante los errores en las pruebas de sistema y de aceptación Hemos dicho ya que los errores que aparezcan en estos tipos de prueba van a llevar a la larga a una depuración, en la medida en que sean errores de codificación. Para llegar a ello, no obstante, se requiere determinar el módulo donde se produjo el error. Esta tarea, en apariencia dificultosa, puede facilitarse considerablemente si trabajamos con un entorno de desarrollo que nos permita recorrer el código en modo de depuración sin necesidad de entrar en todos los módulos. Revisión FormalEl objetivo de la revisión formal es detectar y registrar los defectos de un producto intermedio verificando que satisface sus especificaciones y que se ajusta a los estándares establecidos, señalando las posibles desviaciones. Es un proceso de revisión riguroso en el que hay poca flexibilidad a la hora de llevarlo a cabo debido a que su objetivo es llegar a detectar lo antes posible, los posibles defectos o desviaciones en los productos que se van generando a lo largo del desarrollo. Esta característica fuerza a que se adopte esta práctica únicamente para productos que son de especial importancia, porque de otro modo podría frenar la marcha del proyecto. En este proceso intervienen varias personas del grupo de aseguramiento de calidad, el equipo de desarrollo y según el tipo de revisión formal puede participar también el usuario. El responsable del grupo de aseguramiento de calidad una vez que conoce los productos que se van a revisar formalmente, establece los grupos funcionales que van a llevar a cabo las revisiones, convocando a los participantes por adelantado, e informando del objetivo de la revisión, la agenda y las responsabilidades que tendrán asignadas en la revisión. Es importante que en el transcurso de la revisión se sigan las directrices que estableció el responsable del grupo de aseguramiento de calidad, con el fin de que sea productiva y no se pierda tiempo en discusiones o ataques al responsable del producto. Se concluye determinando las áreas de problemas y elaborando un informe de revisión formal y una lista de acciones correctivas que posee un carácter formal y vinculante. Revisión TécnicaEl objetivo de la revisión técnica es evaluar un producto intermedio del desarrollo para comprobar que se ajusta a sus especificaciones y que se está elaborando de acuerdo a las normas, estándares y guías aplicables al proyecto. Con el fin de asegurar la calidad en el producto final del desarrollo, se deben llevar a cabo revisiones semiformales sobre los productos intermedios durante todo el ciclo de vida del software. Para ello, se fijan los objetivos de la revisión, la agenda que se podrá ir ajustando a lo largo del proyecto y el tipo de informe que se elaborará después de las revisiones. Los participantes en una revisión técnica son el jefe de proyecto y el responsable del grupo de aseguramiento de calidad que, de forma conjunta, revisarán el producto que corresponda en cada momento. Una vez fijado sobre qué productos intermedios se llevarán a cabo las revisiones, el responsable de aseguramiento de calidad recoge la información necesaria de cada producto para poder establecer los criterios de revisión y más adelante, evaluar si el producto cumple las especificaciones, es decir, si se ha elaborado de acuerdo a unas características concretas como pueden ser la aplicación de una técnica específica, la inclusión de algún tipo de información, etc. Además, se debe contar con la normativa y estándares aplicables al proyecto de forma que, no sólo se asegure que el producto cumpla sus especificaciones, sino también del modo adecuado. Si se detecta alguna desviación en cuanto a sus especificaciones o a los estándares aplicados, y se considera que es necesario realizar alguna modificación, el responsable del grupo de aseguramiento de calidad elabora un informe con el que el jefe de proyecto tomará las medidas que estime convenientes. Con dichos informes de calidad, el jefe de proyecto irá confeccionando el dossier de aseguramiento de calidad, que formará parte de la documentación del proyecto al finalizar el desarrollo. Bibliografía UBA (Universidad de Buenos Aires) PAe (Métrica 3) Minería de datos. Aplicación a la resolución de problemas de gestión. Tecnología y algoritmos. Procesamiento analítico en línea (OLAP). Big data. Bases de datos NoSQL.La minería de datos: Data MiningEl término minería o recopilación de datos (data mining) hace referencia al proceso de análisis semiautomático de BD de gran tamaño para hallar estructuras útiles. Al igual que la búsqueda de conocimiento en la inteligencia artificial o el análisis estadístico, la minería de datos intenta descubrir reglas y estructuras a partir de los datos. Es decir, la minería de datos trata de la búsqueda del conocimiento en las BD. Los almacenes de datos guardan todos los datos relevantes para una organización, estando estructurados para que se pueda extraer información a partir de dichos datos. En este tema vamos a ver como la minería de datos permite sacar el máximo provecho del almacén de datos, ofreciendo una serie de técnicas y herramientas que automatizan (o semiautomatizan) el proceso de extracción de información y significado a partir de los datos que éste contiene. El nombre de minería de datos (data mining) deriva de las similitudes entre buscar valiosa información de negocios en grandes BD y minar una montaña para encontrar una veta de metales preciosos. Ambos procesos requieren examinar una inmensa cantidad de material, o investigar inteligentemente hasta encontrar exactamente dónde residen los valores. El proceso de descubrimiento de conocimiento en Bases de DatosEl descubrimiento de conocimiento en las BD es el proceso no trivial de identificación de patrones válidos, potencialmente útiles y comprensibles en los datos. El objetivo es la extracción de conocimiento de los datos, en el contexto de las BD de gran tamaño. El proceso es iterativo, consta de unos pasos básicos e involucra decisiones por parte del usuario, siendo interactivo. Esto quiere decir que el proceso requiere el entendimiento del dominio de la aplicación por parte del usuario. Se han identificado los siguientes pasos como componentes del proceso: Selección de un conjunto de datos objetivo. Preprocesamiento y limpieza de los datos. Transformación y reducción en la dimensión de los datos. Selección del método de minería de datos y de la técnica (algoritmo) de minería de datos e implementación de la técnica para realizar la extracción de patrones. Interpretación o evaluación de los patrones extraídos. Consolidación del conocimiento descubierto. Por otro lado, el consorcio Cross-Industry Standard Process for Data Mining propuso un modelo estándar y de acceso público del proceso. El modelo es jerárquico y consta de cuatro niveles de abstracción: El primer nivel está constituido por una serie de fases las cuales se dividen en tareas generales. El segundo nivel se conoce como genérico, ya que, trata de cubrir todas las posibles situaciones de minería de datos. El tercer nivel es más especializado, describiendo particularmente qué acciones deben llevarse a cabo dependiendo de situaciones específicas. El cuarto nivel es la instanciación del proceso, como un registro de acciones, decisiones y resultados del proceso completo de minería de datos. Definiciones de Minería de DatosVeamos ahora una serie de definiciones de minería de datos, que ayudan a entender mejor en qué consiste: La minería de datos pretende obtener visiones en profundidad de los datos corporativos que no son fácilmente detectables. De hecho, más que analizar los resultados de la actividad, permite modelizarla construyendo patrones o categorías que la identifiquen, respondiendo a las necesidades de información del tipo ¿qué hay en los datos de interés?, o ¿qué podría ocurrir en un futuro?, en base al descubrimiento de tendencias o agrupaciones interesantes de datos. De hecho, las herramientas enmarcadas bajo la denominación de Minería de Datos (MD), permiten no sólo el análisis de información que tradicionalmente ha venido siendo realizado por los Sistemas de Soporte a la Decisión (DSS), sino, y esto es lo realmente importante y diferencial, el planteamiento y descubrimiento automáticos de hechos e hipótesis, ya sean patrones, reglas, grupos, funciones, modelos, secuencias, relaciones, correlaciones, etc. Una cualidad que resalta es la posibilidad de anticiparse a las variaciones del entorno, lo que facilitará darles una mejor y más rápida respuesta. La extracción de información oculta y predecible de grandes BD, es una nueva y poderosa tecnología con gran potencial para ayudar a las compañías a concentrarse en la información más importante de sus Bases de Información (Data Warehouse). Las herramientas de Data Mining predicen futuras tendencias y comportamientos, permitiendo en los negocios tomar decisiones proactivas y conducidas por un conocimiento acabado de la información. Los análisis prospectivos automatizados ofrecidos por un producto así van más allá de los eventos pasados provistos por herramientas retrospectivas típicas de sistemas de soporte de decisión. Las herramientas Data Mining pueden responder a preguntas de negocios que tradicionalmente consumen demasiado tiempo para poder ser resueltas y a los cuales los usuarios de esta información casi no están dispuestos a aceptar. Estas herramientas exploran las BD en busca de patrones ocultos, encontrando información predecible que un experto no puede llegar a encontrar porque se encuentra fuera de sus expectativas. La minería de datos consiste en la búsqueda de relaciones y patrones globales que se hallan presentes en las grandes BD pero que están “ocultos” entre el gran volumen de datos existente. Estas relaciones representan un conocimiento útil sobre los objetos de la BD y la realidad que representan. Los puntos en común que observamos en las definiciones anteriores son: Es necesario disponer de unas BD o, mejor aún, de un almacén de datos, sobre los cuales realizar el proceso de minería. El proceso de minería debe ser automático en la mayor medida posible, debido a los grandes volúmenes de datos que se analizan. Los resultados obtenidos deben representar conocimiento útil y no evidente a primera vista. Después de estudiar el concepto y definiciones de la minería de datos, terminamos este punto poniendo de manifiesto que las aplicaciones de MD extraen conocimiento escondido, patrones de comportamiento no explícitos, relaciones ocultas o información predictiva del almacén, sin necesidad de preguntas o peticiones específicas sino utilizando distintas técnicas, tales como algoritmos matemáticos, métodos estadísticos, modelos lógicos borrosos, algoritmos genéticos, inducciones de reglas, sistemas expertos y sistemas basados en el conocimiento y redes neuronales. Fundamentos de la Minería de DatosLas técnicas de Minería de Datos son el resultado de un largo proceso de investigación y desarrollo de productos. Esta evolución comenzó cuando los datos de negocios fueron almacenados por primera vez en computadoras, y continuó con mejoras en el acceso a los datos, y más recientemente con tecnologías generadas para permitir a los usuarios navegar a través de los datos en tiempo real. La Minería de Datos está lista para su aplicación en la comunidad de negocios porque está soportada por tres tecnologías que ya están suficientemente maduras: Recolección masiva de datos. Potentes computadoras con multiprocesadores. Algoritmos de Data Mining. Los componentes esenciales de la tecnología de Data Mining han estado bajo desarrollo durante décadas, en áreas de investigación como la estadística, la inteligencia artificial y el aprendizaje de máquinas. Hoy, la madurez de estas técnicas, junto con los motores de BD relacionales de alto rendimiento, han hecho que estas tecnologías sean prácticas para los entorno de data warehouse actuales. Fases del proceso de Minería de DatosPara alcanzar buenos resultados es necesario comprender que la minería de datos no se basa en una metodología estándar y genérica que resuelve todo tipo de problemas, sino que consiste en una metodología dinámica e iterativa que va a depender del problema planteado, de la disponibilidad de la fuente de datos, del conocimiento de las herramientas necesarias, de la metodología desarrollada, y de los requerimientos y recursos de la empresa. El procedimiento para resolver un problema a través de la minería de datos se divide en dos grandes etapas: la preparación de los datos y la minería de datos propiamente dicha. Pasos en la fase de preparación de los datos Planteamiento del problema : Definir de manera objetiva cuál es el problema a resolver, determinar con qué recursos humanos y tecnológicos se cuenta, cuáles son las fuentes de información y cuál es la disponibilidad de la información. Selección de los datos : De todas las fuentes de información disponibles se debe establecer cuáles son las que se van a considerar. Es decir, se decide sobre qué datos se va a trabajar, tanto desde el punto de vista físico, como desde el punto de vista lógico. Se debe realizar un tratamiento y estructuración de la información con el objetivo de presentarla de la mejor manera posible para posteriores análisis. Limpieza y preprocesamiento de los datos : En esta fase se analizan los datos con la finalidad de reorganizar la información eliminando aquella que es poco útil o completando la que nos falta. Se eliminan los datos irrelevantes, se unifican los criterios de representación que pueden no ser los mismos en todas las fuentes de datos y se eliminan redundancias y duplicados. Reducción y proyección de datos : Consiste en encontrar las características útiles que representan las dependencias de los datos en el objetivo del proceso. Pasos en la Minería de Datos Selección de técnicas de minería de datos . Selección de los algoritmos de minería de datos . En él son seleccionados los métodos para que sean usados en la búsqueda de patrones de los datos. Esto incluye decidir qué modelos y parámetros son más apropiados para la adquisición del tipo de conocimiento deseado. A través de la entrega de los datos para los algoritmos de minería de datos seleccionados se llega al conocimiento. Extracción del conocimiento. Búsqueda de patrones : Es esta fase donde se escogen y se aplican las técnicas de minería de datos para la determinación de patrones de interés en los datos. Para ello se interpretan los resultados obtenidos a lo largo del proceso para la construcción de modelos o se buscan estructuras subyacentes dentro de la información. Las herramientas de minería de datos, analizan los datos ya preparados para extraer significado e información. Construcción del modelo. Interpretación y evaluación : Con los resultados obtenidos en la fase anterior se lleva a cabo el análisis, interpretación y evaluación para la determinación de un modelo eficiente que sea útil en la toma de decisiones. Validación del modelo : Implementar el modelo desarrollado en el proceso real y determinar su efectividad en diferentes casos de aplicación. Si las pruebas arrojan resultados satisfactorios, el modelo queda comprobado y garantizado para su uso regular. Sin embargo, si los resultados son poco satisfactorios, se debería regresar a las fases anteriores y fortalecer el análisis para mejorar el modelo final. Elementos o Técnicas de la Minería de DatosLa aplicación ideal de la MD se llevaría a cabo sobre las BD corporativas, que como ya hemos comentado pueden ser un Almacén de Datos, o sobre otras específicas de propósito departamental (o Data Marts), contemplando elementos o técnicas como los siguientes: Agentes inteligentes : Se encargan de analizar la información para detectar patrones y relaciones, ya sea de forma automática, o bien interactuando con el analista. Las técnicas que utilizan les permiten identificar grupos, comportamientos y reglas cuyo descubrimiento habría supuesto un enorme esfuerzo de trabajo metódico. Son tomados del campo de la inteligencia artificial y entre ellos destacan los sistemas expertos, el aprendizaje automático, la visión por ordenador o la teoría de juegos. Utilizan estructuras de datos y algoritmos basados en árboles de decisión, redes neuronales, técnicas de agrupamiento y lógica difusa. Estas técnicas son especialmente adecuadas para herramientas de minería que utilizan los modelos predictivo y de descubrimiento, ya que son muy buenas en la detección de patrones. Detección de alarmas : Consiste en la ejecución periódica o permanente de ciertos agentes para detectar acciones o situaciones susceptibles de desencadenar una acción extraordinaria o fuera del ciclo ordinario, pudiéndose activar en tiempo real, o detectarse y almacenarse para su posterior análisis y tratamiento. Análisis multidimensional : Se basa en la estructuración y presentación de la información bajo aquellas perspectivas, ejes o dimensiones de interés. Las técnicas multidimensionales son muy buenas para cruzar los datos de múltiples formas y con distintos niveles de agregación. Se basan en la utilización de BD multidimensionales. Los estudiaremos con detalle en el apartado dedicado a OLAP. Consultas e informes : Ésta es la forma tradicional de obtener información a partir de BD. Las plataformas suelen incorporar herramientas de consulta (lenguaje SQL) con interfaces gráficas muy avanzados, intuitivos y fáciles de usar, cierto grado de análisis multidimensional y agentes inteligentes. Adicionalmente pueden utilizar técnicas matemáticas y estadísticas para analizar los datos obtenidos. Estas técnicas son muy apropiadas si se va a utilizar el modelo de Verificación. Su principal ventaja es que son de eficiencia probada, trabajan sobre las BD relacionales ya existentes y además es muy sencillo encontrar herramientas amigables al usuario que las soporten. Tratamiento de datos : Los datos suelen estar almacenados en los formatos más adecuados para su gestión por parte de los sistemas existentes, pero pueden no ser los más adecuados para su procesamiento por parte de la MD, de ahí que muchos desarrollos de MD incorporen módulos de tratamiento de datos con el objeto de simplificar al máximo las interfaces de datos e información. Aplicación a la Resolución de Problemas de GestiónPlanteamiento Inicial del ProblemaEl desarrollo tecnológico ha aumentado considerablemente la mejora de los sistemas de almacenamiento de datos de las empresas. El problema es que, a medida que aumenta nuestra capacidad para almacenar y acceder a la información, más problemas tenemos para tratarla. Un ejemplo claro lo podemos ver en la “revolución” que ha supuesto internet y en cómo la información que se genera dentro de cualquier campo de nuestro interés aumenta considerablemente cada año, mientras que a su vez, cada vez nos vemos más incapaces de asimilarla. En la industria, igualmente, la preocupación de las empresas por producir “mejor y más barato”, la búsqueda constante de reducir “incertidumbre” en el proceso de fabricación y el aumento creciente de la información que se tiene que los procesos productivos, hace que crezca, cada vez más, la necesidad por analizarla. Bien es cierto, que esta necesidad solo aparece cuando la empresa tiene un volumen de históricos del proceso realmente importante. El Análisis de la InformaciónTambién la evolución de la tecnología ha facilitado y automatizado en gran medida las tareas de análisis de información. Cada paso en esta evolución se apoya en los anteriores, y cada uno de ellos ha supuesto un avance significativo para el usuario, que ha visto como cada progreso le abría nuevas posibilidades de análisis y aumentaba el nivel de abstracción de las consultas. Para decidir cuál es la técnica más adecuada para una determinada situación, es necesario distinguir el tipo de información que se desea extraer de los datos. Según su nivel de abstracción, el conocimiento contenido en los datos puede clasificarse en distintas categorías y requerirá una técnica más o menos avanzada para su recuperación. Éstas son las tres categorías de conocimiento con las que nos podemos encontrar. Conocimiento Evidente Se trata de la información fácilmente recuperable con una simple consulta (por ejemplo con un lenguaje como el SQL). Un ejemplo de este tipo de conocimiento es una pregunta como “¿Cuáles fueron las ventas en España el pasado marzo?”. Conocimiento Multidimensional El siguiente nivel de abstracción consiste en considerar los datos con una cierta estructura. Por ejemplo, en vez de considerar cada transacción individualmente, las ventas de una compañía pueden organizarse en función del tiempo y de la zona geográfica, y analizarse con diferentes niveles de detalle (país, región, localidad, …). Técnicamente, se trata de reinterpretar una tabla con n atributos independientes como un espacio n-dimensional, lo que permite detectar algunas regularidades difíciles de observar con la representación monodimensional clásica. Este tipo de información es la que analizan las herramientas OLAP, que estudiaremos más adelante y que resuelven de forma automática cuestiones como “¿Cuáles fueron las ventas en España el pasado marzo? aumentando el nivel de detalle: mostrar las de Madrid”. Conocimiento Oculto Se trata de la información no evidente, desconocida a priori y potencialmente útil, que puede recuperarse mediante técnicas de minería de datos, como reconocimiento de regularidades. Esta información es de gran valor, puesto que no se conocía y se trata de un descubrimiento real de nuevo conocimiento, del que antes no se tenía constancia, y que abre una nueva visión del problema. Un ejemplo de este tipo sería “¿Qué tipos de clientes tenemos? ¿Cuál es el perfil típico de cada clase de usuario?”. La Minería de Datos resuelve el problemaComo se ha visto en el punto anterior, las técnicas disponibles para extraer la información contenida en los datos son muy variadas y cada una de ellas es complementaria del resto, no excluyentes entre sí. Cada técnica resuelve problemas de determinadas características y para extraer todo el conocimiento oculto, en general será necesario utilizar una combinación de varias. La mayor parte de la información de interés contenida en una BD, aproximadamente el 80% corresponde a conocimiento superficial, fácilmente recuperable mediante consultas sencillas con SQL. El 20% restante corresponde a conocimiento oculto que requiere técnicas más avanzadas de análisis para su recuperación. Estas cifras pueden dar la false impresión de que la cantidad de información recuperable mediante técnicas de minería de datos es despreciable. Sin embargo, se trata precisamente de información que puede resultar de vital importancia para la empresa y que no se puede desdeñar. Básicamente, y como ya hemos comentado, la clave que diferencia la minería de datos respecto de las técnicas clásicas es que el análisis que realiza es exploratorio, no corroborativo. Se trata de descubrir conocimiento nuevo, no de confirmar o desmentir hipótesis. Con cualquiera de las otras técnicas es necesario tener una idea concreta de lo que se está buscando y, por tanto, la información que se obtiene con ellas está condicionada a la idea preconcebida con que se aborde el problema. Con la minería de datos es el sistema y no el usuario el que encuentra la hipótesis, además de comprobar su validez. Por lo tanto, queda claro que el descubrimiento de esta información “oculta” es posible gracias a la minería de datos, que entre otras sofisticadas técnicas aplica la inteligencia artificial para encontrar patrones y relaciones dentro de los datos permitiendo la creación de modelos, es decir, representaciones abstractas de la realidad. La obtención de un buen modelo permitirá una buena comprensión del funcionamiento de una empresa, y será una base idónea para la toma de decisiones. Es decir, dado que el objetivo último de la gestión de los datos corporativos es ofrecer información de calidad a la dirección, cuanto más eficiente sea el proceso de minería, mayor será en cantidad y en calidad la información disponible para soportar la toma de decisiones. Mediante éstas herramientas y técnicas se pueden obtener patrones y estructuras de información muy valiosas para la industria que pueden ayudar, mediante el análisis de los grandes volúmenes de datos de históricos almacenados, a mejorar la calidad y reducir los costes de los procesos productivos así como comprender mejor las causas que generan fallos en los mismos. Los beneficios de la utilización de las técnicas de minería de datos en diversas organizaciones son enormes, de forma que las empresas más innovadoras, las están incorporando con gran éxito de forma extensiva. Aplicaciones de la Minería de DatosLa información hallada a través de las técnicas de minería de datos tiene numerosas aplicaciones en el mundo empresarial. Las aplicaciones más usadas son las que necesitan algún tipo de predicción. Por ejemplo, cuando una persona solicita una tarjeta de crédito, la compañía emisora quiere predecir si la persona constituye un buen riesgo de crédito. La predicción tiene que basarse en los atributos conocidos de la persona, como edad, sus ingresos, sus deudas, etc. Las reglas para realizar la predicción se deducen de los mismos atributos de titulares de tarjetas de crédito pasados y actuales, junto con su conducta observada. Otra clase de aplicaciones busca asociaciones. Por ejemplo, los libros que se suelen comprar juntos. Si un cliente compra un libro, puede que la librería en línea le sugiera otros libros asociados. Puede que otros tipos de asociación lleven al descubrimiento de relaciones causa-efecto. Por ejemplo, el descubrimiento de asociaciones inesperadas entre un medicamento recién introducido y los problemas cardíacos llevó al hallazgo de que el medicamento podía causar problemas cardíacos en algunas personas. El medicamento se retiró del mercado. Las asociaciones son un ejemplo de patrones descriptivos. Las agrupaciones son otro ejemplo de este tipo de patrones. Algunos ejemplos de campos de aplicación de la minería de datos en el mundo empresarial son: Gestión de mercados y de riesgos. Diseño de estrategias competitivas. Ingeniería financiera y promoción comercial. Detección de fraudes. Al igual que en el mundo empresarial, en el medio científico es muy habitual la recolección de gran cantidad de datos, de los que resulta muy difícil extraer conocimiento. Por ello, la minería de datos se está aplicando en campos como: Diagnóstico médico. Clasificación y estudio de señales biomédicas. Detección de patrones en imágenes astronómicas. Análisis de biosecuencias en biomedicina. Técnicas documentales. Estudiamos ahora con más profundidad algunas de las aplicaciones más concretas de la minería de datos dentro de las organizaciones en campos como: marketing, predicción, reducción de riesgos, detección de fraudes y control de calidad. Marketing Éste es uno de los campos donde los éxitos de la minería de datos son más conocidos. Cuanto más precisa sea la información que tengamos sobre los clientes, mayores posibilidades tendremos de aumentar nuestros ingresos y rentabilizar al máximo nuestras acciones. El objetivo fundamental puede resumirse en determinar quién comprará qué, cuándo y dónde. Veamos tres aplicaciones concretas dentro del marketing: Targeting: Podemos aumentar espectacularmente el porcentaje de respuesta a una campaña de marketing si se dirige a los objetivos adecuados. La minería de datos permite detectar entre los potenciales clientes los que presentan una mayor probabilidad de responder a la campaña y dirigirla a ellos específicamente, con lo cual se consigue reducir drásticamente los costes. Fidelización de clientes: Conseguir un nuevo cliente o recuperar uno perdido resulta mucho más costoso que mantener uno que ya lo es. De ahí la rentabilidad de las campañas de fidelización de clientes, que detectan aquéllos que parece más probable que se vayan a perder, permitiendo llevar a cabo iniciativas que eviten dicha pérdida. La minería de datos también permite detectar nuevas oportunidades de mercado, comparando hábitos de consumo de diferentes clientes, por ejemplo, determinando la ubicación más conveniente para un determinado negocio. Predicción Conocer a priori cómo evolucionará una variable en el futuro constituye una información muy valiosa y supone una indudable ventaja competitiva. Se trata de una herramienta de evidente interés tanto desde el punto de vista comercial, como en gestión o control de procesos. A partir de los datos históricos almacenados y utilizando técnicas de minería de datos pueden elaborarse modelos que permitan estimar con precisión la evolución de una variable de futuro. Disponer de esta información con tiempo suficiente permite adecuar la respuesta de forma óptima. Esto puede resulta útil en los campos más diversos: Detección de oportunidades. Prevención de problemas. Gestión óptima del personal. Optimización de stocks. Reducción de Riesgos La minería de datos permite construir sistemas de evaluación automática de riesgos, basados en la experiencia previa. Estos sistemas resultan de gran utilidad cuando la cantidad de casos a evaluar es excesiva para su procesamiento manual. El empleo de técnicas de minería de datos ha aumentado la eficacia y fiabilidad de dichos sistemas, logrando un comportamiento más similar al de los expertos humanos. Detección de Fraudes Aplicando técnicas de minería de datos, pueden obtenerse modelos que permitan descubrir posibles fraudes, basándose en la detección de comportamientos anómalos, en comparación con los datos registrados anteriormente. Podemos encontrar aplicaciones concretas en operadores de telefonía o empresas de gestión de tarjetas de crédito. Estas compañías analizan el uso que los clientes hacen de sus servicios y pueden localizar, de manera muy rápida, un uso fraudulento de los mismos. Control de Calidad Existen numerosos ejemplos en los que se han aplicado técnicas de minería de datos para desarrollar sistemas automáticos de control de calidad. Estos sistemas suponen un considerable ahorro en el proceso productivo, puesto que facilitan: Detección más precisa de productos defectuosos: A menudo el control de calidad se realiza de forma manual y, por tanto, depende de una evaluación subjetiva por parte del personal encargado del mismo. El principal problema de este método es que el criterio de calidad no es estable sino que depende de la persona que realiza el análisis. La minería de datos permite desarrollar sistemas automáticos de control de calidad que discriminan los productos defectuosos con un alto grado de precisión y fiabilidad, según un criterio objetivo. Localización precoz de defectos: El control de calidad no sólo debe realizarse al final del proceso. Cuanto antes se detecte un fallo, menor será su impacto. A menudo no resulta fácil medir la variable que determina la calidad del producto en tiempo real o en la cadena de producción. En estos casos, es imprescindible utilizar técnicas de minería de datos para descubrir posibles relaciones que permitan detectar los fallos utilizando las variables disponibles durante el proceso. Identificación de causas de fallos: La minería de datos no sólo resulta útil para discriminar los productos defectuosos. También ayuda a determinar los fallos más frecuentes así como identificar las causas de los mismos. Esto permite adoptar medidas para evitarlos en el futuro. Análisis no destructivo: A menudo, para obtener la información que se necesita, hay que realizar un análisis destructivo. Un ejemplo típico es la evaluación de la resistencia de un material, medida que se establece forzándolo hasta que se rompe. Utilizando minería de datos es posible estimar con bastante exactitud el valor de este tipo de parámetros en función de otras características que sí pueden medirse sin destruir el producto. Esto permite controlar la calidad de todos los productos fabricados y no sólo de una pequeña muestra, ya que no se destruyen con el examen. Tecnología y AlgoritmosAntes de estudiar las técnicas y algoritmos principales, vamos a ver los modelos que a lo largo del tiempo han ido apareciendo y en los que se apoya la minería de datos. Modelos de la Minería de DatosModelo de Verificación Este es el modelo más parecido al proceso tradicional de extracción de información basado en lenguajes de consulta a BD (por ejemplo SQL). Su principal característica es que no extrae información nueva, sino que, basándose en los datos del almacén, verifica la validez de las afirmaciones que se le presentan. El proceso comienza por el establecimiento de una hipótesis por parte del usuario. Este, a continuación, solicita a la herramienta que verifique su validez. Una vez recibida la respuesta, el usuario puede refinar o detallar la hipótesis, preparar unas preguntas más específicas y solicitar una nueva verificación. De esta manera se consigue un proceso iterativo dirigido por un operador humano. La desventaja de este modelo es, que si al usuario no se le ocurre realizar una pregunta clave, o no ve una relación importante entre diferentes elementos de la BD, la herramienta por sí sola carece de iniciativa para investigar por su propia cuenta. Los nuevos Modelos Automáticos La minería de datos ha dado lugar a una paulatina sustitución del análisis de datos dirigido a la verificación, por un enfoque de análisis de datos dirigido al descubrimiento del conocimiento. La principal diferencia entre ambos se encuentra en que en el último, se descubre información sin necesidad de formular previamente una hipótesis. La aplicación automatizada de algoritmos de minería de datos permite detectar fácilmente patrones en los datos, razón por la cual esta técnica es mucho más eficiente que el análisis dirigido a la verificación cuando se intenta explorar datos procedentes de repositorios de gran tamaño y complejidad elevada. Dichas técnicas emergentes se encuentran en continua evolución como resultado de la colaboración entre campos de investigación tales como BD, reconocimiento de patrones, inteligencia artificial, sistemas expertos, estadística, visualización, recuperación de información, y computación de altas prestaciones. Los algoritmos de minería de datos se clasifican en dos grandes categorías de modelos con distintas denominaciones: Modelos predictivos, también llamados: Modelos supervisados. Modelos basados en la memoria. Minería de datos dirigida. Modelos de descubrimiento del conocimiento, también llamados: Modelos no supervisados. Modelos descriptivos. Minería de datos no dirigida. Por lo tanto con los nuevos modelos usamos la minería de datos para: Predecir : Utilizar algunas variables o campos en un BD para predecir valores desconocidos o futuros. Describir : Encontrar patrones que describan la información (interpretables por el hombre). Modelos Predictivos Los algoritmos supervisados o predictivos predicen el valor de un atributo (etiqueta), de un conjunto de datos, conocidos otros atributos (atributos descriptivos). A partir de datos cuya etiqueta se conoce, se induce una relación entre dicha etiqueta y otra serie de atributos. Esas relaciones sirven para realizar la predicción en datos cuya etiqueta es desconocida. Esta forma de trabajar se conoce como aprendizaje supervisado y se desarrolla en dos fases: Entrenamiento: Construcción de un modelo usando un subconjunto de datos con etiqueta conocida. Prueba: Prueba del modelo sobre el resto de los datos. El usuario indica sobre qué variables se quiere obtener la predicción y el sistema proporciona la respuesta. Esta respuesta la puede proporcionar explicando cómo la consiguió, lo cual a su vez puede ser una información tan valiosa como la respuesta en sí misma, o sin explicarlo. Cuando una aplicación no es lo suficientemente madura no tiene el potencial necesario para una solución predictiva fiable. En este caso se puede optar por diversos caminos alternativos: Modelo predictivo restringido: No se obtiene predicción alguna. Modelo predictivo no restringido: Se obliga a la realización de una predicción de menor fiabilidad. Modelos de descubrimiento del conocimiento: Que descubren patrones y tendencias en los datos actuales (no utilizan datos históricos). Ejemplos: ¿Cuál es el riesgo de este cliente?, ¿Se quedará el cliente? Algunas técnicas asociadas a los modelos predictivos: Clasificación: Clasificar datos en clases predefinidas. Estimación: A diferencia de la clasificación (que trata con resultados discretos), la estimación trata con valores numéricos continuos. A partir de un conjunto de valores de entrada, la estimación obtiene un valor para cierta variable continua, como puede ser una renta, la altura, etc. Predicción de valores: Una predicción no es más que un tipo de clasificación o estimación. Regresión: función que convierte datos en valores de una función de predicción. Árboles de decisión: Son estructuras en forma de árbol que representan conjuntos de decisiones. Estas decisiones generan reglas para la clasificación de un conjunto de datos. Redes neuronales artificiales: Modelos predecibles no lineales que aprenden a través del entrenamiento y semejan la estructura de una red neuronal biológica. Series temporales. Modelos de Descubrimiento del Conocimiento El objetivo de estos modelos es establecer algún tipo de relación entre todas las variables. En estos modelos se utiliza la herramienta de minería para descubrir nueva información que no estaba anteriormente en el almacén de forma explícita. Según este modelo es la propia herramienta quien se plantea sus propias preguntas, sin necesidad de que el usuario establezca hipótesis o realice preguntas concretas, aunque, éste puede intervenir para guiar los caminos a explorar. Habitualmente esta búsqueda se dirige hacia la categorización de los registros en grupos para detectar patrones aplicables o extraer relaciones implícitas en los datos. También es común la búsqueda de elementos extraños o fuera de lo normal. Ejemplo: Un cliente que compra productos dietéticos es tres veces más probable que compre caramelos. Algunas técnicas asociadas a los modelos de descubrimiento del conocimiento: Asociación: Permite establecer las posibles relaciones entre acciones o sucesos aparentemente independientes. Reconocimiento de patrones: Permite la asociación de una señal o información de entrada con aquella o aquellas con las que guarda mayor similitud, y que ya están catalogadas en el sistema. Segmentación o agrupamiento: Esta herramienta posibilita la identificación de tipologías o grupos en los cuales los elementos guardan similitud entre sí y se diferencian de los de otros grupos. Clustering: Es la tarea de segmentar un grupo diverso en un número de subgrupos más similar (denominados clusters). Lo que distingue el clustering de la clasificación es que éste no requiere un conjunto predefinido de clases. Reglas de asociación: Se trata del agrupamiento por afinidad que tiene como objetivo determinar qué cosas van juntas. Detección de desviaciones. ClasificaciónDentro de los modelos de predicción, una de las técnicas más importantes es la clasificación. En este apartado vamos a describir qué es la clasificación, a estudiar técnicas para la creación de un tipo de clasificadores, denominados clasificadores de árboles de decisión y se analizarán otras técnicas de predicción. De manera abstracta, el problema de la clasificación es el siguiente: dado que los elementos pertenecen a una de las clases y dados los casos pasados de los elementos junto con las clases a las que pertenecen, el problema es predecir la clase a la que pertenece un elemento nuevo. La clasificación se puede llevar a cabo hallando reglas que dividan los datos dados en grupos disjuntos. Continuando con el ejemplo de un banco tiene que decidir si debe conceder una tarjeta de crédito a un solicitante. El banco tiene diversa información sobre esa persona, la cual puede utilizar para adoptar una decisión. Para adoptar la decisión el banco asigna un nivel de valor de crédito de: excelente, bueno, mediano o malo a cada integrante de un conjunto de muestras de clientes actuales según su historial de pagos. Luego, el banco intenta hallar reglas que clasifiquen a sus clientes como excelentes, buenos, medianos o malos. El proceso de creación de clasificadores comienza con un muestra de los datos, denominada conjunto de formación . Para cada tupla del conjunto de formación ya se conoce la clase a la que pertenece. Existen diversas maneras de crear clasificadores. Una de las técnicas más utilizadas para este fin son los clasificadores de árboles de decisión. Clasificadores de árboles de decisión Los clasificadores de árboles de decisión son una técnica muy utilizada para la clasificación. Como sugiere su nombre estos clasificadores utilizan un árbol. Cada nodo hoja tiene una clase asociada, y cada nodo interno tiene un predicado o función asociado. Continuando con el ejemplo, para concretar las reglas que clasifican los clientes en excelentes, buenos, medianos o malos, vamos a considerar dos atributos: titulación e ingresos. En la siguiente figura se muestra un árbol de decisión que establece las reglas concretas de clasificación. Para clasificar un nuevo caso se empieza por la raíz y se recorre el árbol hasta alcanzar una hoja. En los nodos internos se evalúa el predicado o función, para hallar a que nodo hijo hay que ir. El proceso continúa hasta llegar a un nodo hoja. Creación de Clasificadores de árboles de decisión La pregunta que se plantea es el modo de crear un clasificador de árboles de decisión, dado un conjunto de casos de formación. La manera más frecuente de hacerlos es utilizar un algoritmo impaciente , que trabaja de manera recursiva, comenzando por la raíz y construyendo el árbol hacia abajo. Inicialmente solo hay un nodo, la raíz, y todos los casos de formación están asociados con este nodo. En cada nodo, si todos o casi todos los ejemplos de formación asociados con el nodo pertenecen a la misma clase, el nodo se convierte en un nodo hoja a esa clase. En caso contrario, hay que seleccionar un atributo de partición o condiciones de partición para crear nodos hijos. En el ejemplo elegido, se escoge el atributo titulación y se crean cuatro hijos, uno por cada valor de la titulación. Las particiones en menor número de conjuntos son preferibles a las particiones en muchos conjuntos, ya que llevan a árboles de decisión más sencillos y significativos. Hay que averiguar el modo de hallar la mejor partición para un atributo. El modo de dividir un atributo depende del tipo de atributo. Los atributos pueden tener dos tipos de valores: Valores continuos : Los valores se pueden ordenar de manera significativa para la clasificación, como la edad o los ingresos. Valores categóricos : No tienen ningún orden significativo para la clasificación, como los nombres de los departamentos o de los paises. Generalmente los atributos que son números se tratan como valores continuos, y los atributos de cadenas de caracteres se tratan como categóricos. En el ejemplo escogido se ha tratado el atributo titulación como categórico y el atributo ingresos como valor continuo. En primer lugar se considera el modo de hallar las mejores particiones para los atributos continuos. Por sencillez solo se consideran particiones binarias de los atributos con valores continuos, es decir, particiones que den lugar a dos hijos. En caso de las particiones múltiples ya es más complicado y se pueden dar con valores continuos o categóricos. Para los atributos categóricos se pueden tener particiones múltiples, con un hijo para cada valor del atributo. Esto funciona muy bien para los atributos categóricos con pocos valores diferentes, como la titulación o el sexo. La idea principal de construcción de árboles de decisión es la evaluación de los diferentes atributos y de las distintas condiciones de partición y la selección del atributo y de la condición de partición que generen el índice máximo de ganancia de información. El mismo procedimiento funciona de manera recursiva en cada uno de los conjuntos resultantes de la partición, lo que hace que se construya de manera recursiva el árbol de decisión. Otros Tipos de Clasificadores Hay varios tipos de clasificadores a parte de los clasificadores de árbol. Dos tipos que han resultado bastante útiles son: Clasificadores de redes neuronales : Utilizan los datos de formación para adiestrar redes neuronales artificiales. Clasificadores bayesianos : Hallan la distribución de los valores de los atributos para cada clase de los datos de formación. Regresión La regresión trata la predicción de valores, no de clases. Dados los valores de un conjunto de variables, X1, X2, …, Xn se desea predecir el valor de una variable Y. Por ejemplo se puede tratar el nivel educativo con un número y los ingresos con otro número, y con base a estas dos variables, querer predecir la posibilidad de impago, que podría ser un porcentaje de probabilidad de impago o el importe impagado. AsociacionesComo ya se dijo, las asociaciones permiten establecer las posibles relaciones entre acciones o sucesos aparentemente independientes. Así, se puede reconocer cómo la ocurrencia de un determinado suceso puede inducir la aparición de otro u otros. Este tipo de herramientas son particularmente útiles, por ejemplo, para comprender los hábitos de compra de los clientes y para la concepción de ofertas, de ventas cruzadas y del “merchandising”. Los comercios en general suelen estar interesados en las asociaciones entre los diferentes artículos que compra la gente. Ejemplos de estas asociaciones son: Alguien que compra pan es bastante probable que compre también leche. Una persona que compró un libro X es bastante probable que también compre el libro Y. Reglas de Asociación Un ejemplo de regla de asociación es: pan =&gt; leche . En el contexto de las compras de alimentación, la regla dice que los clientes que compran pan también tienden a comprar leche con una probabilidad elevada. Una regla de asociación debe tener una población asociada: la población consiste en un conjunto de casos. En el ejemplo de la tienda de alimentación, la población puede consistir en todas las compras en la tienda de alimentación, cada compra es un caso. Las reglas tienen un soporte, así como una confianza asociados. Los dos se definen en el contexto de la población: El soporte : Es una medida de la fracción de la población que satisface tanto el antecedente como el consecuente de la regla. Por ejemplo, supongamos que solo el 0,001% de todas las compras incluyen leche y clavos. El soporte de la regla leche =&gt; clavos es bajo. Las empresas no suelen estar interesadas en reglas que tienen un soporte bajo, ya que afectan a pocos clientes y no merece la pena prestarles atención. La confianza : Es una medida de la frecuencia con que el consecuente es cierto, cuando lo es el antecedente. Por ejemplo la regla pan =&gt; leche tiene una confianza del 80% si el 80% de las compras que incluyen pan incluyen también leche. Hay que tener en cuenta que la confianza de pan =&gt; leche puede ser muy diferente de la confianza leche =&gt; pan , aunque las dos tiene el mismo soporte. Otros Tipos de Asociación El uso de meras reglas de asociación tiene varios inconvenientes. Uno de los principales es que muchas asociaciones no son muy interesantes, ya que pueden predecirse. Por ejemplo, si mucha gente compra cereales y mucha gente compra pan, se puede predecir que un número bastante grande de personas comprará las dos cosas, aunque no haya ninguna relación entre las dos compras. Lo que resultaría interesante es una desviación de la ocurrencia conjunta de las dos compras. O dicho en términos estadísticos, lo que se busca son correlaciones entre los artículos. Otro tipo importante son las asociaciones de secuencias. Las series de datos temporales, como las cotizaciones bursátiles en una serie de días, constituyen un ejemplo de datos de secuencias. Bases de Datos MultidimensionalesLa idea básica empleada por las BD multidimensionales (BDM) es muy sencilla: en lugar de utilizar tablas bidimensionales para almacenar los datos, como se hace en una BD relacional (BDR), emplea tablas n-dimensionales (o hipercubos). Es algo parecido a utilizar una hoja de cálculo para el tratamiento de datos, solo que, se podrán utilizar más de dos dimensiones y se dispondrá de otras capacidades adicionales. Una BDM está diseñada para los sistemas de soporte de decisiones en la cual los datos tienen una estructura matricial (multidimensional) para su almacenamiento. Este tipo de organización admite consultas más complejas. Análisis MultidimensionalEl análisis multidimensional consiste en analizar hechos económicos o, de otros tipos, desde la perspectiva de sus dimensiones, abarcando los diferentes niveles de éstas. Con el análisis multidimensional se da respuesta a las consultas complicadas de los usuarios, que reflejan los diversos componentes que tienen sus organizaciones. Estos componentes puedes ser de dos tipos: cuantitativos y cualitativos. A estos componentes también se les llama dimensiones, y a los valores de los componentes (o dimensiones) se les llama atributos. Además, el detalle con el que se muestran los atributos puede variar, cada dimensión se puede descomponer en diferentes niveles de detalle, y éstos dependen de las necesidades del usuario. Las dimensiones definen dominios como geografía, producto, tiempo, cliente, … Los miembros de una dimensión se agrupan de forma jerárquica (dimensión geográfica: ciudad, provincia, autonomía, país, …). El Esquema Multidimensional La realización del análisis multidimensional a partir de trozos de información no sería nada práctica, lo que se pretende es tener disponible toda la información formando un solo conjunto, al que llamaremos esquema multidimensional. Una de las características principales del esquema multidimensional es la agregabilidad, gracias a la cual se pueden presentar los valores de una determinada dimensión según sus distintos niveles de detalle. Como es lógico para poder realizar agregación es necesario tener datos en el nivel más bajo de cada dimensión, y los niveles superiores se calcularán a partir de éstos. Para un óptimo análisis este esquema se soporta en las BBDD multidimensionales, éstas almacenan los datos en estructuras llamadas hipercubos (más de tres dimensiones). En la práctica estos hipercubos no son grandes matrices, sino que son matrices más reducidas que aparecen como una sola matriz. Esto reduce el espacio de índice requerido. El esquema multidimensional puede ser soportado encima de un SGBD relacional (ROLAP: OLAP sobre BD Relacionales). Para ello el esquema multidimensional deberá ser transformado para poder implementarse sobre un SGBD relacional (que solo soporta tablas planas). Una de las formas de hacer esta transformación es utilizar el “esquema en estrella”, que estudiaremos más adelante. Características del Análisis Multidimensional Navegabilidad: Cuando se habla de navegar se refiere a que se puede pasar de un punto a otro del esquema multidimensional. Estos movimientos son: Perforación (drill-down): Consiste en variar el nivel de detalle de los datos, desde los datos más resumidos a los más detallados. Se dice que drill-down es desagregar y Roll-up es agregar. Segmentación (slice and dice): Consiste en “recortar” un subconjunto de los datos moviéndose por los distintos datos de una misma dimensión o cambiando de dimensión. Es decir, es la capacidad de ver la BD desde diferentes puntos de vistas. El corte suele hacerse a lo largo del eje del tiempo para analizar tendencias. Se dice que slice es proyección y que dice es selección. Visualización: La presentación de los resultados se suele hacer en forma de cuadros o tablas de dos dimensiones, con el cálculo de totales parciales y generales. Se suelen fijar un conjunto de valores de dimensiones y mostrar en la tabla de dos dimensiones los valores en función de esas dimensiones. Representación gráfica: Suele ser un gráfico de dos dimensiones, donde los valores de las dimensiones fijadas aparecen como comentarios y las dimensiones variables son los ejes de coordenadas. Con este tipo de representaciones se suele perder una dimensión. Representación mediante mapas: Muy utilizada para dimensiones geográficas, donde se realizan perforaciones seleccionando la zona deseada. Cálculos dinámicos. Modelo de Datos Multidimensional (MDM)Se define un modelo de datos multidimensional como la disciplina específica para modelizar datos que es una alternativa a la modelización E/R. Es un modelo de datos (estático y dinámico) basado en estructuras multidimensionales. Un modelo multidimensional contiene la misma información que un modelo E/R pero agrupa la información en un formato simétrico cuyos objetivos serían: Que el usuario entienda mejor el modelo. Que el rendimiento y tiempo de respuesta de las consultas sea el óptimo. Que los cambios en el modelo se hagan con menos impacto y mayor facilidad. Veamos ahora los elementos que componen la visión estática de un modelo de datos multidimensional: Esquema de hecho (esquema de cubo): Es el objeto a analizar. Ejemplos: empleados, ventas, stocks, … Atributos de hecho o de medida: Atributos de tipo cuantitativo cuyos valores (cantidades) se obtienen generalmente por aplicación de una función estadística que resume un conjunto de valores en un único valor. Ejemplos: nº de empleados, cantidad vendida, precio medio, … Funciones resumen: Funciones de tipo estadístico que se aplican a los atributos de hecho. Ejemplos: frecuencia, suma, media, máximo, etc. Dimensiones: Cada uno de los ejes en un espacio multidimensional. Ejemplos: tiempo, espacio, productos, intervalos del nº de empleados, departamentos, etc. Atributos de dimensión o de clasificación: Atributos de tipo cualitativo (sus valores son modalidades) que suministran el contexto en el que se obtienen las medidas en un esquema de hecho. Ejemplos: días, semanas, ciudades, provincias, etc. Jerarquías: Varios atributos de dimensión unidos mediante una relación de tipo jerárquico. Ejemplos: día -&gt; semana -&gt; mes -&gt; año. Series temporales: Una de las dimensiones más habituales de cualquier BDM es el tiempo. Para guardar datos en función del tiempo, se utilizan las series temporales, que son tratadas como una dimensión más. Vamos a estudiar ahora con más detalle dos de los elementos fundamentales en las BDM: las dimensiones y las jerarquías. Utilizaremos para ello una serie de ejemplos que nos van a ayudar a entender mejor estos dos elementos. DimensionesEjemplo 1 Supongamos que queremos implementar una sencilla BD para almacenar la cantidad de dinero que se gasta en el pago de las pensiones atendiendo al tipo de pensión y a la comunidad autónoma en que se paga. En el caso de que hubiera dos tipos de pensiones, se podría establecer una BDM con una estructura similar a la de una hoja de cálculo, empleando tantas filas como tipos de pensiones y tantas columnas como comunidades. El gasto correspondiente a cada comunidad y pensión se almacenaría en la celdilla correspondiente, tal como se muestra a continuación: El equivalente relacional sería una tabla de 34 filas y 3 columnas: tipo de pensión, comunidad autónoma y gasto. En este ejemplo sencillo, el espacio de almacenamiento utilizado en ambos casos es el mismo, pero, ¿qué ocurre con los tiempos de acceso a la información? Si se quiere acceder al gasto en un tipo de pensión y una comunidad determinados (una sola fila), el tiempo de acceso será similar, siempre que la tabla relacional esté ordenada o tenga definido un índice por tipo de pensión y comunidad autónoma. Si se quiere obtener el gasto en pensiones de tipo 1 (P1) para todas las comunidades, entonces el tiempo de respuesta de la BDM es mejor, ya que solo tiene que sumar una fila de la matriz (17 sumas). En cambio en la BDR se debe recorrer todos los registros de la tabla para localizar aquellos que cumplan la condición definida (34 registros) o crear más índices. Ejemplo 2 Supongamos ahora, que también es necesario almacenar la forma de pago de las pensiones y que dicha forma de pago puede ser en efectivo, por talón o transferencia. La BDM tendría el aspecto siguiente: En esta estructura se emplea cada una de las tres dimensiones del cubo para representar cada uno de los campos que se utilizarían en el modelo relacional. Las celdas resultantes se emplean para almacenar el gasto para cada tripleta (CA, TP, FP). El equivalente relacional sería una tabla con 102 filas y 4 columnas: tipo de pensión, comunidad autónoma, forma de pago y gasto. De nuevo, las consultas de agregados (totales) serían más costosas en la BDR que en la BDM. JerarquíasOtro aspecto fundamental de las BDM es la posibilidad de jerarquizar las dimensiones. Vamos a ver esto con otro ejemplo. Ejemplo 3 Supongamos que, además de conocer el gasto por comunidades, se quiere saber también el gasto por localidades dentro de cada comunidad. La manera inmediata de representar esto consiste en añadir una nueva dimensión para crear un hipercubo de cuatro dimensiones. Sin embargo esta solución no es eficiente, ya que para cada fila de cada localidad, solo una de las celdillas contendrá el valor. Dicha celdilla será la correspondiente a la comunidad a la que pertenece la localidad. Con esta estructura se gasta mucho espacio de almacenamiento en celdillas que jamás van a contener datos, por lo tanto hay que buscar otro mecanismo que lo evite. La solución a este problema es crear una jerarquía de niveles en cada dimensión para representar los diversos grados de detalle. Si se dispone de este mecanismo, la solución al caso de las localidades sería tan simple como jerarquizar la dimensión de las comunidades autónomas, estableciendo las localidades como escalón inferior en la jerarquía. Para ofrecer esta alternativa el gestor debe ser capaz, de operar con las celdillas, de reconocer si el valor almacenado corresponde a una comunidad o a una localidad, de forma que al hallar totales o realizar cualquier otro tipo de operación no mezcle valores correspondientes a diferentes niveles jerárquicos. Por lo tanto, una celda es una posición formada por la intersección de cada uno de los elementos de las dimensiones que forman el cubo. La celda puede contener cero, uno o varios datos (cantidades). Este concepto de jerarquía es extensible a más de dos niveles, por lo que se puede afinar el grado de detalle obtenido al realizar las consultas. BD Multidimensionales vs BD RelacionalesTerminamos este apartado de BDM realizando una comparación entre estas BD y las BDR que son más conocidas. La utilización de BDM ofrece ventajas sobre las BDR siempre que se vaya a trabajar sobre datos agregados, totales, subtotales, etc. También son superiores a la hora de trabajar con series temporales, obtener vistas de unos datos en función de otros (vistas bidimensionales del hipercubo que forma la BDM) y manejar diversos grados de detalle. En resumen son unas BD adecuadas para el estudio de alto nivel de los datos, al ofrecer una mayor flexibilidad y rapidez de acceso para el análisis de los mismos. Por otra parte, si lo que se quiere es acceder a un dato individual básico, la ventaja de las BDM desaparece a favor de las BDR. Estas son capaces de recuperar un dato individual con la misma eficiencia que las multidimensionales, suelen ser capaces de almacenar mayor cantidad de información y además, dada su utilización masiva en sistemas OLTP, están optimizadas para la inserción de registros y el control concurrente de usuarios. La utilización de ambos tipos de BD no es excluyente. De hecho es frecuente utilizar una BDR para almacenar los datos de nivel más bajo de la jerarquía de una BDM, de forma que si se desea obtener un dato básico, se excava a través de la jerarquía multidimensional hasta acceder a la BDR. Procesamiento Analítico en Línea (OLAP)Dado que el volumen de datos almacenados en las BD suele ser elevado, hay que resumirlos de algún modo si se quiere obtener información que puedan utilizar los usuarios. Las herramientas OLAP soportan el análisis interactivo de la información de resumen. Definición de OLAPEl acrónimo OLAP significa Procesamiento Analítico en Línea (On-Line Analytical Processing), y se utiliza para hacer referencia a sistemas y herramientas de minería de datos que usan técnicas multidimensionales para la extracción y el análisis de los datos. Según E.F. Codd, que fue quién acuñó el término, OLAP es: la síntesis, el análisis y la consolidación dinámica de grandes volúmenes de datos multidimensionales. Según otra definición de OLAP: se trata de un término inventado para describir una aproximación dimensional interactiva al soporte de toma de decisiones (análisis desde la perspectiva de sus componentes o dimensiones, contemplando también los distintos niveles o jerarquías que éstas poseen). Siempre que se habla de tecnología OLAP el adjetivo más utilizado es “multidimensional”, ya sea para referirse a los datos, a su estructura, a la BD que se emplea o a casi cualquier otro aspecto del OLAP. Esta caracterización llega hasta el punto de identificar el OLAP y las BD multidimensionales como una misma cosa. Aunque indudablemente ambas tecnologías están relacionadas, la utilización de OLAP no implica necesariamente la utilización de BD multidimensionales. La pregunta que debemos respondes es, ¿qué requiere el usuario de OLAP? La respuesta es: Conceptos familiares para el usuario final: Dimensiones, medidas y jerarquías. Acceso inmediato a los datos. Información consistente. Navegación y consulta sencillas. Capacidades de generación de informes. Datos precalculados. Soporte de grandes volúmenes de datos. Flexibilidad de manejo y presentación. Potentes capacidades de análisis: Agregaciones, comparaciones, ratios, correlaciones, análisis de situaciones, contraste de hipótesis, descubrimiento de patrones y tendencias, previsiones, series temporales, etc. Características de los Sistemas OLAPLas características básicas de los sistemas OLAP son las siguientes: Ofrecen una visión multidimensional y jerarquizada de los datos. Son capaces de analizar tendencias a lo largo del tiempo. Pueden presentar vistas de un número reducido de dimensiones elegido por el usuario. Permiten ahondar en la jerarquía de los datos para acceder a los de más bajo nivel. Son interactivos y soportan múltiples usuarios concurrentes. Resulta ahora claro, vistas sus características, como los sistemas OLAP pueden beneficiarse de las funcionalidades de una BDM: La visión multidimensional y la jerarquizada van explícitas en la propia estructura de la BD. La herramienta OLAP, que posiblemente esté integrada en la BDM, solo tiene que ocuparse del manejo del cubo hiperdimensional para extraer los datos conforme a los criterios establecidos por el usuario. El estudio de tendencias se puede realizar aprovechando las series temporales de la BDM o, si no se dispone de dicho tipo de datos, realizando las operaciones y conversiones necesarias para manejar el tiempo como una dimensión adicional de la BD. La presentación de vistas se conoce en la jerga OLAP como “slice and dice” (cortar y trocear) y se podría traducir en algo así como segmentación. Esta característica de una herramienta OLAP consiste en la capacidad de extraer “rodajas” del hipercubo que forma la BDM. Estas rodajas se extraen dado un valor fijo para una o varias dimensiones y tomando el hipercubo resultante. La capacidad de perforar en los niveles de jerarquía se realiza, de nuevo, aprovechando la propia estructura de la BDM subyacente. En el caso de que se utilice una BDR como escalón inferior de la jerarquía, la herramienta OLAP debe ocuparse de que el acceso a dicho nivel sea transparente para el usuario. La interactividad y el soporte de múltiples usuarios simultáneos son capacidades que dependen en gran medida de los tiempos de respuesta del gestor de BD empleado, por lo que se puede utilizar como criterio orientativo a la hora de elegir el producto que se va a adquirir para construir el sistema. Implementación de Sistemas OLAPComo ya hemos comentado, debido a su orientación hacia el manejo de los datos organizados en dimensiones, el entorno natural de trabajo de los sistemas OLAP son las bases de datos multimensionales. No obstante también pueden trabajar sobre BD Relacionales, aunque en este caso sus prestaciones se ven disminuidas. Atendiendo a este criterio, los sistemas OLAP se pueden dividir en tres tipos principales, que estudiamos a continuación. MOLAP (Multidimensional-OLAP) Los primeros sistemas OLAP utilizaban arrays de memoria multidimensionales para almacenar los cubos de datos y se denominan OLAP multidimensional (MOLAP). Por lo tanto, funcionan sobre BD multidimensionales. Requieren un esfuerzo previo de modelización y construcción de la BD multidimensional y de otro continuo consistente en migrar los datos en formato relacional al nuevo formato multidimensional. A cambio ofrecen un rendimiento muy superior a la hora de realizar la extracción y el análisis de los datos, puesto que los datos a los que acceden están organizados en dimensiones y jerarquías. Los datos se almacenan en un sistema de matrices (hipercubo) en donde cada eje es una dimensión. ROLAP (Relational-OLAP) Posteriormente, los servicios OLAP se integraron en los sistemas relacionales y los datos se almacenaron en las BD relacionales. Estos sistemas se denominan sistemas OLAP relacionales (ROLAP). Estos sistemas permiten trabajar sobre las BD corporativas ya establecidas, ahorrando así el trabajo de crear y mantener nuevas BD multidimensionales. A cambio deben ocuparse de realizar la conversión entre la visión relacional de los datos mantenida por el SGDBR y el manejo multidimensional y jerárquico que debe ofrecer al usuario, lo cual acarrea un coste en tiempo y recursos de máquina. El almacenamiento se suele realizar en un esquema en estrella (no normalizado) o copo de nieve (normalizado), que vamos a estudiar posteriormente con detalle. Las tendencias actuales en estos sistemas ROLAP son: Desarrollo de técnicas específicas para el almacenamiento (índices join, bitmap, …) y optimización de consultas. Crear servidores SQL ampliado especializados en funcionar como Almacén de Datos. A su vez, estas tendencias dan lugar a dos tipos de modelos ROLAP: SGBD especializados de SQL: Proporcionan un lenguaje de consulta avanzado y soporte para el proceso de consultas SQL sobre esquemas en estrella y copo de nieve en entornos de solo lectura. Servidores ROLAP: Servidores intermedios que se sitúan entre el SGBDR y las herramientas cliente. Este middleware está especializado en el soporte de consultas OLAP multidimensionales que se optimizan para servidores relacionales específicos. Respecto a la elección entre MOLAP y ROLAP, en la práctica resulta mucho más habitual encontrar sistemas de almacén de datos, junto con sus correspondientes herramientas OLAP y de minería de datos, implementadas mediante BD relacionales. Esto es debido a la mayor experiencia de que se dispone para trabajar sobre BD Relacionales, a la gran cantidad de productos ya disponibles en el mercado y a la confianza que las organizaciones tienen en este tipo de BD. HOLAP (Hybrid-OLAP) Además de los dos sistemas descritos, aparecen los sistemas híbridos, que almacenan algunos resúmenes en la memoria y los datos básicos y otros resúmenes en las BD Relacionales, se denominan sistemas OLAP híbridos (HOLAP). Dicho de otra forma, los sistemas HOLAP proporcionan análisis multidimensional accediendo indistintamente a BD Multidimensionales o Relacionales. Muchos sistemas OLAP se implementan como sistemas cliente-servidor. El servidor contiene la BD Relacional y los cubos de datos MOLAP. Los sistemas clientes obtienen vistas de los datos comunicándose con el servidor. ROLAP: Tipos de DiseñoNos detenemos ahora en los sistemas ROLAP, que a pesar de no ser los que mejor se adaptan a una herramienta OLAP, si son muy utilizados. Veamos los diferentes tipos de diseño que se deben realizar para que estos sistemas puedan dar una respuesta eficiente. Esquema en Estrella Esquema relacional adaptado a la representación de datos multidimensionales. Se basa en una serie de tablas que representan dimensiones unidas mediante claves ajenas, a una principal que actúa como nexo llamada tabla de hechos y que almacena datos agregados y precalculados (tablas no normalizadas). Tabla de Hechos El contenido de una tabla de hechos está formado por: Clave principal: Concatenación de las claves de todas las tablas de dimensión asociadas a la tabla de hechos. Claves ajenas: Que referencian a las claves de las correspondientes dimensiones. Atributos de Hecho: atributos de tipo cuantitativo cuyos valores (cantidades) se obtienen generalmente por aplicación de una función estadística que resume un conjunto de valores en un único valor. Ejemplos: nº de empleados, cantidad vendida, precio medio, et. Por otro lado, las características principales de una tabla de hechos son: Filas con pocas columnas (pocos atributos). Nº de filas: Desde millones a más de miles de millones (tantas como celdas tenga el cubo). Acceso, en general, vía dimensiones. Tablas de Dimensión Las características de las tablas de dimensión son: Definen las dimensiones de negocio en términos familiares para los usuarios. Filas con numerosas columnas de texto, altamente descriptivas. Normalmente menos de un millón de filas. Combinadas con las tablas de hecho mediante claves ajenas. Altamente indexadas. No están relacionadas entre sí. Se utilizan como puntos de acceso a los datos detallados de la tabla de hechos. A veces se tienen que desnormalizar. Figura de Tabla de Hechos con Tabla de Dimensiones Al igual que sucede al manejar un hipercubo multidimensional, las consultas típicas en un esquema en estrella consisten en fijar un valor o rango de ellos para las dimensiones y, a continuación, obtener la información solicitada. La respuesta se encuentra realizando operaciones de unión natural (join) entre tablas de dimensiones y la tabla de hechos. Para optimizar las consultas, el gestor de BD debe ser capaz de reconocer que está trabajando con un esquema en estrella y hacer en primer lugar los join entre las tablas de dimensiones y, con el resultado, hacer un único join con la tabla de hechos, minimizando el número de accesos físicos. Esquema en Copo de Nieve El esquema en copo de nieve es una variante del esquema en estrella que presenta las tablas de dimensión estructuradas a más de un nivel (tablas normalizadas). Se utiliza cuando hay jerarquías en las dimensiones, lo que supone más claves ajenas. Ejemplo: Constelación de Estrellas La constelación de estrellas la forman varios esquemas en estrella y/o en copo de nieve que comparten dimensiones. Ejemplo: Índices Bitmap Para poder conseguir una cierta eficiencia en los accesos, hay que considerar una serie de aspectos en el diseño físico, tales como: Estructuras de índices (mapas de bits, índices de combinación, índices textuales). Vistas materializadas: Identificación de las vistas a materializar. Explotación de la vista materializada durante la consulta. Actualización de las vistas materializadas durante la carga y refresco. En este apartado vamos a estudiar cómo es la estructura de los índices bitmap. Los índices bitmap son un tipo especial de índice que almacena la información en bits en vez de múltiplos de bit (byte, doble byte) y que sirve para acelerar el acceso a filas con atributos de baja cardinalidad. Se dice que un atributo es de baja cardinalidad si su dominio está formado por pocos elementos. Ejemplo: el atributo sexo (H o M). Se trata de guardar un mapa de bits para cada posible valor del atributo, por lo que, como se dijo anteriormente, no es eficiente usar estos índices para valorares de alta cardinalidad. Ejemplo: el índice para sexo tendrá dos bitmaps. Para responder a consultas que se realicen sobre esquemas relacionales con índices bitmap, basta con hacer las operaciones lógicas apropiadas (AND, OR, NOT) sobre los bits de cada índice implicado en la consulta, lo cual es una operación muy rápida, mucho más que la comparación de cadenas o números que implica la utilización de índices de otro tipo. Este tipo de índices son útiles para indexar las tablas de dimensiones en esquemas en estrella o en copo de nieve, ya que muchas de estas dimensiones suelen tener su clave principal formada por un atributo de baja cardinalidad. Ejemplo: código de provincia, sexo, estado civil, etc. Elección de una Herramienta OLAPA la hora de elegir una herramienta OLAP hay que tener en cuenta, entre otros, los puntos siguientes: Si obliga a trabajar con una BD multidimensional (MOLAP), relacional (ROLAP) o si soporta ambas. En el caso de herramientas MOLAP es conveniente estudiar las capacidades de la BDM subyacente. Además hay que fijarse en su capacidad de aceptar accesos concurrentes y la carga de usuarios que admite, ya que el objetivo del OLAP es permitir el análisis interactivo. En el caso de herramientas ROLAP, la penalización en que se incurre al no utilizar BD multidimensional, y las facilidades que ofrece la herramienta para ofrecer una vista multidimensional de los datos (optimización de accesos a esquemas en estrella, en copo de nieve e índices bitmap). El límite en cuanto al número de dimensiones y de celdillas que puede manejar, sea o no multidimensional la BD subyacente. También la profundidad de los niveles de jerarquías y el manejo de series temporales. La capacidad de cálculo y la facilidad para especificar qué métodos y operaciones hay que aplicar a los datos. También debe disponer de herramientas y presentación de informes. El mantenimiento de las dimensiones y las jerarquías mediante herramientas automatizadas. Facilidad a la hora de modificar cualquiera de ambos elementos. Comparativa de OLAP y Otros SistemasTerminamos el estudio de los sistemas OLAP haciendo una comparativa de los mismos frente a otros. Por un lado sistemas muy relacionados, como son los Sistemas de Soporte a las Decisiones y la propia Minería de Datos, y por otros, sistemas antagónicos como los sistemas OLTP. Minería de Datos frente a OLAP y DSS Los sistemas de ayuda a la decisión (DSS) son herramientas sobre las que se apoyan los responsables de una empresa, directivos y gestores, en la toma de decisiones. Para ello, utilizan: Un Data Warehouse, en el que se almacena la información de interés para la empresa. Herramientas de análisis multidimensional (OLAP). Los DSS permiten al responsable de la toma de decisiones consultar y utilizar de manera rápida y económica las enormes cantidades de datos operacionales y de mercado que se generan en una empresa. Gracias al análisis OLAP, pueden verificarse hipótesis y resolverse consultas complejas. Además, en el curso del análisis, la interpretación de los datos puede dar lugar a nuevas ideas y enfoques del problema, sugiriendo nuevas posibilidades de análisis. Sin embargo, el análisis OLAP depende de un usuario que plantee una consulta o hipótesis. Es el usuario el que lo dirige y, por tanto, el análisis queda limitado por las ideas preconcebidas que aquél pueda tener. La minería de datos constituye un paso más en el análisis de los datos de la empresa para apoyar la toma de decisiones. No se trata de un técnica que sustituya los DSS ni el análisis OLAP, sino que los complementa, permitiendo realizar un análisis más avanzado de los datos y extraer más información de ellos. Como ya se ha comentado anteriormente, utilizando minería de datos es el propio sistema el que descubre nuevas hipótesis y relaciones. De este modo, el conocimiento obtenido con estas técnicas no queda limitado por la visión que el usuario tiene del problema. Las diferencias entre minería de datos y OLAP radican esencialmente en que el enfoque desde el que se aborda el análisis con cada una de ellas es completamente distinto. Fundamentalmente: El análisis que realizan las herramientas OLAP es dirigido por el usuario, deductivo, parte de una hipótesis o de una pregunta del usuario y se analizan los datos para resolver esa consulta concreta. Por el contrario, la minería de datos permite razonar de forma inductiva a partir de los datos para llegar a una hipótesis general. Además, las aplicaciones OLAP trabajan generalmente con datos agregados, para obtener una visión global del negocio. Por el contrario, la minería de datos trabaja con datos individuales, concretos, descubriendo las regularidades y patrones que presentan entre sí y generalizando a partir de ellos. Un ejemplo clarificará la diferencia entre ambas técnicas es el siguiente: Una pregunta típica de un sistema OLAP/DSS sería: “El año pasado, ¿se compraron más furgonetas en Cataluña o en Madrid?”. La respuesta de sistema sería del tipo “En Cataluña se compraron 12.000 furgonetas, mientras que, durante el mismo intervalo, en Madrid se compraron 10.000”. Obviamente es una información interesante y útil, pero restringida por la hipótesis realizada a priori. En cambio, un problema típico para resolver utilizando minería de datos sería, por ejemplo: “Hallar un modelo que determine las características más relevantes de las personas que compran furgonetas”. A partir de los datos del pasado, el sistema de minería de datos proporcionaría una respuesta del tipo: “Depende de la época del año y la situación geográfica. En invierno, los habitantes de Madrid que pertenecen a un cierto grupo de edad y nivel de ingresos probablemente comprarán más furgonetas que gente de las mismas características en Cataluña”. Como puede verse, se trata de problemas distintos, de modo que según los objetivos perseguidos deberá utilizarse una técnica u otra. Además, puesto que sus conclusiones son complementarias, en general será conveniente combinar ambas para obtener los mejores resultados. Sistemas OLTP vs Sistemas OLAP Como ya sabemos OLAP (On-Line Analytical Processing) se define como análisis rápido de información multidimensional compartida. El término OLAP aparece en contraposición al concepto tradicional OLTP (On-Line Transactional Processing), de designa el procesamiento operacional de los datos, orientado a conseguir la máxima eficacia y rapidez en las transacciones (actualizaciones) individuales de los datos, y no su análisis de forma agregada. Existen, por lo tanto, dos grupos de aplicaciones que se realizan en una empresa: Aplicaciones que ejecutan operaciones del día a día (compra, inventario, nóminas, …). Son los Sistemas de Procesamiento de transacciones en línea (OLTP). Aplicaciones que se encargan de analizar el negocio, interpretar lo que ha ocurrido y tomar decisiones (para mejorar los servicios al cliente, incrementar ventas,…). Son los Sistemas de Procesamiento analítico en línea (OLAP). Los dos son sistemas de procesamiento muy diferentes. Veamos las diferencias principales entre los dos sistemas: OLAP permite que una compañía decida qué debe hacer y OLTP ayuda a llevar a cabo la decisión. OLTP representa una “imagen” de los asuntos de la organización que se actualiza constantemente (con cada operación realizada). Los sistemas OLAP son estáticos, refrescándose periódicamente (cada semana, cada mes, …) a partir de las fuentes OLTP. El diseño de los sistemas OLTP elimina redundancias, y se piensa más en la eficiencia (transacciones rápidas) que en el usuario (dificultad para navegar). Los sistemas OLAP almacenan datos redundantes para conseguir un acceso sencillo al usuario y buenos tiempos de respuesta. OLTP proporciona capacidades muy limitadas para la toma de decisiones (los usuarios examinan la BD registro a registro). OLAP trabaja con un resumen de miles de registros “condensados” en una respuesta. Los sistemas transaccionales (u operacionales) automatizan el día a día del negocio, buscando la eficiencia. Los sistemas analíticos se centran en la estrategia a largo plazo y están dirigidos por el negocio. En cuanto a la implementación de OLTP y OLAP: Surgen los sistemas EIS y DSS (basados en OLAP) para soportar la toma de decisiones. Presentan problemas para recuperar datos de la BD Operacionales. No se puede implementar OLTP y OLAP en una sola BD. Actuando el SGBD como interfaz entre datos y usuarios. Se necesita una arquitectura dual de BD. En el siguiente cuadro, se observa de forma resumida, las características de los sistemas OLTP y OLAP, quedando así más claras sus diferencias. Big DataCon la irrupción de internet, llegaron nuevos conceptos que con el tiempo se han vuelto de uso cotidiano y que nos acompañan en nuestro día a día. Han repercutido para bien en nuestras vidas y casi no podemos entender las nuevas tecnologías sin estas geniales ideas. Uno de estos conceptos que han resonado mucho últimamente es Big Data ; aunque como ya ha pasado en anteriores ocasiones, el halo de escepticismo y desconfianza ha planeado en torno a todo lo que lo rodea. Hay muchas dudas (fundadas) en cuanto a su concepto, uso y alcance; de esta manera se crea un ambiente de recelo aparejado a algo que parece intangible, incontrolable y sobre todo, que puede atentar nuestra privacidad. Qué significa Big DataBig Data ( datos masivos en español, aunque apenas se utiliza la traducción) es el proceso de recolección de grades cantidades de datos y su inmediato análisis para encontrar información oculta, patrones recurrentes, nuevas correlaciones, etc; el conjunto de datos es tan grande y complejo que los medios tradicionales de procesamiento son ineficaces. Y es que estamos hablando de desafíos como analizar, capturar, recolectar, buscar, compartir, almacenar, transferir, visualizar, etc, ingentes cantidades de información, obtener conocimiento en tiempo real y poner todos los sentidos en la protección de datos personales. El tamaño para albergar todo el proceso ha ido aumentando constantemente para poder recopilar e integrar toda la información. La recolección de datos ha existido casi desde siempre, cuando en el amanecer el hombre hacía muescas en piedras o huesos para hacer seguimiento de las actividades cotidianas o de los suministros esenciales para subsistir. La invención de ábaco supuso un determinante empuje al cálculo y análisis que tanto necesitábamos cuando los dedos y la memoria no eran suficientes, y las primeras bibliotecas representaron además un primer intento de almacenar datos. En la época actual, todo lo que hacemos está continuamente dejando un rastro digital que se puede utilizar y analizar; los avances en tecnología, junto a la expansión de Internet y el almacenamiento en la nube, han provocado que crezca la cantidad de datos que podemos almacenar. Para resumir, se puede utilizar 5 V’s como definición de Big Data (empezaron siendo 3), que es lo que caracteriza al sistema y al mismo tiempo explica sus ventajas: Volumen . La más evidente y la que hace honor al nombre; captar y organizar absolutamente toda la información que nos llega es esencial para tener registros completos e insesgados, y que las conclusiones que obtengamos sirvan eficientemente a la hora de la toma de decisiones. Es el Business Intelligence que todos conocemos, pero a lo grande; aunque la diferencia con la clásica inteligencia de negocio viene marcada por el resto de V’s. Velocidad . Siempre es importante el tiempo si afrontamos tanto la necesidad de generar información (y recordemos que estamos hablando de muchos datos) como de analizarla, pero lo es más si necesitamos reaccionar inmediatamente; todo el proceso pide agilidad para extraer valor de negocio a la información que se estudia y que no se pierda la oportunidad. Variedad . Hay que dar uniformidad a toda la información, que tendrá su origen en datos de lo más heterogéneos, tal como veremos en el siguiente apartado. Una de las fortalezas del Big Data reside en poder conjugar y combinar cada tipo de información y su tratamiento específico para alcanzar un todo homogéneo. Veracidad . Se refiere a la calidad del dato y su disponibilidad; en un entorno descrito por la anterior V, Variedad , hay que encontrar herramientas para comprobar la información recibida; las tecnologías creadas al servicio del Big Data se muestran imprescindibles y eficientes para afrontar los retos. Valor . Trabajar con Big Data tiene que servier para aportar valor a la sociedad, las empresas, los gobiernos, en definitiva, a las personas; todo el proceso tiene que ayudar a impulsar el desarrollo, la innovación y la competitividad, pero también mejorar la calidad de vida de las personas. Tipos de datos en Big DataPara aclarar qué es lo que se recoge para el análisis, podemos dividirlos en dos grandes categorías: Datos estructurados . Aquellos que tienen longitud y formato (por ejemplo fechas) y que pueden ser almacenados en tablas (como las BDR). En esta categoría entran los que se compilan en los censos de población, los diferentes tipos de encuestas, los datos de transacciones bancarias, las compras en tiendas online, etc. Datos no estructurados . Son los que carecen de un formato determinado y no pueden ser almacenados en una tabla. Pueden ser de tipo texto (los que generan los usuarios de los foros, redes sociales, documentos de Word), y los de tipo no-texto (cualquier fichero de imagen, audio, vídeo). Dentro de esta categoría, podemos añadir los Datos semiestructurados , que son los que no pertenecen a BDR ya que no se limitan a campos determinados, aunque poseen organización interna o marcadores que facilita el tratamiento de sus elementos; estaríamos hablando de documentos XML, HTML o los datos almacenados en BD NoSQL. El uso del análisis de datosPara poder analizar todo esto, se precisa de técnicas potentes y avanzada; las clásicas medias o varianzas no son por sí solas suficientes para extraer toda esa cantidad de información, ni para entender los diferentes tipos de datos que hemos descrito. Antes de la irrupción Big Data , ya existían algoritmos matemáticos que nos facilitaban descubrir información oculta en los datos, como todos los que engloban el Data Mining (minería de datos): k-medias, árboles de decisión, redes neuronales, etc, que con la llegada de la potencia de cálculo de los ordenadores permitieron acortar el tiempo que se tardaba en obtener resultados. Aunque no se pensó para ser en tiempo real si no a posteriori, permite analizar datos para encontrar correlaciones entre ellos y de este modo desarrollar por ejemplo una estrategia de marketing adaptada a las conclusiones. Por eso el análisis de datos siempre ha tenido un gran peso en el marketing, un mejor conocimiento del consumidor y sus necesidades propicia saber cómo aumentar las ventas; el análisis de datos nos permite establecer relaciones entre variables, predecir comportamientos, realizar agrupaciones (clustering) de grupos homogéneos, e incluso analizar textos para extraer información. Ahora con Big Data , todo esto se consigue en tiempo real y con cada nueva actualización de nuestro repositorio de datos es posible ver los cambios en las estadísticas inmediatamente. Qué utilidad puede tenerComo todas las cosas en esta vida, puede tener un buen uso o usarse para propósitos “malvados”. Lo primero que llama la atención es el tema de la privacidad, ya que cada vez más detalles de nuestras vidas son almacenados y analizados por empresas y gobiernos; por supuesto, no es algo que nos debamos tomar a la ligera, pero a medida que siga avanzando la tecnología, habrá que ir adaptando las leyes y regulaciones para proteger a las personas. Por ahora, no hay más rastro de nosotros que los que ya estamos dejando día a día, y que ya están siendo analizados por terceros; a partir de este momento, todos esos registros se unen para formar un todo. Sí, podemos hablar de una representación de nosotros, pero no deja de ser un número entre millones de números, sin cara ni alma. Lo único que va a contar para estudiar es el comportamiento de grupos homogéneos tratados como tendencias en un segundo, para que al siguiente empiece de nuevo el proceso. En cambio los beneficios son muchos, y muy importantes. Veamos ejemplos. Una eCommerce puede optimizar el stock de sus almacenes a través de la información extraída de lo que busca la gente en su web o analizando las tendencias en redes sociales y foros; también fijar precios dinámicos en sus productos extrayendo datos de múltiples fuentes (las acciones de los clientes, preferencias de los proveedores o recopilación de precios de la competencia). El sector de las telecomunicaciones es una industria privilegiada, gracias a sus redes y a la proliferación de dispositivos móviles; la oportunidad más evidente es extraer información de la experiencia del usuario gracias al tráfico de voz y datos, y así poder ofrecer altas en contratos personalizados, ampliar la batalla por la competencia e incluso crear nuevas fuentes de ingresos. La banca tiene ante si un reto, y una oportunidad, de poner medios para luchar contra el fraude, los delitos financieros y las brechas de seguridad, mediante Big Data . Las entidades financieras están invirtiendo enormes cantidades de dinero en perfeccionar algoritmos y la tecnología de análisis para minimizar riesgos y fortalecer su imagen de cara al cliente. La Federación Alemana de Fútbol empezó a usar el análisis de grandes volúmenes de datos para mejorar el rendimiento de sus jugadores, y con los deberes bien hechos se presentaron en el Mundial de Brasil 2014. Si piensas que todo lo que puede dar de sí Big Data es sólo aprovechable por grandes corporaciones, vas mal encaminado; por ejemplo, las fuerzas de seguridad utilizan estas herramientas para perseguir criminales y luchar contra el terrorismo de cualquier tipo. En materia de sanidad, el cruce de información de historiales clínicos, antecedentes familiares, clima y entorno, junto a los hábitos de consumo, permitirá un modelo predictivo personal para cada paciente, y de esta manera ayudar en la detención precoz de enfermedades y estrategias más efectivas para combatirlas. En muchas ciudades, ya se usa el análisis de datos para transformarse en más modernas e inteligentes: transportes públicos interconectados para minimizar los tiempos de espera, o semáforos que ante la previsión de un aumento del tráfico e regulan para minimizar los atascos. Y por supuesto, las pymes también pueden subirse al carro del Big Data , ya que no es necesaria una gran inversión. Es suficiente con tener un CRM y a un analista de datos para extraer conclusiones de la información que utiliza una pyme, aunque siempre cabe la posibilidad de externalizarlo. Big Data, modelando el futuroTodo el mundo habla cada día más, es una tendencia en aumento y ha llegado para quedarse. A medida que las herramientas se hagan más accesibles, se integrará poco a poco en nuestras vidas y pasará de ser algo desconocido o temido, a una forma más de comprender el comportamiento humano y nuestra relación con el entorno. Es como el Social Media, al principio las empresas lo veía como algo ajeno a ellas, que no debían destinar recursos porque creían que no reportaría ningún beneficio; ahora, lo más normal es hacer Social Marketing y elaborar informes exhaustivos con las estadísticas derivadas de su presencia online. Pues ahora es el momento de cruzar esos datos con el resto de aspectos de la organización, como ventas, tráfico web, interacción con distribuidores, etc, para encontrar nuevas vías de negocio y crear nuevas estrategias. Y por supuesto, para analizar toda esta información, es necesario contar con profesionales que tengan parte analista y parte creativa; estos “ científicos de datos ” serán muy demandados por las empresas y organizaciones, por lo que se abre un interesantísimo campo laboral para los amantes de los números. Bibliografía Scribd (Roger Fabian Molina) MiBloguel"},{"title":"Bloque 2","date":"2019-01-16T15:11:23.000Z","path":"bloque-2/index.html","text":"Tecnologías actuales de ordenadores: de los dispositivos móviles a los superordenadores y arquitecturas escalables y de altas prestaciones. Computación en la nube. Base tecnológica. Componentes, funcionalidades y capacidades.Tecnologías actuales de ordenadores: de los dispositivos móviles a los superordenadores y arquitecturas escalables (grid, cluster, MPP, SMP, arquitecturas multinúcleo y otros).Tecnologías actuales de ordenadoresEl acrónimo TIC “Tecnologías de la Información y de la Comunicación” agrupa elementos y técnicas utilizadas en el tratamiento y transmisión de la información, principalmente de informática, internet y telecomunicaciones. Podemos decir que las TIC son herramientas informáticas que almacenan, procesan y presentan información de formas muy diferentes. Estas Tecnologías incluyen ordenadores, internet, tecnologías de radiodifusión (radio y televisión) y telefonía. Dispositivos móvilesGeneralmente, los dispositivos móviles los definimos como aquellos micro-ordenadores que son lo suficientemente ligeros como para ser transportados por una persona, y que disponen de la capacidad de batería suficiente como para poder funcionar de forma autónoma. Es importante destacar que los ordenadores portátiles no se consideran dispositivos móviles debido a que consumen más batería y suelen ser más pesados. A grandes rasgos se pueden dividir los dispositivos móviles en tres amplios grupos que son: teléfonos, PDAs y consolas. Teléfonos Son los más pequeños del grupo, y por tanto los más ligeros y más transportables. Su función primordial era clara históricamente, lo que hace un teléfono cualquiera: recibir y realizar llamadas; aunque desde hace ya tiempo es impensable concebir un teléfono móvil que solamente haga eso. Funcionalidades propias de ordenadores, o de dispositivos de otro tipo, como la grabación y edición de vídeo, realización de fotografías, lectura de documentos, localización en mapas, navegación por internet, y muchas cosas más. PDAs (Personal Digital Assistant) (Asistente Personal Digital) También son conocidos como ordenadores de mano u organizadores electrónicos. Su funcionalidad principal es servir como organizadores, con agenda, calendario, gestión de contactos, y posteriormente han ido creciendo, de forma que actualmente sirven tanto como aparatos en los que leer un libro como en los que encontrarse en un mapa. La línea que los separa de los teléfonos es cada vez más difusa. Consolas En realidad esta categoría debería llamarse “dispositivos orientados a jugar”, porque son más que simples consolas. Dos de los ejemplos en el mercado son la Sony PlayStation Portable (PSP) y la Nintendo DS, que no sólo sirven para jugar, sino que integran algunas de las funcionalidades típicas de una PDA, como reproducción de archivos multimedia, integración con agenda y calendario, o navegador de internet. Dispositivos fijos. Computadoras.Estos dispositivos requieren de una ubicación física permanente o prácticamente permanente ya que necesitan de una toma de corriente, a no ser que lleven una batería, como los ordenadores portátiles. Aun así, la autonomía de los portátiles es limitada lo que les convierte en dispositivos fijos. Además de su dependencia de la corriente eléctrica, una computadora tiene una envergadura y peso que les convierte en incómodos de trasladar. MicroordenadoresOrdenador portátil Llamamos ordenador portátil al ordenador que puede funcionar autónomamente sin necesidad de tenerlo enchufado a la red eléctrica, y el cual puede ser trasladado de un lugar a otro con facilidad. Se distinguen tres tipos de portátiles: Deskbook, Desktop y Mobile. Deskbook : Son portátiles de bajo precio. En realidad son utilizados como ordenadores de sobremesa pero que se pueden transportar. Son grandes, pesados y no traen batería. Desktop : Estos son los portátiles por excelencia. Son iguales que los deskbook pero estos sí que tienen batería incorporada, lo que hace que se puedan transportar con mayor facilidad. Son más ligeros y tienen prácticamente las mismas prestaciones que los de sobremesa. Mobile : Pertenecen a la última generación de ordenadores portátiles. Son los más ligeros, se calientan mucho menos y su batería tienen una autonomía mayor. Todo esto hace que sean más manejables. Además, hacen menos ruido. Tienen el mismo rendimiento y prestaciones que un desktop. Ordenador de sobremesa El conocido como PC (Personal Computer), Ordenador personal, etc. Es el computador por excelencia. Están fabricados para el uso de una persona, son de un tamaño medio… y cumple multitud de funcionalidades. Fueron concebidos para usuarios domésticos, pero su potencial y sus programas los han implantado en el ámbito laboral y profesional. Estaciones de Trabajo o Workstation Son equipos de gran potencia. Son sofisticados y especialmente diseñados para niveles de alto rendimiento. Suelen ser utilizados para ingeniería, cálculos técnicos, diseño, gráfico, diseño de software, … Estas computadoras de gama alta están equipadas con funciones adicionales como por ejemplo, procesadores más rápidos, monitores de alta resolución, tarjetas gráficas potentes, y aplicaciones integradas que vienen instaladas por defecto. Servidor Es un ordenador que ha sido optimizado para proveer de servicios a otros ordenadores sobre una red local o de internet. Usualmente disponen de procesadores de alta potencia, mucha memoria y varios discos duros de gran tamaño. MiniordenadoresSon ordenadores de tamaño medio, con unas capacidades intermedias entre ordenadores personales y los grandes ordenadores. Pueden ser utilizados por varios usuarios al mismo tiempo y disponen de mayores recursos que los microordenadores. Cuentan con una mayor capacidad de proceso, mayor memoria, periféricos más sofisticados y posibilidad de conectar más de un puesto de trabajo. Son también conocidos como ordenadores departamentales. Ordenadores grandes o MainframesSon ordenadores de gran capacidad, tanto de procesamiento como de almacenamiento, comunicaciones, etc. Son capaces de gestionar múltiples bases de datos, procesar miles de transacciones al minuto procedentes de miles de terminales a la vez. Es frecuente encontrar varios procesadores trabajando en paralelo, lo cual requiere sistemas más complejos y equipos especialistas. SuperOrdenadoresSon ordenadores de gran potencia y elevadísimas prestaciones. Se utilizan principalmente para cálculos científicos que necesitan una gran capacidad de proceso. Es capaz de realizar miles de millones de operaciones por segundo. Arquitecturas EscalablesUna arquitectura escalable es aquella que tiene la capacidad de incrementar el rendimiento sin que tenga que rediseñarse y simplemente aprovecha el hardware adicional que se le disponga. Generalmente podemos definir la escalabilidad como la capacidad que tiene un sistema informático de modificar su configuración o su tamaño, para ajustarse a los cambios. Dimensiones . La escalabilidad de un sistema se puede medir en distintas dimensiones. Escalabilidad de carga . Esto se hace más fácil mediante un sistema distribuido, podemos ampliar y reducir los recursos con mayor facilidad para adecuar las cargas ya sean pesadas o ligeras según sea necesario. Escalabilidad geográfica . Un sistema es escalable geográficamente cuando su uso y sus ventajas se conservan sin que afecte la distancia de los usuarios. Escalabilidad administrativa . Este debe de manejarse con facilidad sin importar las organizaciones que necesiten compartir un solo sistema distribuido. Escalabilidad vertical . También se dice escala hacia arriba, quiere decir que en un solo nodo del sistema es donde se han agregado más recursos. Ejemplo, añadir memoria a un disco duro de una computadora. Escalabilidad horizonal . Quiere decir que se agregan más nodos a un sistema. Ejemplo, agregar una nueva computadora a un programa de aplicación para espejo. MPP (Massive Parallel Processing)Un computador masivamente paralelo es un sistema de Memoria Distribuida que consiste en muchos nodos individuales, cada uno de los cuales es esencialmente un ordenador independiente en sí mismo, y consiste en al menos un procesador, su propia memoria y un enlace a la red que lo une con el resto de nodos. El término masivo supone cientos o miles de nodos. Los nodos se comunican por paso de mensajes, usando estándares como MPI. MPI (Message Passing Interface) Una API que permite a procesos comunicarse con otros enviando y recibiendo mensajes. Es un estándar de facto para programas paralelos ejecutándose en clusters de ordenadores y supercomputadores. SMP (Symmetric Multiprocessing)Una arquitectura multiprocesador donde dos o más procesadores idénticos se conectan a una Memoria Principal compartida y controlado por una sola instancia del sistema operativo. Está claro que está hablando de las máquinas PC de hoy día, donde cada chip es de doble o cuádruple núcleo, hay una memoria principal y hay un sistema operativo en ejecución. GridPuede ser visto como un sistema distribuido. Lo que lo distingue de un clúster es que el grid tiende a estar más débilmente acoplado, heterogéneo y geográficamente disperso. Aunque se puede dedicar el grid a una aplicación específica, es más común que un solo grid se use para una variedad de propósitos diferentes. Normalmente los recursos no se administran de forma centralizada, se usan estándares abiertos y se obtiene calidad de servicio. ClústerUn grupo de ordenadores enlazados, trabajando juntos, colaborando estrechamente, formando uno solo en muchos aspectos. Normalmente están conectados por redes de área local rápidas. El servicio que se suele dar es mejorar la disponibilidad y el rendimiento. Distinguimos estas categorías: Clústers de Alta Disponibilidad. Linux-HA es un proyecto que sirve para esto. Clústers de Balanceo de Carga. Clústers de computación. Donde empezamos a colisionar con el concepto de grid. Se usan para simulaciones, modelado, predicción del tiempo, etc. MultinúcleoLos núcleos pueden o no compartir caché y pueden implementar métodos de comunicación: paso de mensajes memoria compartida comunicación internúcleo Algunas topologías son: bus anillo malla bidimensional crossbar Ley de Amdahl Encontrar el máximo nivel de mejora para un sistema completo cuando solo una parte es mejorada. Se usa normalmente en computación paralela para predecir la mejora máxima teórica utilizando múltiples procesadores o núcleos. Por ejemplo, si un programa necesita 20 horas en un solo núcleo y una porción particular de 1 hora no puede ser paralelizada, pero las 19 horas restantes sí pueden serlo, entonces independientemente de cuantos procesadores utilicemos el mínimo tiempo de ejecución posible no puede ser menor a esa hora. Bibliografía Escuela de Administración Pública de Castilla y León . Danielside . Conceptos de sistemas operativos: Características, evolución y tendencias. Estructuras, componentes y funciones. Sistemas operativos multiprocesador.Conceptos de Sistemas OperativosIntroducciónUn Sistema Operativo es un programa que administra el hardware de una computadora. También proporciona las bases para los programas de aplicación, y actúa como intermediario entre el usuario y el hardware. Estas tareas pueden ser llevadas a cabo de varias formas, lo que permite que algunos Sistemas Operativos se diseñen para ser prácticos, otros eficientes y otros para ser ambas cosas. ¿Qué hace un Sistema Operativo?Un sistema informático puede dividirse en cuatro componentes: el hardware, el Sistema Operativo, los programas de aplicación y los usuario. El Sistema Operativo controla y coordina el uso del hardware entre los diversos programas de aplicación por parte de los distintos usuarios. También podemos ver un sistema informático como hardware, software y datos. El Sistema Operativo proporciona los medios para hacer un uso adecuado de estos recursos durante el funcionamiento del sistema informático. Definición Sistema OperativoUn Sistema Operativo es un programa, o conjunto de programas eficiente y productivo en el uso de un computador (hardware), permitiendo la ejecución de aplicaciones de usuario. Es el intermediario entre las aplicaciones de usuario y el hardware. Metas: Brindar un ambiente de realización y ejecución de aplicaciones. Proveer un entorno sin interferencias a cada usuario (interferencia: lo que un usuario modifica en su entorno, no interfiera ni modifique lo de otro usuario). Administrar de forma equitativa los recursos (hardware y software). Hacerlo de la forma más amigable e intuitiva posible. Todas las aplicaciones de usuario requieren un conjunto común de operaciones que son incorporadas al Sistema Operativo. Tareas principales: Implementar diferentes entornos para diferentes usos (interfaz gráfica, shells, tipo web, etc). Proveer una o más interfaces con el usuario. Proveer a las aplicaciones un conjunto de servicios (a través de los “system services”). Eficiencia y equidad en la administración de recursos. Se puede decir que el Sistema Operativo es un: Administrador de recursos. Sus tareas consisten en administrar los recursos disponibles y decidir como asignar estos recursos según los pedidos y asignaciones que tenga. Programa de control. Controla la ejecución de los programas para la prevención de errores y mal uso del sistema. Frecuentemente la porción residente del propio Sistema Operativo se denomina núcleo del sistema (Kernel) . Evolución de los Sistemas OperativosLa informática tal y como se le conoce hoy día, surgió a raíz de la II Guerra Mundial, en la década de los 40. En esos años no existía siquiera el concepto de “Sistema Operativo” y los programadores interactuaban directamente con el hardware de las computadoras trabajando en lenguaje máquina (es decir, en binario, programando únicamente 0s y 1s). Sistemas Batch o por Lotes (Años 70 y comienzo de los 80)En las primeras épocas los sistemas eran grandes y costosos. Constaban de una entrada de trabajos y una salida impresa, por lo cual la interacción con el usuario era prácticamente nula. Las principales características eran que el sistema soportaba un único trabajo a la vez, y que las tareas relacionadas se agrupaban en conjuntos o lotes, para su procesamiento más eficiente. A comienzo de los 80, utilizando las técnicas de Spooling (proceso mediante el cual la computadora introduce trabajos en un buffer, de manera que un dispositivo pueda acceder a ellos cuando esté listo) y multiprogramación (ejecución de múltiples tareas compartiendo recursos) se pudo comenzar a desarrollar técnicas de planificación de despacho. Esta técnica consistía en seleccionar un lote de trabajos que estaban en memoria secundaria para cargarlos en memoria principal. Luego, el SO seleccionaba uno de ellos para ejecutar, y si este debía esperar por alguna tarea (por ejemplo ejecución de E/S) el sistema elegía otro del lote para utilizar el procesador. Esto incrementó el uso del procesador. Sistemas de Tiempo Compartido (Finales de los 80)Estos sistemas eran multiusuarios. Ejecutaban programas de forma concurrente con una elevada tasa de procesos, de forma tal que permitía a los usuarios interactuar directamente con el sistema como si fuera un único usuario. La necesidad de acceder y actualizar datos de forma concurrente, creó la necesidad de evolucionar el sistema de archivos a uno multiusuario, incorporando técnicas de protección de accesos. Sistemas de Computadores Personales (Años 80)Con costos de hardware decrecientes, fue posible el diseño y uso de computadores personales. Los Sistemas fueron diseñados en base a que serían utilizados por un único usuario, y todo el énfasis en el desarrollo estuvo en mejorar la interacción con el usuario. Se desarrolló la interfaz de ventanas que conocemos hoy. Sistemas Paralelos (Comienzo de los 90)Son sistemas donde se dispone de más de un procesador, permitiendo ejecución simultánea y sincronizada de procesos. Se clasifican en: Altamente integrados: “tightly coupled”. Son sistemas en donde los canales de interconexión son de alta velocidad (bus común o memoria compartida). Poco integrados: “closely coupled”. Son sistemas en donde los canales de interconexión son de baja velocidad (sistemas en red). Veamos ahora otra clasificación de los Sistemas Paralelos: Asimétricos : se designa una CPU (master) para ejecutar el código del núcleo, para no lidiar con la concurrencia, los demás (slaves) ejecutarán lo que éste les designe. Simétricos : todos los procesadores son considerados iguales, el código del núcleo se dispone en memoria común y es ejecutado por cualquier procesador. Y otra clasificación más: UMA (Uniform Memory Access) : cada CPU accede a cualquier lugar de la memoria en el mismo tiempo. NUMA (Non-Uniform Memory Access) : las CPU tienen áreas de memoria a las que acceden más rápido que el resto. Veamos ahora una clasificación de Arquitecturas (Taxonomía de Flynn): SISD (Single Instruction, Single Data) : Arquitectura secuencial, no hay paralelismo, son arquitecturas monoprocesadores. SIMD (Single Instruction, Multiple Data) : Son sistemas que ejecutan la misma instrucción sobre un conjunto de datos (Arquitectura Vectorial). MISD (Multiple Instruction, Single Data) : Paralelismo redundante. MIMD (Multiple Instruction, Multiple Data) : Varios procesadores autónomos que ejecutan en forma simultanea varias instrucciones sobre datos diferentes memoria compartida : escalan poco, acceso a memoria es cuello de botella memoria distribuida : escalan a miles de procesadores, conectados en una red de alta velocidad. Como ejemplo de sistemas computacionales que utilizan sistemas paralelos tenemos los clusters. Estos son sistemas en la cual participan varias computadoras. Los clusters brindan alta disponibilidad (mantiene una serie de servicios, a pesar de posibles fallos), alto rendimiento (en cuanto a capacidad de cálculo) y balance de carga (técnica usada para compartir el trabajo a realizar entre varios procesos, ordenadores, etc) Se clasifican en: Simétricos: todos los nodos ejecutan tareas y asumen las de otros ante fallos. Asimétricos: nodos primarios ejecutan tareas y nodos secundarios esperan fallos. Sistemas de Tiempo RealSon sistemas en los cuales todo resultado debe producirse en un cierto tiempo. De lo contrario se considera que el sistema ha fallado. Estructura de los Sistemas OperativosComponentes de un Sistema Operativo Gestión de procesos Gestión de memoria Gestión de Entrada/Salida Administración de Almacenamiento Secundario Gestión de Archivos Sistema de Protección Gestión de Procesos Un proceso es un programa en memoria + CPU + acceso a dispositivos + otros recursos. Un proceso necesita de ciertos recursos (CPU, memoria, archivos, dispositivos E/S, etc) para realizar su tarea. Podemos ver entonces que un proceso es una entidad activa, mientras que un programa es una entidad pasiva. Sabiendo entonces qué es un proceso, podemos decir entonces que el sistema operativo es el encargado de su administración. Es el encargado de proveer servicios para que cada proceso pueda realizar su tarea. Entre los servicios se encuentran: Crear y destruir procesos Suspender y reanudar procesos Proveer mecanismos para la sincronización y comunicación entre procesos Proveer mecanismos para prevenir dead-locks o lograr salir de ellos Gestión de Memoria La memoria es un área de almacenamiento común a los procesadores y dispositivos, donde se almacenan programas, datos, etc. El sistema deberá administrar el lugar libre y ocupado, y será el encargado de las siguientes tareas: Mantener qué partes de la memoria están siendo usadas, y por quién. Decidir qué procesos serán cargados a memoria cuando exista espacio de memoria disponible, pero no suficiente para todos los procesos que deseamos. Asignar y quitar espacio de memoria según sea necesario. Gestión de Entrada/Salida El sistema operativo deberá ocultar las características específicas de cada dispositivo y ofrecer servicios comunes a todos. Estos servicios serán, entre otros: Montaje y desmontaje de dispositivos Una interfaz entre el cliente y el sistema operativo para los device drivers Técnicas de caché, buffering y spooling Device drivers específicos Administración de Almacenamiento Secundario Dado que la memoria RAM es volátil y pequeña para todos los datos y programas que se precisan guardar, se utilizan discos para guardar la mayoría de la información. El sistema operativo será el responsable de: Administrar el espacio libre Asignar la información a un determinado lugar Algoritmos de planificación de disco (estos algoritmos deciden quien utiliza un determinado recurso del disco cuando hay competencia por él) Gestión de Archivos Proporciona una vista uniforme de todas las formas de almacenamiento, implementando el concepto de archivo como una colección de bytes. El Sistema Operativo deberá proveer métodos para: Abrir, cerrar y crear archivos Leer y escribir archivos Organización de directorios Sistema de Protección Por Protección nos referimos a los mecanismos por los que se controla el acceso de los procesos a los recursos. En un sistema multiusuario donde se ejecutan procesos de forma concurrente se deben tomar medidas que garanticen la ausencia de interferencia entre ellos. Estas medidas deben incorporar la posibilidad de definir reglas de acceso, entre otras cosas. Servicios del Sistema OperativoEl sistema brindará un entorno de ejecución de programas donde se dispondrá de un conjunto de servicios. Los servicios principales serán: Ejecución de programas : el SO deberá ser capaz de cargar un programa a memoria y ejecutarlo. El programa deberá poder finalizar, de forma normal o anormal. Operaciones de E/S : el SO deberá proveer un mecanismo de acceso ya que por eficiencia y protección los usuarios no accederán directamente al dispositivo. Manipulación del Sistema de Archivos : se deberá tener acceso al sistema de archivos y poder, como mínimo, leer, escribir, borrar y crear. Comunicación entre procesos : los procesos deberán poder comunicarse, ya sea que estén en el mismo computador o en diferentes. Manipulación de errores : el sistema deberá tomar decisiones adecuadas ante eventuales errores que ocurran, como fallo de un dispositivo de memoria, fallo en un programa, etc. Estructura del SistemaLa estructura interna de los sistemas operativos pueden ser muy diferentes, ya que se debe tener en cuenta las metas de los usuarios (fácil, uso, confiable, rápido, etc) y las del sistema (fácil de diseñar, implementar y mantener, eficiente, etc). Veremos 3 posibles diseños del sistema: sistema monolítico, sistema en capas, sistema con micronúcleo. Sistema Monolítico Estos sistemas no tienen una estructura definida, sino que son escritos como una colección de procedimientos donde cualquier procedimiento puede invocar a otro. Ejemplo de estos sistemas pueden ser MS-DOS. Es importante tener en cuenta que ningún sistema es puramente de un tipo. Sistema en Capas o Niveles El diseño se organiza en una jerarquía de capas, donde los servicios que brinda una capa son consumidos solamente por la capa superior. La capa 0 es del Hardware y la N es la de los procesos de Usuario. Estos sistemas tienen como ventaja que son modulares y la verificación se puede hacer a cada capa por separado (son más mantenibles). Sin embargo el diseño es muy costoso y es menos eficiente que el sistema monolítico ya que pierde tiempo pasando por cada capa. Sistema con micronúcleo (microkernels) La idea consiste en tener un núcleo que brinde los servicios mínimos de manejo de procesos, memoria y que provea la comunicación entre procesos. Todos los restantes servicios se construyen como procesos separados del micronúcleo, que ejecutan en modo usuario. Estos sistemas tienen como ventaja un diseño simple y funcional, que aumenta la portabilidad y la escalabilidad. Para agregar un nuevo servicio no es necesario modificar el núcleo, y es más seguro ya que los servicios corren en modo usuario. Cliente/Servidor Los procesos se diferencian en servidores, que proporcionan ciertos servicios y clientes que disponen de esos servicios. Máquinas Virtuales Se ejecuta un monitor de máquinas virtuales que proporciona copias virtuales del hardware al resto de procesos. En cada una de las máquinas se ejecuta un SO. Exokernels Un programa se ejecuta en modo kernel, asignando los recursos a las máquinas virtuales. Híbrido Implica que el núcleo en cuestión usa conceptos de arquitectura o mecanismos tanto del diseño monolítico como del micronúcleo. Tipos de Sistemas OperativosSe pueden clasificar los SO en función de: Nº de usuarios Monousuario : solo 1 usuario puede usar los recursos del sistema simultáneamente. Multiusuario : varios usuarios pueden usar los recursos del sistema simultáneamente. Por tanto, aunque haya más de un usuario dado de alta en el sistema, si no pueden trabajar de forma simultánea, el SO no es multiusuario. Nº de procesos o tareas Monotarea : solo puede ejecutar 1 tarea a la vez. Multitarea o multiprogramación : puede ejecutar varios programas a la vez. Nº de procesadores Monoproceso / monoprocesador : el SO es capaz de gestionar solo 1 procesador, de manera que si tuviese más sería inútil. En estos SO los procesos irán alternando su ocupación en la CPU. Multiproceso / multiprocesador : el SO es capaz de gestionar varios procesadores, de modo que puede usarlos simultáneamente para distribuir su carga de trabajo. Estos sistemas trabajan de dos formas: Asimétrica : el SO reparte las tareas, que está realizando, entre los procesadores. Determinados procesos los ejecutará siempre un procesador, y el otro procesador sólo se utilizará para realizar procesos de usuario. En este caso, es posible que un procesador esté siempre trabajando y el otro, en ocasiones, sin actividad. Simétrica : los procesos son enviados indistintamente a cualquiera de los procesadores disponibles. Tiempo de respuesta (tiempo que tarda el usuario en obtener los resultados después de iniciar la ejecución de un programa): Procesamiento por lotes : el tiempo de respuesta no es importante y suele ser alto. Los procesos se ejecutan secuencialmente unos tras otro. No existe interacción con el usuario. Ejemplo: copias de seguridad. Tiempo compartido : el procesador divide su tiempo entre todos los procesos (usando algoritmos de planificación como Round Robin). Ejemplo: sistemas multiusuarios interactivos (los usuarios interactúan con el sistema). Tiempo real : en estos SO, los procesos requieren un tiempo de respuesta muy bajo o inmediato. Ejemplos donde esto es especialmente importante: sistema donde el tiempo de respuesta es crucial como sistemas médicos de monitorización de pacientes, sistemas bancarios, tráfico aéreo… Administración de MemoriaMemoria PrincipalIntroducción En sistemas multiprogramados, para sacarle jugo a la multiprogramación, se necesita tener varios procesos cargados en memoria a la vez. Recordemos, que con respecto a la administración de memoria, el SO es responsable de: Mantener qué partes de la memoria están en uso y por quién. Decidir qué procesos cargar cuando haya memoria libre. Asignar y quitar espacio de memoria según sea necesario. Preparación de un programa para ejecutar Los programas son normalmente escritos en lenguajes de alto nivel, y deben pasar por distintas etapas antes de ser ejecutados: Compilación (compile): traducción de código fuente a código objeto. Ensamblaje (linker): ensamblar varios códigos objeto en un archivo ejecutable. Surge ante la necesidad de modularizar los programas y reutilizar código. Carga (load): asigna el archivo ejecutable a la memoria principal del sistema (crea en memoria el espacio necesario para diferentes áreas y las carga con la información). El tamaño de un proceso en memoria principal está limitado por la cantidad de memoria física que exista. Para aprovechar mejor la memoria, se puede utilizar la carga dinámica, la cual no cargará en memoria principal una rutina hasta que ésta no sea invocada. La gran ventaja es que las rutinas que no son utilizadas, no son cargadas a memoria física, y por lo tanto no consumen este recurso. Direcciones Relativas y Absolutas La mayoría de los SO permiten que un proceso de usuario resida en cualquier parte de la memoria principal. Es así que, aunque el espacio de direcciones comience en el 0000, la primera dirección de usuario no tiene porqué ser 0000. Esta posibilidad afecta a las direcciones que el programa de usuario puede utilizar. En cada una de las etapas que hemos visto para poder ejecutar un programa, las direcciones pueden representarse de diferentes formas. Las direcciones de un programa fuente son normalmente simbólicas. Al compilar, el compilador se encarga de reasignar estas direcciones simbólicas a direcciones relativas. El cargador, se encargará, a su vez, de reasignar direcciones relativas a direcciones absolutas. Asociación de direcciones (address binding) ¿En qué momento el SO reasigna las instrucciones y los datos a direcciones de memoria?: Tiempo de Compilación: Si sabemos en el momento de la compilación donde va a residir el proceso en memoria, podemos generar código absoluto (con direcciones absolutas). Ahora, si en algún momento deseamos cambiar su ubicación, deberemos recompilar el código. Tiempo de Carga: El compilador deberá generar código reubicable (con direcciones relativas), y en este caso se retarda la reasignación a direcciones absolutas hasta el momento de la carga. Si en algún momento deseamos cambiar su ubicación, deberemos solamente volver a cargarlo. Tiempo de Ejecución: Si el proceso puede variar su ubicación en memoria durante su ejecución, entonces es necesario retardar su asignación a direcciones absolutas hasta el momento de ejecución. Para que este esquema pueda funcionar, se requiere soporte de hardware. Espacios de direcciones lógico y físico Una dirección generada por la CPU se denomina normalmente dirección lógica, mientras que una dirección vista por la unidad de memoria se denomina dirección física. Los métodos de reasignación en tiempo de compilación y de carga generan direcciones físicas y lógicas idénticas; no es el caso para el tiempo de ejecución. En este caso decimos que la dirección lógica es una dirección virtual. Al conjunto de todas las direcciones lógicas de un programa se le denomina espacio de direcciones lógicas; mientras que al conjunto de todas las direcciones físicas de un programa se le denomina espacio de direcciones físicas. La correspondencia entre direcciones virtuales y físicas en tiempo de ejecución es establecida por un dispositivo de hardware que se denomina Unidad de Gestión de Memoria (MMU = Memory Management Unit). Estrategia de asignación o reubicación ¿Cómo elige el SO en que porción de memoria colocaremos un proceso? Existen varias estrategias: First fit: Asigna el primer “agujero”de memoria libre que satisface la necesidad. Best fit: Asigna el mejor “agujero” de memoria libre que exista en la memoria principal. Worst fit: Asigna en el “agujero” más grande que exista en la memoria principal. Estudios de simulación han mostrado que first-fit y best-fit lograron mejores rendimientos en tiempo de asignación y utilización de la memoria que la estrategia worst-fit . Veamos un ejemplo: Si quisiéramos asignar a memoria un proceso de 212 kb, y tenemos los siguientes espacios libres (espacios en blanco): Veamos en qué hueco asigna al proceso cada estrategia: Problema de asignación de memoria La memoria física puede ser asignada a los diversos procesos en ejecución siguiendo diversas técnicas: Asignación contigua Asignación dispersa Asignación contigua El espacio de direcciones lógicas de un proceso se mapea sobre una única zona de la memoria física: las direcciones de memoria son contiguas. Métodos: Particiones fijas Particiones variables Asignación dispersa La memoria lógica se divide en fragmentos (páginas o segmentos), que se mapean sobre zonas no contiguas de la memoria física. Técnicas de asignación dispersa: Paginación Paginación multinivel Segmentación Segmentación paginada Para implementar estas técnicas se necesita el apoyo de la MMU. Fragmentación Existen dos tipos de fragmentación: Fragmentación Interna: Es la pérdida de espacio en disco debido al hecho de que el tamaño de un determinado archivo sea inferior al tamaño del clúster, ya que teóricamente el archivo estaría obligado a ser referenciado como un clúster completo. Fragmentación Externa: Se da cuando existe suficiente memoria libre en el sistema para satisfacer un requerimiento de memoria, pero no es posible asignarlo debido a que no es un espacio contiguo. Dicho lo anterior, vemos que las estrategias presentadas en el ejemplo anterior muestran problemas de fragmentación externa, ya que en la memoria van quedando una gran cantidad de espacios pequeños que no son asignados. Intercambio (Swapping) Como ya vimos, un proceso debe estar en memoria principal para ser ejecutado. Sin embargo, los procesos pueden ser intercambiados temporalmente, sacándolos de memoria y almacenándolos en el disco, y volviéndolos a llevar a memoria para continuar su ejecución. Al mecanismo de llevar un proceso desde memoria principal a disco se le denomina s wap-out. Al inverso se le denomina swap-in . El mayor tiempo consumido en el swaping es el tiempo de transferencia. Memoria VirtualIntroducción La memoria virtual permite ejecutar procesos que requieren más memoria que la disponible en el sistema, manteniendo en memoria principal solo aquella memoria que el proceso esté utilizando y el resto en el disco. De esta forma el usuario ya no debe preocuparse por las limitaciones de memoria física. Cada proceso tiene su propio espacio de direccionamiento virtual (o lógico) y la MMU es la encargada de mapear las direcciones virtuales (o lógicas) a físicas. Implementación La implementación de memoria virtual es realizada a través de la técnica de paginación bajo demanda. En la paginación bajo demanda los procesos residen en un dispositivo de disco y son puestos en memoria principal cuando es necesario cargarlos para ejecutar. La carga del proceso en memoria no es total, sino que implementa un cargador “perezoso” (lazy swapper), que cargará las páginas según se vayan necesitando. Utilizar un esquema de este tipo requiere el conocimiento de las páginas que están activas en memoria. Para ello se utiliza el valid-invalid bit, que consiste en agregar a la tabla de páginas un nuevo campo (bit de validez), que indique para cada entrada, si la página se encuentra o no en memoria. Al inicio, la tabla de páginas indicará que ninguna página está en memoria (todos los bits de validez se encontrarán en i (invalid)). En este ejemplo tenemos que el proceso tiene para usar 8 páginas, de las cuales solo usa 6, y de las cuales solo 3 están en memoria principal (A, C, F). Todas las páginas estarán el el disco (incluidas aquellas que también están en memoria principal). Fallo de página La memoria cargada en memoria principal se le denomina memoria residente. El acceso a memoria residente por parte de un proceso es tomado como un acceso normal, pero el acceso a memoria no residente genera un fallo de página. El fallo de página genera un trap a nivel del SO, que activa una rutina de atención que carga la página en memoria principal. Acceso a Memoria El acceso a memoria genera la siguiente secuencia de pasos: Verificar que el proceso referencia una página correcta dentro de su espacio virtual, ya que no todas las direcciones dentro de su espacio son válidas. Por ejemplo, el acceso fuera de un array puede generar un acceso a una página virtual que no fue asignada al proceso. Si el proceso referencia a una página incorrecta, se genera un errar y el proceso termina. Si el acceso fue correcto, se busca en la tabla de páginas el frame correspondiente, verificando el bit de validez-invalidez. Si el bit es de validez se accede al frame correspondiente y se termina el acceso. Si no es válido se genera un trap de page fault, que involucra los siguientes pasos: Se busca frame libre en memoria principal, si no hay se ejecuta el algoritmo de reemplazo. Se lee de disco la página a cargar, y se carga en el frame obtenido en el paso anterior. Se actualiza la tabla de páginas, indicando que la página está disponible en memoria principal. Se devuelve el control a la instrucción que fue interrumpida por el PF (page fault). Si se aplica este método se tendrá un sistema puro de paginación bajo demanda. Tener en cuenta que para poder llevarlo a cabo se precisa una tabla de páginas y espacio swap de disco. Algoritmos de reemplazo La necesidad de traer a memoria principal una página en una memoria principal llena, genera la búsqueda de un frame a reemplazar, mediante un algoritmo de reemplazo. Un mal algoritmo de reemplazo puede generar un impacto significativo de degradación del sistema. Cuando se elige un frame a reemplazar, este será puesto en memoria swap, y ante un eventual uso futuro, volverá a memoria principal a través de un page fault. Los pasos a seguir cuando reemplazamos frames son los siguientes: Elegir el frame mediante algún algoritmo de reemplazo. Escribir el frame en memoria swap (swap out) y ajustar la tabla de páginas. Cargar la página en el frame correspondiente (swap in). Ajustar la tabla de página. Veamos ahora algunos algoritmos: FIFO (First in First out) El algoritmo reemplaza la página que lleva más tiempo en memoria principal. Es un algoritmo fácil de implementar ya que requiere únicamente de una estructura tipo cola, pero reemplaza las páginas sin tener en cuenta las referencias que tuvo. Segunda Oportunidad Este algoritmo intenta disminuir la cantidad de fallos de páginas del algoritmo FIFO, teniendo en cuenta las referencias a las páginas. El algoritmo será igual al anterior, salvo que cada página tendrá un bit que indicará si fue o no referenciada luego de ser cargada a memoria. Al momento del reemplazo, se verifica el bit de referencia; si está encendido, a la página se le da una segunda oportunidad y es puesta al final de la cola. Luego se continúa con la siguiente página que está al principio de la cola. Si el bit está apagado, esta página será seleccionada para ser reemplazada. Es un tanto ineficiente, pero disminuye la cantidad de fallos de páginas. Óptimo En este algoritmo se reemplaza la página que no va a ser usada por el mayor periodo de tiempo. Es imposible de implementar porque requiere conocer a qué páginas accederá el proceso. LRU (Least Recently Used – Recientemente Menos Usada) Este algoritmo asocia a cada página el tiempo en que fue referenciada. La página elegida por el algoritmo de reemplazo será la que fue accedida hace más tiempo. Este algoritmo es el que más se aproxima al óptimo y es bastante utilizado por los SO. NRU (No Recientemente Usada) En este algoritmo a las páginas se les asigna un bit de referencia y otro de modificación. El bit de referencia se enciende cada vez que se lee o escribe la página, mientras que el de modificación solo se enciende cada vez que se escribe. Cada cierto tiempo el bit de referencia es apagado. Al ocurrir un fallo de página, los frames son divididos en 4 clases. Se reemplazará un frame al azar de la clase más baja que no esté vacía: Clase 0: No referenciada, no modificada Clase 1: No referenciada, modificada Clase 2: Referenciada, no modificada Clase 3: Referenciada, modificada Al ejecutar el algoritmo de reemplazo, existen dos opciones de páginas a reemplazar: Reemplazo global: Un proceso puede reemplazar un frame utilizado por otro. Aunque los PF de un proceso afectan a otros, es el método más usado. Reemplazo local: Un proceso reemplaza únicamente los frames que tiene asignado, es por eso que la cantidad de frames de un proceso no varía. La desventaja es que hay marcos que se pueden desperdiciar. Asignación de frames a procesos e hiperpaginación Si el SO no implementa una estrategia de asignación de memoria, un proceso que requiera mucha memoria puede hacer colapsar el sistema. Una forma de asignar frames a procesos podría ser dividir la cantidad de frames del sistema en partes iguales para cada proceso. Este método puede ser ineficiente ya que no todos los procesos consumen la misma cantidad de memoria. Si un proceso utiliza en forma activa una cantidad mayor de frames de los asignados por el sistema, tendrá un algo porcentaje de fallos de página, dando lugar a que el proceso esté continuamente realizando PF, pasando más tiempo paginando que ejecutando, lo que se conoce como hiperpaginación . Se degrada significativamente el rendimiento del sistema. ProcesosDefinición de proceso Un proceso es un programa en ejecución que necesita estar cargado en memoria y disponer de recursos (CPU, memoria, archivos, dispositivos de E/S) para cumplir su objetivo. Se trata de una entidad activa. Mientras que los programas son un conjunto de archivos que están almacenados en algún dispositivo de almacenamiento (disco duro, pendrive …) y cuyo código fuente está escrito en algún lenguaje de programación. Cuando este conjunto de archivos se ejecutan, entonces pasa a ser un proceso. Procesos en memoria Un proceso en memoria se constituye de varias secciones: Código (text) : instrucciones del proceso. Datos (data) : variables globales del proceso. Memoria dinámica (Heap) : Memoria dinámica que genera el proceso. Pila (Stack) : utilizado para preservar el estado en la invocación anidada de procedimientos y funciones. Estados de los procesos El estado de un proceso se define por su actividad actual, cambiando a medida que se ejecuta. La ejecución de un proceso alterna una serie de ráfagas de CPU y E/S. Los estados de un proceso son: Nuevo : Cuando el proceso es creado. En Ejecución : El proceso tiene asignado un procesador y está ejecutando sus instrucciones. Bloqueado : El proceso está esperando por un evento (que se complete un pedido de E/S o una señal). Preparado : El proceso está listo para ejecutar, solo necesita del recurso procesador. Terminado : El proceso finalizó su ejecución. Transiciones entre los estados Veamos ahora como los procesos pueden cambiar de estados a partir de determinados hechos. A continuación se muestra el diagrama de estados y transiciones de los procesos: Nuevo -&gt; Preparado : el SO está preparado para admitir un proceso más. Preparado -&gt; Ejecución : el planificador escoge un proceso para la ejecución. Ejecución -&gt; Preparado : el proceso en ejecución es interrumpido y expulsado del procesador porque ya ha consumido su tiempo asignado o porque otro proceso de mayor prioridad está esperando. Ejecución -&gt; Bloqueado : el proceso abandona voluntariamente la CPU y espera a un evento externo. Bloqueado -&gt; Preparado : finaliza el evento que estaba esperando el proceso y pasa al estado preparado. Ejecución -&gt; Terminado : el proceso termina su ejecución (terminación normal). Preparado/Bloqueado -&gt; Terminado : el proceso es eliminado (terminación anormal). Listas y colas de procesos Los procesos, según su estado, deberán esperar por determinados eventos, como ya vimos. Puede suceder, que más de un proceso esté esperando por el mismo evento, es por eso que se deben organizar en diferentes colas o listas. Lista de procesos del sistema (job queue) : Esta será una lista especial, porque los procesos que están en ella no esperan por nada en particular, sino que es la lista de todos los procesos del sistema. Al crearse un nuevo proceso se agrega el PCB a esta lista. Cuando el proceso termina su ejecución es borrado. Cola de procesos listos (ready queue) : Esta cola se compondrá de los procesos que estén en estado listo. La estructura de esta cola dependerá de la estrategia de planificación utilizada. Cola de espera de dispositivos (device queue) : Los procesos que esperan por un dispositivo de E/S particular son agrupados en una lista específica al dispositivo. Cada dispositivo de E/S tendrá su cola de espera, por lo que existirán varias device queue. Bloque de Control de Proceso (PCB) Cuando un proceso se ejecuta, el SO le asigna un espacio de direcciones de memoria (que contiene las instrucciones, los datos y la pila que es una estructura para almacenar y recuperar datos del proceso) y lo añade a la tabla de procesos. El SO guarda en la tabla de procesos por cada proceso una estructura de datos llamada Bloque de Control de Proceso (PCB) que almacena la siguiente información: Identificación de proceso: del proceso en sí (PID), del proceso padre (PPID) y de usuario. Información de estado del proceso: preparado, en ejecución, bloqueado, …. Prioridad del proceso. Dirección de memoria donde se ha cargado el proceso Otros : recursos utilizados, valores de los registros del procesador, propietarios, permisos. Cambio de contexto (context switch) Para dar sensación de ejecución simultánea o multiprogramación, el tipo de CPU debe repartirse entre los procesos. Esto implica cambios de contexto que consisten en quitarle la CPU al proceso “en ejecución” y asignársela a otro estado “preparado”. Esta operación la realiza un componente del SO llamado dispatcher o planificador a corto plazo y en ella se guarda el contexto del proceso en ejecución en su PCB y se restaura el contexto del nuevo proceso a ejecutar mediante su PCB. Los cambios de contexto pueden suponer una sobrecarga si se utilizan con mucha frecuencia. En general, suelen producirse cuando un proceso finaliza, es expulsado o se suspende. Comunicación entre procesos Procesos que se ejecutan concurrentemente pueden ser procesos independientes o cooperativos. Un proceso es cooperativo si puede afectar o verse afectado por los restantes procesos que se ejecuten en el sistema, y es independiente si no. Evidentemente, cualquier proceso que comparta datos con otro será cooperativo. Veamos algunas razones por las cuales es bueno tener un entorno que permita la cooperación de procesos: Compartir información. Dado que varios usuarios pueden estar interesados en la misma información, se debe proveer un acceso concurrente a ella. Acelerar cálculos. Si deseamos que una determinada operación se ejecute rápidamente, debemos dividirla en subtareas ejecutándose cada una de ellas en paralelo. Esto se consigue solo si hay múltiples CPU o varios canales de E/S. El mecanismo que provee esto es IPC (InterProcess Comunication), que permite intercambiar datos e información. Hilos (Thread)La mayoría de los SO proporcionan caracteríscas que permiten que un proceso tenga múltiples hilos de control. ¿Qué es un hilo? Un hilo es una unidad básica de utilización de CPU, la cual contiene un id de hilo, su propio program counter, un conjunto de registros y una pila; se representa a nivel del SO con una estructura llamada TCB (Thread Control Block) Los hilos comparten con otros hilos que pertenecen al mismo proceso la sección de código, la sección de datos, entre otras cosas. Si un proceso tiene múltiples hilos, puede realizar más de una tarea a la vez (esto es real cuando se posee más de una CPU). Ventajas de usar hilos Respuesta : el tiempo de respuesta mejora, ya que el programa puede continuar ejecutándose, aunque parte de él esté bloqueado. Compartir recursos : los hilos comparten la memoria y los recursos del proceso al que pertenecen, por lo que se puede tener varios hilos de ejecución dentro del mismo espacio de direcciones. Economía : es más fácil la creación, cambio de contexto y gestión de hilos que de procesos. Utilización múltiples CPUs : permite qué hilos de un mismo proceso ejecuten en diferentes CPUs a la vez. En un proceso mono-hilo, un proceso ejecuta en una única CPU, independientemente de cuantas tenga disponibles. Hilos a nivel de usuario y de kernel Hilos a nivel de usuario : son implementados en alguna librería. Estos hilos se gestionan sin soporte del SO, el cual solo reconoce un hilo de ejecución. Hilos a nivel de kernel : el SO es quien crea, planifica y gestiona los hilos. Se reconocen tantos hilos como se hayan creado. Los hilos a nivel de usuario tienen como beneficio que su cambio de contexto es más sencillo que el cambio de contexto entre hilos de kernel. Además, se pueden implementar aún si el SO no utiliza hilos a nivel de kernel. Otro de los beneficios consiste en poder planificar diferente a la estrategia del SO. Los hilos a nivel de kernel tienen como gran beneficio poder aprovechar mejor las arquitecturas multiprocesadores, y que proporcionan un mejor tiempo de respuesta, ya que si un hilo se bloquea, los otros puedes seguir ejecutándose. PlanificaciónLa planificación es la base para lograr la multiprogramación. En un sistema multiprogramado, generalmente en un determinado instante existirán varios procesos que requieren el procesador a la vez, el componente del SO que realiza la operación de elegir el proceso a utilizar es el planificador. Principales planificadores de CPU Planificador a largo plazo : Selecciona procesos de la cola de esperando ejecución y los carga a memoria Controla el grado de multiprogramación. Es importante que elija un conjunto equilibrado de procesos. Se ejecuta con poca frecuencia. Planificador a corto plazo : Selecciona entre los procesos preparados en memoria y les asigna la CPU. Se ejecuta con mucha frecuencia. Planificador a medio plazo : Decide qué proceso pasa de la memoria principal a la secundaria (memoria virtual) o viceversa. El SO enlaza losPCB’s de los procesos que están en el mismo estado a las diversas colas que puedan existir. Esquemas de planificación Se invoca al planificador cuando: Cuando un proceso cambia de ejecutando a bloqueado. Cuando un proceso finaliza. Cuando un proceso cambia de ejecutando a listo. Cuando un proceso cambia de bloqueado a listo. Cuando se crea un nuevo proceso. Cuando ocurren los dos primeros casos, el planificador es invocado debido a que el proceso en ejecución libera el procesador. Los últimos tres casos se dan solamente cuando el planificador es expropiativo, ya que puede quitar el procesador a un proceso que estaba ejecutando para dárselo a otro. Planificación no apropiativa y apropiativa Planificación no apropiativa (non-preemptive): Algoritmos no expulsivos. Los procesos se ejecutan hasta que terminan o se bloquean. Sencillo de implementar. Rendimiento negativo en general. Planificación aproviativa (preemptive): Algoritmos expulsivos. Los procesos puedes ser expulsados de la CPU. Mayor coste de implementación. Necesitan soporte hardware adicional (relojes). Mejora el servicio y evita monopolización de la CPU. Medidas para poder evaluar los algoritmos de planificación Utilización de CPU : es el porcentaje de uso útil que tiene un procesador. Rendimiento (Throughput) : número de procesos terminados por unidad de tiempo. Tiempo de retorno : tiempo desde que un proceso se carga hasta que finaliza su ejecución. Tiempo de espera : es la suma de los tiempos que un proceso estuvo en la cola de procesos listos. Tiempo de respuesta : tiempo desde la carga hasta que el proceso da su primera respuesta. Algoritmos de planificación FCFS (First Come First Served) La CPU es asignada a los procesos en el mismo orden que lo solicitan. Es un algoritmo no expulsivo. Ventajas Sencillo de implementar (cola FIFO). Inconvenientes Mal tiempo de espera Efecto convoy (procesos con largar ráfagas de CPU retrasan a procesos con ráfagas cortas). No válido para procesos interactivos. SJF (Shortest Job First) Primero el que menos tiempo total de CPU requiere. Se escoge el proceso de la cola de preparados con una próxima racha de CPU más corta y se ejecuta hasta que se termine o se suspenda. Si hay varios procesos con rachas de CPU iguales, se puede aplicar FIFO. Algoritmo no expulsivo. Ventajas Optimiza el tiempo de espera Favorece los procesos orientados a E/S Desventajas Es costoso averiguar cuándo dura la siguiente racha de CPU Inanición de los procesos con rachas de CPU largas SRTF (Shortest Remaining Time First) Primero al que menos tiempo de CPU le queda para acabar. Versión apropiativa de SJF (como el SJF, solo que puede echar a los procesos) Planificación por prioridades Primero el que tiene más prioridad. Cada proceso tiene asignada una prioridad. El planificador selecciona el proceso con prioridad más alta (a igual prioridad se selecciona con FCFS). Las prioridades pueden ser dinámicas (cambian con el tiempo) o estáticas (se mantienen). Inconvenientes Riesgo de inanición de procesos con prioridad baja. Una solución sería aumentar la prioridad con el incremento del tiempo de espera. Planificación Circular (Round Robin) Todos el mismo tiempo por turnos. A cada proceso se le asigna una cantidad de tiempo de CPU llamada “quantum”. Si el proceso tiene un intervalo de CPU mayor que el quantum es expulsado de la CPU. La cola de preparados se gestiona con una política FIFO. Si el valor del quantum es grande el algoritmo degenera en FCFS. Si es pequeño se generará sobrecarga debido a cambios de contexto. Es Equitativo. Multilevel Queue Este algoritmo propone dividir la lista de procesos listos en varias colas, una para cada tipo de proceso. Cabe destacar que los procesos no podrán cambiar de cola, que cada cola tendrá su propio algoritmo de planificación, y que existirá un algoritmo de planificación entre colas. Multilevel Feedback Queue Este algoritmo se diferencia con el anterior en que los procesos si pueden cambiar de nivel, dependiendo del uso del CPU que tengan. La cola de más alta prioridad corresponderá a los I/O bound, la más baja a los CPU-bound. Un algoritmo así se define por: Cantidad de colas Algoritmo de cada cola Criterio para subir de nivel un proceso Criterio para bajar de nivel un proceso Criterio para asignar un proceso nuevo a una de las colas Sistemas Multiprocesador En un sistema simétrico, cualquier procesador ejecuta procesos de usuario. Se puede asignar una cola de listos a cada CPU, lo cual es conveniente para el uso de la caché. Pueden haber desbalances de trabajo entre procesadores, por lo cual se pueden migrar procesos de cola para balancear la carga nuevamente. Programación Concurrente¿Qué es la programación concurrente? Se conoce por programación concurrente a la rama de la informática que trata de las técnicas de programación que se usan para expresar el paralelismo entre tareas y para resolver los problemas de comunicación y sincronización entre procesos. El principal problema de la programación concurrente corresponde a no saber en qué orden se ejecutan los programas (en especial los programas que se comunican). Se debe tener especial cuidado en que este orden no afecte el resultado de los programas. Sección crítica y exclusión mutua El método más sencillo de comunicación entre procesos de un programa concurrente es el uso común de unas variables de datos. Esta forma tan sencilla de comunicación puede llevar, no obstante, a errores en el programa ya que el acceso concurrente puede hacer que la acción de un proceso interfiera en las acciones de otro de una forma no adecuada. Para evitar este tipo de errores se pueden identificar aquellas regiones de los procesos que acceden a variables compartidas y dotarlas de la posibilidad de ejecución como si fueran una única instrucción. Se denomina Sección Crítica a aquellas partes de los procesos concurrentes que no pueden ejecutarse de forma concurrente o, también, que desde otro proceso se ven como si fueran una única instrucción. Esto quiere decir que si un proceso entra a ejecutar una sección crítica en la que se accede a unas variables compartidas, entonces otro proceso no puede entrar a ejecutar una región crítica en la que acceda a variables compartidas con el anterior. Las secciones críticas se pueden excluir mutuamente . Para conseguir dicha exclusión se deben implementar protocolos software que impidan el acceso a una sección crítica mientras está siendo utilizada por un proceso. Semáforos Dijkstra dio en 1968 una solución al problema de la exclusión mutua con la introducción del concepto de semáforo binario. Esta técnica permite resolver la mayoría de los problemas de sincronización entre procesos y forma parte del diseño de muchos SO. Un semáforo binario es un indicador (S) de condición que registra si un recurso está disponible o no. Un semáforo binario solo puede tomar dos valores: 0 y 1. Si, para un semáforo binario S=1 entonces el recurso está disponible y la tarea lo puede utilizar; si S=0 el recurso no está disponible y el proceso debe esperar. Los semáforos se implementan con una cola de tareas a la cual se añaden los procesos que están en espera del recurso. Un semáforo binario se puede definir como un tipo de datos especial que sólo puede tomar los valores 0 y 1, con una cola de tareas asociada y con sólo tres operaciones para actuar sobre él: La operación INIT se debe sellar a cabo antes de que comience la ejecución concurrente de los procesos ya que su función exclusiva es dar una valor inicial al semáforo. Un proceso que corre la operación P y encuentra el semáforo a 1, lo pone a 0 y prosigue su ejecución. Si el semáforo está a 0 el proceso queda en estado de bloqueado hasta que el semáforo se libera. Cuando se ejecuta la operación V puede haber varios procesos en la lista o cola. El proceso que la dejará para pasar al estado listo dependerá del esquema de gestión de la Cola. Si no hay ningún proceso en espera el semáforo se deja libre para el primero que lo requiera. El semáforo binario resulta adecuado cuando hay que proteger un recurso que pueden compartir varios procesos, pero cuando lo que hay que proteger es un conjunto de recursos similares, se puede usar una versión más general de semáforo que lleve la cuenta del número de recursos disponibles. En este caso el semáforo se inicializa con el número total de recursos disponibles (N) y las operaciones P y V se diseñan de modo que se impida el acceso al recurso protegido por el semáforo cuando el valor de éste es meno o igual que cero. Cada vez que se solicita y obtiene un recurso, el semáforo se decrementa y se incrementa cuando uno de ellos se libera. Las operaciones que tenemos son las mismas, con algunas diferencias en su semántica: Semáforos: mutua exclusión La exclusión mutua se realiza fácilmente utilizando semáforos. La operación P se usará como procedimiento de bloqueo antes de acceder a una sección crítica y la operación V como procedimiento de desbloqueo. Se utilizarán tantos semáforos como clases de secciones críticas se establezcan. Gestión de Entrada/SalidaLa gestión de entrada/salida es una de las funciones más importantes del SO, ya que el SO debe ser capaz de manejar los diferentes periféricos existentes. Para ello debe : Enviar órdenes a los dispositivos de E/S Determinar el dispositivo que necesita la atención del procesador Detectar las interrupciones Controlar los errores Proporcionar una interfaz entre los dispositivos y el resto del sistema. Esta interfaz debe ser: Sencilla y fácil de usar Debe ser la misma para todos los dispositivos El SO tiene varias maneras de llevar a cabo la E/S: E/S programada : el procesador ejecuta un programa que controla las operaciones de E/S. El problema es que el procesador se tiene que quedar esperando (parado) a recibir respuesta. E/S controlada por interrupciones : los dispositivos envían una señal de interrupción para llamar la atención del sistema. E/S mediante el uso de DMA (acceso directo a memoria) : un chip se encarga de la transferencia y accede a la memoria para leer o escribir datos que recibe y envía el dispositivo sin pasar por el procesador. Actualmente los discos duros, unidades de CD, DVD, Blueray, admiten DMA y la tienen activada por defecto. Dado que la velocidad del procesador es muy superior a la de los dispositivos de E/S, se utilizan técnicas de almacenamiento intermedio para mejorar el rendimiento del sistema: Caching : consiste en almacenar una caché temporal, de rápido acceso, los datos que se usan con más frecuencia. Buffering : consiste en utilizar un área de memoria como buffer, simulando un dispositivo o un periférico lógico, que hará de dispositivo intermedio entre el periférico real y el procesador. El buffer es independiente del dispositivo de entrada y/o salida, por lo que permite que el procesador comience a trabajar leyendo o almacenando en el buffer mientras la información del periférico se va almacenando o extrayendo del buffer. Esto evita que un periférico lento afecte al rendimiento del equipo informático. Spooling : técnica en la cual la computadora introduce trabajos en un buffer (un área especial en memoria o en un disco), de manera que un dispositivo pueda acceder a ellos cuando esté listo. El spooling es útil en caso de dispositivos que acceden a los datos a distintas velocidades. El buffer proporciona un lugar de espera donde los datos pueden estar hasta que el dispositivo (generalmente más lento) los procesa. Esto permite que la CPU pueda trabajar en otras tareas mientras que espera que el dispositivo más lento acabe de procesar el trabajo. La aplicación más común del spooling es la impresión. En este caso, los documentos son cargados en un área de un disco, y la impresora los saca de éste a su propia velocidad. El usuario puede entonces realizar otras operaciones en el ordenador mientras la impresión tiene lugar en segundo plano. El spooling permite también que los usuarios coloquen varios trabajos de impresión en una cola de una vez, en lugar de esperar a que cada uno acabe para enviar el siguiente. Unidad de Entrada/Salida La Unidad de Entrada/Salida (chipset) permite la comunicación de la CPU y la Memoria Principal con el exterior: impresoras, monitor, teclado, etc. Para que se pueda llevar a cabo el intercambio de información se deben realizar las siguientes tareas: Direccionamiento : selección del dispositivo de E/S implicado en una transferencia determinada. Sincronización de CPU y periféricos: es necesario coordinar la actividad de la CPU con los periféricos, ya que sus velocidades de trabajo son distintas. Transferencia de datos desde o hacia el dispositivo seleccionado. Software de Entrada/Salida El software de E/S se organiza en niveles. Los del nivel inferior ocultan las particularidades del hardware a los del nivel superior que presentan una interfaz simple y uniforme al usuario. Hardware de Entrada/Salida En general, las unidades de E/S constan de: Un componente electrónico denominado controladora Un componente mecánico, que es el dispositivo mismo Controladoras Las principales funciones de las controladoras son: Comunicación en el periférico, intercambio de órdenes, información del estado. Detección de errores. Comunicación con el procesador, descodificadores de órdenes, datos, información de estado, reconocimiento de dirección. Al código específico que el SO utiliza para programar una controladora se le conoce como manejador o driver de la controladora. De este modo, para llevar a cabo las tareas de E/S, el SO, usando el driver, se comunica con la controladora a través de una serie de registros específicos que cada controladora tiene. Cuando la orden ha sido cumplida, la controladora produce una interrupción con el fin de permitir que la CPU atienda al SO para comprobar los resultados de la operación de E/S. Para dicha comprobación se utilizan los valores de los registros de la controladora que informan sobre el estado final. Dispositivos de E/S Los periféricos se pueden clasificar en función de si gestionan la información por bloques o caracteres. Gestión de FicherosIntroducción Fichero o archivo: Conjunto de información de un determinado tipo que está almacenada en un dispositivo de almacenamiento. Ejemplo: documento de texto, sonido, imagen, … Carpeta o directorio: Tipo especial de fichero que se utiliza para organizar ficheros (u otras carpetas). Sistema de ficheros: Parte del SO que permite “administrar” la información almacenada de los dispositivos de E/S en forma de ficheros. Objetivos: Crear, modificar o borrar ficheros (o carpetas) Controlar el acceso a los ficheros (mediante permisos) Permitir intercambio de datos entre ficheros Permitir realizar copias de seguridad de los ficheros Permitir el acceso a los ficheros mediante nombres simbólicos Ficheros -&gt; Nombre y extensión de los ficheros Los archivos generalmente se componen de: Nombre: La mayoría de SO permiten usar nombres de hasta 255 caracteres y algunos SO, como Linux, distinguen entre mayúsculas y minúsculas. Extensión: Sirve para saber el programa que permite ejecutar o abrir un fichero. Algunos SO como Linux no necesitan el uso de extensiones. -&gt; Tipos de ficheros: Ficheros normales o regulares: Aquellos ficheros que contienen datos (información). Directorios: Fichero que se utiliza para organizar los ficheros (u otras carpetas). Ficheros especiales de dispositivos: representan a dispositivos de E/S. -&gt; Información que contiene un fichero: Nombre Tamaño Fechas: de creación, modificación, … Propietario Permisos (lectura, escritura, ejecución, …) Ubicación Enlaces: puntos desde los que se puede acceder al fichero -&gt; Operaciones que se puedes hacer sobre un fichero: Crear Abrir Escribir Cerrar Borrar Directorios -&gt; Operaciones que se pueden hacer sobre un directorio: Crear Entrar Salir Leer su contenido Añadir o Eliminar en él archivos o directorios Borrar La mayoría de los SO tienen un sistema de archivos de estructura jerárquica, en el que los directorios parten de uno llamado directorio raíz, y del que cuelgan todos los demás en forma de árbol, de ahí que se utilicen términos como árbol de subdirectorios. -&gt; Directorios especiales: Existen dos tipos: . directorio actual .. directorio padre -&gt; Rutas: Concatenación de directorios y subdirectorios para llamar a un archivo en una estructura de directorios. Tipos: Absolutas: se llama al archivo desde el directorio raíz hasta el archivo. Ejemplo: c:\\web\\imagenes\\logo.gif Relativas: se llama al archivo desde el directorio actual en el que estemos. Ejemplo: si estamos en la carpeta “web” la ruta hasta llegar al archivo “logo.gif” sería: imagenes\\logo.gif Métodos de asignación Métodos para asignar espacio a cada fichero dentro del disco. Asignación contigua: los bloques de un fichero se encuentran de forma contigua en el disco. Asignación enlazada: en cada bloque está parte de los datos del fichero y una pequeña parte para indicar el siguiente bloque que contiene los datos del fichero. Bibliografía Facultad de ingeniería. Universidad de la República. Uruguay. Wikipedia IES Serra Perenxisa Características técnicas y funcionales de los sistemas operativos: Windows, Linux, Unix y otros. Sistemas operativos para dispositivos móviles.Sistemas WindowsLos SO Windows han ido evolucionando desde 1981, en que empezó a comercializarse MS-DOS. Era un SO monotarea, monousuario y monoprocesador. El interfaz gráfico de usuario (GUI) se empezó a probar con Windows 1.0 en 1985 y se lanzaba desde MS-DOS. En 1990 aparece Windows 3.0, evolucionando hasta el famoso Windows 3.1. Windows NT aparece en 1993 y representa un SO de 32 bits para el Intel 60386. La versión NT 4.0 (1996) incluía el GUI de Windows 95 en que los componentes gráficos, anteriormente en modo usuario, pasaron a modo núcleo (NT Executive). Windows 2000 heredó la arquitectura NT incluyendo el servicio de directorio (AD, Active Directory). Case en paralelo, Windows 95 evolucionó a Windows 98 y Windows Me, que mantenían el código de 16 bits lo que no les hacía tan eficientes en procesadores 386 y posteriores. No soportan NTFS. La tecnología NT y el interfaz mejorado de Windows 95 se fusionan en Windows XP (2001), evolucionando a Windows Vista (2006), Windows 7, Windows 8 y Windows 10. Las caracterísiticas de la tecnología NT se aplica a los SO profesionales de Microsoft, destacando: SO de 32 bits: No compatible hacia atrás. Por tanto supone una ruptura con Windows Me. Las versiones Windows 2000 y XP son la evolución del código original de Windows NT. Independencia de memoria separada: La ejecución de un programa se hace en regiones de memoria distintas, lo que evita que las inestabilidades afecten al resto. El SO gestiona el uso de la memoria, evitando perder el control de la máquina. Multitarea apropiativa (preemptive): O ejecución simultánea de aplicaciones. El SO asigna los recursos evitando inanición. Se opone a la multitarea colaborativa (no apropiativa) tipo Windows 95. Multiusurio y multiprocesador: Se gestiona la concurrencia al sistema de distintos usuarios en red y se pueden usar varios procesadores en la misma máquina, asignándoles distintas tareas. Portabilidad: Se refiere a la independencia del hardware, implementado con una capa HAL (Hardware Abstraction Layer). El resto de código es común a cualquier sistema. Seguridad de dominio: Consiste en la inclusión de autenticación de usuarios para el acceso a recursos de red. Los controladores de dominio se encargan de validar usuarios de alta en la BBDD llamada SAM (Security Account Manager). A partir de Windows 2000 Server, los dominios se integran en el servicio de directorio Microsoft, el AD. NTFS: La nueva tecnología de sistema de ficheros (NTFS) incluye seguridad y se basa en la creación de listas de control de acceso (ACL) para cada archivo o directorio. NT también incluye soporte para FAT y HPFS (OS/2). Tolerancia a fallos: Se incluyen mecanismos de continuidad en presencia de fallos y soporte RAID. Usuarios, Grupos y Dominios Cada usuario necesita ser identificado, es decir, disponer de una cuenta para iniciar sesión en el sistema. La identificación de una cuenta de usuario se realiza con un nombre y se asocia una contraseña. La cuenta de máximo privilegio es la de Administrador, similar a root en entornos Unix. Para la gestión de cuentas, usuarios, se agrupan en Grupos, en general con criterios de privilegios o perfiles comunes. Los tipos de grupos que se distinguen en entornos Windows son: globales: pueden contener usuarios de un mismo dominio locales: con usuario y grupos globales de distintos dominios universales: que incluyen usuarios, grupos globales y universales de distintos dominios. No soportado en Windows NT. NTFS incorpora seguridad a nivel de archivo y carpeta. Con las ACL se definen permisos de usuario o grupo independientemente. Los permisos de Windows son más completos que los nativos de Unix, que también permite el uso de ACL. Cuando los usuarios de una red son de escala, se hace necesaria una gestión centralizada. Es la razón de ser de la idea de Dominio. Los entornos servidores Windows permiten implementar la arquitectura cliente-servidor, en lo que se da en llamar Dominio. Un dominio es entonces un conjunto de equipos que comparten un servicio de directorio (BBDD de usuarios, recursos y permisos). El directorio se soporta con controladores de dominio (DC, Domain Controller) y lo forman cuentas de usuario y directivas de seguridad. Cuando un usuario inicia sesión en un equipo cliente, debe indicar nombre, contraseña y dominio. Un controlador del dominio verificará las credenciales, permitiendo, o no, el acceso. Un servidor en un dominio puede desempeñar 3 roles: Controlador de dominio principal (PDC): Servidor SAM. Todo dominio NT tiene que tener un controlador principal de dominio. Controlador de reserva (BDC): Es una copia de seguridad del SAM. No es obligatoria su presencia, pero sí muy recomendable. Pueden existir varios controladores de reserva, que se sincronizan con el PDC. Servidor miembro: Es un servidor específico. No contiene copias del SAM. Participa en el dominio para ofrecer recursos y servicios. Si un servidor no pertenece a un dominio entonces es independiente, como pueda ser un servidor web público. El controlador de reserva puede cambiar su función a PDC si éste cae. En Windows 2003 Server no se distingue entre PDC y BDC, se denominan controladores de dominio. Para configurar un servidor comp PDC se ejecuta el comando DCPROMO (Asistente de instalación del directorio activo). Los nombre de dominio siguen la sintaxis del sistema DNS. AD es una estructura basada en el servicio de directorio LDAP (Lightweight Directory Access Protocol) que almacena información sobre recursos facilitando su acceso. Los componentes del directorio se llaman objetos y se almacenan en contenedores, siguiendo una estructura jerárquica. La información del AD se replica en los controladores de dominio para mantener consistencia. Requiere un servidor DNS dinámico. Otras estructuras del modelo de dominio Windows son: Sitio: Lugar físico de un controlador de dominio. Los clientes tratan de iniciar sesión en controladores de dominio de su mismo sitio para acelerar y optimizar el uso de la red. Árbol: Conjunto de dominios en una misma jerarquía DNS. Bosque: Conjunto de árboles con distintas jerarquías DNS. OU (Unidad Organizativa): Conjunto de recursos agrupados para facilitar su administración. Por fin, para que los usuarios de un dominio accedan a recursos de otro dominio hay que definir una relación de confianza entre dominios. Un sistema tipo Unix puede participar en una red Microsoft, actuando incluso como controlador de dominio. Para ello hay que instalar y configurar samba en un sistema tipo Unix. Administración La administración de servidores Windows se realiza con consolas administrativas del Menú Inicio/Programas. Destacan las siguientes: Usuarios y equipos de AD: Menú que permite gestionar cuentas de usuarios en el dominio. Administración de equipos: Para administrar equipos locales o remotos. Permite administrar servicios iniciados, dispositivos, visor de sucesos y recursos compartidos. Administrador de servicios de Internet (IIS): Para configurar servidores web, FTP, SMTP y NNTP de Windows. DHCP, DNS y WINS: Son las herramientas de administración del servidor DHCP, DNS y el servicio WINS, de resolución de nombres Windows. Directivas de seguridad: Herramienta de gestión de los privilegios de seguridad a nivel de dominio, de controlador de dominio y local. Dominios y confianzas de AD: En particular, para definir las relaciones de confianza entre dominios. Enrutamiento y acceso remoto: Permite definir opciones de enrutamiento, acceso remoto (RAS) y redes privadas virtuales (VPN). Rendimiento y visor de sucesos: Es la herramienta que ofrece información gráfica del rendimiento de los componentes del sistema. Permite definir alertas de rendimiento. El visor de sucesos registra la actividad del sistema de ficheros de registro (logs). Servicios: Permite iniciar y detener servicios. Sistema de archivos distribuidos (DFS): Configura DFS para acceder a recursos compartidos. Sitios y servicios de AD: Define los servidores que integran cada sitio y la comunicación entre ellos. El registro de Windows El registro de Windows es una BBDD jerárquica centralizada dispuesta para almacenar la información de configuración del sistema para usuarios, aplicaciones y dispositivos hardware. La información del registro es la referencia que usa el SO Windows continuamente, como puedan ser perfiles de usuario, aplicaciones instaladas, tipos de documentos que gestiona cada aplicación, elementos hardware del sistema, etc. El registro reemplaza a la mayoría de archivos .ini basados en texto usados en versiones anteriores de Windows como autoexec.bat y config.sys. Aunque es común a distintas versiones de Windows, existen diferencias. Una sección del registro es un grupo de claves, subclaves y valores que cuentan con archivos auxiliares con copias de seguridad de sus datos. Los archivos auxiliares de cada sección excepto HKEY_CURRENT_USER suelen estar en la carpeta %SystemRoot%\\System32\\Config. Para la clave HKEY_CURRENT_USER suelen disponerse en %SystemRoot%\\Profiles\\nombreDeUsuario. Las extensiones de los archivos de estas carpetas indican el tipo de datos que contienen. A veces, la falta de extensión también puede indicar el tipo de datos que contienen. Los tipos de valores más importantes usados en el registro para almacenar la información se resumen en la siguiente tabla. La siguiente tabla enumera las claves predefinidas que usa el sistema. El tamaño máximo del nombre de una clave es de 255 caracteres. Comandos DOS A continuación se muestra un resumen de los comandos de administración DOS habituales. Sistemas Unix y LinuxLos sistemas informáticos, en origen sólo permitían el proceso por lotes. Por tanto, se hizo necesaria la evolución (década de los 60) a sistema de proceso de tiempo compartido. Así, se permitía la interacción con la máquina. El primer sistema de tiempo compartido fue CTSS (Compatible Time-Sharing System), desarrollado en el MIT. El MIT, Bell Laboratories y General Electrics diseñaron el SO MULTICS (Multiplexed Information and Computing Service), programado en PL/1. MULTICS sirvió de base a Ken Thompson, para desarrollar otro SO en lenguaje ensamblador, para una máquina PDP-7: UNICS, por oposición a MULTICS, que acabó por llamarse “UNIX” (1970). Dennis Ritchie y Ken Thompson reescribieron Unix en lenguaje B, para otra máquina, la PDP-11. Así, se evitaba reescribir el código fuente cuando el sistema se migraba, consiguiendo portabilidad. El lenguaje B fue mejorado y evolucionó al lenguaje C, reescribiendo Unix en C, que sigue empleándose y evolucionando. Unix fue proporcionado por AT&amp;T a universidades, con su código fuente. Esto generó mejoras en el código y su amplia difusión. La universidad de Berkeley incluyó memoria virtual, mejoró el sistema de archivos, incorporó el editor de texto vi, el shell csh y la pila de protocolos TCP/IP. Sus SO se llamaron BSD. AT&amp;T terminó comercializando Unix, en particular su versión más conocida, Unix System V. Las distintas versiones de Unix generaron problemas de compatibilidad de programas, lo que originó su estandarización. El IEEE abrió el proyecto POSIX (Portable Operating System IX) para dicha tarea, lo que materializó en el estándar IEEE 1003. Linux es un SO de código libre basado en Unix. Fue desarrollado por Linus Torvalds. Actualmente su desarrollo lo coordina la FSF (Free Software Foundation) y su proyecto GNU. El código libre usa licencias GPL (General Public License), que básicamente obliga a proporcionar el código fuente modificado y mantener la licencia GPL para el software desarrollado a partir de otro software protegido con GPL. Las versiones de Linux se denominan distribuciones. Sus variaciones se refieren a aspectos como el GUI, la instalación, etc. El núcleo del sistema es común. Ejemplos son Ubuntu, Red Hat, Fedora, Suse, etc. El proyecto GNU define software libre como el que dispone de libertad de ejecución, de modificación, de distribución y mejora. Si se incluye código GPL en un proyecto, todo el código pasará a ser libre. Esta condición evita que el software comercial use código GPL. La aplicación estricta de esta licencia generaría problemas con las bibliotecas del compilador, para programas comerciales. Para evitarlo, se dispone la licencia GNU LGPL (Lesser GPL), menos restrictiva que GPL, que permite integrar partes LGPL sin que todo el código pase a ser software libre. Los entornos Unix/Linux permiten al usuario elegir el intérprete de comandos (shell). Las Shell difieren en la definición de instrucciones y la programación de scripts. Destacan entre las comunes bsh (Bourne Shell, /bin/sh), csh (C-Shell, /bin/csh, basada en C), ksh (Korn Shell, /bin/ksh), bash (Bourne Again Shell, /bin/bash, mejora csh y ksh) o tcsh (Tab C-Shell, /bin/tcsh, una mejora de csh). Sistema de archivos En entornos Unix, el sistema de archivos sigue el estándar de jerarquía de ficheros (FHS, Filesystem Hierarchy Standard, 1993). FHS define la estructura de directorios y sus contenidos. Comenzó en 1994 con el FSSTND (Filesystem Stardard), que ha sufrido varias revisiones hasta el actual FHS (1996). FHS es mantenido por el Free Stardards Group, una organización constituida por compañías como Hewlett Packard, Dell, IBM o Red Hat. La mayoría de las distribuciones Linux, no lo aplican de forma estricta. La estructura de directorios es tal que todos los ficheros y directorios aparecen bajo el directorio raíz (/), aun si están almacenados en dispositivos físicos diferentes. La estructura es la siguiente: Para instalar un sistema Linux, suele ser habitual la recomendación de usar al menos tres particiones: /, /boot y swap (a la que no se asigna punto de montaje). Archivos, permisos e inodos Los archivos de un sistema de archivos Unix distinguen los tipos ordinarios (datos o programas), directorios (lista de archivos con punteros a sus inodos), especiales (dispositivos tales como puertos y discos) y tuberías con nombre (named pipes, comunican dos procesos). Los nombres de archivos pueden tener hasta 255 caracteres. Se pueden crear archivos ocultos con un punto como primer carácter del nombre. Un enlace o vínculo (link) permite que un mismo archivo pueda llamarse desde varios directorios). Sólo existirá una copia del archivo, aunque podrá accederse desde varios directorios. Si se borra el archivo en un directorio sólo se borra el enlace. El archivo sólo se borra si se borra en todos los directorios en que posee enlace. En general los enlaces se refieren a enlaces duros (físicos o hard link), en oposición a los enlaces simbólicos, archivos que apuntan a otro (similar a un acceso directo en Windows). Todos los directorios y subdirectorios se tratan como archivos. El directorio actual se nota con un punto y el directorio padre con dos. En entornos Unix se definen 3 tipos de permisos básicos: lectura (r) escritura ejecución (x) Se definen tres perfiles de usuario: el propietario del archivo (user, u) usuario del grupo del propietario (group, g) usuario que no pertenece al grupo del usuario (other, o) Los permisos se suelen representar con 10 bits: -rwxrwxrwx. El primer carácter corresponde al tipo de fichero (‘-‘, fichero ordinario; ‘d’, directorio; ‘c’, fichero especial tipo carácter; ‘b’, fichero especial tipo bloque; …). Dependiendo del tipo de Unix hay otras opciones (‘l’, ‘s’, ‘=’). El resto de caracteres, en bloques de tres, especifican qué tipo de usuario puede realizar qué operación. Por ejemplo, la respuesta -rwxr-x-r– indica un fichero ordinario, los tres bits del propietario (rwx), le dan permiso de lectura, escritura y ejecución, los tres segundos (r-x), del grupo, le permiten leer y ejecutar el fichero y los tres últimos (r–) permiten al resto, solo lectura. Aparte de los anteriores bits de permisos, hay un cuarto tipo más especial, en el que se engloban setuid, setgid y sticky bit. Setuid o modo “s”, significa que la identidad efectiva de usuario con la que se ejecuta el programa es la del propietario. Este permiso no tiene sentido en ficheros no ejecutables. Por ejemplo, la salida -rwsr-x— 1 usuario 1499 Jun 6 10:17 fichero, indica un fichero modo s. Setgid, o modo s del grupo es similar al anterior, referido al grupo. Ejemplo, -r-xr-sr-x 1 usuario grupo 9984 Jul 16 1994 fichero. Por fin, el sticky bit, o modo “t”, cuando está activado indica que el fichero nunca se elimina del área de swap. Suele ser útil para programas ejecutados a menudo y por diferentes usuarios. Sobre un directorio el comportamiento es distinto, permitiendo que sólo el propietario del fichero, el propietario del directorio o el superusuario “root”, puedan renombrar o borrar los ficheros contenidos en él. Es útil para áreas compartidas. Por ejemplo, en el directorio /tmp al hacer ls -ald, se puede obtener la salida drwxrwxrwt 5 root 309 Jun 7 11: 41 ./. Los permisos suelen indicarse también mediante un código octal de 3 números. Cada número codifica una terna. Así,la terna rw- se codificaría como 110 en binario, que en octal es 6. En entornos Unix los archivos se gestionan con nodos índice o inodos, estructuras de 64B con información del tipo de archivo y permisos, número de referencias del archivo en directorios (enlaces), identificador del propietario y su grupo, tamaño del archivo en Bytes, fecha de último acceso y modificación del archivo e inodo y dirección, formada por 39B divididos en 13 punteros de 3B. En la dirección, los 10B primeros son directos. Contienen direcciones de bloques de datos. Los 3B siguientes son punteros indirectos: indirecto simple (puntero a bloque de 256 punteros directos, 256 bloques), indirecto doble (puntero a bloque de 256 punteros indirectos simples, 2e8 · 2e8 = 2e16 bloques) o indirecto simple (puntero a bloque de 256 punteros indirectos dobles, 16 millones de bloques). La asignación de bloques a un archivo es dinámica, según necesidad. Por eso, los bloques pueden no asignarse secuencialmente, generando fragmentación, lo que perjudica el rendimiento. En Unix System V se usan bloques de 1KB (en FAT pueden llegar a 32KB). Los bloques pequeños evitan desaprovechar espacio en disco. Los punteros de dirección de un archivo se van usando a medida que se necesitan, comenzando por los punteros directos (más rápidos). Suponiendo bloques de 1KB las capacidades que ofrece cada sistema es, para direccionamiento directo 10KB (10 bloques); indirecto simple 256KB (256 bloques); indirecto doble 65MB (65536 bloques); indirecto triple 16GB (2e8 · 2e16 = 2e24 bloques). Así, el tamaño máximo teórico de un archivo sería la suma de la capacidad de un inodo total (los 16GB aprox.). Las ventajas de los inodos es que al ser pequeños pueden mantenerse en memoria, ofreciendo un rápido acceso y aprovechan mejor el espacio en disco. En los entornos Unix tradicionales, la organización de un disco cuenta con un Bloque de arranque (bloque 0), con información para el arranque del SO; un Superbloque (bloque 1), con información de la organización del sistema de archivos; Inodos, tabla de inodos con la estructura expuesta dividida en Inodo 1 (reservado para gestión de bloques defectuosos) e Inodo 2 (gestionado por el directorio raíz) y Bloque de datos, que guarda el contenido de los archivos. Gestores de arranque, entornos de escritorio y editores de texto Un gestor de arranque es un programa que se instala en el sector de arranque del disco duro (MBR, Master Boot Record) y al encender la máquina, permite elegir el SO que ejecutar. Los más populares en entornos Unix son LILO (LInux LOader) y GRUB (GRand Unified Bootloader). El de Microsoft se llama NT Loader (NTLDR) instalado en el sector de arranque de la partición primaria de windows. Los dos tipos pueden convivir ya que se instalan en lugares distintos. En cuanto a las GUI, en entornos Unix existen algunas típicas, como KDE, que usa Konqueror como gestor de archivos y navegador web; GNOME, que usa Nautilus como gestor de archivos, pudiéndose usar otro programa como navegador web y Xfce, que usa Thunar como gestor de archivos, típico en distribuciones de BSD y Solaris. Debido a su importancia, a la hora de editar archivos de texto para la configuración, los entornos Unix suelen incluir editores de texto típicos como vi, emacs (de más fácil manejo), vim o xemacs. Cuentas de usuario Para acceder a un sistema tipo Unix, un usuario se modela con una cuenta del sistema. Un usuario tendrá además un perfil, que definirá, entre otras cosas, sus privilegios. Los entornos Unix definen un perfil de usuario de máximo privilegio llamado root o superusuario, y por añadidura, su cuenta se denomina cuenta de root. Sus privilegios le permiten realizar cualquier tarea administrativa. Una cuenta de usuario se define con un nombre de usuario o login, un identificador (UID, User IDentificator), un identificador de su grupo (GID), una contraseña (password), su Shell (el CLI que ejecutará por defecto), su directorio particular (conocido como /home) y comentarios. La información de una cuenta de usuario se almacena básicamente en tres archivos: passwd, shadow y group del directorio /etc. El primero posee el listado completo de usuarios con información para cada uno de su login, contraseña, uid, gid, comentario, directorio home y shell. El campo contraseña suele aparecer con “x”, lo que evita que se pueda ver directamente, porque se encripta en el archivo shadow. El archivo shadow contiene las contraseñas encriptadas de los usuarios. Para cada usuario se almacena su login, contraseña encriptada, fecha de última modificación, mínimo y máximo de días entre modificaciones, días de aviso de expiración, máximo de días con la cuenta inactiva y fecha de expiración. El archivo group lista los grupos de usuarios. Para cada grupo se tiene la información del nombre, contraseña, gid y lista de usuarios. La contraseña no se usa. Suele presentarse un asterisco o un espacio en blanco. La lista de usuario sindica los UID de los miembros secundarios del grupo. Como un usuario puede pertenecer a varios grupos, su grupo principal se indica en el archivo passwd y los secundarios se indican incluyendo su UID en la lista de usuario de cada grupo en el archivo group. Comandos UNIX A continuación se muestra un resumen de los comandos de administración UNIX habituales. Sistemas Operativos para dispositivos móvilesUn SO móvil es un conjunto de programas de bajo nivel que permite la abstracción de las peculiaridades del hardware específico del teléfono móvil y provee servicios a las aplicaciones móviles, que se ejecutan sobre él. Capas de un Sistema Operativo móvil Kernel . El núcleo o kernel proporciona el acceso a los distintos elementos del hardware del dispositivo. Ofrece distintos servicios a las capas superiores como son los controladores o drivers para el hardware, la gestión de procesos, el sistema de archivos y el acceso y gestión de la memoria. Middleware . El middleware es un conjunto de módulos que hacen posible la propia existencia de aplicaciones para móviles. Es totalmente transparente para el usuario y ofrece servicios claves como el motor de mensajería y comunicaciones, códecs multimedia, intérpretes de páginas web, gestión del dispositivo y seguridad. Entorno de ejecución de aplicaciones . El entorno de ejecución de aplicaciones consiste en un gestor de aplicaciones y un conjunto de interfaces programables abiertas y programables por parte de los desarrolladores para la creación de software. Interfaz de usuario . Las interfaces de usuario facilitan la interacción con el usuario y el diseño de la presentación visual de la aplicación. Los servicios que incluye son el de componentes gráficos (botones, pantallas, listas, …) y el marco de interacción. Aparte de estas capas también existe una familia de aplicaciones nativas del teléfono que suelen incluir los menús, el marcador de números de teléfono, … Sistemas Operativos móviles Android Android Inc. es la empresa que creó el SO móvil. Se fundó en 2003 y fue comprada por Google en 2005 y en 2007 fue lanzado al mercado. Su nombre se debe a su inventor, Andy Rubin. Originalmente era un sistema pensado para las cámaras digitales. Android está basado en Linux, disponiendo de un Kernel en este sistema y utilizando una máquina virtual sobre este Kernel que es la responsable de convertir el código escrito en Java a código capaz de comprender el Kernel. Una de las grandes cualidades o características de este SO es su carácter abierto. Android se distribuye bajo dos tipos de licencias, una que abarca todo el código del Kernel y que es GNU GPLv2 (implica que su código se debe poner al alcance de todos y que todos podremos hacer con este código lo que nos parezca oportuno, modificarlo, ampliarlo, recortarlo, pero siempre estaremos en la obligación de volver a licenciarlo con la misma licencia). Google también tiene otra licencia para el resto de componentes del sistema que se licencia bajo APACHE v2, una licencia libre y de código abierto (implica que este código se pueda distribuir para ser modificado y usado a antojo del que lo utilice, pero a diferencia del primer caso, las modificaciones y el código resultante no es obligatorio licenciarlo bajo las mismas condiciones en las que se encontraba). La estructura del SO Android se compone de aplicaciones que se ejecutan en un framework Java de aplicaciones orientadas a objetos sobre el núcleo de las bibliotecas de Java en una máquina Virtual Dalvik con compilación en tiempo de ejecución hasta la versión 5.0, luego cambió al entorno Android Runtime (ART). Las bibliotecas escritas en lenguaje C incluyen un administrador de interfaz gráfica (surface manager), un framework OpenCore, una base de datos relacional SQLite, una interfaz de programación de API gráfica OpenGL ES 2.0 3D, un motor de renderizado WebKit, un motor gráfico SGL, SSL y una biblioteca estánde de C Bionic. Las características y especificaciones son: Información General ArquitecturaLos componentes principales del SO Android son: Aplicaciones : las aplicaciones base incluyen un cliente de correo electrónico, programa de SMS, calendario, mapas, navegador, contactos y otros. Todas las aplicaciones están escritas en lenguaje de programación Java. Marco de trabajo de aplicaciones : los desarrolladores tienen acceso completo a las mismas API del entorno de trabajo usados por las aplicaciones base. La arquitectura está diseñada para simplificar la reutilización de componentes; cualquier aplicación puede publicar sus capacidades y cualquier otra aplicación puede luego hacer uso de esas capacidades (sujeto a reglas de seguridad del framework). Este mismo mecanismo permite que los componentes sean reemplazados por el usuario. Bibliotecas : Android incluye un conjunto de bibliotecas de C/C++ usadas por vaios componentes del sistema. Estas características se exponen a los desarrolladores a través del marco de trabajo de aplicaciones de Android. Algunas son: System C library (implementación biblioteca C estándar), bibliotecas de medios, bibliotecas de gráficos, 3D y SQLite, entre otras. Runtime de Android : Android incluye un set de bibliotecas base que proporcionan la mayor parte de las funciones disponibles en las bibliotecas base del lenguaje Java. Cada aplicación Android corre su propio proceso, con su propia instancia de la máquina virtual Dalvik. Dalvik ha sido escrito de forma que un dispositivo puede correr múltiples máquinas virtuales de forma eficiente. Dalvik ejecutaba hasta la versión 5.0 archivos en el formato de ejecutable Dalvik (.dex), el cual está optimizado para memoria mínima. La Máquina Virtual está basada en registros y corre clases compiladas por el compilador de Java que han sido transofrmadas al formato .dex por la herramienta incluida dx. Desde la versión 5.0 utiliza el ART, que compila totalmente al momento de la instalación de la aplicación. Núcleo Linux : Android depende de Linux para los servicios base del sistema como seguridad, gestión de memoria, gestión de procesos, pila de red y modelo de controladores. El núcleo también actúa como una capa de abstracción entre el hardware y el resto de la pila de software. Versiones Honeycomb fue la primera actualización exclusiva para TV y Tablet, no era apta para móviles. iOS iOS es un SO que da vida a dispositivos como el iPhone, el iPad, el iPod Touch o el Apple TV. Anteriormente denominado iPhone OS creado por Apple originalmente para iPhone, siendo después usado en el iPod Touch e iPad. Es un derivado de Mac OS X que a su vez está basado en Darwin BSD y por lo tanto es un SO tipo Unix, se lanzó en el año 2007. iOS cuenta con cuatro capas de abstracción: la capa del núcleo del SO la capa de “Servicios Principales” la capa de “Medios” la capa de “Cocoa Touch” Información General Actualmente va por la versión iOS 11. Windows Phone Windows Phone (abreviado WP) es un SO móvil desarrollado por Microsoft, como sucesor de Windows Mobile. Con Windows Phone; Microsoft ofrece una nueva interfaz de usuario que integra varios de sus servicios propios como OneDrive, Skype y Xbox Live en el SO. Microsoft pasa a enfocarse en un único sistema denominado Windows 10 Mobile, disponible para todo tipo de plataformas (teléfonos inteligentes, tabletas y computadoras). Está diseñado para ser similar a las versiones de escritorio de Windows estéticamente. Información General Versiones Windows Phone 7 Windows Phone 8 Windows Phone 8.1 BlackBerry 6 El BlackBerry OS es un SO móvil de código cerrado desarrollado por BlackBerry, antigua Research In Motion (RIM). El sistema permite multitarea y tiene soporte para diferentes métodos de entrada adoptados por RIM para su uso en computadoras de mano. Su desarrollo se remonta a la aparición de los primeros handleds en 1999. Información General Symbian Fue producto de la alianza de varias empresas de telefonía móvil, entre la que se encuentra Nokia como la más importante. El SO Symbian es una colección compacta de código ejecutable y varios archivos, la mayoría de ellos son bibliotecas vinculadas dinámicamente (DLL) y otros datos requeridos, incluyendo archivos de configuración, de imágenes y de tipografía, entre otros recursos residentes. Symbian se almacena, generalmente, en un circuito flash dentro del dispositivo móvil. Información General Firefox OS Firefox OS es un SO móvil, basado en HTML5 con núcleo Linux, para smartphones y tabletas. Es desarrollado por Mozilla Corporation bajo el apoyo de empresas y voluntarios de todo el mundo. Este SO está enfocado especialmente en los dispositivos móviles. Está diseñado para permitir a las aplicaciones HTML5 comunicarse directamente con el hardware del dispositivo usando Javascript y Open Web APIs. Información General Ubuntu Touch Ubuntu Touch es un SO móvil basado en Linux. Se caracteriza por ser un sistema diseñado para plataformas móviles. Información general Listado Sistemas Operativos Móviles Bibliografía Apunte de caramelo RUA. Repositorio Institucional de la Universidad de Alicante Wikipedia. Sistemas operativos móviles Características técnicas de los lenguajes y paradigmas actuales de programación.Lenguajes y paradigmas actuales de programación.Categorías de Lenguajes de ProgramaciónLos lenguajes de programación se pueden clasificar atendiendo a varios criterios: Según el nivel de abstracción Según el paradigma de programación que posee cada uno de ellos Según su campo de aplicación Según su traducción Según su nivel de abstracción Lenguajes Máquina Están escritos en lenguajes directamente legibles por la máquina (computadora), ya que sus instrucciones son cadenas binarias (0 y 1). Da la posibilidad de cargar (transferir un programa a la memoria) sin necesidad de traducción posterior lo que supone na velocidad de ejecución superior, solo que con poca fiabilidad y dificultad de verificar y poner a punto los programas. Lenguajes de bajo nivel Los lenguajes de bajo nivel son lenguajes de programación que se acercan al funcionamiento de una computadora. El lenguaje por excelencia es el lenguaje ensamblador, éste trabaja con los registros de memoria de la computadora de forma directa. La principal utilización de este tipo de lenguajes es para programar los microprocesadores, utilizando el lenguaje ensamblador correspondiente a dicho procesador. Lenguajes de medio nivel Hay lenguajes de programación que son considerados como lenguajes de medio nivel (como es el caso del C) al tener ciertas características que los acercan a los lenguajes de bajo nivel pero teniendo, al mismo tiempo, ciertas cualidades que lo hacen un lenguaje más cercano al humano y, por tanto, de alto nivel. Lenguajes de alto nivel Los lenguajes de alto nivel son normalmente fáciles de aprender porque están formados por elementos de lenguajes naturales, como el inglés. Esta forma de trabajar puede dar la sensación de que las computadoras parecen comprender un lenguaje natural; en realidad lo hacen de una forma rígida y sistemática, sin que haya cabida, por ejemplo, para ambigüedades o dobles sentidos. Lenguajes utilizados Pascal, Basic, … Según el paradigma de programación Un paradigma de programación indica un método de realizar cómputos y la manera en que se deben estructurar y organizar las tareas que debe llevar a cabo un programa. Un paradigma es un modelo que, a su vez, es una representación abstracta de la realidad. Un paradigma de programación es un modelo de programación que representa un estilo o forma de programar o construir programas para realizar ciertas tareas o actividades. Cada modelo tiene sus propias estructuras y reglas de construcción. El modelo de programación por emplear depende del problema que se desee solucionar. Los paradigmas fundamentales están asociados a determinados modelos de cómputo. También se asocian a un determinado estilo de programación. Los lenguajes de programación suelen interpretar, a menudo de forma parcial, varios paradigmas. Existen muchos paradigmas de programación diferentes, cada uno de ellos tiene sus propias características y tratan de solucionar los problemas clásicos del desarrollo de software desde diferentes perspectivas y filosofías. Los paradigmas de programación solo son propuestas tecnológicas adoptadas por la Comunidad de desarrolladores que se enfocan a resolver uno o varios problemas definidos y delimitados. Existen muchos paradigmas de programación diferentes, posiblemente el más ampliamente utilizado hoy en día sea el de la programación orientada a objetos . Algunos lenguajes de programación pueden soportar múltiples paradigmas de programación. Por ejemplo, C++ puede ser empleado para desarrollar software utilizando para ello un modelo de programación puramente orientado a objetos o bien puramente estructurado. Otros lenguajes han sido diseñados para soportar un único paradigma de programación, ese es el caso de Smalltalk que soporta únicamente la programación orientada a objetos o Haskell que solo soporta la programación funcional . Es común el diseño de lenguajes que soporten múltiples paradigmas de programación. Estos lenguajes son aquellos que soportan al menos dos paradigmas: Scala : Imperativo, orientado a objetos, funcional, genérico y concurrente Erlang : Funcional, orientado a objetos y funcional Perl : Imperativo, orientado a objetos y funcional PHP : Imperativo, orientado a objetos, funcional y reflexivo JavaScript : Imperativo, orientado a objetos (prototipos) y funcional Java : Imperativo, orientado a objetos, reflexivo y genérico Python y Ruby : Imperativo, orientado a objetos, reflexivo y funcional C++ : Imperativo, orientado a objetos, funcional y genérico C# : Imperativo, orientado a objetos, funcional (lambda), reflexivo y genérico Lisp : Orientado a objetos, funcional y declarativo Prolog : Lógico y declarativo Estos son algunos ejemplos, existen lenguajes como Oz que soporta nueve paradigmas de programación. Paradigmas de programación Un paradigma define un conjunto de reglas, patrones y estilos de programación que son usados por un grupo de lenguajes de programación. Cada lenguaje tiene sintaxis y semántica: La sintaxis de un lenguaje de programación está relacionada con la forma de los programas, por ejemplo, las expresiones, comandos, declaraciones, etc. son puestos juntos en un programa. La semántica de un lenguaje de programación está relacionada con el significado de los programas, por ejemplo, cómo se comportarán cuando se ejecutan en una computadora. La sintaxis de un lenguaje influye en cómo los programas son escritos por el programador, leídos por otro programador y traducidos por el computador. La semántica de un lenguaje determina como los programas son compuestos por el programador, entendidos por otros programadores e interpretados por el computador. Tipos de Paradigmas: Paradigma imperativo (por procedimientos) Paradigma declarativo Programación funcional Programación lógica Programación Reactiva (Dataflow) Paradigma orientado a objetos Paradigma imperativo (por procedimientos) En el paradigma por procedimientos, los programas se desarrollan a través de procedimientos. Pascal, C y BASIC son tres de los lenguajes imperativos más importantes. El paradigma se inició a principios de los años 50 cuando los diseñadores reconocieron que las variables y lo comandos o instrucciones de asignación constituían una simple pero útil abstracción del acceso a memoria y actualización del conjunto de instrucciones máquina. Describe cómo debe realizarse el cálculo, no el porqué Un cómputo consiste en una serie de sentencias, ejecutadas según un control de flujo explícito, que modifican el estado del programa Las variables son celdas de memoria que contienen datos (o referencias), pueden ser modificadas y representan el estado del programa La sentencia principal es la asignación Definición de procedimientos Definición de tipos de datos Chequeo de tipos en tiempo de compilación Cambio de estado de variables Pasos de ejecución de un proceso Asociados al paradigma imperativo se encuentran los paradigmas procedural , modular y la programación estructurada Lenguajes: FORTRAN-77, COBOL, BASIC, PASCAL, C, ADA, … También lo implementan: Java, C++, C#, Eiffel, Python, … La programación imperativa es una forma de escribir programas secuenciales; es decir, que tienes que ir indicando en el programa los pasos o tareas que debe realizar según las siguientes reglas: Paradigma declarativo El paradigma declarativo o paradigma de programación lógica se basa en el hecho de que un programa implementa una relación antes que una correspondencia. Debido a que las relaciones son mas generales que las correspondencias (identificador – dirección de memoria), la programación lógica es potencialmente de más alto nivel que la programación funcional o la imperativa. El lenguaje más popular es el lenguaje PROLOG. Describe qué se debe calcular, sin explicar el cómo No existe un orden de evaluación prefijado Las variables son nombres asociados a definiciones, y una vez instanciadas son inmutables No existe sentencia de asignación El control de flujo suele estar asociado a la composición funcional, la recursividad y/o técnicas de reescritura y unificación Existen distintos grados de pureza en las variantes del paradigma Las principales variantes son los paradigmas funcional , lógico , la programación reactiva y los lenguajes descriptivos Programación funcional La programación funcional se caracteriza por el uso de expresiones y funciones. Un programa dentro del paradigma funcional, es una función o un grupo de funciones compuestas por funciones más simples estableciéndose que una función puede llamar a otra, o el resultado de una función puede ser usado como argumento de otra función. El lenguaje por excelencia es el LISP. Basado en los modelos de cómputo cálculo lambda (Lisp, Scheme) y lógica combinatoria (famila ML, Haskell) Las funciones son elementos de primer orden Evaluación por reducción funcional. Técnicas: recursividad parámetros acumuladores CPS Mónadas Familia LISP (Common-Lisp, Sheme): Basados en s-expresiones Tipado debil Meta-programación Familia ML (Miranda, Haskell, Scala): Sistema estricto de tipos (tipado algebraico) Concordancia de patrones Transparencia referencial Evaluación perezosa (estructuras de datos infinitas) La computación se realiza mediante la evaluación de expresiones Definición de funciones Funciones como datos primitivos Valores sin efectos laterales, no existe la asignación Programación declarativa Lenguajes: LISP, Scheme, Haskell, Scala, … Las funciones matemáticas son una correspondencia entre un dominio y un rango. Una definición de función especifica el dominio y el rango, de manera implícita o explicita, junto con una expresión que describe la correspondencia. Las funciones son aplicadas a un elemento del dominio y devuelve uno del rango. Programación lógica Basado en la lógica de predicados de primer orden Los programas se componen de hechos, predicados y relaciones Evaluación basada en resolución SLD: unificación + backtracking La ejecución consiste en la resolución de un problema de decisión, los resultados se obtienen mediante la instanciación de las variables libres Definición de reglas Unificación como elemento de computación Programación declarativa Lenguajes: Prolog, Mercury, Oz, … Programación Reactiva (Dataflow) Basado en la teoría de grafos Un programa consiste en la especificación del flujo de datos entre operaciones Las variables se encuentran ligadas a las operaciones que proporcionan sus valores. Un cambio de valor de una variable se propaga a todas las operaciones en que participa. Las hojas de cálculo se basan en este modelo Lenguajes representativos: Simulink, Oz, Clojure, … Paradigma orientado a objetos El paradigma orientado a objetos, se basa en los conceptos de objetos y clases de objetos. Un objeto es una variable equipada con un conjunto de operaciones que le pertenecen o están definidas para ellos. Los objetos pueden usarse una y otra vez para construir múltiples objetos con las mismas propiedades o modificarse para construir nuevos objetos con propiedades similares pero no exactamente iguales. Los objetos y clases son conceptos fundamentales. Una clase es un conjunto de objetos que comparten las mismas operaciones. Objetos (o al menos referencia a objetos) deben ser valores de la clase base. Así, cualquier operación puede tomar un objeto como un argumento y puede devolver un objeto como resultado. De esta manera el concepto de clase de objetos está relacionado con el concepto de tipo de dato. Herencia es también vista como un concepto clave dentro del mundo de los objetos. En este contexto, la herencia es la habilidad para organizar las clases de objetos en una jerarquía de subclases y superclases y las operaciones dadas para una clase se pueden aplicar a los objetos de la subclase. Definición de clases y herencia Objetos como abstracción de datos y procedimientos Polimorfismo y chequeo de tipos en tiempo de ejecución Lenguajes: Smalltalk, Java, … Las características más importantes de la programación orientada a objetos son las siguientes: Abstracción: Denota las características esenciales de un objeto, donde se captura su comportamiento. Cada objeto en el sistema sirve como modelo de un “agente” abstracto que puede realizar trabajo, informar y cambiar su estado, y “comunicarse” con otros objetos en el sistema sin revelar “cómo” se implementan estas características. Los procesos, las funciones o los métodos, pueden también ser abstraídos, y cuando sucede esto, una variedad de técnicas son requeridas para ampliar una abstracción. Encapsulamiento: Significa reunir a todos los elementos que pueden considerarse pertenecientes a una misma entidad, al mismo nivel de abstracción. Esto permite aumentar la cohesión de los componentes del sistema. Algunos autores confunden este concepto con el principio de ocultación, principalmente porque se suelen emplear conjuntamente. Principio de ocultación : Cada objeto está aislado del exterior, es un módulo natural, y cada tipo de objeto expone una “interfaz” a otros objetos, que especifica cómo pueden interactuar con los objetos de la clase. El aislamiento protege a las propiedades de un objeto contra su modificación por quien no tenga derecho a acceder a ellas, solamente los propios métodos internos del objeto pueden acceder a su estado. Esto asegura que otros objetos no pueden cambiar el estado interno de un objeto de maneras inesperadas, eliminando efectos secundarios e interacciones inesperadas. Algunos lenguajes relajan esto, permitiendo un acceso directo a los datos internos del objeto de una manera controlada y limitando el grado de abstracción. La aplicación entera se reduce a un agregado o rompecabezas de objetos. Polimorfismo : Comportamientos diferentes, asociados a objetos distintos, pueden compartir el mismo nombre; al llamarlos por ese nombre se utilizará el comportamiento correspondiente al objeto que se esté usando. Dicho de otro modo, las referencias y las colecciones de objetos pueden contener objetos de diferentes tipos y la invocación de un comportamiento en una referencia producirá el comportamiento correcto para el tipo real del objeto referenciado. Cuando esto ocurre en “tiempo de ejecución”, esta última característica se llama “asignación tardía” o “asignación dinámica”. Algunos lenguajes proporcionan medios más estáticos (en “tiempo de compilación”) de polimorfismo, tales como las plantillas y la [[ sobrecarga | sobrecarga de operadores ]] de C++. Herencia : Las clases no están aisladas, sino que se relacionan entre sí, formando una jerarquía de clasificación. Los objetos heredan las propiedades y el comportamiento de todas las clases a las que pertenecen. La herencia organiza y facilita el polimorfismo y el encapsulamiento, permitiendo a los objetos ser definidos y creados como tipos especializados de objetos preexistentes. Estos pueden compartir (y extender) su comportamiento sin tener que volver a implementarlo. Recolección de basura: La recolección de basura o Garbage Collection es la técnica por la cual el ambiente de Objetos se encarga de destruir automáticamente, y por tanto, desasignar de la memoria los Objetos que hayan quedado sin ninguna referencia a ellos. Esto significa que el programador no debe preocuparse por la asignación o liberación de memoria, ya que el entorno la asignará al crear un nuevo Objeto y la liberará cuando nadie lo esté usando. En la mayoría de los lenguajes híbridos que se extendieron para soportar el Paradigma de Programación Orientada a Objetos como C++ u Object Pascal, esta característica no existe y la memoria debe desasignarse manualmente. Según su campo de aplicación Aplicaciones científicas En este tipo de aplicaciones predominan las operaciones numéricas o matriciales propias de algoritmos matemáticos. Lenguajes adecuados son FORTRAN y PASCAL. Aplicaciones en procesamiento de datos En estas aplicaciones son frecuentes las operaciones de creación, mantenimiento y consulta sobre ficheros y bases de datos. Dentro de este campo estarían aplicaciones de gestión empresarial, como programas de nóminas, contabilidad, facturación, etc. Lenguajes adecuados son COBOL y SQL. Aplicaciones de tratamiento de textos Estas aplicaciones están asociadas al manejo de textos en lenguaje natural. Un lenguaje muy adecuado para este tipo de aplicaciones es el C. Aplicaciones en inteligencia artificial Dentro de este campo, destacan las aplicaciones en sistemas expertos, juegos, visión artificial, robótica. Los lenguajes más populares son LISP y PROLOG. Aplicaciones de programación de sistemas En este campo se incluirían la programación de software de interfaz entre el usuario y el hardware, como son los módulos de un SO y los traductores. Tradicionalmente para estas aplicaciones se utilizaba Ensamblador, en la actualidad ADA, MODULA-2 y C. Según su Traducción Intérpretados Un intérprete es un programa que analiza y ejecuta un código fuente, toma un código, lo traduce y a continuación lo ejecuta; y así sucesivamente lo hace hasta llegar a la última instrucción del programa, siempre y cuando no se produzca un error en el proceso. Como ejemplo de lenguajes interpretedos están PHP, Perl, Python, … Compilados Como ejemplo de lenguajes que utilizan un compilador tenemos a C, C++, Visual Basic, … La compilación permite crear un programa de computadora que puede ser ejecutado por una computadora. La compilación de un programa se hace en tres pasos: La forma en que se lleva a cabo el enlace variará entre distintos compiladores, pero la forma general es: Principales Lenguajes de Programación Lenguaje Máquina. Es el sistema de códigos directamente interpretable por un circuito microprogramable, como el microprocesador de una computadora. Ensamblador . Es el que proporciona poca o ninguna abstracción del microprocesador de una computadora. Es fácil su traslado al lenguaje máquina. FORTRAN (FORmula TRANslation). Es un lenguaje de programación desarrollado en los años 50 y activamente utilizado desde entonces. Se utiliza principalmente en aplicaciones científicas y análisis numérico. ALGOL (ALGOrithmic Language) COBOL (COmmon Business Oriented Language) BASIC Visual BASIC Visual BASIC Script Pascal Modula-2 COMAL (COMmon Algorithmic Language) APL (A Programming Language) LOGO HYPERTALK ADA C C++. Es un lenguaje de programación, diseñado a mediados de los años 80, por Bjarne Stroustrup. Por otro lado, es un lenguaje que abarca dos paradigmas de la programación: la programación estructurada y la programación orientada a objetos. Visual C++ C# LISP (LISt Processing) PROLOG (PROgramacion LOGica) FORTH Perl . Lenguaje práctico para la extracción e informe. Es un lenguaje de programación diseñado por Larry Wall creado en 1987. Perl toma características de C, del lenguaje interpretado shell sh, AWK, sed, Lisp y, en un grado inferior, muchos otros lenguajes de programación. Python . Es un lenguaje de programación creado por Guido van Rossum en el año 1990. En la actualidad Python se desarrolla como un proyecto de código abierto, administrado por la Python Software Foundation. Clipper Delphi HTML XHTML PHP SQL PL/1 Java . Es un lenguaje de programación orientado a objetos desarrollado por Sun Microsystems a principios de los años 90. Las aplicaciones Java están típicamente compiladas en un bytecode, aunque la compilación en código máquina nativo también es posible. Javascript Otros CaracterísticasTipos de datos elementalesLas formas de organizar datos están determinadas por los tipos de datos definidos en el lenguaje. Un tipo de dato determina el rango de valores que puede tomar el objeto, las operaciones a que puede ser sometido y el formato de almacenamiento en memoria. Los tipos de datos pueden ser: Predefinidos Definidos por el usuario, a partir de los básicos Tipos de datos elementales: Palabras reservadas Las palabras reservadas son símbolos cuyo significado está predefinido y no se pueden usar para otro fin; aquí tenemos algunas palabras reservadas en el lenguaje C. Identificadores Hay que decir que no todos los lenguajes distinguen entre mayúsculas y minúsculas. Operadores Existen diferentes tipos de operadores: asignación, aritméticos, relacionales, lógicos y de bits. Asignación Al utilizarlo se realiza esta acción: el operador destino (parte izquierda) debe ser siempre una variable, mientras que en la parte derecha puede estar cualquier expresión válida. Con esto el valor de la parte derecha se asigna a la variable de la derecha. Aritméticos Los operadores aritméticos pueden aplicarse a todo tipo de expresiones. Son utilizados para realizar operaciones matemáticas sencillas, aunque uniéndolos se puede realizar cualquier tipo de operaciones. En la siguiente tabla se muestran los operadores aritméticos: Los operadores – , + , * , / , % se corresponden con operaciones matemáticas. Son binarios porque cada uno tiene dos operandos. Hay un operador unario – , pero no hay un operador +. La división de enteros devuelve el cociente entero y desecha la fracción restante. El operador módulo se aplica así: con dos enteros positivos, devuelve el resto de la división. Ejemplos: 12%3 tiene el valor 0. 12%5 tiene el valor 2. Los operadores — y ++ son unarios. Tienen la misma prioridad que el – unario. Se asocian de derecha a izquierda. Pueden aplicarse a variables, pero no a constantes ni a expresiones. Se pueden presentar como prefijo o como sufijo. Aplicados a variables enteras, su efecto es incrementar o decrementar el valor de la variable en una unidad. ++i es equivalente a i=i+1; –i es equivalente a i=i-1; Con ++a el valor de “a” se incrementa antes de evaluar la expresión. Con a++ el valor de “a” se incrementa después de evaluar la expresión. Relacionales Los operadores relacionales hacen referencia a la relación entre unos valores y otros. Lógicos Los operadores lógicos hacen referencia a la forma en que las relaciones pueden conectarse entre sí. De Bits Los operadores de bits son operadores binarios, excepto la negación que es unario. Se aplican a variables enteras, resultando otro entero aplicando operaciones lógicas correspondientes a los bits de los operandos. Al desplazar bits a la izquierda, suele rellenarse con 0 por la derecha y al desplazar a la derecha, se rellena por la izquierda con 0 si el dígito más significativo es 0, y con 1 si es 1. El tipo de variable es con signo. Expresiones y reglas de prioridad Una expresión se forma combinando constantes, variables, operadores y llamadas a funciones. Una expresión representa un valor, el resultado de realizar las operaciones indicadas siguiendo las reglas de evaluación establecidas en el lenguaje. Con expresiones se forman sentencias; con éstas, funciones, y con éstas últimas se construye un programa completo. Cada expresión toma un valor que se determina tomando los valores de las variables y constantes implicadas y la ejecución de las operaciones indicadas. Una expresión consta de operadores y operandos. En la tabla de prioridades veremos operadores no estudiados, la línea de separación indica diferentes prioridades: Variables, constantes, conversión y ámbito de variables Todas las definiciones serán respecto al lenguaje C. Variables No en todos los lenguajes hace falta declarar antes las variables, aunque siempre es recomendable. Las variables, también conocidas como identificadores, deben cumplir las siguientes reglas: Constantes Las constantes son entidades cuyo valor no se modifica durante la ejecución del programa. Conversión Este tipo de conversiones es temporal y la variable por convertir mantiene su valor. Ámbito de variables Las variables pueden ser: Estructura de un programa La estructura de un programa está compuesto de bibliotecas (#include), función principal (main), llaves ({}), y dentro de éstas, la declaración de variables y desarrollo del programa (conjunto de instrucciones). Control de flujoEl teorema del programa estructurado, demostrado por Böhm-Jacopini, demuestra que todo programa puede escribirse utilizando únicamente las tres instrucciones de control siguientes: Solamente con estas tres estructuras se pueden escribir cualquier tipo de programa. La programación estructurada crea programas claros y fáciles de entender, además los bloques de código son auto explicativos, lo que facilita la documentación. No obstante, cabe destacar que en la medida que los programas aumentan en tamaño y complejidad, su mantenimiento también se va haciendo más difícil. Estructura secuencial El control de flujo se refiere al orden en que se ejecutan las sentencias del programa. A menos que se especifique expresamente, el flujo normal de control de todos los programas es secuencial. Estructura alternativa Las estructuras utilizadas son: if-else . La cláusula else es opcional. Se pueden anidar varios if. switch. Realiza distintas operaciones con base en el valor de la única variable o expresión. El valor de la expresión se compara con cada uno de los literales de la sentencia, si coincide alguno, se ejecuta el código, si no se realiza la sentencia default (opcional), si no existe default no se ejecuta nada. La sentencia break realiza la salida de un bloque de código. Ejemplo: Estructuras repetitiva while do-while for FuncionesEn el ámbito de la programación, una función es el término para describir una secuencia de órdenes que hacen una tarea específica. Características: Las funciones son las que realizan las tareas principales de un programa. Las funciones realizan una tarea específica. Subdivide en varias tareas un programa, con pocas líneas y haciendo una tarea simple. Las funciones pueden o no devolver y recibir valores del programa. Hay dos tipos de funciones: Funciones Internas. Funciones internas del lenguaje que realizan tareas específicas. Por ejemplo, hay funciones para el manejo de caracteres y cadenas, matemáticas, de conversión, … Definidas por el el usuario. Primero hay que declarar el prototipo de la función, a continuación debemos hacer la llamada y por último desarrollar la función. Ámbito de las variables (locales y globales) Recursividad La principal ventaja de las funciones recursivas es que se pueden usar para crear versiones de algoritmos más claros y sencillos. Cuando se escriben funciones recursivas, se debe tener una sentencia _if_ para forzar a la función a volver sin que se ejecute la llamada recursiva. Las funciones recursivas pueden ahorrar la escritura de código, sin embargo, se deben usar con precaución, pues pueden generar un excesivo consumo de memoria. Tipos de datos compuestos (estructuras)Un array es una colección de variables del mismo tipo que se referencian por un nombre en común. A un elemento específico de un array se accede mediante un índice. Todos los arrays constan de posiciones de memoria contiguas. La dirección más baja corresponde al primer elemento. Los arrays pueden tener una o varias dimensiones. Una estructura es una colección de variables que se referencia bajo un único nombre, y a diferencia del array, puede combinar variables de tipos distintos. Tanto los arrays como los registros son estructuras de datos que sirven para almacenar valores en memoria, la diferencia radica en que el array solo te permite almacenar un tipo específico de datos: entero, caracteres, fechas, … y los registros, como se ha indicado, admite diferentes tipos de datos. Arrays unidimensionales y bidimensionales La dirección más baja corresponde al primer elemento, y la más alta al último. Un array puede tener una o varias dimensiones. Para acceder a un elemento en particular de un array se usa un índice. El formato para declarar un array unidimensional es: _tipo nombre_array[tamaño]_ El formato para declarar un array bidimensional es : _tipo nombre_array[tamaño1][tamaño2]…[tamañoN]_ En la mayoría de los lenguajes los arrays usan el cero como índice para el primer elemento. Arrays multidimensionales El formato para declarar un array multidimensional es: _tipo nombre_array[fila][columna]_ Estructuras Una definición de estructura forma una plantilla que se puede usar para crear variables de estructura. Las variables que forman la estructura son llamados elementos estructurados. Bibliografía genbetadev Escuela de Ingeniería Eléctrica – UCR Departamento de Informática (Universidad de Valladolid) SISBIB (Sistema de bibliotecas) Universidad Nacional Autónoma de México Apunte de Caramelo McGraw-Hill Inteligencia de negocios: cuadros de mando integral, sistemas de soporte a las decisiones, sistemas de información ejecutiva y almacenes de datos. OLTP y OLAP.Introducción. Evolución de los sistemas tradicionales a las Bases de Datos.Sistemas tradicionales de ficheros. Inconvenientes.Los sistemas tradicionales de ficheros son sistemas orientados hacia el proceso, es decir, se pone el énfasis en el tratamiento que reciben los datos, los cuales se almacenan en archivos diseñados para una aplicación concreta. Las aplicaciones se diseñan e implantan independientemente unas de otras y los datos se duplican si las diferentes aplicaciones los necesitan, en lugar de transferirse entre ellas. Los principales inconvenientes de estos sistemas tradicionales de ficheros son: Redundancia: duplicidad innecesaria de información. Mal aprovechamiento del equipo de almacenamiento: como consecuencia inmediata de la redundancia. Aumento de los tiempos de proceso: se repiten los mismos controles y operaciones en los distintos ficheros, con lo que se consume más tiempo de CPU del necesario. En el caso de modificar un campo hay que hacerlo en todos los registros de todos los ficheros que lo contengan. Inconsistencia de la información: por la alta redundancia. Si se deja de actualizar un dato en uno de los archivos donde aparece, la información proporcionada por este dato se vuelve inconsistente. Aislamiento de los datos: Cada archivo pertenece a un programa y no es posible que estos sean usados por nuevos programas. Un nuevo programa necesitará sus propios archivos de datos que habrán de crearse aunque parte de los datos ya existan en otros archivos de otros programas, contribuyendo a aumentarla redundancia y las consecuencias de esta. Imposibilidad de responder a demandas inesperadas de información: los sistemas tradicionales de archivo son inoperantes para conseguir un sistema de información orientado a la toma de decisiones. Dependencia total entre los programas y la estructura física de los datos: no es posible modificar las características físicas (estructura y métodos de acceso) de los archivos sin afectar a los programas que los usan. Conseguir la independencia entre datos y aplicaciones va a ser uno de los principales objetivos de los sistemas de bases de datos. Sistemas orientados a la base de datosAnte los problemas descritos con los sistemas tradicionales de ficheros, surge la necesidad de una gestión más racional del conjunto de los datos. Poco a poco se fue poniendo más énfasis en un enfoque distinto, en el cual la información se organizaba y se mantenía como un conjunto estructurado que no se diseñaba para una aplicación concreta. Es decir, surge así un nuevo enfoque que se apoya sobre una base de datos en la que los datos son recogidos y almacenados una sola vez con independencia de los tratamientos que se van a aplicar sobre ellos. De esta forma, la información contenida en una base de datos está integrada y compartida. Integrada porque puede considerarse como una unificación de varios archivos de datos de los que hemos eliminado la redundancia y compartida porque los programas que antes accedían a los archivos individuales acceden ahora al depósito común de datos, por lo que cada usuario o aplicación tendrá acceso a un subconjunto de los datos y como consecuencia diferentes usuarios verán de formas muy diferentes la misma base de datos. Es importante destacar que los subconjuntos de datos a los que acceden las diferentes aplicaciones o usuarios no tienen por qué ser disjuntos, por lo que usuarios o aplicaciones distintas pueden acceder a la misma parte de la base de datos para utilizarla con propósitos diferentes. Concepto de Base de DatosUna primera definición de bases de datos sería: “Una Base de Datos (BD) es una colección o depósito de datos integrados, almacenados en soporte secundario (no volátil) y con redundancia controlada. Los datos, que han de ser compartidos por diferentes usuarios y aplicaciones, deben mantenerse independientes de ellos y su definición (estructura de la BD), única y almacenada junto con los datos, se ha de apoyar en un modelo de datos, el cual ha de permitir captar las interrelaciones y restricciones existentes en el mundo real. Los procedimientos de actualización y recuperación, comunes y bien determinados, facilitarán la seguridad del conjunto de los datos”. Veamos en qué consiste cada uno de los aspectos mencionados en esta definición de Base de Datos que no son más que distintas definiciones según distintas perspectivas. La BAD es un conjunto de datos relativos a una determinada parcela del mundo real que se almacenan en un soporte informático no volátil. Además, no debe existir redundancia, es decir, no deben existir duplicidades perjudiciales ni innecesarias (a ser posible un determinado tipo de dato, sólo deben aparecer en un sitio en la BD). En ocasiones, es necesaria cierta redundancia (a nivel de almacenamiento físico) que mejora la eficiencia de la BD. Sin embargo, esta redundancia siempre debe ser controlada por el sistema para que no se produzcan inconsistencias. Por otro lado, las BD han de atender a múltiples usuarios de la organización así como a distintas aplicaciones. Otras definiciones de BD son: Conjunto de datos de la empresa memorizados en un ordenador, que es utilizado por numerosas personas y cuya organización está recogida en un modelo de datos. Colección o depósito de datos, donde estos se encuentran lógicamente relacionados entre sí, tienen una definición y una descripción comunes y están estructurados de una forma particular. Colección o depósito de datos integrados, con redundancia controlada y una estructura que refleje las interrelaciones y restricciones existentes en el mundo real; los datos, que han de ser compartidos por diferentes usuarios y aplicaciones, deben mantenerse independientes de estos, y su definición y descripción, únicas para cada tipo de dato, han de estar almacenados junto con los mismos. Los procedimientos de actualización y recuperación, comunes y bien determinados, habrán de ser capaces de conservar la integridad, seguridad y confidencialidad del conjunto de los datos. Principales características de los datos almacenados en una BD: Están organizados Están relacionados Son accesibles de diferentes formas sin grandes dificultades Se almacenan solo una vez Independencia de los datos. Niveles de abstracción.En los sistemas de base de datos se plantean dos objetivos principales: Independencia de la base de datos de los programas para su utilización. Proporcionar a los usuarios una visión abstracta de los datos. El sistema esconde los detalles de almacenamiento físico (como almacena y mantiene los datos), pero deben extraerse eficientemente. Independencia de los Datos. Definición de ANSI. La independencia de los datos es la capacidad de un sistema para permitir que las referencias a los datos almacenados, especialmente en los programas y en sus descriptores de datos, estén aislados de los cambios y de los diferentes usos en el entorno de los datos, como puede ser la forma de almacenar dichos datos, el modo de compartirlos con otros programas y como se reorganizan para mejorar el rendimiento del sistema de Bases de Datos. Niveles de abstracción Para conseguir esta independencia entre los datos y las aplicaciones es necesario separar la representación física y lógica de los datos, distinción que fue reconocida oficialmente en 1978, cuando el comité ANSI/X3/SPARC propuso un esqueleto generalizado para sistemas de BD. Este esqueleto propone una arquitectura de tres niveles, los tres niveles de abstracción bajo los que podía verse una base de datos: el nivel interno, el nivel conceptual y el nivel externo. Esta arquitectura de tres niveles nos proporciona la deseada independencia, que definimos como la capacidad para cambiar el esquema en un nivel sin tener que cambiarlo en ningún otro nivel. Distinguimos dos tipos de independencia: Independencia lógica de datos : cambio del esquema conceptual sin cambiar las vistas externas o las aplicaciones. Independencia física de los datos : cambio del esquema interno sin necesidad de cambiar el esquema conceptual o los esquemas externos. Ventajas e inconvenientes del uso de bases de datos Ventajas Control sobre las inconsistencias y redundancias : en los sistemas tradicionales de ficheros cada aplicación tiene sus datos privados, lo que provoca una alta redundancia y desaprovechamiento del espacio en disco. La redundancia debe minimizarse y controlarse con las BD. Aunque se mantenga cierto grado de redundancia por motivos de rendimiento u otros, el sistema proporciona mecanismos para garantizar la consistencia. Se controla la redundancia garantizando que los datos redundantes se actualicen de forma automática. Mejor servicio a los usuarios : en los sistemas convencionales suele ser difícil obtener una información para la cual no fueron diseñados. Una vez que varios de estos sistemas se combinan para crear una BD centralizada, además de mejorar sustancialmente la disponibilidad de la información también garantiza que los datos son actuales. Asimismo es posible responder mas ágilmente a nuevas necesidades del usuario no planteadas anteriormente. Los datos pueden compartirse : las necesidades de datos de nuevas aplicaciones pueden atenderse con los ya existentes sin tener que almacenar nuevos datos. Mejora la flexibilidad del sistema : a menudo se plantea la necesidad de cambiar los datos almacenados. Estos cambios, a través de un sistema de BD, no tienen el impacto sobre las aplicaciones que tendrían un sistema convencional. Menor coste de desarrollo y mantenimiento : aunque el coste inicial de una base de datos puede ser superior al de un sistema tradicional, los costes en mantenimiento y desarrollo de aplicaciones son menores. Pueden hacerse cumplir las normas establecidas : al tener un control centralizado de las BD, el administrador (a instancias del responsable de la información de la organización) puede garantizar que se observen las normas de la empresa aplicables a la representación de los datos. Restricciones de seguridad : el administrador puede asegurar que el único modo de acceso sea a través de los canales establecidos, y definir controles de autorización que pueden afectar a cada modo de acceso (modificación, inserción, borrado o lectura), según las necesidades de cada usuario. Se desarrolla un modelo de datos : en los sistemas convencionales los ficheros de datos se diseñan teniendo en mente las necesidades particulares de la aplicación que los va a utilizar, dejando de lado la visión más general. En los sistemas de BD al estar la información centralizada, se hace necesario un punto de vista más general a la hora de diseñar el modelo de datos. Se reduce el espacio de almacenamiento : al integrar en un solo espacio los datos de varios sistemas aislados y evitar que se repitan, se requiere un menor volumen para almacenar los mismos datos. Por otro lado esto facilita las tareas de realización de copias de seguridad y su recuperación. Inconvenientes Instalación costosa. Necesidad de personal especializado. Necesidad de hardware adicional: al ser los requerimientos superiores a los de un sistema tradicional, ya sea en cuanto a memoria, capacidad de proceso, etc. El sistema adquiere mayor complejidad: al integrarse dentro del SO un nuevo sistema activo que interacciona con él e influye en la capacidad de responder al usuario. Implantación larga y difícil. Falta de rentabilidad a corto plazo. Principales componentes de un entorno de BDAhora vamos a desmenuzar un poco más el concepto de BD, y vamos a describir sus componentes básicos, justificando además la necesidad de cada uno de ellos. DatosUna BD no tiene sentido sino está compuesta por datos. Lo que no está tan claro es la forma en que estos datos se deben disponer, qué datos se deben almacenar y cómo los debe entender la máquina. La disposición de los datos depende del ámbito de aplicación concreto en que se enmarque la BD. No es lo mismo una BD que almacene un dibujo vectorial de monumentos históricos que una BD para almacenar las reservas de clientes en un hotel. Lo que varía fundamentalmente en uno y otro caso, es la forma en que los datos se relacionan entre sí, y el tipo de accesos a la información que va a realizar el usuario. Además, los datos deben disponerse de manera que las consultas sean lo más eficientes posible, evitando a la vez la existencia de datos duplicados que pueden dar al traste con la coherencia de la BD. De esta forma, no sólo se consideran datos aquellos que el usuario desea almacenar, sino toda la estructura de apoyo que el sistema necesite para hacer más eficiente una consulta. Los índices son ficheros auxiliares que facilitan el acceso a los datos que se encuentran en el fichero principal de la BD. La gran ventaja de los índices es que el mantenimiento de los mismos lo realiza de forma automática el sistema. De esta forma se obtiene una gran eficiencia en las consultas, sin que el usuario tenga que trabajar más. Pues bien, este fichero aparte, creado con el único objetivo de facilitar el acceso a la información, también lo consideramos datos, aunque deban ser gestionados automáticamente, e incluso algunos usuarios no tengan ni la menor idea de su existencia. Por otro lado, es muy importante decidir qué datos se van a almacenar. Hay que encontrar un equilibrio entre las situaciones en las que almacenamos datos de más (históricos muy poco utilizados) y en las que se almacenan datos de menos (guardando los estrictamente necesarios en el momento de definir la BD). Por último, también es interesante, por cuestiones de seguridad y aprovechamiento de los recursos, decidir en qué formato se van a almacenar los datos: si se van a almacenar encriptados para que nadie los pueda copiar, o si se van a codificar o a comprimir de alguna forma para hacer que ocupen menos espacio. MetadatosDesde el momento en que se crea una BD, hasta el momento en que se desecha porque se compra un sistema mejor, o se instala una nueva BD, las estructura de la BD (o sea, los datos que se deciden almacenar, y la estructura con que se almacenan) cambia a medida que cambian las necesidades sobre la información a obtener de la BD. Dado que la BD que solucione unas necesidades concretas puede adoptar muchas formas posibles, es muy interesante poseer algún lugar que indique al personal encargado de mantener la BD, cuál es el objetivo de cada dato particular almacenado en la base, así como en qué aplicaciones es utilizado, y con qué propósito, si es un dato fundamental, o si puede ser omitido por el que introduce los datos, etc. De esta forma, antes de modificar el esquema o estructura de la BD, el departamento de proceso de datos debe consultar esta información sobre los datos de la base, con cuidado para no cometer errores graves que repercutan sobre el buen funcionamiento de todo el sistema. Esta información que el sistema guarda sobre los datos almacenados, es lo que se llaman metadatos, es decir, datos acerca de los datos. Es más, estos metadatos se almacenan en el diccionario de datos o catálogo. El Sistema Gestor de Bases de DatosEn un Sistema de BD, debe existir una capa intermedia entre los datos almacenados enla BD, las aplicaciones y los usuarios del mismo. Se trata del Sistema de Gestión de la BD (SGBD). Actúa de intermediario entre los usuarios y aplicaciones y los datos proporcionando medios para describir, almacenar y manipular los datos, y proporciona herramientas al administrador para gestionar el sistema, entre ellas las herramientas de desarrollo de aplicaciones, generadores de informes, lenguajes específicos de acceso a los datos, como SQL (Structure Query Language) o QBE (Query By Example) en BD relacionales. Un SGBD se puede definir como un conjunto coordinado de programas, procedimiento, lenguajes, etc. que suministra, tanto a los usuarios finales como a los analistas, programadores o el administrador, los medios necesarios para describir, recuperar y manipular los datos almacenados en la base, manteniendo su integridad, confidencialidad y seguridad. Entre sus funciones destacan: Definición y control centralizado de los datos : definición de todos los elementos de datos en la BD en los tres niveles definidos anteriormente (interno, conceptual y externo). Descripción de los datos (campos, grupos, registros, tablas), e interrelaciones entre las diferentes estructuras de datos. La BD es autodescriptiva, es decir, contiene información que describe su estructura (los metadatos). Manipulación de los datos : suministrar mecanismos que faciliten la interacción con la BD. El SGBD debe ser capaz de atender las solicitudes del usuario para extraer, modificar o añadir datos a la BD. Estos mecanismos suelen venir dados en forma de lenguajes de manipulación y definición de datos. Además, garantizan la independencia de los datos, en el sentido de que, pese a la evolución del esquema de los datos, las aplicaciones deben sufrir las mínimas modificaciones imprescindibles. La Seguridad y la Integridad : debe proporcionar los medios para definir y gestionar las autorizaciones de acceso, ya sea mediante claves de acceso al sistema o mediante la definición de vistas externas de usuario, para evitar así accesos fraudulentos a los datos. Por otro lado, también proporciona los medios para garantizar la integridad y la consistencia de los datos definiendo (en el diccionario de datos) restricciones sobre los valores que pueden tomar. Permitir hacer copias de seguridad : proporciona capacidades de recuperación ante fallos y de copia de seguridad. Garantiza la disponibilidad de la información : permite el acceso simultáneo a los datos por parte de varios usuarios. Debe controlar que la información representada por los datos al final de cada acceso de usuario siga siendo coherente. Interactuar con el SO : como el SO es el único que puede acceder a los dispositivos de E/S, si el SGBD debe leer o escribir en estos dispositivos debe interactuar con el SO. Usuarios de la Base de DatosLas personas que trabajan con una BD se pueden catalogar como usuarios de BD. Podemos clasificar en cuatro clases los usuarios de un sistema de BD: Usuarios normales : no saben nada de la estructura interna de la BD. Interaccionan con ella a través de las aplicaciones desarrolladas por los programadores, y son incapaces de acceder a los datos directamente a través del lenguaje del SGBD. La interfaz de este tipo de usuarios es una interfaz de formularios, donde el usuario puede rellenar los campos apropiados del formulario. Programadores de aplicaciones . Reciben peticiones de otros usuarios, para el acceso a los datos, y se encargan de escribir los programas que satisfacen dichas necesidades. Normalmente estos programas están escritos en lenguajes de programación: convencional (Pascal, Basic, C, Cobol, etc.), en el que se insertan órdenes especiales que es capaz de comprender el SGBD. De esta forma el SGBD suministra los datos, y el lenguaje convencional (también llamado anfitrión porque alberga las sentencias reconocidas por el SGBD) los procesa, presenta al usuario, modifica, etc. de cuarta generación que combinan estructuras de control imperativo, con instrucciones del lenguaje de manipulación de datos, y además incluyen características especiales para facilitar la generación de formularios y la presentación de datos en pantalla. La mayoría de los sistemas de BD comerciales incluyen un lenguaje de cuarta generación. Usuarios sofisticados : son aquellos que interactúan con la BD sin programas escritos, haciendo uso directamente del lenguaje que proporciona el SGBD. Por ejemplo, SQL. Usuarios especializados : son usuarios sofisticados que escriben aplicaciones de BD especializadas. Entre estas aplicaciones están los sistemas de diseño asistido por ordenador, sistemas de bases de conocimiento, sistemas expertos etc. Finalizamos este apartado con la quinta clase de usuarios de BD. Son los administradores de BD . Usuarios especiales que son los responsables del control general del sistema desde el punto de vista técnico. Entre sus funciones cabe destacar: Definir el esquema conceptual, con el lenguaje de Definición de Datos. Definir el esquema interno. La estructura de almacenamiento y los métodos de acceso. Diseño físico e implementación de la BD. Modificar el esquema y la organización física de los datos. Para reflejar las necesidades cambiantes de la organización y para mejorar el rendimiento. Establecer las restricciones de seguridad, integridad y confidencialidad. Concesión de permisos y privilegios para el acceso a los datos. Definir los procedimientos de copia de seguridad y recuperación. Supervisar el rendimiento del sistema y responder a los cambios en los requerimientos. Elementos de seguridadEl administrador debe conocer en profundidad los elementos de seguridad que suministra el SGBD y sacar el máximo partido posible de ellos. En general, se pueden tener niveles de acceso clasificados por: La información a que se tiene acceso. En una empresa grande y ampliamente informatizada, cada usuario debe poder acceder exclusivamente a los datos que competen a su tarea. Por tanto, debe existir un mecanismo de seguridad que restrinja el ámbito de acceso de cada usuario en función de sus competencias. Las operaciones que se pueden realizar sobre la información. No sólo es importante el acceder o no a los datos, sino también la forma en que este acceso se produce, en función de las características propias de la sección. En general, las operaciones que se pueden efectuar se agrupan en cuatro grandes bloques: Altas, Bajas, Modificaciones y Consultas. El acceso al diccionario de datos y a la estructura de la BD. Como se ha comentado anteriormente, los metadatos almacenados en el diccionario de datos y que almacenan información sobre la estructura de la BD, son gestionados, a su vez, como si de una BD especial se tratase. Sin embargo, dada su primordial importancia, su acceso debe estar muy restringido, ya que cualquier modificación puede dar lugar a resultados desastrosos en la BD: pérdida de información, corrupción en los datos, falta de integridad, etc. Por ello, es necesaria la existencia de prioridades o privilegios especiales que sólo permitan el acceso al personal que compone la administración de la BD, que es el único capacitado para modificar estos metadatos. Lenguajes de Bases de DatosLa interacción del usuario con la BD debe efectuarse a través de alguna técnica que haga fácil la comunicación, y que permita al usuario centrarse en el problema que desea solucionar, más que en la forma de expresarlo con las técnicas que se le suministran. La mejor forma de alcanzar este objetivo, es darle un lenguaje parecido al lenguaje natural, que le permita expresar de forma sencilla los requerimientos. En función de estos requerimientos, podemos tener, fundamentalmente dos tipos de lenguajes para comunicarnos con el SGBD: Lenguaje de definición de datos (LDD) . Este lenguaje es utilizado en exclusiva por el administrador de la BD, ya que permite la construcción de sentencias que le indican al SGBD las características particulares de la BD sobre la que se está trabajando, así como la creación de nuevas BD. La creación de esquemas y su modificación, la creación y supresión de índices, la especificación de unidades de almacenamiento en los ficheros, etc. Lenguaje de manipulación de datos (LMD). El lenguaje de manipulación de datos es el que usan los usuarios sofisticados para efectuar sus operaciones sobre la BD. Como se indicó, estas operaciones son básicamente de inserción, eliminación, modificación y consulta de datos, aunque también se pueden introducir capacidades para crear vistas de los datos que faciliten otros accesos. Los usuarios sofisticados interaccionan con el SGBD a través de este lenguaje, mediante una interfaz agradable y fácil de usar. Los programadores de aplicaciones emplean el LMD dentro de un lenguaje de programación que les da potencia expresiva. Para ello, el LMD que emplean se extiende de diferentes formas para poderse integrar fácilmente en el lenguaje anfitrión, ya que ambos, LMD y lenguaje anfitrión deben poderse comunicar adecuadamente para que las aplicaciones resultantes sean simples de programar y de utilizar. Utilización de la Base de Datos en la OrganizaciónLas BD son ampliamente usadas en multitud de aplicaciones: Banca, Líneas aéreas, Universidades, Telecomunicaciones, Ventas, Recursos Humanos, etc. Esto significa que las BD forman una parte esencial de las empresas actuales. Sistema de información en una organización. Componentes.Concepto de Sistema de información Toda organización necesita para su funcionamiento un conjunto de informaciones que han de transmitirse entre sus diferentes elementos, así como desde y hacia el exterior de la propia organización. Un sistema de información se diseña con el fin de satisfacer las necesidades de información de una organización, en la que está inmerso. El sistema de información toma datos del entorno (tanto de la organización, como de las fuentes externas) y los resultados de las operaciones sobre esos datos serán la información que dicha organización necesita para su gestión y toma de decisiones. Componentes de un sistema de información Contenido: (Datos: hechos conocidos con significado implícito que pueden ser almacenados). Es el centro del sistema de información. Los datos contenidos en un sistema de información pueden ser: de tipo referencial: son aquellos que contienen información acerca de donde se encuentra la información buscada. de tipo factual: son aquellos que contienen la información en sí. A su vez pueden ser: estructurados y no estructurados. Equipo físico. Ordenadores y periféricos. Soporte Lógico. Incluye todo el software necesario para la implantación del sistema de información: SO, Sistemas de BD, software de comunicaciones y otros programas para tratamientos específicos. Administrador. Los datos y las informaciones manejadas por nuestro sistema de información han de ser gestionadas por las personas adecuadas. Tendremos, por un lado los responsables de tomar las decisiones estratégicas y políticas con respecto a la información de la empresa y por otro los responsables de dar apoyo técnico para poner en práctica estas decisiones. Niveles de gestión y de usuarios en una organizaciónEn toda organización hay tres niveles de gestión (operacional, táctico y estratégico) y el sistema de información debe diseñarse para satisfacer las necesidades y facilitar informaciones adecuadas a cada uno de los niveles. En el nivel operacional, los usuarios manejan datos elementales que describen los sucesos que caracterizan las actividades de la organización. Esta información, compuesta por datos totalmente desagregados (microdatos) es necesaria para los procesos comúnmente denominados administrativos (tareas diarias y de rutina) y el volumen de datos manejado será muy grande. En este nivel situamos a los llamados sistemas transaccionales. En el nivel táctico se definen los objetivos específicos y el control de gestión. En el nivel estratégico se definen los objetivos generales y la elaboración de planes. En los niveles táctico y estratégico, cuyos usuarios tienen necesidades de información muy distintas, obtendrán del nivel anterior (operacional), mediante procesos de elaboración adecuados (generalmente de agregación) junto con datos provenientes el exterior, las informaciones necesarias para la ayuda a la decisión. Junto con estos niveles de gestión también se pueden distinguir 3 niveles de usuarios: personal, mandos intermedios y ejecutivos. Estos niveles se corresponden con los 3 diferentes tipos de automatización de los sistemas de negocios: Los PED (Procesamiento Electrónico de Datos) o DP (Data Processing) que tienen el foco de atención en el nivel operativo de almacenamiento, procesamiento y flujo de los datos, así como procesar eficientemente las transacciones y realizar informes resúmenes para los dirigentes. Típicamente el enfoque DP se usa para transformar un conjunto de datos “brutos” en la siguiente información: Estadística (Ejemplo: Números que representan la media, la moda y la varianza de los datos). Representaciones gráficas (Ejemplo: Histogramas, Diagramas de barras, Diagramas de pastes, etc). Los SIG (Sistemas de Información de Gestión) o MIS (Management Information Systems) que se caracterizan porque su foco de atención está en la información orientada a mandos intermedios, por la integración de las tareas de PED, por sus funciones en los negocios y por la generación de informes. Los STD (Sistema de Apoyo a la Toma de Decisiones) o DSS (Decision Support Systems), más centrados en la decisión y orientados a los altos ejecutivos. Por otro lado los datos provenientes del nivel operacional almacenados en bases de datos, así como en otros soportes, pueden estar organizados en almacenes de datos (Data Warehouses) que sirven de base para la extracción y el descubrimiento de conocimiento en BD (KDD, Knowledge Discovering inDatabases) y para la minería de datos (Data Mining). Los sistemas transaccionales (PED)Concepto de transacción bajo un sistema de BBDDUno de los problemas más complejos que se plantea en los sistemas de BD es garantizar la consistencia de la base de datos a pesar de los fallos del sistema de la ejecución concurrente. Para dar solución a los problemas de recuperación y concurrencia, se introduce el concepto de transacción. Una transacción es una secuencia de operaciones llevadas a cabo como una unidad lógica de trabajo simple. Para asegurar la integridad de los datos se necesita que el sistema de base de datos mantenga las siguientes propiedades de las transacciones: Atomicidad : Una transacción debe ser una unidad atómica de trabajo o todas sus operaciones se llevan a cabo o no se realiza ninguna de ellas. Consistencia : Cuando termina, una transacción debe dejar la BD en un estado consistente (suponiendo que está ejecutándose de forma aislada, sin otras transacciones concurrentes). Asegurar la consistencia es responsabilidad de los mecanismos de control de concurrencia. Aislamiento : Las modificaciones realizadas por una transacción deben aislarse de las modificaciones llevadas a cabo por otras posibles transacciones concurrentes. Una transacción debe ver los datos en el estado en el que estaban antes de que cualquier otra transacción concurrente los modificara o bien los ve tras su modificación, pero nunca en un estado intermedio. El aislamiento es una propiedad de las transacciones por lo cual una transacción ve en todo momento la BD en un estado consistente. Una transacción en ejecución no hace visibles sus datos a otras transacciones concurrentes hasta que no termina y hace permanentes sus cambios. Durabilidad : Una vez que la transacción ha terminado con éxito, sus efectos deben hacerse permanentes en la BD. Las modificaciones deben persistir incluso en caso de fallo del sistema. El SGBD garantiza que los resultados de las transacciones terminadas sobrevivan a fallos posteriores del sistema. Suele referirse a estas propiedades como ACID , por sus siglas en inglés (Atomicity, Consitency, Isolation, Durability). Es frecuente también encontrar referencias a la “acidity” de una transacción. Los programadores son los responsables de establecer el inicio y el final de cada transacción en puntos que hagan cumplir la consistencia lógica de los datos. Es responsabilidad de SGBD proporcionar los mecanismos que garanticen la integridad física de cada transacción, aquí destacamos: Facilidades que protejan el aislamiento de las transacciones. Facilidades de registro que aseguren la durabilidad de las transacciones. Si la ejecución de la transacción es interrumpida por cualquier tipo de fallo, el SGBD es el responsable de determinar qué hacer con la transacción una vez recuperado del fallo: Terminar la transacción (hacer lo que queda por hacer). Abortarla (deshacer lo que se haya hecho) Características de gestión de transacciones que garantizan la atomicidad y la consistencia de las transacciones. Una vez que la transacción ha comenzado, esta debe ser completada con éxito o el gestor deshace todas las modificaciones de datos realizadas por esta desde el comienzo de la transacción. Transacciones y procesamiento multiusuarioCuando dos o más usuarios acceden concurrentemente a una BD, el procesamiento de transacciones adquiere una nueva dimensión. Ahora el SGBD no solo debe recuperarse adecuadamente de los fallos del sistema sino que también debe asegurar que las operaciones de los usuarios o aplicaciones no interfieran unas con otras. Idealmente, cada usuario debería poder acceder a la BD como si tuviera acceso exclusivo a ella, sin preocuparse de las acciones del resto de los usuarios. Esto se logra mediante diferentes mecanismos que se conocen como esquemas de control de concurrencia. Sistemas OLTP. Características.Para cualquier organización actual, la captura de datos sobre sus operaciones diarias es indispensable para su funcionamiento continuado, por lo que el almacenamiento sistemático de transacciones es una actividad común en el día a día. Los sistemas que se utilizan con tal fin son los llamados OLTP (On-Line Transaction Processing) y suelen estar constituidos por BD y sistemas on-line optimizados para la inserción de grandes volúmenes de registros, habitualmente recogidos de uno en uno. En adelante, denominaremos a los datos así recogidos como datos operativos, para distinguirlos de los que se guardan en los almacenes de datos. En principio, la información necesaria para la gestión corporativa se podría extraer de estos sistemas y sus BD asociadas. No obstante, los sistemas OLTP tienen características que hacen que esa solución no sea la más adecuada: Heterogeneidad : viene producida por la cantidad de fuentes de las que proceden los datos operativos y se manifiesta en la falta de consistencia a la hora de elegir formatos de representación, la aparición de registros redundantes, erróneos, contradictorios o, simplemente, inútiles para la extracción de información. Falta de organización : es un producto tanto de la necesidad de efectuar el almacenamiento de forma rápida, para que no se produzcan cuellos de botella en los tiempos de respuesta de sistema, como de la dispersión de los sistemas en que se almacenan estos datos diarios. Esto conduce a que, de forma natural, los datos queden almacenados atendiendo a criterios geográficos (donde se recogen) y de transacción (que operación los produjo). Inadecuación para dar respuesta a consultas complejas : esto se debe a que las BD utilizadas están optimizadas para ofrecer grandes rendimientos en las operaciones de inserción y actualización, tanto en velocidad como en volumen. Para lograr este objetivo suelen tener un nivel de redundancia muy bajo, es decir, que distribuyen la información en gran número de tabla relacionadas entre si y con un mínimo de información agregada o precalculada. Esta organización de las tablas, útil en BD que van a estar sufriendo constantes actualizaciones, se convierte en un obstáculo para realizar consultas complejas, ya que al estar la información dispersa entre múltiples tablas, la recuperación de la información requiere una gran cantidad de operaciones de unión natural (join), costosas tanto en recursos de máquina como en tiempo. Además, realizar una consulta compleja, cuya respuesta va a demandar gran cantidad de recursos por parte de la máquina OLTP, producirá un impacto negativo en la capacidad de procesamiento y almacenaje de nuevas transacciones. Dado el número suficiente de usuarios y consultas concurrentes de este tipo, puede suceder incluso que se inutilice el sistema. Por supuesto, el hecho mismo de que sigan llegando sin cesar nuevos registros mientras se realiza la consulta implica que sus resultados no va a ser todo lo fiables que sería de desear. Por estos motivos, trabajar sobre las BD operativas, que utilizan para llevar el día a día,no es la forma más eficaz de extraer información útil para la toma de decisiones, tanto por los errores que se pueden producir al tratar con datos dispersos y sin limpiar, como por la repercusión que podría tener sobre el rendimiento del sistema el trabajar directamente sobre los equipos que se usan para recoger transacciones on-line, vitales para el funcionamiento de la organización. Sistemas de Información de Gestión (MIS)¿Qué es un M.I.S.?Existen varias definiciones, veamos dos de ellas: Conjunto de medios para reunir los datos necesarios para la gestión y difundir la información obtenida con el tratamiento de estos datos. Proceso por el que los datos importantes para la empresa son identificados, analizados, recolectados y puestos a su disposición. De estas definiciones se deduce que el primer objetivo de un sistema de gestión es incrementar la “inteligencia” de los procesos del negocio y el conocimiento de los trabajadores implicados en estos procesos. Un poco de historia. Antecedentes. En los primeros días de los sistemas de información vieron la luz las aplicaciones. Su diseño estaba marcado por las necesidades puntuales del día a día de diferentes departamentos. La integración entre ellas no era un objetivo. Pronto dio comienzo el mantenimiento de esas aplicaciones. Las aplicaciones necesitaban cambios por muchas razones: nuevos requisitos, cambios en el negocio, nuevas oportunidades. Al mismo tiempo que comenzaba el mantenimiento surgía la necesidad de extraer más información de estas aplicaciones. El primer intento para satisfacer esta necesidad fue la escritura de programas que listasen informes. La primera limitación de estos listados es que accedían a una única aplicación. Hubo que definir interfaces entre las aplicaciones para que pudiesen compartir datos entre ellas. La segunda limitación es que los informes había que modificarlos de forma constante. Se introdujeron herramientas 4GL para poder escribir y modificar informes a gran velocidad. Listaban muchos más informes, pero con los mismos problemas de antes. Se introducen entonces herramientas de extracción. Surge el PC. Con las herramientas de extracción los usuarios ya pueden acceder y manipular directamente la información. A medida que aumentaba la potencia de los PCs aumentaba el volumen de los datos almacenados en ellos. Luego las redes, … Aquí llegados los usuarios padecen la misma falta de integración, consistencia, coherencia y las mismas limitaciones que antes de la llegada del PC. Pero esto no trae como consecuencia que decrezca la demanda de información, al contrario, ésta siempre crece. Un cambio en la arquitectura El corazón del problema es que las aplicaciones están profundamente marcadas por las primeras consideraciones que dirigieron su desarrollo: las necesidades departamentales enfocadas sobre las necesidades del día a día. La arquitectura sobre la que se construyeron estas aplicaciones (OLTP) no es válida para soportar las necesidades de los sistemas de información de gestión actuales. En estos sistemas la arquitectura de datos nunca fue un objetivo del negocio. La complejidad y dinamismo de la “economía digital” ha colocado en un lugar predominante la habilidad de los gestores para ver lo que está ocurriendo, desvelando las dificultades de acceso a la información de la empresa. Es en este momento cuando la calidad y disponibilidad de la información se convierte en un objetivo primordial del negocio. ¿Cómo compensar la carencia de una arquitectura de datos? Creando una gran BD virtual para integrar los datos de las aplicaciones existentes, que pasarán a formar parte de esta BD una vez que hayan sido depurados y reconciliadas sus disparidades. Esto posibilitará que los datos sean utilizados para la gestión. La solución pasa por separar el procesamiento en dos grandes categorías: Proceso operacional OLTP Procesamiento para el sistema de soporte de decisiones (DSS/OLAP) Puesta en marcha del M.I.S. dentro de la organizaciónLa implementación consiste, en una primera fase, en el análisis de las necesidades de información a las que desea acceder cada empresa. Para ello se integrarán en el sistema todos aquellos datos operacionales necesarios, además de otras fuentes de información que sea necesario incorporar. Definida la estructura de las BD se procederá a la carga de la información y se crearán las agregaciones de datos para mejorar el rendimiento del sistema en los procesos de consulta más habituales. Finalmente, se incluirán en el sistema los procedimientos que permitan la actualización de la información, cuya periodicidad dependerá de las necesidades de cada usuario. Sistemas de soporte a la decisión (DSS)Como ya se comentó, los STD (Sistema de Apoyo a la Toma de Decisiones) o DSS (Decision Support Systems), están más centrados en la decisión y orientados a los altos ejecutivos. Las BD que soportan estos sistemas son de gran tamaño y pueden resultar minas de información para adoptar decisiones empresariales, como los artículos que debe haber en inventario y los descuentos que hay que ofrecer. La utilización de Sistemas de Soporte a la Decisión (DSS) es muy adecuada para afrontar una variedad de situaciones al interactuar: interfaces de usuarios, que hacen que dichos sistemas se adecuen a las necesidades de los usuarios; BD, que incluyen toda la información necesaria; y finalmente; procesos de decisión que ayudan al experto en la difícil tarea de establecer posibles soluciones. Se puede definir DSS como “programas informáticos interactivos que utilizan métodos analíticos, tales como análisis de decisión, algoritmos de optimización, programas de planificación de rutinas, etc., para el desarrollo de modelos ayudando a los creadores de decisión a formular alternativas, analizar sus impactos, e interpretar y seleccionar opciones apropiadas para la implementación”. Los sistemas de soporte a la decisión pueden considerarse como una tercera generación de aplicaciones asistidas por ordenador. Al principio, los ordenadores mainframe fueron usados mayormente para el procesamiento de transacciones. Durante los años 70 y 80, el concepto de DSS creció y se desarrolló en los campos de búsqueda, desarrollo y práctica. Los Sistemas de Información de Gestión (MIS) suministraron: Informes planificados para desarrollar bien las necesidades de información Informes de demandas para la información específica solicitada Habilidad para consultaren una BD, datos específicos Los MIS carecían de algunos de los atributos necesarios para soportar la creación de decisión. Atributos tales como, enfoque, metodología de desarrollo, manejo de gestión de datos, uso de ayuda analítica, y diálogo entre el usuario y el sistema. El DSS se extendió y combinó la tecnología de la BD y la tecnología de modelado dando a los usuarios finales acceso a ellos. Los datos y modelos se unieron íntimamente junto con el usuario. Arquitectura del DSSUna manera útil de pensar en las partes de los componentes de un DSS y las relaciones entre las partes está en utilizar el diálogo, los datos, y el modelo (DDM). En esta conceptualización, hay un diálogo (D) entre el usuario y el sistema, los datos (D) que soporta el sistema, y los modelos (M) que suministra el análisis de las capacidades. Estudiamos ahora con más detalle cada una de estas partes. El componente de Diálogo (D) Una apreciación de la importancia del componente de diálogo se obtiene reconociendo que desde la perspectiva del usuario, el diálogo es el sistema. Lo importante es lo que el usuario debe conocer para usar el sistema, las opciones para dirigir las acciones del sistema, y las presentaciones alternativas de las respuestas del sistema. Dentro del componente diálogo destacan: La Base de Conocimiento . La base de conocimiento incluye lo que el usuario conoce acerca de la decisión y acerca de cómo usar el DSS. El Lenguaje de acción . Las acciones que el usuario realiza para controlar el DSS se describen de varias formas, dependiendo del diseño del sistema. El Lenguaje de Presentación . El PC o la estación de trabajo usada en una base autónoma, como una unidad en la red de área local, o como un terminal inteligente conectado a un mainframe tiene una significativa expansión y mejora la salida desde que está presente un DSS. Una de las mayores contribuciones del PC es su capacidad de presentación de gráficos. Estilos de Diálogo . Las combinaciones o conjuntos de opciones para implantar la base de conocimiento, el lenguaje de acción, y el lenguaje de presentación, tomados a la vez, son llamados “estilo de diálogo”. El componente de Datos (D) Los datos juegan un papel importante en el DSS: se acceden directamente por el usuario o son una entrada para el procesamiento de los modelos. El componente de datos es manejado en: Las Fuentes de Datos . Mientras ha crecido la importancia del DSS, llega a ser cada vez más crítico para el DSS utilizar todas las fuentes de datos importantes dentro de la organización, y también desde fuentes externas. Desde luego, el concepto de fuentes de datos debe expandirse para incluir documentos que contienen conceptos, ideas, y opiniones que son muy importantes para crear la decisión. Los Almacenes de Datos . Las BD separadas para aplicaciones de apoyo a la decisión se están desarrollando mediante la creación de almacenes de datos. Estas son BD especiales que están diseñadas para permitir a los creadores de decisión hacer sus propios análisis. También se conocen a veces como BD de información. Es un típico almacén de datos, los datos que se necesitan primero se extraen del mainframe y de otras BD. Con anterioridad a ponerlos en el almacén de datos, los datos se procesan (es decir, “se limpian”) para hacerlos más útiles para el apoyo a la decisión. Entonces los datos son mantenidos por un servidor de BD. Los administradores hacen los análisis de apoyo a la decisión, utilizando el comúnmente conocido como procesamiento analítico en línea (OLAP). El componente Modelo (M) El modelo suministra las capacidades de análisis para un DSS. Hay muchos tipos diferentes de modelos y varias formas en las que se pueden catalogar. Se pueden hacer distinciones importantes en base a su propósito, tratamiento de aleatoriedad, y por su aplicación o uso: El propósito de un modelo puede ser o la optimización o la descripción. El modelo de optimización es uno que busca identificar puntos de maximización o minimización. Un modelo descriptivo describe el comportamiento del sistema. Pero un modelo descriptivo sólo describe el comportamiento del sistema; no suiere perfeccionar las condiciones. Con respecto a la aleatoriedad, casi todos los sistemas son probabilísticos. Esto es, que el comportamiento del sistema no se puede predecir con seguridad porque se presenta un grado de aleatoriedad. Aunque la mayoría de los sistemas son probabilísticos, la mayoría de los modelos matemáticos son deterministas. Según para que se emplean existe una variedad de modelos, tales como: Los modelos estratégicos los usan los directivos para ayudar a determinar los objetivos de la organización, los recursos que se necesitan para cumplir sus objetivos, y las políticas que rigen la adquisición, el uso y la disposición de estos recursos. Los modelo tácticos comúnmente son empleados para la gestión media para ayudar a atribuir y controlar el uso de los recursos de la organización. Los modelos operacionales normalmente son para soportar decisiones de términos pequeños (es decir, diariamente, semanalmente) usualmente encontrados en niveles organizativos inferiores. El área de ayuda a la toma de decisionesDentro de una organización el área de ayuda a la toma de decisiones puede abarcar a su vez todas o algunas de las áreas que se muestran a continuación: El área de procesamiento analítico en línea (Online Analytical Processing, OLAP) trata de las herramientas y las técnicas para el análisis de los datos que pueden dar respuestas casi instantáneas a las consultas que soliciten datos resumidos, aunque la BD sea extremadamente grande. El campo del análisis estadístico, también se incluye en la ayuda a la toma de decisiones. Los lenguajes de consulta de BD no resultan adecuados para el rendimiento de los análisis estadísticos detallado de los datos. Se han creado una serie de paquetes que ayudan en el análisis estadístico. A estos paquetes se les ha añadido interfaces con las BD para permitir que se almacenen en la BD grandes volúmenes de datos y se recuperen de forma eficiente para su análisis. Las técnicas de búsqueda de información intentan descubrir de manera automática las reglas y las pautas estadísticas de los datos. El campo de la minería de datos combina las técnicas de búsqueda de la información creadas por los investigadores en inteligencia artificial y los expertos en análisis estadísticos con las técnicas de implantación eficiente que permiten utilizarlas en BD muy grandes. Las grandes empresas tienen varios orígenes de datos que necesitan utilizar para adoptar decisiones empresariales. Para ejecutar de manera eficiente las consultas sobre datos tan diferentes, las empresas han creado los almacenes de datos. Los almacenes de datos reúnen los datos de varios orígenes bajo un esquema unificado en un solo sitio. Por tanto, ofrecen al usuario una sola interfaz uniforme para los datos. Ventajas y desventajas de usar DSS Ventajas Aumento en el número de alternativas examinadas. Mejor entendimiento del sistema. Respuesta rápida a situaciones inesperadas. Capacidad para efectuar análisis específico. Comunicación mejorada. Control y Ahorro de costes. Mejores decisiones. Ahorro de tiempo. Mejor uso de elaboración de recursos informáticos. Desventajas Alto coste de adquisición y mantenimiento. Para pequeños volúmenes de información no son rentables. Alto grado de sofisticación requerido; tiene algo grado de incertidumbre y potencial para el error. El DSS no es un experto, sólo comunica a los usuarios los resultados de suposiciones y modelos correspondientes de sus constructores a problemas actuales. Almacenes de Datos (Data Warehouse)Ya se ha comentado que tanto los Sistemas de Soporte a la Decisión (DSS), como los Sistemas de Información de Gestión (MIS), presentan problemas para recuperar datos de las BD operacionales. Para lograr la integración de estos tipos de sistemas se deberá contar con un repositorio de datos preparado para tal fin. Este repositorio se creará bajo las características de un Data Warehouse (DW). El DW, convertirá entonces los datos operacionales en una herramienta competitiva, por hacerlos disponibles a los usuarios que lo necesiten para el análisis y toma de decisiones. Una vez definido el DW se implementarán las aplicaciones de acceso a los datos, estas aplicaciones están determinadas por las características nombradas en los sistemas MIS y DSS. Concepto de Almacén de DatosLa tecnología de los almacenes de datos “Data Warehouse”, se encuentra dentro de la línea de evolución de las BD hacia una mayor funcionalidad e inteligencia. Las empresas actuales han visto aumentada su capacidad de generar y recoger datos (introducción de internet en las empresas, tecnologías de entrada de datos …). Estas grandes cantidades de datos (obtenidas a un coste relativamente bajo) no aportan, en principio, información a las organizaciones. Ante esta situación se puede llegar a la siguiente conclusión: “Una organización puede ser rica en datos y pobre en información, si no sabe como identificar, resumir y categorizar los datos”. Los encargados de adoptar las decisiones empresariales necesitan tener acceso a la información de todas las fuentes que contienen datos relevantes de la empresa. La formulación de consultas a cada una de las fuentes es a la vez engorrosa e ineficiente. Además, puede que los orígenes de datos solo almacenen los datos actuales, mientras que es posible que los encargados de adoptar las decisiones empresariales necesiten tener acceso también a datos anteriores. Los almacenes de datos proporcionan una solución a estos problemas. El almacén de datos pretende dar un soporte a la organización para proporcionarle una buena gestión de sus datos, que le ayude en la toma de decisiones estratégicas y tácticas. Antecedentes de los Almacenes de Datos A finales de los 80 aparece el DRI (Diccionario de recursos de información), cuyos antecedentes son: Directorio de datos: Componente del SGBD encargado de describir donde y como se almacenan los datos, y modo de acceder a los datos contenidos en la BD. Diccionario de datos: Reúne la información sobre los datos almacenados para que los usuarios comprendan su significado. Diccionario / Directorio de datos: Que aúna las dos tareas anteriores. Enciclopedia o Repositorio: Donde se almacenan los datos generados durante el ciclo de vida de un SI (Sistema de Información): esquemas, información relativa a la gestión de proyectos, etc. Diccionario de recursos de información: Engloba las capacidades y funciones de todos los “almacenes” de datos anteriores. Pretende ser el corazón de toda arquitectura de información de la empresa, sirviendo de soporte para la integración de sistemas. Los almacenes de datos recogen la herencia de los DRI con algunos matices derivados de la nueva tecnología y de la experiencia. ¿Qué es un Data Warehouse? Vamos a dar varias definiciones, para entender mejor lo que es un almacén de datos: Depósitos de información reunida de varios orígenes, almacenada bajo un esquema unificado en un solo sitio. Una vez reunida, los datos se almacenan mucho tiempo, lo que permite el acceso a datos históricos. Así, los almacenes de datos proporcionan a los usuarios una sola interfaz consolidada con los datos, lo que hace más fáciles de escribir las consultas de ayuda a la toma de decisiones. Un Data Warehouse (DW a partir de ahora) es una nueva arquitectura informática para dar soporte a la obtención de información relevante. Combina potentes herramientas de modelado multidimensional con herramientas de acceso a BD, contribuyendo, no sólo a mostrar los hechos (datos), sino a comprender las causas de los hechos. Un DW es la creación de una vista lógica unificada de los datos, aún cuando estos estén dispersos entre varias BD físicas, para así disponer de un único modelo de trabajo de los datos de la organización. Se puede considerar un DW como un repositorio lógico central (aunque los componentes físicos pueden estar distribuidos), que almacena los datos de la organización a diferentes niveles (desde el más bajo del dato puro hasta los niveles más altos que contienen agregados o resúmenes de los datos de niveles inferiores), que solo contiene datos relevantes para la toma de decisiones y que está optimizado para permitir el análisis y la recuperación de información corporativa. Características Esta colección de datos tiene las siguientes características: Orientado a las materias : Se centra en entidades de alto nivel (como por ejemplo cliente, producto, …) y no en los procesos (como hacen los sistemas operacionales). Integrado : La integración implica que los datos del almacén son consistentes al elegir convenciones en nombres, unidades de medida, representación de campos comunes, etc. Se construye mediante la integración de fuentes de datos múltiples, y heterogéneas: BD relacionales, ficheros planos, registros de transacciones on-line. Se aplican técnicas de limpieza e integración para asegurar la consistencia en el nombrado, estructuras codificadas, medidas de los atributos, y demás aspectos entre las múltiples BD. Cuando los datos se mueven al DW, éstos se tienen que transformar. No volátil : Los datos no cambian una vez que se encuentran en el almacén. Las únicas operaciones que permite un almacén de datos con la carga de nuevos datos y el acceso a los ya almacenados. Sirve de soporte a consultas de ayuda a la decisión . Es dependiente del tiempo : Los datos están asociados a un instante en el tiempo (semestre, año). Por lo tanto, los datos representan una imagen estática del estado de la organización en cada momento. Es decir, que si, por ejemplo, se accede a los datos de hace un mes de obtendrán los datos que describían la organización en aquel momento, sin que se hayan modificado de ninguna manera. Objetivos Después de ver las características de un DW, podemos ahora comprender mejor cuales son sus objetivos: Debe conseguir que la información sea fácilmente accesible para una organización y sus contenidos comprensibles. Los datos deben ser intuitivos y obvios para el usuario y no sólo para el desarrollador. Los contenidos del DW deben estar etiquetados con nombres comprensibles. Las herramientas de acceso al DW deben ser fáciles de usar y ofrecer resultados en el mínimo tiempo posible. Debe presentar a la organización información consistente y “creíble”. La información debe ser cuidadosamente agrupada según los orígenes de datos sin inconsistencias o duplicidades. Debe adaptarse a los cambios y al crecimiento. Las necesidades de usuario, las condiciones del negocio, los datos y la tecnología cambian inevitablemente a lo largo del tiempo. Estos cambios no deben en ningún caso invalidar la información que había hasta el momento en el DW. Debe proteger la información relevante, sensible y confidencial de una organización. Los accesos al DW deben mostrar sólo la información relevante a aquellas personas que están autorizadas para verla. Debe ser la base para tomar decisiones dentro de las organizaciones. Es decir, la información necesaria para tomar ciertas decisiones, operacionales, tácticas y estratégicas debe estar contenida en el DW. Componentes de un almacén de datosLa estructura básica de un sistema de almacén de datos está compuesta por un modelo de datos corporativo, que representa la vista conceptual de los datos de la organización, un procedimiento de limpieza e inserción de los datos operativos en el almacén, por el propio almacén junto con unos metadatos que proporcionan información sobre su contenido y, finalmente por unos procesos de extracción de información que son las herramientas de minería de datos y OLAP. En la siguiente figura se muestran estos componentes y las relaciones entre ellos: Vamos a presentar a continuación cada uno de estos componentes, su funcionamiento y cómo encajan en el proceso global de construcción y mantenimiento de un almacén de datos. Modelo de datos corporativos Es probable que los orígenes de datos que se han creado de manera independiente tengan esquemas diferentes. Parte de la labor de los almacenes de datos es llevar a cabo la integración de los esquemas y convertir los datos a un único modelo de datos corporativo. Este modelo establece un único esquema lógico de datos para toda la organización, evitando la fragmentación producida por la existencia de sistemas de información departamentales. Uno de los fundamentos de un DW y paso previo obligatorio a emprender su construcción, es determinar un modelo de datos corporativo que identifique y estructure los requisitos de información del almacén. Como resultado se obtiene un esquema conceptual válido para toda la organización, que ofrece una visión estratégica global de los datos, permitiendo a la vez la creación de vistas parciales con el grado de detalle adecuado a las necesidades de cada departamento. Esta estrategia “top-down” supone la construcción de un modelo de datos que comprenda todas las entidades y objetos manejados por una organización, lo que es una tarea suficientemente compleja como para que esta sea una de las principales causas de fracaso en la implantación de DW. Para disminuir el impacto negativo de esta tarea, se ha creado el concepto de Supermercado de Datos (Data Marts) que no son sino el resultado de aplicar una estrategia “bottom-up”, esto es, comenzar construyendo almacenes de datos departamentales, con sus correspondientes modelos de datos, antes de construir el almacén de datos corporativos. La creación del modelo de datos corporativo es, por lo tanto, la etapa del diseño del DW. Se trata de definir: Qué información, relevante para los usuarios, se va a incluir en el DW. Qué datos se precisan para obtener la información. La mayoría vienen de las BD de producción pero otros pueden venir de fuentes externas. Representación de la información: nombres, formatos, unidades, etc. La descripción detallada de la información del DW está en el diccionario de datos, éste suele incluir: Identificación de la fuente de datos. Estructura, unidades, precisión, etc, de los datos. Estructura, unidades, precisión, etc, de la información. Estructura de la información que ven los usuarios finales. Normas para encontrar, limpiar, transformar y agregar los datos para transformarlos en información. Normas de seguridad aplicables a los datos y a la información. Limpieza y carga de datos operativos Identificación de las fuentes de datos Esta operación sirve para reconocer a qué sistemas (datos propios de la empresa, fuentes externas …) hay que acceder para conseguir los datos que se van a introducir en el almacén. Esto permitirá identificar los sistemas OLTP afectados, el volumen de datos que se va a capturar, la frecuencia con la que se deberá realizar la captura y los departamentos o áreas de la organización que se verán afectados por el proyecto de creación del DW. Limpieza de los datos operativos El proceso de limpieza de los datos importados, consiste en: El descubrimiento y la corrección de inconsistencias, errores, repeticiones o cualquier otra anomalía en los datos que se van a introducir en el almacén. Transformación de los términos operacionales en términos de negocio uniformes, estándar y auto-explicativos. Definición física de un atributo: usar tipos de datos y longitudes significativos. Uso consistente de los valores de los atributos de las entidades: valores diferentes pero que signifiquen lo mismo, se convierten a un único valor. Asuntos relacionados con valores por defecto y valores perdidos: si no se tiene claro es más seguro dejar estos valores en blanco. Documentación de los formatos Es necesario documentar los formatos de los datos que se van a transferir al almacén, para ello hay que tener en cuenta el significado de cada uno de ellos, el procedimiento que se ha seguido para su obtención a partir de los datos operativos originales y los procesos de sumarización o agregación que se les ha aplicado. Los datos que conforman esta información acerca del contenido del propio almacén reciben el nombre especial de metadatos que dan lugar al Diccionario de datos. Forman lo que se podría considerar el manual técnico del almacén y son imprescindibles para realizar el mantenimiento del almacén e interpretar adecuadamente los resultados de las consultas que se realicen sobre el mismo. Transformación y carga Este es el último paso y consiste, lógicamente, en la transformación y carga efectiva de los datos una vez procesados y documentados. En este proceso se incluye la inserción en el almacén tanto de los datos operativos procesados y limpiados, como de los metadatos que sirven para documentarlos. Esta tarea debe realizarse periódicamente para mantener un grado de sincronización aceptable entre el contenido del almacén y el de las bases de datos OLTP, ya que sino la información proporcionada por las herramientas de extracción de datos no sería lo suficientemente actual como para ser útil. Por otro lado, el proceso de carga suele ser largo y costoso, ya que hay que procesar una gran cantidad de registros, lo que hace necesario buscar un equilibrio entre el deseo de mantener actualizado el DW y el coste que supone en recursos máquina. Resultado: Almacén de datos Tras realizar todas estas etapas, se obtiene como resultado un almacén que contiene unos datos tratados, documentados y listos para ser utilizados como materia prima de las herramientas de extracción de información. Extracción y recuperación de los datos Acceso a los datos El DW debe ofrecer soluciones a los problemas de acceso a los datos. Algunos de estos problemas suelen ser: Los datos están en sistemas a los que el usuario no puede acceder. El usuario no tiene herramientas para leer correctamente los datos y para ello se le ofrecen una serie de herramientas de varios tipos: Visualización de datos, Análisis estadístico y Generadores de informes. Los datos podrían estar siendo usados por aplicaciones que impiden su utilización por otras aplicaciones. Recuperación de los datos Un DW debe contener toda la información sobre el tema correspondiente y debe contar con mecanismos para recuperarla. Existen tres conceptos fundamentales, que ayudan a llevar a cabo dicha recuperación, son: Base de datos. Una BD sobre un tema determinado es un conjunto de datos sobre dicho tema que cumple los criterios de: Exhaustividad, Ausencia de redundancia y Estructura adecuada. Diccionario de datos. Define los datos, se sabe cuáles existen, qué significa cada elemento de datos. Además determina el tema y los criterios que deben cumplirse para que los datos sean exhaustivos. Complementación relacional. Garantiza la recuperación de cualquier subconjunto de la información, basándose en cinco operadores: selección, proyección, intersección, unión y diferencia. Complejidad de las consultas Cuando se realizan consultas complejas a BD de producción, estas deben ser hechas por especialistas que saben cómo y dónde buscar esos datos. El DW elimina la intervención de estos especialistas, eliminando los posibles problemas de disponibilidad. Análisis multidimensional Con el análisis multidimensional se da respuesta a consultas complejas de los usuarios que reflejan los diversos componentes de sus organizaciones. Estos componentes pueden ser de dos tipos: Cuantitativos o Cualitativos. A estos componentes también se les llama dimensiones, y a los valores de los componentes o dimensiones se les llama atributos. Además el detalle con el que se muestran los atributos puede variar, cada dimensión se puede descomponer en diferentes niveles de detalle, estos dependen de las necesidades del usuario. Las dimensiones definen dominios como geografía, producto, tiempo, cliente, etc. Los miembros de una dimensión se agrupan de forma jerárquica (dimensión geográfica: ciudad, provincia, autonomía, país…). El usuario puede navegar por los datos de diferentes maneras: Perforación (drill-down): Consisten en variar el nivel de detalle de los datos, desde los datos más resumidos a los más detallados. Se dice que drill-down es desagregar y Roll-up es agregar. Segmentación (slicing and dicing): Consisten en “recortar” un subconjunto de los datos moviéndose por los distintos datos de una misma dimensión o cambiando de dimensión. Es decir, es la capacidad de ver la BD desde diferentes puntos de vistas. El corte suele hacerse a lo largo del eje del tiempo para analizar tendencias. Se dice que slicing es proyección y que dicing es selección. Estructura lógica del almacén: datos y metadatosLa estructura lógica de un almacén se encuentra dividida en cuatro niveles más uno adicional de metadatos: Veamos a continuación cada uno de estos niveles. Metadatos Este nivel no es el superior jerárquico de los otros cuatro, sino que se encuentra completamente aparte del resto. Esto se debe a que no está compuesto por datos extraídos a partir de sistemas OLTP, sino por la descripción del tratamiento a que se han sometido dichos datos originales. Los metadatos describen la estructura de los datos contenidos en el almacén, de donde proceden y que tratamiento sufrieron. También detallan los algoritmos utilizados para crear los resúmenes de los dos niveles superiores de la estructura del almacén (datos ligeramente resumidos y datos muy resumidos). Esta información, será de utilidad para las herramientas de extracción de información, que la usarán para determinar estrategias de navegación y recuperación. Datos detallados actuales Son los datos obtenidos directamente al limpiar y homogeneizar los datos provenientes de sistemas OLTP. Constituyen el nivel más bajo de detalle, representan el estado de la organización en en momento presente y, debido a que están sin resumir, constituyen una gran porción del volumen total de los datos almacenados. Estos datos son de acceso frecuente, ya que son los más actualizados, por tanto, es conveniente que se almacenen en dispositivos de acceso rápido como discos. Datos detallados históricos Son los datos detallados correspondientes a momentos anteriores al presente, por lo que el nivel de detalle es el mismo que el de los datos actuales. Al no ser datos a los que se deba acceder con frecuencia, se almacenan en cintas o cualquier otro dispositivo de almacenamiento masivo. Datos ligeramente resumidos Es el primer nivel de agregación de los datos detallados actuales. Corresponden a consultas o informes de uso habitual, por lo que al tenerlos preparados de antemano se consigue acelerar considerablemente el rendimiento global del almacén. Es importante identificar sobre que variables se van a realizar estos resúmenes así como su frecuencia de actualización. Datos muy resumidos Representan el nivel más elevado de agregación, tanto de los datos ligeramente resumidos como de los de detalle. Corresponden a consultas o informes que se solicitan muy a menudo y que deben obtenerse con gran rapidez. Dado el alto grado de accesibilidad que deben tener estos datos muy resumidos es normal encontrarlos fuera del almacén de datos corporativo, formando parte de los almacenes de datos departamentales o Data Mart. Estructura física del almacén: arquitecturaLa estructura física del almacén puede presentar cualquiera de las siguientes arquitecturas: Arquitectura centralizada Consiste en utilizar un único servidor para guardar todo el almacén de datos. La ventaja de esta configuración reside en que maximiza la potencia de cálculo disponible para trabajar sobre el almacén y facilita el mantenimiento del mismo. La desventaja estriba en que la realización de consultas que consumen muchos recursos puede afectar seriamente al resto de usuarios que sólo necesiten acceder a datos de alto nivel (resumidos o muy resumidos). Además, un fallo en este servidor puede resultar catastrófico para la organización, por lo que la seguridad del mismo cobra una especial relevancia. Arquitectura distribuida Esta segunda opción se basa en la existencia de varios servidores entre los que se reparten los datos del almacén. Dado que una de las características del almacén es que está organizado en torno a temas, resulta lógico que la distribución física de los datos refleje esta propiedad, asignando así cada servidor a uno o varios temas lógicos. La ventaja de esta arquitectura es una mayor distribución de la carga de proceso a cambio de una mayor complejidad en el mantenimiento de la estructura del almacén. También sigue presentando el problema de la no-discriminación de los datos de más alto nivel de los de menor nivel, por lo que una operación que requiera muchos recursos máquina seguirá bloqueando el acceso al resto de usuarios. Arquitectura distribuida por niveles Esta arquitectura refleja la estructura lógica del almacén, ya que asigna los servidores en función del nivel de agregación de los datos que contienen. De esta manera se tendrá un servidor para los datos de detalle, otro para los resumidos y otro para los muy resumidos. Un caso particular se presenta cuando los datos muy resumidos no están en un único servidor, sino que se duplican en varios para agilizar el acceso a los mismos. En este caso los servidores que los mantienen son Data Marts. La ventaja de esta arquitectura es que permite un acceso rápido a los datos que se utilizan con más frecuencia, sin que sea penalizado por las consultas que se realicen sobre datos de detalle. Comparación de DW y BDDiferencias entre un sistema de BD tradicional y un DW: Elementos básicos de un DWSistema Origen Es un sistema operacional cuya función es capturar las transacciones del negocio. A menudo se le llama “legacy system”. Las principales características de este sistema son la disponibilidad y la actualidad de su información. Asumiremos que el sistema origen mantiene poca información histórica y que se realizará un cierto tipo de reporting directamente sobre el sistema fuente que no tiene porque estar recogido en el propio DW. Los sistemas fuentes son normalmente independientes entre sí, por lo que seguramente en ellos no se habrá invertido para conformar dimensiones básicas como los productos, los clientes, la geografía o el tiempo. Los sistemas origen tienen claves que identifican ciertos objetos del análisis de manera única, como por ejemplo la clave de producto o la clave de cliente. A las calves de estos sistemas fuente se les denomina “claves de producción”, pero no las usaremos directamente en el DW como claves, trataremos estas claves como atributos de las dimensiones que se creen. Data staging área Es un área de almacenamiento y un conjunto de procesos que limpian, transforman, combinan, unifican, almacenan, archivan y preparan la fuente de datos para su uso en el DW. Está compuesta por un sistema de ficheros planos. Consiste en todo aquello que hay entre el origen de datos y el servidor de presentación. Aunque sería ideal que incluso consistiera en una única máquina, la realidad es que está distribuida por un conjunto de máquinas. Esta área está dominada por las actividades de ordenación y procesamiento secuencial y en algunos casos ni siquiera tiene que estar basada en tecnología relacional. Una restricción de esta área es que no se utilizará en ningún caso para hacer consultas directamente sobre ella o para generar informes a partir de ella misma. Servidor de presentación (Presentation server) Es la máquina física destino donde el DW es organizado y almacenado para ser directamente consultado por los usuarios finales y otras aplicaciones. En este sistema los datos se presentarán y almacenarán en un marco multidimensional. Si el servidor está basado en una BD relacional, entonces las tablas estarán organizadas como “esquemas en estrella”. Si el servidor de presentación está basado en una tecnología de procesamiento analítico on-line no relacional (tecnología OLAP), los datos tendrán dimensiones reconocidas como tal. ESQUEMA – RESUMENUna BD es una colección o depósito de datos integrados, almacenados en soporte secundario (no volátil) y con redundancia controlada. Los datos, que han de ser compartidos por diferentes usuarios y aplicaciones, deben mantenerse independientes de ellos y su definición (estructura de la BD), única y almacenada junto con los datos, se ha de apoyar en un modelo de datos, el cual ha de permitir captar las interrelaciones y restricciones existentes en el mundo real. Los procedimientos de actualización y recuperación, comunes y bien determinados, facilitarán la seguridad del conjunto de los datos. En los sistemas de BD se plantean dos objetivos principales: Independencia de la BD de lo programas para su utilización. Proporcionar a los usuarios una visión abstracta de los datos. El sistema esconde los detalles de almacenamiento físico (como se almacenan y se mantienen los datos), pero estos deben extraerse eficientemente. Niveles de abstracción: externo, conceptual e interno. Principales componentes de un entorno de BD: Datos : una BD no tiene sentido si no está compuesta por datos. Hay que definir la forma en que estos datos se deben disponer, qué datos se deben almacenar y cómo los debe entender la máquina. Metadatos : la información que el sistema guarda sobre los datos almacenados, es lo que se llaman metadatos, es decir, datos acerca de los datos. Es más, estos metadatos se almacenan como otra BD propiamente dicha, y puede ser gestionada y consultada como tal. Estos metadatos suelen conformar lo que se da en llamar diccionario de datos o catálogo. El Sistema Gestor de BD : se puede definir como un conjunto coordinado de programas, procedimientos, lenguajes, etc. que suministra, tanto a los usuarios finales como a los analistas, programadores o el administrador, los medios necesarios para describir, recuperar y manipular los datos almacenados en la base, manteniendo su integridad, confidencialidad y seguridad. Usuarios de la BD : usuarios normales, programadores de aplicaciones, usuarios sofisticados y especializados y finalmente los administradores de BD. Lenguajes de BD : Lenguaje de Definición de Datos (LDD) y Lenguaje de Manipulación de Datos (LMD). En cuanto a la utilización de las BD en las organizaciones, hablamos de 3 diferentes tipos de automatización de los sistemas de información: Los PED (Procesamiento Electrónico de Datos) o DP (Data Processing) que se caracterizan por tener el foco de atención en el nivel operativo de almacenamiento, procesamiento y flujo de los datos, así como procesar eficientemente las transacciones y realizar informes resúmenes para los dirigentes. Los SIG (Sistemas de Información de Gestión) o MIS (Management Information Systems) que se caracterizan porque su foco de atención está en la información orientada a mandos intermedios, por la integración de las tareas de PED, por sus funciones en los negocios y por la generación de informes. Los STD (Sistema de Apoyo a la Toma de Decisiones) o DSS (Decision Support Systems) que están más centrados en la decisión y orientados hacia altos ejecutivos. En los sistemas transaccionales basados en BD, una transacción es una secuencia de operaciones llevadas a cabo como una unidad lógica de trabajo simple. Para asegurar la integridad de los datos se necesita que el sistema de BD mantenga las siguientes propiedades de las transacciones: Atomicidad, Consistencia, Aislamiento y Durabilidad. Son conocidas como propiedades ACID. Dos definiciones de un Sistema de Información de Gestión (MIS): Un Sistema de Información de Gestión puede definirse como un conjunto de medios para reunir los datos necesarios para la gestión y difundir la información obtenida con el tratamiento de estos datos. Definimos Sistema de Información de Gestión como el proceso por el cual los datos que son importantes para la empresa son identificados, analizados, recolectados y puestos a disposición de la empresa. Se pueden definir los Sistemas de Soporte a la Decisión (DSS) como programas informáticos interactivos que utilizan métodos analíticos, tales como análisis de decisión, algoritmos de optimización, programas de planificación de rutinas, etc., para el desarrollo de modelos ayudando a los creadores de decisión a formular alternativas, analizar sus impactos, e interpretar y seleccionar opciones apropiadas para la implementación. Una manera útil de pensar en las partes de los componentes de un DSS y las relaciones entre las partes está en utilizar el diálogo, los datos, y el modelo (DDM). En esta conceptualización, hay un diálogo (D) entre el usuario y el sistema, los datos (D) que soporta el sistema, y los modelos (M) que suministra el análisis de las capacidades. Los DSS abarcan diversos campos como: procesamiento analítico en línea (Online Analytical Processing, OLAP), análisis estadístico, minería de datos y almacenes de datos. Se define un DW como un repositorio lógico central (aunque los componentes físicos pueden estar distribuidos), que almacena los datos de la organización a diferentes niveles (desde el más bajo del dato puro hasta los niveles más altos que contienen agregados o resúmenes de los datos de niveles inferiores), que solo contiene datos relevantes para la toma de decisiones y que está optimizado para permitir el análisis y la recuperación de información corporativa. Los almacenes de datos presentan las siguientes características: es orientado a materias, integrado, no volátil, sirve de soporte a consultas de ayuda a la decisión y es dependiente del tiempo. Además el DW busca los objetivos siguientes: Debe conseguir que la información sea fácilmente accesible para una organización. Debe presentar a la organización información consistente y “creíble”. Debe adaptarse a los cambios y al crecimiento. Debe proteger la información relevante, sensible y confidencial de una organización. Debe ser la base para tomar decisiones dentro de las organizaciones. Componentes que forman parte de un DW: Modelo de datos corporativo : La creación del modelo de datos corporativo es la etapa del diseño del DW, que identifica y estructura los requisitos de información que va a tener que satisfacer el almacén. Limpieza y carga de datos operativos : Este proceso está compuesto de las siguientes etapas: Identificación de las fuentes de datos. Limpieza de los datos operativos. Documentación de los formatos. Transformación y carga. El almacén de datos : se obtiene como resultado un almacén que contiene unos datos tratados, documentados y listos para ser utilizados como materia prima de las herramientas de extracción de información. Extracción y recuperación de los datos : En este proceso hay que tener presente los puntos siguientes: Acceso y Recuperación de los datos. Complejidad de las consultas. Análisis multidimensional. La estructura lógica de un almacén se encuentra dividida en cuatro niveles más uno adicional de metadatos que está por encima de ellos. Los cuatro niveles son: Datos detallados actuales Datos detallados históricos Datos ligeramente resumidos Datos muy resumidos La estructura física del almacén puede presentar cualquiera de las siguientes arquitecturas: Arquitectura centralizada : Consiste en utilizar un único servidor para guardar todo el almacén de datos. Arquitectura distribuida : Esta segunda opción se basa en la existencia de varios servidores entre los que se reparten los datos del almacén. Dado que una de las características del almacén es que está organizado en torno a temas, resulta lógico que la distribución física de los datos refleje esta propiedad, asignando así cada servidor a uno o varios temas lógicos. Arquitectura distribuida por niveles : Esta arquitectura refleja la estructura lógica del almacén, ya que asigna los servidores en función del nivel de agregación de los datos que contienen. De esta manera se tendrá un servidor para los datos de detalle, otro para los resumidos y otro para los muy resumidos. Bibliografía Scribd (Ibiza Ales) Sistemas de gestión de bases de datos relacionales: características y elementos constitutivos. Antecedentes históricos. El lenguaje SQL. Estándares de conectividad: ODBC y JDBC.Antecedentes históricosSe suele hablar de tres generaciones en la historia de las BD, son: Primera generación: sistema jerárquico y sistema de red. Requieren complejos programas de aplicación. La independencia de datos es mínima. No tienen un fundamento teórico. Segunda generación: modelo relacional. Lenguaje de consultas estructurado: SQL. Desarrollo de SGBD relacionales comerciales. Limitada capacidad para modelar datos. Tercera generación: modelo orientado a objetos y modelo relacional extendido. Veamos ahora con más detalle la historia de cada uno de estos modelos, soportados en diferentes SGBD. Las Bases de Datos JerárquicasA finales de los 60, coincidiendo en el tiempo con el desarrollo de los sistemas gestores de archivos, IBM y North American Aviation desarrollan el modelo jerárquico. Con la finalidad de resolver problemas de diseño aeroespacial y de producción se desarrolla Information Management System (IMS) con su lenguaje DL/1. Fue el primer sistema de gestión de BD comercial basado en el modelo jerárquico. Aparece IMS DB/DC (Database/Data Communication), el primer sistema de BD de gran escala. Sobre 1969, IMS dio como resultado un sistema de gestión de BD de tipo jerárquico de propósito general: el IMS/1 de IBM que constituye la primera familia de sistemas de gestión de BD. American Airlines e IBM desarrollan SABRE, el primer sistema que proporciona acceso a datos compartidos por múltiples usuarios a través de una red de comunicación. Las Bases de Datos en RedA mitad de los sesenta, se desarrolló IDS (Integrated Data Store), de General Electric. Este trabajo fue dirigido por uno de los pioneros en los sistemas de BD, Charles Bachman. IDS era un nuevo tipo de sistema de BD conocido como estructura en red, que produjo un gran efecto sobre los sistemas de información de aquella generación. El sistema en red se desarrolló, en parte, para satisfacer la necesidad de representar relaciones más complejas entre datos que las que se podían modelar con los sistemas jerárquicos, y, en parte, para imponer un estándar de BD. Para ayudar a establecer dicho estándar, CODASYL (Conference on Data Systems Languages), formado por el gobierno de EEUU y representantes del mundo empresarial, organiza el grupo DBTG (Data Base Task Group), para definir especificaciones estándar que permitan la creación y el manejo de BD. El DBTG presentó su informe final en 1971 y aunque no fue formalmente aceptado por ANSI (American National Standards Institute), muchos sistemas se desarrollaron según la propuesta del DBTG. Estos sistemas se conocen como sistemas en red, sistemas CODASYL o DBTG. Los modelos jerárquico y de red constituyen la primera generación de los sistemas de BD, pero presentan algunos de los siguientes inconvenientes: no tienen un fundamento teórico, la independencia de datos es mínima y es necesario escribir complejos programas de aplicación para cualquier consulta de datos, por simple que sea. En la década de los 70, la tecnología de BD experimenta un rápido crecimiento. Algunos sistemas, desarrollados a lo largo de los años 70, que siguen las propuestas de CODASYL son: DMS-1.110 de UNIVAC, DMS-170 de CDC, IDMS de DF Goodrich, DBMA-11 de DIGITAL, etc. Sin embargo ninguna de estas implementaciones desarrolló completamente las propuestas de CODASYL. El modelo de datos en red siempre tuvo pretensiones de generalización y estandarización, mientras que la familia de sistemas jerárquicos está constituida por una serie de sistema de gestión de BD de los que posteriormente se obtuvo la abstracción del modelo de datos jerárquico. Ambos tipos de SGBD eran accesibles desde un lenguaje de programación, usualmente Cobol, usando un interfaz de bajo nivel. Esto hacía que la creación de una aplicación, el mantenimiento de la BD, así como el ajuste y el desarrollo fuesen controlables, pero aún a costa de una gran inversión de tiempo. Hasta 1980 los modelos de red y jerárquico fueron populares. Cullinet, una empresa fundada por Bachman, fue la mayor empresa de software y con más rápido crecimiento en el mundo, en aquellos años. Las Bases de Datos RelacionalesA pesar del éxito del modelo de datos en red, muchos diseñadores de software reconocieron que la interfaz de programación para navegación por los registros era de demasiado bajo nivel. En 1970 E.F.Codd, basándose en el álgebra y la teoría de conjuntos, propone un nuevo modelo de datos llamado modelo relacional. Sugiere que todos los datos de la BD se podrían representar como una estructura tabular (tablas con columnas y filas, que denominó relaciones) y que esas relaciones se podrían acceder con un lenguaje no procedimental (declarativo). En este tipo de lenguajes, en lugar de escribir algoritmos para acceder a los datos, sólo se necesita un predicado que identifica los registros o combinación de registros deseados. Es más, este nuevo modelo integraba los lenguajes de definición, navegación y manipulación en un solo lenguaje unificado. El modelo relacional encontró inicialmente una gran oposición debido a que requería más recursos informáticos que los SGBD existentes en la época y sus implementaciones no estaban lo suficientemente refinadas como para competir con el resto de modelos y, por tanto, resultaban demasiado lentos. Los SGBD relacionales no fueron prácticos hasta la década de los ochenta en que se desarrollaron computadores más rápidos y a menor precio. Los programadores se debieron adaptar a una nueva forma de pensar en el tratamiento de los datos. Hasta ahora los programadores estaban acostumbrados a procesar los datos en registro, en lugar de procesar simultáneamente los datos. Se desarrollaron proyectos de investigación que dieron lugar a algunos prototipos entre los que destacan: INGRES de la Universidad de Berkeley (1973-1975) System R de IBM (1974-1977) System 2000 de la Universidad de Austin en Texas El poyecto Sócrates de la Universidad de Grenoble en Francia ADABAS de la Universidad técnica de Darmstadt en Alemania Durante este periodo se desarrollaron diversos lenguajes de consulta: SQUARE, SEQUEL (SQL), QBE y QUEL. De fundamental importancia es el lenguaje SQL, que fue el resultado de la convergencia de muchos de los prototipos desarrollados en la época. El trabajo de investigación en IBM conducido por Ted (E.F.) Codd, Raymond Boyce y Don Chamberlain y el trabajo en la Universidad de Berkeley conducido por Michael Stonebraker, dieron como resultado SQL. Se estandarizó por primera vez en 1986 por el comité ANSI X3H2 como estándar de ANSI, que fue denominado SQL-86. ANSI publicó un estándar extendido en 1989, SQL-89. La siguiente versión del estándar fue SQL-92 y la más reciente SQL-99. Ya la primera estandarización de SQL, provocó la desaparición de su más inmediato competidor, QUEL. Sin embargo, QBE ha sobrevivido hasta nuestros días gracias a las interfaces de usuario amigables y porque supone un primer contacto más intuitivo y rápido con las BD relacionales. Posteriormente a los prototipos aparecieron numerosos sistemas relacionales comerciales, tales como: INGRES de RTI (1980), SQL/DS de IBM (1981), ORACLE de RSI (1981), DB2 de IBM (1983), RDB de DIGITAL (1983), etc. En la década de los 80 se desarrolla SQL Server en Sybase para sistemas UNIX y posteriormente se transportó a sistemas Windows NT. Desde 1994 Microsoft ha lanzado nuevas versiones de este producto de BD independientemente de Sybase, que dejó de usar el nombre SQL Server a finales de los 90. El modelo de datos relacional ha proporcionado beneficios inesperados además del aumento de productividad y facilidad de uso. Es muy adecuado para el enfoque cliente/servidor, el procesamiento paralelo y las interfaces gráficas de usuario. El modelo relacional constituye la segunda generación de los sistemas de BD. Hoy en día, existen cientos de SGBD relacionales, tanto para ordenadores personales como para sistemas multiusuario, aunque muchos no son completamente fieles al modelo relacional. El modelo relacional también tiene sus fallos, siendo uno de ellos su limitada capacidad para modelar los datos. En 1976, Chen presentó el modelo entidad-relación, que es la técnica más utilizada en el diseño de BD. En 1979, Codd intentó subsanar algunas de las deficiencias de su modelo relacional con una versión extendida denominada RM/T (1979) y posteriormente RM/V2 (1990). Como respuesta a la creciente complejidad de las aplicaciones que requieren BD, ha surgido un nuevo modelo: el modelo de datos orientado a objetos. Esta evolución representa la tercera generación de los sistemas de BD. Sistemas de Gestión de Bases de Datos (SGBD)En un sistema de BD debe existir una capa intermedia entre los datos almacenados en la BD, las aplicaciones y los usuarios del mismo. Se trata del Sistema de Gestión de la BD (SGBD). Actúa de intermediario entre los usuarios y aplicaciones y los datos, proporcionando medios para describir, almacenar y manipular los datos y proporciona herramientas al administrador para gestionar el sistema, entre ellas las herramientas de desarrollo de aplicaciones, generador de informes, lenguajes específicos de acceso a los datos, como SQL (Structured Query Language) o QBE (Query By Example) en BD relacionales. Un SGBD se puede definir como un conjunto coordinado de programas, procedimientos, lenguajes, etc. que suministra, tanto a los usuarios no informáticos como a los analistas, programadores o el administrador, los medios necesarios para describir, recuperar y manipular los datos almacenados en la BD, manteniendo su integridad, confidencialidad y seguridad. El objetivo primordial de un SGBD es proporcionar un entorno conveniente y eficiente para extraer, almacenar y manipular información de la BD. El SGBD gestiona de forma centralizada todas las peticiones de acceso a la BD, por lo que este paquete funciona como interfaz entre los usuarios y la BD. Además, el SGBD gestiona la estructura física de los datos y su almacenamiento. Por lo tanto, el SGBD libera al usuario de conocer exactamente la organización física de los datos y de crear algoritmos para almacenar, actualizar o consultar dicha información que está contenida en la BD. Todos los SGBD no presentan la misma funcionalidad, depende de cada producto y del modelo de datos que implanten. Los sistemas más grandes son conjuntos de programas complejos y sofisticados. Los SGBD están en continua evolución, tratando de satisfacer los requerimientos de todo tipo de usuarios. Veamos a continuación las principales funciones o características que debe proporcionar un SGBD. Características de un SGBDEn general todos los SGBD presentan unas características comunes. Estas fueron ya definidas por Codd y posteriormente revisadas en función de las nuevas necesidades detectadas con la generalización del uso de las BD. Idealmente, el SGBD debe poseer una serie de características indispensables para satisfacer a los usuarios, tales como: Mantener la independencia entre los programas y la estructura de la BD. Así se simplifica el mantenimiento de las aplicaciones que acceden a la BD. Aunque esta independencia nunca es absoluta, los SGBD, principalmente los relacionales, van respondiendo cada vez mejor a esta exigencia. Asegurar la coherencia de los datos. En lo posible, no debe existir redundancia de datos, los datos deben estar almacenados una sola vez en la BD. Permitir a los usuarios almacenar datos, acceder a ellos y actualizarlos. Además, el SGBD debe hacerlo de forma transparente al usuario, ocultando la estructura física interna de los datos y la forma de almacenarlos. Contener un catálogo accesible por los usuarios en el que se almacenen las descripciones de los datos de forma centralizada. Este catálogo se denomina diccionario de datos y permite identificar y eliminar las redundancias y las inconsistencias. Garantizar que todas las actualizaciones correspondientes a una determinada transacción se realicen, o que no se realice ninguna. Una transacción es un conjunto de acciones que cambian el contenido de la BD. Si la transacción falla durante su realización, la BD quedará en un estado inconsistente. Algunos de los cambios se habrán hecho y otros no, por lo tanto, los cambios realizados deberán ser deshechos para devolver la BD a un estado consistente. Permitir que varios usuarios tengan acceso al mismo tiempo a los datos. Cuando dos o más usuarios acceden a la BD y al menos uno de ellos está actualizando datos, el SGBD deberá gestionar el acceso concurrente, impidiendo que haya datos corruptos o inconsistentes. Aquí el SGBD puede permitir la simultaneidad de accesos mediante el manejo eficiente de los bloqueos de la BD. Garantizar la recuperación de la BD en caso de que algún suceso la dañe. El fallo puede ser debido a una avería en algún dispositivo hardware o un error del software, que hagan que el SGBD aborte, o puede ser debido a que el usuario detecte un error durante la transacción y la aborte antes de que finalice. En todos estos casos, el SGBD debe proporcionar un mecanismo capaz de recuperar la BD llevándola a un estado consistente. Garantizar la seguridad de la BD. Esto es, sólo los usuarios autorizados pueden acceder a la BD, permitiendo diferentes niveles de acceso. La protección debe ser contra accesos no autorizados, tanto intencionados como accidentales. Garantizar la integridad de la BD. Esto requiere la validez y consistencia de los datos almacenados. Normalmente se expresa mediante restricciones, que son una serie de reglas que la BD no puede violar. Mantener la disponibilidad continua. La BD debe estar siempre disponible para su acceso. El SGBD debe proporcionar utilidades de administración, mantenimiento y gestión que puedan realizarse sin detener el funcionamiento de la BD. Proporcionar herramientas de administración de la BD. Estas herramientas permiten entre otras funcionalidades: importar y exportar datos, monitorizar el funcionamiento y obtener estadísticas de utilización de la BD, reorganizar índices y optimizar el espacio liberado para reutilizarlo. Integrarse con algún software gestor de comunicaciones. Muchos usuarios acceden a la BD desde terminales remotos, por lo que la comunicación con la máquina que alberga al SGBD se debe hacer a través de una red. Todas estas transmisiones de mensajes las maneja el gestor de comunicaciones de datos. Aunque este gestor no forma parte del SGBD, es necesario que el SGBD se pueda integrar con él. Garantizar la escalabilidad y elevada capacidad de proceso. El SGBD debe aprovechar todos los recursos de máquina disponibles en cada momento, aumentando su capacidad de proceso, conforme disponga de más recursos. Poseer un lenguaje de definición de datos que permita fácilmente la creación de nuevas BD, así como la modificación de su estructura. Poseer un lenguaje de manipulación de datos, que permita la inserción, eliminación, modificación y consulta de los datos de la base, de la forma más eficiente y conveniente posible. Permitir el almacenamiento de enormes cantidades de datos (miles de millones de caracteres), sin que el usuario perciba una degradación en cuanto al rendimiento global del sistema. Para ello el SGBD debe utilizar índices, partición de tablas, etc. La forma en que las distintas BD comerciales y académicas abordan estas características difieren enormemente, no sólo por las técnicas utilizadas sino también por las aproximaciones o paradigmas con que se han desarrollado. En este tema nos centraremos exclusivamente en el tipo más extendido: las BD relacionales, ya que tienen un formalismo subyacente que las hace muy potentes. Además, fueron desarrolladas hace ya bastantes años, y han evolucionado lo suficiente como para suministrar poderosas herramientas que hacen fácil su gestión. De hecho, todas las características que hemos visto que debe poseer un SGBD, son suministradas a través de entornos e interfaces amigables y comprensibles que permiten un rápido aprendizaje de todas las funciones propias de una BD. En contraposición, otro tipo de BD, como las orientadas a objetos, requieren que casi todas las funciones de creación de BD, manipulación, etc, se efectúen a través de programas, lo cual requiere un profundo conocimiento de la técnicas de programación. Por otro lado, sistemas igual de evolucionados, como el jerárquico o en red, han caído en desuso, y su aprendizaje supone un esfuerzo que aporta más bien poco al diseñador que debe enfrentarse de inmediato ante un mundo de datos básicamente relacional. Niveles de abstracción: Interno, Conceptual y ExternoSe puede observar en los Sistemas de Información la existencia de dos niveles distintos: Un nivel lógico o externo, que es la vista que tiene el usuario del sistema. Un nivel físico o interno, que es la forma en la que los datos están almacenados. En la BD aparece un nuevo nivel de abstracción llamado: nivel conceptual. Este nivel intermedio pretende una representación global de los datos que se interponga entre el nivel lógico y el físico, y que sea independiente tanto del equipo, como de cada usuario en particular. Una de las características más importantes de los SGBD es la independencia entre programas y datos. Según ANSI (American National Standard Institute), “la independencia de los datos es la capacidad de un sistema para permitir que las referencia a los datos almacenados, especialmente en los programas y en sus descriptores de los datos, están aislados de los cambios y de los diferentes usos en el entorno de los datos, como pueden ser la forma de almacenar dichos datos, el modo de compartirlos con otros programas y como se reorganizan para mejorar el rendimiento del sistema de BD”. Para asegurar esta independencia entre los datos y las aplicaciones es necesario separar la representación física y lógica de los datos, distinción que fue reconocida oficialmente en 1978, cuando el comité ANSI/X3/SPARC propuso una arquitectura de 3 niveles: nivel interno, nivel conceptual y nivel externo: Nivel interno : Es la representación del nivel más bajo de abstracción, en éste se describe en detalle la estructura física de la BD: dispositivos de almacenamiento físico, estrategias de acceso, índices, etc. Ningún usuario necesita conocer este nivel, su organización y conocimiento está reservado a los administradores de la BD. Nivel conceptual : El siguiente nivel de abstracción, describe qué datos son almacenados realmente en la BD y las relaciones que existen entre los mismos, describe la BD completa en términos de su estructura de diseño. El nivel conceptual de abstracción lo usan los administradores de BD, quienes deben decidir qué información se va a guardar en la BD. En el nivel conceptual la BD aparece como una colección de registros lógicos, sin descriptores de almacenamiento. En realidad los archivos conceptuales no existen físicamente. La transformación de registros conceptuales a registros físicos para el almacenamiento se lleva a cabo por el sistema y es transparente al usuario. Consta de las siguientes definiciones: Definición de los datos: Se describen las características y tipos de campo de todos los elementos direccionables en la BD. Relaciones entre datos: Se definen las relaciones entre datos para enlazar tipos de registros relacionados para el procesamiento de archivos múltiples. Nivel externo : Nivel más alto de abstracción, es lo que el usuario final puede visualizar del sistema terminado, describe sólo una parte de la BD al usuario acreditado para verla. El sistema puede proporcionar muchas visiones para la misma BD. La arquitectura de tres niveles es útil para explicar el concepto de independencia de datos que podemos definir como la capacidad para modificar el esquema en un nivel del sistema sin modificar el esquema del nivel superior. Se pueden definir dos tipos de independencia de datos: La independencia lógica permite modificar el esquema conceptual sin tener que alterar los esquemas externos ni los programas de aplicación. La independencia física permite modificar el esquema interno sin alterar el esquema conceptual (o los externos). Por ejemplo, permite cambiar el disco en que se almacenan parte de los ficheros físicos con el fin de mejorar el rendimiento de las operaciones de consulta o aumentar la capacidad de almacenamiento de datos. La independencia física es más fácil de conseguir que la independencia lógica. La siguiente figura, muestra los tres niveles de abstracción mencionados. Lenguajes de los SGBDPara proporcionar a los usuarios las diferentes facilidades, los SGBD deben ofrecen lenguajes especializados e interfaces apropiadas para cada tipo de usuario: administradores de la BD, diseñadores, programadores de aplicaciones y usuarios finales. La interacción del usuario con la BD debe efectuarse a través de alguna técnica que haga fácil la comunicación, y que permita al usuario centrarse en el problema que desea solucionar, más que en la forma de expresarlo. La mejor forma de alcanzar este objetivo, es darle un lenguaje parecido al lenguaje natural, que le permita expresar de forma sencilla los requerimientos. Los lenguajes que interactúan con los SGBD, se pueden clasificar en dos grandes grupos: Unos orientados hacia la función: Son los lenguajes de definición, manipulación y control. Otros orientados a los diferentes tipos de usuarios o de procesos. Dentro del segundo grupo se encuentran los lenguajes de programación a los que están habituados los usuarios informáticos: programadores, analistas, etc. A este tipo de lenguajes se les conoce como “lenguaje anfitrión”. A las sentencias de manipulación de los lenguajes de las BD que son utilizadas en estos lenguajes se les conoce como “lenguaje huésped”. Los SGBD pueden admitir varios lenguajes de tipo anfitrión para manipulación de datos, como: Cobol, Ensamblador, Fortran, PL/I, Basic, Pascal, C, etc. Ahora nos vamos a centrar en los lenguajes del primer grupo, orientados hacia la función. Lenguaje de Definición de Datos (LDD) o Data Definition Language (DDL) El lenguaje de definición de datos está orientado a la definición, descripción y mantenimiento de la estructura de la BD. Permite al administrador definir los datos con facilidad y precisión, especificando sus distintas estructuras. Debe tener facilidad para describir la estructura del esquema conceptual, hacer las especificaciones relativas al esquema físico, y declarar las estructuras del esquema externo, requeridas por las aplicaciones. Para el caso concreto de los SGBD relacionales, se utiliza como estándar el SQL, para crear las BD a partir del esquema relacional. Mediante el DDL del SQL se crean tablas, columnas con los dominios correspondientes, índices, claves, las restricciones de integridad, etc. El SGBD posee un compilador de DDL cuya función consiste en procesar las sentencias del lenguaje para identificar las descripciones de los distintos elementos de los esquemas y almacenarlas generalmente en una BD especial que contiene los “metadatos”. Esta BD especial, es comúnmente llamada diccionario de datos o catálogo del SGBD. Dicho catálogo es el que se consulta, para obtener la estructura de la BD, toda vez que se quiere leer, modificar o eliminar los datos de la BD. El DDL permite especificar: Elementos de datos Estructura de datos Relaciones entre datos Reglas de integridad Vistas lógicas Espacio reservado para la BD Formato de representación (binario, decimal, …) Modo de acceso (punteros, índices, …) Lenguaje de Manipulación de Datos (LMD) o Data Manipulation Language (DML) El lenguaje de consulta y manipulación de datos sirve para obtener, insertar, eliminar y modificar los datos de la BD. Al igual que el programador necesita el LMD como lenguaje huésped dentro de un lenguaje anfitrión que maneja, el usuario no informático necesita de un instrumento para comunicarse con la BD. Este instrumento suele ser un LMD autocontenido, que da facilidades a los usuarios con pocos conocimientos de programación a acceder y manipular los datos en modo interactivo. El lenguaje de manipulación de datos SQL, puede actuar al mismo tiempo como huésped y como autocontenido, cumpliendo la propiedad dual (Codd 1990). En una primera clasificación de los LMD, hay dos tipos de lenguajes según su definición: DML procedural . El programador especifica qué datos se necesitan y cómo obtenerlos. Se deben especificar todas las operaciones de acceso a datos llamando a los procedimientos necesarios para obtener la información requerida. Estos lenguajes acceden a un registro, lo procesan y basándose en los resultados obtenidos, acceden a otro registro, que también deben procesar. Así se va accediendo a registros y se van procesando hasta que se obtienen los datos deseados. Las sentencias de un DML procedural deben estar embebidas en un lenguaje de alto nivel. Como ya hemos comentado este es el lenguaje conocido como lenguaje anfitrión. DML no procedural . El usuario o programador especifica qué datos quiere obtener sin decir cómo se debe acceder a ellos. El SGBD traduce las sentencias del DML en uno o varios procedimientos que manipulan los conjuntos de registros necesarios. Esto libera al usuario de tener que conocer cuál es la estructura física de los datos y qué algoritmos se deben utilizar para acceder a ellos. A los DML no procedurales también se les denomina lenguajes declarativos. El lenguaje DML no procedural más conocido es el SQL. Los lenguajes no procedurales son más fáciles de utilizar y conocer que los procedurales porque el SGBD oculta al usuario los detalles sobre cómo se ha realizado la operación solicitada. En una segunda clasificación de los LMD, hay dos tipos de lenguajes según como recuperan la información: Navegacionales : Recuperan o actualizan los datos registro a registro, debiendo el programador indicar el camino que se ha de recorrer, a través de la estructura definida, hasta llegar al registro buscado. Se utilizan estos lenguajes en BD en red y jerárquicas. No navegacionales : Actúan sobre un conjunto de registros. Una única sentencia puede dar lugar a recuperar o actualizar todos los registros que cumplan una determinada condición. El SQL es de este tipo. En el caso del SQL, asociado al LMD se suele encontrar un módulo optimizador que se ocupa de analizar la petición contra la BD y decidir el mejor camino de acceso con el fin de acelerar la ejecución. Para la toma de decisiones, el optimizador necesita de la información contenida en el catálogo o diccionario del SGBD. La manipulación de datos comprende las siguientes operaciones: Recuperación de información Inserción de nueva información Eliminación de información existente Modificación de información almacenada Lenguaje de Control de Datos (LCD) o Data Control Language (DCL) El lenguaje de Control de Datos sirve para trabajar en un entorno multiusuario, donde es muy importante la protección y la seguridad de los datos y la compartición de datos por parte de usuarios. Se encarga principalmente de tres actividades sobre la BD: Control de permisos de acceso Control de concurrencia Control de transacciones Estructura de un SGBDLos SGBD son paquetes de software muy complejos que deben proporcionar los servicios comentados anteriormente. Los elementos que componen un SGBD varía mucho unos de otros. El SO proporciona servicios básicos al SGBD, que es construido sobre él. Los principales módulos del SGBD son: El compilador del DDL. Chequea la sintaxis de las sentencias del DDL y actualiza las tablas del diccionario de datos o catálogo que contienen los metadatos. El precompilador del DML. Convierte las sentencias del DML embebidas en el lenguaje anfitrión, en sentencias listas para su procesamiento por parte del compilador de lenguaje anfitrión y además extrae dichas sentencia DML para que puedan ser procesadas de forma independiente por el compilador del DML. El compilador del DML. Chequea la sintaxis de las sentencias del DML y se las pasa al procesador de consultas. El procesador de consultas. Realiza la transformación de las consultas en un conjunto de instrucciones de bajo nivel que se dirigen al gestor de la BD. El gestor de la BD. Es el interfaz con los programas de aplicación y las consultas de los usuarios. El gestor de la BD acepta consultas y examina los esquemas externo y conceptual para determinar qué registros se requieren para satisfacer la petición. Entonces el gestor de la BD realiza una llamada al gestor de ficheros para ejecutar la petición. Los principales componentes del gestor de la BD son los siguientes: El gestor de transacciones. Realiza el procesamiento de las transacciones. El gestor de buffers. Transfiere los datos entre memoria principal y los dispositivos de almacenamiento secundario. El gestor de ficheros. Gestiona los ficheros en disco en donde se almacena la BD. Este gestor establece y mantiene la lista de estructuras e índices definidos en el esquema interno. Para acceder a los datos pasa la petición a los métodos de acceso al SO que se encargan de leer o escribir en los ficheros físicos que almacenan la información de la BD. En la figura se ilustra cómo se relacionan entre sí todos los elementos. La primera fila de esta figura son los distintos tipos de usuarios que pueden acceder al SGBD (usuarios inexpertos, programadores de aplicaciones, usuarios sofisticados y administradores). La segunda fila son los distintos métodos de acceso a la información que utilizan los usuarios (interfaces de aplicaciones, programas de aplicación, consultas interactivas o el esquema de la BD). Los usuarios inexpertos y los programadores de aplicaciones utilizan aplicaciones informáticas para acceder a la BD y los usuarios sofisticados y administradores acceden directamente a ella. El tercer bloque es el SGBD y se subdivide en dos partes: La primera recibe las peticiones de los cuatro tipos de usuarios y las dirige al gestor de la BD o directamente al diccionario de datos. Independientemente de si las peticiones de manipulación de datos llegan de programas de aplicación o de una consulta directa a la BD por un usuario sofisticado, finalmente es el procesador de consultas el que las redirige al gestor de la BD. La segunda es el gestor de la BD, que consta de los gestores de transacciones, de buffer y de ficheros. El gestor de buffer sólo se comunica con el gestor de ficheros y es este último el que accede a la estructura física de la BD. Por último tenemos la BD formada por los datos, sus índices y el diccionario de datos. Esta estructura es general y algunos de los SGBD actuales incorporan otras funciones y módulos para dotar al SGBD de nuevas facilidades o incrementar su eficiencia. Entre las funciones adicionales deseables en un SGBD se encuentran las siguientes: Utilidades de carga de datos (importación y exportación de datos) con posibilidad de conversión de formatos de ficheros. Copia de seguridad (backup). Estadísticas de utilización. Reorganización de ficheros (mejora del rendimiento). Control del rendimiento. Registro de transacciones. Gestor de bloqueos. Distribución de procesos entre máquinas. SGBD Relacionales (SGBD-R)A continuación vamos a estudiar los Sistemas de Gestión de BD Relacionales (SGBD-R) y para ello veremos antes a nivel conceptual el modelo relacional. Este modelo es fundamental porque dio origen a los primeros sistemas comerciales de SGBD-R, que son los que hoy en día dominan el mercado de BD. El modelo relacionalEl modelo de datos relacional fue presentado por Codd en 1970 y se basa en la representación del universo del discurso mediante el álgebra relacional. Codd, que era un experto matemático, utilizó una terminología perteneciente a las matemáticas, en concreto de la teoría de conjuntos y de la lógica de predicados. Las características principales del modelo son las siguientes: Está basado en un modelo matemático con reglas y algoritmos algebraicos establecidos, lo cual permite el desarrollo de lenguajes de acceso y manipulación potentes y de fiabilidad demostrable. Estructura los dato en forma de relaciones que se modelan mediante tablas bidimensionales. Estas tablas representan tanto las entidades del universo del discurso como las relaciones entre las mismas. Permite incorporar aspectos semánticos del universo del discurso mediante el establecimiento de reglas de integridad. Estas reglas permiten trasladar al esquema conceptual restricciones o comportamientos de los datos presentes en el universo del discurso que no se podrían modelar exclusivamente con tablas. Características de los SGBD-RLos SGBD construidos sobre el modelo relacional se caracterizan, por tanto, por las estructuras de datos, los operadores asociados y los aspectos semánticos. A continuación vamos a ver estos tres conceptos. Estructuras de datos: Relaciones y Claves En la estructura básica del modelo relacional se distinguen los siguientes elementos: Relación : Es un subconjunto de un producto cartesiano entre conjuntos formados por atributos. Por ejemplo, una relación R, definida sobre los atributos A1, A2, …, AN , sería un subconjunto formado por _m_ elementos del producto cartesiano A1, A2, …, AN . En el modelo relacional se representa mediante una tabla con _m_ filas y _n_ columnas. Como las tablas son esencialmente relaciones, se utilizarán los términos matemáticos relación y tupla, en lugar de los términos tabla y fila. Atributos : Son las columnas de la tabla. Corresponden a las propiedades de las entidades presentes en el universo del discurso que nos interesa almacenar en la BD. Cada uno de estos atributos puede tomar valores dentro de un rango determinado, que se llama dominio. Varios atributos pueden compartir un único dominio. Dominio : Rango de valores aceptable para un atributo dado. Este rango depende exclusivamente del atributo y va a condicionar los valores posibles dentro de cada celda de la tabla. Los valores que forman el dominio deben ser homogéneos, es decir, del mismo tipo y atómicos, o sea, indivisibles. Un valor de dominio que es miembro de todos los dominios posibles, es el valor nulo , que indica que el valor es desconocido o no existe. Tuplas : Es el nombre que recibe cada una de las filas de la tabla. Corresponden a cada una de las ocurrencias de la relación que representa la tabla o, lo que es lo mismo, a cada uno de los elementos que forman el conjunto R de la relación. El orden en el que aparecen las tuplas es irrelevante,dado que la relación es un conjunto de tuplas. Cardinalidad de la relación : es el número _m_ de tuplas de la relación. Grado de la relación : es el número _n_ de atributos que intervienen en la relación. Una vez visto qué es una tabla o relación, vamos a enumerar sus propiedades principales: Todas las filas de una tabla están compuestas por el mismo número y tipo de atributos que, además, aparecen siempre en el mismo orden. No puede haber filas repetidas. Es decir, todas las filas de la tabla deben diferenciarse entre sí al menos en el valor de un atributo. El orden en que aparecen las filas dentro de la tabla no es relevante. En cada celda de la tabla sólo puede aparecer un valor. Además este valor debe estar dentro del dominio de la columna correspondiente. Una tabla no puede contener dos filas iguales. Esto obliga, necesariamente, a que haya uno o varios atributos que se puedan utilizar para distinguir unas tuplas de otras. Cualquier atributo o conjunto mínimo de ellos que sirva para este propósito se denomina clave candidata . Es decir, una clave candidata permite identificar de forma única una fila de una tabla. Por conjunto mínimo se entiende aquel conjunto de atributos tal que si se elimina uno de ellos el conjunto resultante deja de ser clave candidata Es posible que la única clave candidata de una relación esté formada por todos los atributos de la misma. A la clave candidata que el usuario escoge para identificar las tuplas de una relación se la denomina clave primaria . La elección de esta claves es decisión del usuario, aunque se suele utilizar la más corta por razones de eficiencia. Una propiedad fundamental de la clave primaria consistes en que, bajo ninguna circunstancia, puede adoptar el valor nulo, ya que si así lo hiciera perdería su capacidad para identificar las tuplas de la relación. El resto de claves candidatas que no han sido elegidas como clave primaria reciben el nombre de claves alternativas o sencundarias . Una relación R1 puede incluir entre sus atributos la clave primaria de otra relación R2. Esta clave es una clave ajena de R1 que hace referencia a R2. La relación R1 también se denomina relación de referencia de la dependencia de clave ajena, y R2 se denomina la relación referenciada de la clave ajena. Operadores asociados Los operadores asociados al modelo de datos relacional forman el álgebra relacional. Se puede demostrar matemáticamente que éste álgebra es completa, o sea, que por medio de ella se puede realizar cualquier acceso a la BD. Los operandos con los que trabaja el álgebra son relaciones del modelo relacional y los operadores básicos son: Unión . La unión de dos relaciones R y S (R U S) es el conjunto formado por todas las tuplas de R más todas las tuplas de S. Este operador sólo se puede aplicar a relaciones del mismo grado y con los mismos atributos. Diferencia . La diferencia de de dos relaciones R y S (R – S) es el conjunto formado por todas las tuplas de R que no están en S. Este operador sólo se puede aplicar a relaciones del mismo grado y con los mismos atributos. Producto cartesiano . El producto cartesiano de dos relaciones R y S, de grados _m_ y _n_ respectivamente, se denota R x S y es el conjunto formado por todas las posibles tuplas de m + n elementos en las que los _m_ primeros elementos son de R y los _n_ restantes pertenecen a S. Proyección . Si x es un subconjunto de atributos de la relación R, entonces la proyección πx(R) es la relación formada por las columnas de R correspondientes a los atributos x. Selección . Si F es una fórmula compuesta por operadores lógicos, aritméticos y de comparación y sus operandos son los valores de los atributos de una relación R, entonces la selección σF(R) es el conjunto de tuplas de la relación R que hacen verdadera la condición establecida por la fórmula F. A partir de estos cinco operadores básicos, es posible definir otros derivados tales como la intersección, el cociente y la unión natural. Aspectos semáticos Los aspectos semánticos son todos aquellos aspectos del universo del discurso que no pueden modelarse mediante la definición de relaciones, sino que necesitan de un nivel superior de descripción. Este nivel superior de descripción del modelo se traduce, en la práctica, en el establecimiento de restricciones adicionales a las propias del modelo relacional que ya se han mencionado (tuplas no repetidas, orden de las columnas constante, etc.) y que tienen como fin mantener la integridad y validez de los datos almacenados así como aumentar el grado de información que el esquema lógico de datos proporciona. A continuación describiremos las dos principales restricciones que se manejan en el modelo relacional: Restricciones de usuario . Son restricciones a los valores del dominio de los atributos. La capacidad de definir estas restricciones de usuario, así como su potencia y los elementos sobre los que se pueden aplicar (dominios, atributos, tuplas, tablas, etc.) dependen del gestor de BD. Integridad referencial . Otro aspecto que se puede incluir en el modelo relacional es la denominada integridad referencial, que se ocupa del mantenimiento de referencias entre las propias relaciones o tablas. Formalmente, se define integridad referencial de la siguiente manera “Sean dos relaciones R1 (relación que referencia) y R2 (relación referenciada), no necesariamente distintas entre sí. Si ocurre que la relación R1 tiene un atributo o conjunto de atributos que es clave primaria de R2, entonces cualquier valor de dicho atributo o conjunto de atributos en R1 debe concordar con un valor de la clave primaria de R2 o bien ser nulo”. El mantenimiento de la integridad referencial supone la realización de alguna acción cuando se borra o modifica una tupla en la tabla referenciada R2. Esta acción debe ser alguna de las siguientes: Impedir la operación de borrado o modificación. Así se asegura que una vez establecida no se puede romper la relación entre dos tuplas de ambas tablas. Transmitir en cascada la modificación. O sea, borrar o modificar en consecuencia las tuplas que hacen referencia a la que se acaba de borrar o modificar. Poner a nulo. Esto quiere decir que se asigna el valor nulo al atributo que ejerce de clave referencial para mantener la integridad. Poner valor por omisión o lanar una procedimiento de usuario. En este caso cuando se altera el valor del atributo referenciado, se pone como valor de la clave referencial un valor por omisión o se ejecuta un procedimiento por el usuario que establezca algún valor que sirva para mantener la integridad referencial. Lenguajes de interrogación de Bases de DatosUn lenguaje de interrogación o consulta es un lenguaje en el que un usuario solicita información de la BD. Estos lenguajes suelen ser de un nivel superior que el de los lenguajes de programación habituales. Los lenguajes de consulta pueden clasificarse, tal como vimos al estudiar los LMD, en dos grupos: Lenguajes procedurales o procedimentales : El usuario instruye al sistema para que lleve a cabo una serie de operaciones en la BD para calcular el resultado deseado. Lenguajes no procedurales o no procedimentales : El usuario describe la información deseada sin dar un procedimiento concreto para obtener esa información. El álgebra relacionalEl álgebra relacional forma la base del lenguaje de consulta SQL. El álgebra relacional es un lenguaje de consulta procedimental. Consta de un conjunto de operaciones que toman como entrada una o dos relaciones y producen como resultado una nueva relación. Las operaciones asociadas a este modelo de datos forman el álgebra relacional. Se puede demostrar matemáticamente que ésta álgebra es completa, o sea, que por medio de ella se puede realizar cualquier acceso a la BD. Las operaciones fundamentales del álgebra relacional son: selección, proyección, unión, diferencia de conjuntos, producto cartesiano y renombramiento . Además de estas operaciones fundamentales hay otras operaciones que se definen a partir de las fundamentales, tales como: intersección de conjuntos, unión natural y asignación . Operaciones Fundamentales Se dividen estas operaciones en dos tipos: Unarias : Porque operan con una sola relación o tabla. Son: selección, proyección y renombramiento. Binarias : Porque operan con dos relaciones. Son: unión, diferencia de conjuntos y producto cartesiano. La operación selección La operación selección selecciona tuplas que satisfacen un predicado dado. Se utiliza la letra griega sigma minúscula (σ) para denotar la selección. El predicado aparece como subíndice de σ. La relación de entrada es el argumento que aparece entre paréntesis a continuación de σ. Por lo tanto, la definición formal dice: Si P es un predicado compuesto por operadores lógicos, aritméticos y de comparación y sus operandos son los valores de los atributos de una relación R, entonces la selección σp(R) es el conjunto de tuplas de la relación R que hacen verdadera la condición establecida por el predicado P. En general, se permite las comparaciones que utilizan los signos: =, ≠, &lt;, ≤, &gt; o ≥, en el predicado de selección. Además se pueden combinar varios predicados en uno mayor utilizando las conectivas y(^) y o(v). El predicado de selección puede incluir comparación entre dos atributos. Ejemplo: La operación proyección La operación proyección es una operación unaria que dada una relación de entrada, devuelve todos los atributos de dicha relación que aparecen en los argumentos de dicha operación. Dado que las relaciones son conjuntos, se eliminan todas las filas duplicadas. La proyección se denota por la letra griega pi (π). Se crea una lista de atributos que se desea que aparezcan en el resultado, como subíndice de π. La relación de entrada es el argumento que aparece entre paréntesis a continuación de π. Por lo tanto, la definición formal dice: Si x es un subconjunto de atributos de la relación R, entonces la proyección πx(R) es la relación formada por las columnas de R correspondientes a los atributos x. Por ejemplo: Composición de operaciones relacionales Es importante el hecho de que el resultado de una operación relacional sea también una relación. En general, dado que el resultado de una operación de álgebra relacional es del mismo tipo (relación) que los datos de entrada, las operaciones del álgebra relacional pueden componerse para formar una expresión del álgebra relacional. La composición de operaciones del álgebra relacional para formar expresiones del álgebra relacional es igual que la composición de operaciones aritméticas (como +, -, * y ÷) para formar expresiones aritméticas. Por ejemplo: La operación unión Se debe asegurar que las uniones se realicen entre relaciones compatibles, es decir, que deben cumplir las dos condiciones siguientes: Las dos relaciones deben ser de la misma aridad , es decir, deben tener el mismo número de atributos. Los dominios de los atributos, deben ser iguales. Por lo tanto, la definición formal dice: La unión de dos relaciones R y S (R U S) es el conjunto formado por todas las tupas de R más todas las tuplas de S. Este operador sólo se puede aplicar a relaciones del mismo grado y con los mismos atributos. Por ejemplo: La operación diferencia de conjuntos La operación diferencia de conjuntos, denotada por -, permite buscar las tuplas que estén en una relación pero no en otra. La definición formal dice: La diferencia de dos relaciones R y S (R – S) es el conjunto formado por todas las tuplas de R que no están en S. Este operador, al igual que el operador unión, solo puede realizarse entre relaciones compatibles. Por lo tanto el operador diferencia sólo se puede aplicar a relaciones del mismo grado y con los mismos atributos. Por ejemplo: La operación producto cartesiano La operación producto cartesiano se denotada por un aspa (x), permite combinar información de cualesquiera dos relaciones. Hay que considerar dos posibles problemas: Si las dos relaciones de entrada tienen un atributo con el mismo nombre, se adjunta a dicho atributo el nombre de la relación, para así distinguir uno de otro. Si el nombre de las dos relaciones de entrada es el mismo (producto cartesiano de una relación consigo misma) o si se utiliza el resultado de una expresión del álgebra relacional en un producto cartesiano, se debe dar un nuevo nombre a una de las relaciones o a la expresión del álgebra relacional utilizando una operación de renombramiento que veremos en el apartado siguiente. La definición formal dice: El producto cartesiano de dos relaciones R y S, de grados _m_ y _n_ respectivamente, se denota R x S y es el conjunto formado por todas las posibles tuplas de m + n atributos en las que los _m_ primeros atributos son de R y los _n_ restantes pertenecen a S. La operación renombramiento A diferencia de las relaciones de BD, los resultados de las expresiones del álgebra relacional no tienen un nombre que se pueda utilizar para referirse a ellas. Resulta, por lo tanto, útil ponerles nombre. La operación renombramiento denotado por la letra griego rho (p), permite realizar esta tarea. La definición formal dice: Data una expresión E del álgebra relacional, la exprexión px(E), devuelve el resultado de la expresión E con nombre x. Las relaciones por sí mismas también se consideran expresiones triviales del álgebra relacional. Por lo tanto, también se puede aplicar la operación renombramiento a una relación dada, para obtener la misma relación con un nuevo nombre. Por ejemplo: Otras operaciones derivadas Las operaciones fundamentales del álgebra relacional son suficientes para expresar cualquier consulta del álgebra relacional. Sin embargo, si uno se limita únicamente a las operaciones fundamentales, algunas consultas habituales resultan ser algo más complejas de expresar. Por este motivo, se definen otras operaciones que no añaden potencia al álgebra, pero que simplifican las consultas habituales. A partir de las operaciones básicas, es posible definir otras operaciones derivadas tales como la intersección, la unión natural y la asignación. La operación intersección de conjuntos La operación intersección de conjuntos denotada por ∩, permite buscar las tuplas que estén al tiempo en las dos relaciones sobre las que actúa. Se observa que esta operación no es fundamental y no añade potencia al álgebra relacional, ya que puede ser expresada en función de la operación de conjuntos, de la manera siguiente: R ∩ S = R – (R – S). Por ejemplo: La operación unión natural Suele resultar deseable simplificar ciertas consultas que exigen producto cartesiano. Generalmente, las consultas que implican producto cartesiano incluyen un operador selección sobre el resultado del producto cartesiano. La unión natural es una operación binaria que permite combinar ciertas selecciones y un producto cartesiano en una sola operación. Se denota por el símbolo de la “reunión”. La operación unión natural forma un producto cartesiano de sus dos argumentos, realiza una selección forzando la igualdad de los atributos que aparecen en ambas relaciones y, finalmente elimina los atributos duplicados. La operación asignación En ocasiones resulta conveniente escribir una expresión de álgebra relacional por partes utilizando la asignación a una variable de relación temporal. La operación asignación, denotada por ←, actúa de manera parecida a la asignación de los lenguajes de programación. Con la operación asignación se puede escribir las consultas como programas secuenciales consistentes en una serie de asignaciones seguida de una expresión cuyo valor se muestra como resultado de la consulta. Por ejemplo: El cálculo relacional de tuplasCuando escribimos una expresión de álgebra relacional proporcionamos una serie de procedimientos que generan la respuesta a la consulta. El cálculo relacional de tuplas, en cambio, es un lenguaje de consulta no procedimental. Describe la información deseada sin dar un procedimiento específico para obtenerla. Las consultas se expresan en el cálculo relacional de tuplas como: { _t_ | P( _t_ )}, es decir, son el conjunto de todas las tuplas tales que el predicado P es cierto para _t_ . Se utiliza la notación _t_ [A] para denotar el valor de la tupla _t_ en el atributo A y _t_ Є R para denotar que la tupla _t_ está en la relación R. Para poder hacer una definición formal del cálculo relacional de tuplas, debemos conocer los tres conceptos siguientes: Constructor “existe” de la lógica matemática. La notación ∃ _t_ Є R(Q( _t_ )) significa: existe una tupla _t_ en la relación R que el predicado Q( _t_ ) es verdadero. Implicación denotada por ⇒, es decir P implica Q, se escribe: P ⇒ Q Constructor “para todo” de la lógica matemática. La notación ∀ _t_ Є R(Q( _t_ )) significa: Q es verdadera para todas las tuplas _t_ de la relación R. Ahora podemos dar una definición formal del cálculo relacional de tuplas. Como ya hemos dicho, las expresiones del cálculo relacional de tupla son de la forma: { _t_ | P( _t_ )} donde P es un predicado o una fórmula. En una fórmula pueden aparecer varias variables tupla. Se dice que una variable tupla es una variable libre a menos que esté cuantificada mediante ∃ o ∀. Las fórmulas del cálculo relacional de tuplas se construyen con átomos. Los átomos tienen una de las formas siguientes: s Є R, donde s es una variable tupla y R es una relación. s[x] θ u[y], donde s y u son variables tuplas, x es un atributo en el que está definida s, y es un atributo que está definida en u y θ es un operador de comparación (&lt;, &lt;=, &gt;, &gt;=, &lt;&gt;, =). Es necesario que los atributos x e y tengan dominios cuyos miembros puedan compararse mediante θ. s[x] θ c, donde s es una variable tupla, x es un atributo en el que está definida s, θ es un operador de comparación y c es una constante en el dominio de atributo x. Las fórmulas se construyen a partir de los átomos utilizando las reglas siguientes: Si P1 es una fórmula, también lo son ¬ P1 y (P1) Si P1 y P2 son fórmulas, también lo son P1 ∨ P2, P1 ∧ P2 y P1 ⇒ P2 Si P1(s) es un fórmula que contiene una variable tupla libre s, y R es una relación: ∃ s Є R(P1(s)) y ∀ s Є R(P1(s)) también son fórmulas. El cálculo relacional de dominiosHay una segunda forma de cálculo relacional denominada cálculo relacional de dominios. Esta forma utiliza variables de dominio que toman sus valores del dominio de un atributo, en vez de tomarlos de una tupla completa. El cálculo relacional de dominios, sin embargo, se halla estrechamente relacionado con el cálculo relacional de tuplas. Las expresiones de cálculo relacional de dominios son de la forma: {&lt;x1, x2, …, xn&gt; | P(x1, x2, …, xn)}, donde x1, x2, …, xn representan las variables de dominio, P representa una fórmula compuesta de átomos, como era el caso en el cálculo relacional de tuplas. Los átomos del cálculo relacional de dominios tienen una de las formas siguientes: &lt;x1, x2, …, xn&gt; Є R, donde R es una relación con _n_ atributos y x1, x2, …, xn son variables de dominio o constantes de dominio. x θ y, donde x e y son variables de dominio y θ es un operador de comparación (&lt;, &lt;=, &gt;, &gt;=, &lt;&gt;, =). Se exige que los atributos x e y tengan dominios que puedan compararse mediante θ. x θ _c_ , donde x es una variable de dominio, θ es un operador de comparación y _c_ es una constante de dominio del atributo para el que x es una variable de dominio. Las fórmulas se construyen a partir de los átomos utilizando las reglas siguientes: Un átomo es una fórmula. Si P1 es una fórmula, también lo son ¬ P1 y (P1). Si P1 y P2 son fórmulas, también lo son P1 ∨ P2, P1 ∧ P2 y P1 ⇒ P2. Si P1(x) es una fórmula en x, donde x es una variable de dominio: ∃ x (P1(x)) y ∀ x (P1(x)) también son fórmulas. El lenguaje SQL (Structured Query Language)Los lenguajes formales descritos en el epígrafe anterior proporcionan una notación concisa para la representación de las consultas. Sin embargo, los sistemas de BD comerciales necesitan un lenguaje de consulta cómodo para el usuario. SQL es una combinación de álgebra relacional y construcciones de cálculo relacional. Aunque el lenguaje SQL se considere un lenguaje de consultas, contiene muchas otras capacidades además de la consulta en BD. Incluye características para definir la estructura de los datos, para la modificación de los datos en la BD y para especificación de restricciones de integridad. El lenguaje SQL es un lenguaje de alto nivel para dialogar con los SGBD-R. Como todo lenguaje de un SGBD, está formado por tres componentes claramente diferenciados, según muestra la figura: Destacamos algunas de las características principales del lenguaje SQL: Utilizado por todo tipo de usuarios: Administradores de BDR. Programadores. Usuarios Finales. Lenguaje no procedimental: Se especifica QUÉ se quiere obtener, sin decir CÓMO. Permite especificar cualquier consulta. Lenguaje de Definición de Datos (DDL)Tipos básicos de datos Datos Alfanuméricos o Cadenas de Caracteres: CHAR(longitud), donde: longitud = número máximo de caracteres del campo VARCHAR(longitud) Datos Numéricos: SMALLINT, INTEGER DECIMAL ó DECIMAL(precisión, decimal), donde: precisión = número de dígitos del número y decimal = número de dígitos decimales del nº decimal REAL FLOAT Datos tipo fecha y tiempo: DATE: Se puede elegir entre varios formatos TIME: También tiene diferentes formatos TIMESTAMP: Su valor es: fecha + tiempo + nanosegundos Creación y borrado de bases de datos Creación de una BD: CREATE DATABASE nombre_base_datos; Borrado de la BD: DROP DATABASE nombre_base_datos; Creación, modificación y borrado de tablas Creación CREATE TABLE nombre_tabla ( &lt;definición_atributo_1&gt; [NOT NULL][CHECK Condicion], &lt;definición_atributo_2&gt; [NOT NULL][CHECK Condicion], ··················································· &lt;definición_atributo_n&gt; [NOT NULL][CHECK Condicion], [PRIMARY KEY (ListadeAtributos)]); Donde: definición_atributo = nombre_atributo tipo_dato (tamaño) NOT NULL: no se permiten valores nulos en la columna ListadeAtributos: uno o más atributos separados por comas Modificación Añadir un nuevo atributo: ALTER TABLE &lt;nombre_tabla&gt; ADD &lt;def_atributo&gt;|&lt;def_integridad&gt;; Modificar un atributo ya existente: ALTER TABLE &lt;nombre_tabla&gt; ALTER TYPE &lt;nuevo_tipo&gt;; Borrar un atributo ya existente: ALTER TABLE &lt;nombre_tabla&gt; DROP ; Eliminación DROP TABLE &lt;nombre_tabla&gt;; Definición de vistas Una vista es una estructura tabular no física (tabla virtual), que permite consultar y/o modificar datos de la tabla real. Las principales características de las vistas son: Se utilizan como si fuesen tablas reales. No contienen datos propios. No tienen asociada estructura física. Las ventajas del uso de vistas son: Meno complejidad en consultas: Permiten obtener igual información de forma más simple. Aumento de confidencialidad: Permiten acceder sólo aciertos datos de las tablas reales. Las vistas se pueden crear y borrar con las siguientes sentencias: Creación de vistas: CREATE VIEW &lt;nombre_vista&gt; [&lt;lista_atributos&gt;)] AS (&lt;cláusula SELECT&gt;); Las filas de la vista serán aquellas que resulten de ejecutar la consulta cobre la que está definida. Eliminación de vistas: DROP VIEW &lt;nombre_vista&gt;; Creación y borrado de índices Es el sistema el encargado de utilizar los índices, para optimizar el acceso a los datos, el usuario sólo puede crear o eliminar índices, pero no indicar su utilización. Creación de índices: CREATE [UNIQUE] INDEX &lt;nombre_índice&gt; ON &lt;nombre_tabla&gt; (&lt;lista_atributos&gt;); Eliminación de índices: DROP INDEX &lt;nombre_índice&gt;; Definición de claves referenciales Justo debajo de PRIMARY KEY cuando estamos creando una tabla: [FOREIGN KEY (lista_de_columnas) REFERENCES nombre_de_tabla(lista_de_columnas)ON UPDATE [NO ACTION | SET DEFAULT | SET NULL | CASCADE]ON DELETE [NO ACTION | SET DEFAULT | SET NULL | CASCADE] Lenguaje de Manipulación de Datos (DML)Inserción, actualización y borrado de filas Inserción Inserción de una fila: INSERT INTO &lt;nombre_tabla&gt; [(&lt;lista_atributos&gt;)] VALUES (, …, ); Inserción de varias filas: INSERT INTO &lt;nombre_tabla&gt; [(&lt;lista_atributos&gt;)] (&lt;cláusula SELECT&gt;); La cláusula “SELECT” especifica una consulta cuyo resultado (filas) se insertará en la tabla especificada. Modificación UPDATE &lt;nombre_tabla&gt; SET &lt;atributo_1&gt; = &lt;valor_1&gt;, &lt;atributo_2&gt; = &lt;valor_2&gt;, ………… &lt;atributo_n&gt; = &lt;valor_n&gt;[WHERE &lt;condición&gt;]; La modificación afectará a todas las filas que cumplan la condición, si se especifica ésta. Si no se especifica condición, la modificación afectará a todas las filas de la tabla. Eliminación DELETEFROM &lt;nombre_tabla&gt;[WHERE &lt;condición&gt;]; No se pueden eliminar partes de una fila. Si no aparece la cláusula “WHERE” se eliminarán todas las filas de la tabla, no eliminándose la definición de ésta en el esquema. Operaciones de consulta Sintaxis de la sentencia: SELECT [DISTINCT] &lt;expresión&gt;FROM &lt;lista_de_tablas&gt; [WHERE ][GROUP BY &lt;lista_de_atributos&gt; [HAVING &lt;condición_de_grupo&gt; ]][ORDER BY &lt;lista_de_atributos&gt; [ASC/DESC]]; SELECT: especifica la información que se desea obtener. DISTINCT: elimina los valores repetidos. FROM: indica las tablas o vistas en las que se encuentran los atributos implicados en la consulta. WHERE: especifica la condición de búsqueda. GROUP BY: permite agrupar el resultado. HAVING: especifica una condición de grupo. ORDER BY: permite ordenar el resultado. Operadores: Los operadores que se pueden utilizar para expresar condiciones de fila (cláusula WHERE) o de grupo (cláusula HAVING) son: De comparación: &lt;, &lt;=, &gt;, &gt;=, &lt;&gt;, = Lógicos: AND, OR, NOT BETWEEN … AND …: establece una comparación dentro de un intervalo cerrado. También se puede utilizar NOT BETWEEN. LIKE: establece una comparación entre cadenas de caracteres, también se puede utilizar NOT LIKE, emplea los siguientes comodines: %: sustituye a una cadena de caracteres cualquiera. _: sustituye a un único carácter cualquiera. IN: comprueba la pertenencia de un valor a un conjunto dado. IS NULL: comprueba si un valor determinado es nulo (NULL). También se puede utilizar IS NOT NULL. Cuantificadores: ANY (alguno), ALL (todos). Existencial: EXISTS, indica la existencia o no de un conjunto. También se puede utilizar NOT EXISTS. Reglas de Evaluación de Operadores: El Orden de Evaluación es el siguiente: Operadores de Relación: BETWEEN, IN, LIKE, IS, NULL y después NOT, AND, OR. Se pueden utilizar paréntesis para establecer el orden de evaluación deseado por el usuario. Consultas con UNION, DIFERENCIA e INTERSECCIÓN: Unión de conjuntos: operador UNION. Diferencia de conjuntos: operador MINUS. Intersección de conjuntos: operador INTERSECT. Expresiones en la cláusula SELECT: No sólo se pueden seleccionar atributos, sino expresiones en las que aparezcan atributos y/o constantes y operadores aritméticos. Funciones agregadas: Devuelven un valor único, numérico. No se pueden combinar, con columnas que devuelvan más de un valor, a menos que la consulta contenga una cláusula GROUP BY. COUNT (*): contador de tuplas (totalizador) COUNT (DISTINCT Atributo): contador de tuplas (parcial), no tiene en cuenta valores nulos ni duplicados. AVG(Atributo): media aritmética de un atributo numérico. SUM(Atributo): suma de atributos o expresiones numéricas. MAX(Atributo): valor máximo de un atributo o expresión numérica. MIN(Atributo): valor mínimo de un atributo o expresión numérica. Cláusula GROUP BY: GROUP BY &lt;lista_de_atributos&gt; Agrupa el resultado, devolviendo una única fila por grupo. El agrupamiento no se realiza ordenado. Los atributos que aparezcan en GROUP BY, deben aparecer en la cláusula SELECT. Cláusula HAVING: HAVING &lt;condición_de_grupo&gt; Siempre va acompañada de la cláusula GROUP BY. Especifica una condición de grupo. Cláusula ORDER BY: ORDER BY &lt;lista_de_atributos&gt; [ASC | DESC] El resultado de la consulta se ordena en base a los atributos que se indiquen en la lista. Los atributos de ordenación deben aparecer en SELECT. Lenguaje de Control de Datos (DCL)Este lenguaje se preocupa principalmente del control de acceso a los datos (seguridad) y del control de la integridad de los datos. Control de acceso a los datos Niveles de acceso soportados por los SGBDR: Se establecen privilegios de acceso por los niveles siguientes: Base de Datos Tabla Atributo Tupla Concesión de Privilegios: Permite dar a los usuarios el acceso completo o restringido a la BD: GRANT &lt;privilegio_de_acceso&gt; [ON &lt;lista_de_objetos&gt;] TO &lt;lista_de_usuarios&gt; [WITH GRANT OPTION] &lt;privilegio_de_acceso&gt;: CONNECT, RESOURCE, DBA, ALL PRIVILEGES, SELECT, UPDATE, INSERT, DELETE WITH GRANT OPTION concede permiso para que el usuario a su vez, conceda esos permisos a otros usuarios. Nivel de Base de Datos: El SGBDR chequea los privilegios del usuario al iniciar la sesión. Los posibles privilegios o permisos son: CONNECT: Conectarse a la BDR. RESOURCE: Crear objetos. DBA: Ejecución de comandos restrictivos. Acceso a cualquier objeto. Privilegio RESOURCE implícito. Nivel de Tabla: Las tablas son propiedad del usuario que las creó. Los posibles privilegios o permisos son: DELETE: Autoriza el borrado de tuplas. INSERT: Autoriza la inserción de nuevas tuplas. SELECT: Permite la realización de consultas. UPDATE: Permite la actualización de tuplas. ALL PRIVILEGES: Concede todos los privilegios. Niveles Atributo y Tupla: Se implantan a través de la definición de vistas. Nivel de Atributo: Se crea una vista sin condiciones. Se establecen los permisos sobre la vista. Nivel de Tupla. Se crea una vista con sólo las tuplas permitidas. Se establecen los permisos sobre la vista. Revocación de privilegios: Se utiliza para anular privilegios ya concedidos a los usuarios: REVOKE &lt;privilegio_de_acceso&gt; [ON &lt;lista_de_objetos&gt;] TO &lt;lista_de_usuarios&gt;; Control de integridad Este control está asociado al concepto de Transacción. Existen dos sentencias principales: COMMIT: Los cambios que se puedan estar realizando sobre la BD se hacen fijos únicamente al completar la transacción (COMMIT automático) o al hacer un COMMIT explícito. ROLLBACK: Elimina todos los cambios que se hayan podido producir en la BD desde la ejecución de la última instrucción COMMIT. Si se produce un error de programa o un fallo hardware el sistema realiza un ROLLBACK automáticamente. Estándares de conectividad: ODBC y JDBCLos programas de aplicación son programas que se usan para interaccionar con la BD. Como ya se comentó, estos programas se escriben usualmente en un lenguaje anfitrión, tal como Cobol, C, C++, Java, etc. Para acceder a la BDR, las instrucciones del LMD del SQL necesitan ser ejecutadas desde el lenguaje anfitrión. Hay dos manera de hacerlo: SQL incorporado: Extendiendo la sintaxis del lenguaje anfitrión para incorporar las llamadas del LMD dentro del programa del lenguaje anfitrión. Usualmente, un carácter especial o una sentencia concreta precede a las llamadas del LMD y un precompilador LMD, convierte las instrucciones LMD en llamadas normales a procedimientos del lenguaje anfitrión. SQL dinámico: Proporcionando una interfaz de programas de aplicación, API (Application Program Interface), que se deben usar para enviar tanto instrucciones LMD, como LDD, a la BD, y recuperar los resultados. Existen dos estándares: El estándar de conectividad abierta de BD (ODBC, Open Data Base Connectivity) definido por Microsoft para el uso con el lenguaje C, usado comúnmente. El estándar de conectividad de Java con BD (JDBC, Java Data Base Connectivity) que proporciona las características correspondientes para el lenguaje Java. En el resto del apartado, vamos a examinar las dos normas de conexión de BD, ODBC y JDBC, utilizando el lenguaje SQL. Para comprender estas normas es necesario comprender el concepto de sesión SQL. El usuario o aplicación se conecta a un servidor de BD SQL, estableciendo una sesión. Así todas las actividades del usuario o aplicación están en el contexto de una sesión SQL. Además de las órdenes normales de SQL (LMD y LDD), una sesión también puede contener órdenes para comprometer el trabajo realizado hasta ese momento en la sesión (COMMIT) o para echarlo atrás (ROLLBACK). Las normas ODBC y JDBC, se desarrollaron para hacer de interfaz entre clientes y servidores. Cualquier cliente que utilice interfaces ODBC o JDBC puede conectarse a cualquier servidor de BD que proporcione esta interfaz. ODBC¿Qué es ODBC? ODBC son las siglas Open Database Connectivity. Es un interface estándar de programas de aplicación (API) para acceso a BD. Impulsado por SQL Access Group, principalmente por Microsoft, desde 1992. Usando sentencias ODBC en un programa, se puede acceder a las tablas de diferentes BD, tales como: Access, dBase, DB2, etc. Permite a los programas utilizar sentencias SQL que acceden a las BD, sin tener que conocer los interfaces propietarios de dichas BD. ODBC maneja la sentencia SQL requerida y la convierte en una petición a la BD. No soporta el COMMIT en dos fases, para coordinar el acceso simultáneo a varias BD. ODBC presenta una arquitectura de cuatro niveles: La aplicación propiamente dicha. ODBC driver manager: Módulo separado por cada BD a la que se quiere acceder. A este módulo es al que se conecta dinámicamente la aplicación. Driver DBMS/OS/Network: es un controlador que hace transparente el gestor de BD, el SO y los protocolos de red. El propio servidor de datos o fuente de datos. Esta arquitectura, es la que muestra la figura: Las aplicaciones como las interfaces gráficas de usuario, los paquetes estadísticos y las hojas de cálculo pueden usar la misma API ODBC, para conectarse a cualquier servidor de BD compatible con ODBC. Cada sistema de BD que sea compatible con ODBC proporciona una biblioteca que se debe enlazar con el programa cliente. Cuando este programa cliente realiza una llamada a la API ODBC, el código de la biblioteca se comunica con el servidor de BD para realizar la acción solicitada y obtener los resultados. ODBC se base en las normas SQL de interface de nivel de llamada, CLI (Call-Level Interface) desarrolladas por el consorcio industrial X/Open y el grupo SQL Access, pero tienen varias extensiones. La API ODBC define una CLI, una definición de sintaxis SQL y reglas sobre las secuencias admisibles de llamadas CLI. La norma también define los niveles de conformidad para CLI y la sintaxis SQL: El nivel central de la CLI tiene comandos para conectarse con BD, para preparar y ejecutar sentencias SQL, para devolver resultados o valores de estado y para administrar transacciones. El nivel uno, siguiente nivel de conformidad, exige el soporte de la recuperación de información de los catálogos de los SGBD, como la información sobre las relaciones existentes y los tipos de sus atributos, y otras características que superan la CLI del nivel central. El nivel dos exige más características, como la capacidad de enviar y recuperar arrays de valores de parámetros y de recuperar información de catálogo más detallada. JDBC¿Qué es JDBC? JDBC son las siglas de Java Database Connectivity. Es un Java API, para conectar programas escritos en Java a datos almacenados en SGBDR. Consiste en un conjunto de clases e interfaces escritos en el lenguaje de programación Java. Suministra un API estándar para los programadores, haciendo posible desarrollar aplicaciones con acceso a BD usando “puro” Java API. Este estándar es definido por Sun Microsystems, y permitiendo a los diversos suministradores de BD, implementar y extender dicho estándar con su propios JDBC drivers. JDBC establece una conexión con la BD, envía sentencias SQL y procesa los resultados. Con un pequeño programa “puente” se puede utilizar el interface JDBC, para acceder a las BD a través de ODBC. Pasos que hay que realizar en un programa Java utilizando el API JDBC: Importación de paquetes: Estos paquetes contienen el propio JDBC y los drivers para una determinada BD. Registrar los drivers de JDBC: DriveManager.registerDriver(new oracle.jdbc.driver.OracleDriver()); En este caso se registra un driver para acceder a Oracle. Abrir una conexión a la BD: Connection conn = DriverManager.getConnection(“jdbc:oracle:thin:@aardvark:1256:teach”, user, password); Se indica el tipo de driver, nombre de host, puerto, nombre de la BD, usuario y password, es decir, todo lo necesario para localizar la BD y poder acceder a ella. Crear un objeto de tipo Statement: PreparedStatement pstmt = conn.prepareStatement(x); En este caso x es la sentencia SQL que se quiere ejecutar. Procesar el Result Set que nos ha devuelto la BD. Cerrar los objetos creados: Result Set y Statement. Cerrar la conexión. En la figura siguiente, se muestra las cuatro arquitecturas JDBC que existen actualmente: Se reflejan los cuatro tipos de driver con los que puede trabajar JDBC. Son: Driver tipo 1: JDBC/ODBC bridge. Acceso a BD a través del API ODBC. Driver tipo 2: JDBC Driver (DBMS específico). Acceso directo a una BD concreta. Driver tipo 3: JDBC/native bridge. Acceso a BD a través de un driver Java nativo, que está arrancando en la parte del Servidor. Driver tipo 4: JDBC middleware. Acceso directo a varias BD. Soporte de COMMIT en dos fases para coordinar las actualizaciones en las diversas BD. ESQUEMA – RESUMENEl modelo relacional constituye la segunda generación de los sistemas de BD. En 1970 E.F. Codd, basándose en el álgebra y la teoría de conjuntos, propone un nuevo modelo de datos llamado modelo relacional. Sugiere que todos los datos de la BD se podrían representar como una estructura tabular (tablas con columnas y filas, que denominó relaciones) y que esas relaciones se podrían acceder con un lenguaje no procedimental (declarativo). En este tipo de lenguajes, en lugar de escribir algoritmos para acceder a los datos, sólo se necesita un predicado que identifica los registros o combinación de registros deseados. Es más, este nuevo modelo integraba los lenguajes de definición, navegación y manipulación en un solo lenguaje unificado. El modelo de datos relacional ha proporcionado beneficios inesperados además del aumento de productividad y facilidad de uso. Es muy adecuado para el enfoque cliente/servidor, el procesamiento paralelo y las interfaces gráficas de usuario. El sistema de gestión de BD (SGBD) es una colección de numerosas rutinas de software interrelacionadas, cada una de las cuales es responsable de una tarea específica. El objetivo primordial de un SGBD es proporcionar un entorno conveniente y eficiente para extraer, almacenar y manipular información de la BD. El SGBD gestiona centralizadamente todas las peticiones de acceso a la BD, por lo que este paquete funciona como interfaz entre los usuarios y la BD. Además, el SGBD gestiona la estructura física de los datos y su almacenamiento. Una de las características más importantes de los SGBD es la independencia entre programas y datos. Para asegurar esta independencia es necesario separar la representación física y lógica de los datos, distinción que fue reconocida oficialmente en 1978, cuando el comité ANSI/X3/SPARC propuso una arquitectura de 3 niveles: Nivel interno: Es la representación del nivel más bajo de abstracción, en éste se describe en detalle la estructura física de la BD: dispositivos de almacenamiento físico, estrategias de acceso, índices, etc. Nivel conceptual: El siguiente nivel de abstracción describe que datos son almacenados realmente en la BD y las relaciones que existen entre los mismos, esto es, describe la BD completa en términos de su estructura de diseño. Nivel externo: Nivel más alto de abstracción, es lo que el usuario final puede visualizar del sistema terminado, describe sólo una parte de la BD al usuario acreditado para verla. Los SGBD deben ofrecer lenguajes e interfaces apropiadas para cada tipo de usuario: administradores de la BD, diseñadores, programadores de aplicaciones y usuarios finales. Estos lenguajes son básicamente tres: El lenguaje de definición de datos (DDL) define y mantiene la estructura de la BD, es decir, creación, borrado y mantenimiento de BD, tablas, columnas, índices, claves, etc. El lenguaje de consulta y manipulación de datos (DML) sirve para obtener, insertar, eliminar y modificar los datos de la BD. El lenguaje de Control de Datos (DCL) sirve para trabajar en un entorno multiusuario, donde es importante la protección y la seguridad de los datos y la compartición de datos entre usuarios. Los principales módulos del SGBD son: El compilador del DDL (Data Definition Language) El precompilador del DML (Data Manipulation Language) El compilador del DML (Data Manipulation Language) El procesador de consultas El gestor de BD El modelo de datos relacional fue presentado por Codd en 1970, se basa en la representación del universo del discurso mediante el álgebra relacional. La estructura básica del modelo relacional es la tabla, que representa una relación, y en la cual se distinguen los siguientes elementos: Relación, Atributos, Dominio, Tuplas, Cardinalidad de la relación y Grado de la relación. Los operadores asociados al modelo de datos relacional forman el álgebra relacional. Se puede demostrar matemáticamente que ésta álgebra es completa, o sea, que por medio de ella se puede realizar cualquier acceso a la BD. Los operandos con los que trabaja el álgebra son relaciones del modelo relacional y los operadores básicos son: Unión, Diferencia, Producto cartesiano, Proyección y Selección. En el nivel superior de la descripción del modelo se establecen restricciones adicionales a las propias del modelo relacional que tienen como fin mantener la integridad y validez de los datos almacenados así como aumentar el grado de información que el esquema lógico de datos proporciona. Estas restricciones son dos: Restricciones de Usuario y de Integridad referencial. Ahora vamos a estudiar los lenguajes formales de consulta de lenguajes “puros”. Los tres que se estudian no son cómodos de usar, pero a cambio sirven como base formal para lenguajes de consulta que sí resultan cómodos. El álgebra relacional es un lenguaje de consulta procedimental. Consta de un conjunto de operaciones que toman como entrada una o dos relaciones y producen como resultado una nueva relación. Operaciones fundamentales del álgebra relacional: Unarias: Porque operan con una sola relación de la tabla. Son: Selección: Si P es un predicado compuesto por operadores lógicos, aritméticos y de comparación y sus operandos son los valores de los atributos de una relación R, entonces la selección σP(R) es el conjunto de tuplas de la relación R que hacen verdadera la condición establecida por el predicado P. Proyección: Si x es un subconjunto de atributos de la relación R, entonces la proyección πx(R) es la relación formada por las columnas de R correspondientes a los atributos x. Renombramiento: Dada una expresión E del álgebra relacional, la expresión ρx(E), devuelve el resultado de la expresión E con nombre x. Binarias: Porque operan con dos relaciones. Son: Unión: La unión de dos relaciones R y S (R U S) es el conjunto formado por todas las tuplas de R más todas las tuplas de S. Este operador sólo se puede aplicar a relaciones del mismo grado y con los mismos atributos. Diferencia de conjuntos: La diferencia de dos relaciones R y S (R – S) es el conjunto formado por todas las tuplas de R que no están en S. Poroducto cartesiano: La definición formal dice: El producto cartesiano de dos operaciones R y S, de grados _m_ y _n_ respectivamente, se denota R x S y es el conjunto formado por todas las posibles tuplas de m + n atributos en las que los _m_ primeros atributos son de R y los _n_ restantes pertenecen a S. Otras operaciones derivadas de las fundamentales: La operación intersección de conjuntos denotada por ∩, permite buscar las tuplas que estén al tiempo en las dos relaciones sobre las que actúa. La operación unión natural forma un producto cartesiano de sus dos argumentos, realiza una selección forzando la igualdad de los atributos que aparecen en ambas relaciones y, finalmente elimina los atributos duplicados. La operación asignación, denotada por ←, actúa de manera parecida a la asignación de los lenguajes de programación. Las consultas se expresan en el cálculo relaciona de tuplas como: { _t_ │ P( _t_ )}, es decir, son el conjunto de todas las tuplas tales que el predicado P es cierto para _t_ . Se utilizar la notación _t_ [A] para denotar el valor de la tupla _t_ en el atributo A y _t_ Є R para denotar que la tupla _t_ está en la relación R. Hay una segunda forma de cálculo relacional denominada cálculo relacional de dominios. Esta forma utiliza variables de dominio que toman sus valores del dominio de un atributo, en vez de tomarlos de una tupla completa. El cálculo relacional de dominios, sin embargo, se halla estrechamente relacionado con el cálculo relacional de tuplas. El lenguaje SQL es un lenguaje de alto nivel para dialogar con los SGBD-R. Como todo lenguaje de un SGBD, está formado por tres componentes claramente diferenciados: Lenguaje de definición de datos: CREATE, ALTER y DROP. Lenguaje de manipulación de datos: INSERT, UPDATE, DELETE y SELECT. Lenguaje de control de datos: GRANT, REVOKE, COMMIT y ROLLBACK. Existen dos estándares de conectividad para SQL: El estándar de conectividad abierta de BD (ODBC, Open Data Base Connectivity) definido por Microsoft para el uso con cualquier lenguaje de programación. El estándar de conectividad de Java con BD (JDBC, Java Data Base Connectivity) que proporciona las características correspondientes para el lenguaje Java. ¿Qué es ODBC? ODBC son las siglas de Open Database Connectivity. Es un interface estándar de programas de aplicación (API) para acceso a BD. Impulsado por SQL Access Group, principalmente por Microsoft, desde 1992. Usando sentencias ODBC en un programa, se puede acceder a las tablas de diferentes BD, tales como: Access, dBase, DB2, etc. Permite a los programas utilizar sentencias SQL que acceden a las BD, sin tener que conocer los interfaces propietarios de dichas BD. ODBC maneja la sentencia SQL requerida y la convierte en una petición a la BD. No soporta COMMIT en dos fases, para coordinar el acceso simultáneo a varias BD. ¿Qué es JDBC? JDBC son las siglas de Java Database Connectivity. Es un Java API, para conectar programas escritos en Java a datos almacenados en SGBDR. Consiste en un conjunto de clases e interfaces escritos en el lenguaje de programación Java. Suministra un API estándar para los programadores, haciendo posible desarrollar aplicaciones con acceso a BD usando un “puro” Java API. Este estándar es definido por Sun Microsystems, y permitiendo a los diversos suministradores de BD, implementar y extender dicho estándar con sus propios JDBC drivers. JDBC establece una conexión con la BD, envía las sentencias SQL y procesa los resultados. Con un pequeño programa “puente” se puede utilizar el interface JDBC, para acceder a las BD a través de ODBC. Bibliografía Sribd (Ibiza Ales) Arquitectura de sistemas cliente-servidor y multicapas: tipología. Componentes. Interoperabilidad de componentes. Ventajas e inconvenientes. Arquitectura de servicios web (WS).Arquitecturas Cliente/ServidorConcepto de Arquitectura Cliente/ServidorLa tecnología Cliente/Servidor es el procesamiento cooperativo de la información por medio de un conjunto de procesadores, en el cual múltiples clientes, distribuidos geográficamente, solicitan requerimientos a uno o más servidores centrales. Desde el punto de vista funcional, se puede definir la computación Cliente/Servidor como una arquitectura distribuida que permite a los usuarios finales obtener acceso a la información de forma transparente aún en entornos multiplataforma. Se trata pues, de la arquitectura más extendida en la realización de Sistemas Distribuidos. Un sistema Cliente/Servidor es un Sistema de Información distribuido basado en las siguientes características: Servicio: unidad básica de diseño. El servidor los proporciona y el cliente los utiliza. Recursos Compartidos: Muchos clientes utilizan los mismos servidores y, a través de ellos, comparten tanto recursos lógicos como físicos. Protocolos asimétricos: Los clientes inician “conversaciones”. Los servidores esperan su establecimiento pasivamente. Transparencia de localización física de los servidores y clientes: El cliente no tiene por qué saber dónde se encuentra situado el recurso que desea utilizar. Independencia de la plataforma Hardware y Software que se emplee. Sistemas débilmente acoplados. Interacción basada en envío de mensajes. Encapsulamiento de servicios. Los detalles de la implementación de un servicio son transparentes al cliente. Escalabilidad horizontal (añadir clientes) y vertical (ampliar potencia de los servidores). Integridad: Datos y programas centralizados en servidores facilitan su integridad y mantenimiento. En el modelo Cliente/Servidor, un servidor (daemon en la terminología sajona basada en sistemas UNIX/LINUX, traducido como “demonio”) se activa y espera las solicitudes de los clientes.Habitualmente, programas cliente múltiples comparten los servicios de un programa servidor común. Tanto los programas clientes como los servidores son con frecuencia parte de un programa o aplicación mayores. El Esquema de funcionamiento de un Sistema Cliente/Servidor sería: El cliente solicita una información al servidor. El servidor recibe la petición del cliente. El servidor procesa dicha solicitud. El servidor envía el resultado obtenido al cliente. El cliente recibe el resultado y lo procesa. Componentes de la arquitectura Cliente/ServidorEl modelo Cliente/Servidor es un modelo basado en la idea del servicio, en el que el cliente es un proceso consumidor de servicios y el servidor es un proceso proveedor de servicios. Además esta relación está establecida en función del intercambio de mensajes que es el únicos elemento de acoplamiento entre ambos. Cliente Un cliente es todo proceso que reclama servicios de otro. Una definición un poco más elaborada podría ser la siguiente: cliente es el proceso que permite al usuario formular los requerimientos y pasarlos al servidor. Se lo conoce con el término front-end. Éste normalmente maneja todas las funciones relacionadas con la manipulación y despliegue de datos, por lo que están desarrollados sobre plataformas que permiten construir interfaces gráficas de usuario (GUI), además de acceder a los servicios distribuidos en cualquier parte de la red. Las funciones que lleva a cabo el proceso cliente se resumen en los siguientes puntos: Administrar la interfaz de usuario. Interactuar con el usuario. Procesar la lógica de la aplicación y hacer validaciones locales. Generar requerimientos de bases de datos. Recibir resultados del servidor. Formatear resultados. La funcionalidad del proceso cliente marca la operativa de la aplicación (flujo de información o lógica de negocio). De este modo el cliente se puede clasificar en: Cliente basado en aplicación de usuario. Si los datos son de baja interacción y están fuertemente relacionados con la actividad de los usuarios de esos clientes. Cliente basado en lógica de negocio. Toma datos suministrados por el usuario y/o la base de datos y efectúa los cálculos necesarios según los requerimientos del usuario. Servidor Un servidor es todo proceso que proporciona un servicio a otros. Es el proceso encargado de atender a múltiples clientes que hacen peticiones de algún recurso administrado por él. Al proceso servidor se lo conoce con el término back-end. El servidor normalmente maneja todas las funciones relacionadas con la mayoría de las reglas del negocio y los recursos de datos. Las principales funciones que lleva a cabo el proceso servidor se enumeran a continuación: Aceptar los requerimientos de bases de datos que hacen los clientes. Procesar requerimientos de bases de datos. Formatear datos para transmitirlos a los clientes. Procesar la lógica de la aplicación y realizar validaciones a nivel de bases de datos. Puede darse el caso que un servidor actúe a su vez como cliente de otro servidor. Existen numerosos tipos de servidores, cada uno de los cuales da lugar a un tipo de arquitectura Cliente/Servidor diferente. El término “servidor” se suele utilizar también para designar el hardware, de gran potencia, capacidad y prestaciones, utilizado para albergar servicios que atienden a un gran número de usuarios concurrentes. Desde el punto de vista de la arquitectura cliente/servidor y del procesamiento cooperativo un servidor es un servicio software que atiende las peticiones de procesos software clientes. Para conectar cliente con servidor y viceversa, existe un software llamado middleware y se ejecuta en ambas partes. CaracterísticasLas características básicas de una arquitectura Cliente/Servidor son: Combinación de un cliente que interactúa con el usuario, y un servidor que lo hace con los recursos compartidos. El proceso del cliente facilita la interfaz entre el usuario y el resto del sistema. El proceso del servidor actúa como si fuese un motor de software que maneja recursos compartidos como bases de datos, impresoras, módems, etc. Las tareas del cliente y del servidor tienen distintos requerimientos en cuanto a recursos de cómputo como velocidad del procesador, memoria, velocidad y capacidades del disco y dispositivos de E/S. Se establece una relación entre procesos distintos, los cuales pueden ser ejecutados en la misma máquina o en máquinas diferentes distribuidas a lo largo de la red. Existe un clara distinción de funciones basada en el concepto de “servicio”, que se establece entre clientes y servidores. La relación establecida puede ser de muchos a uno, en la que un servidor puede dar servicio a muchos clientes, regulando su acceso a recursos compartidos. Los clientes corresponden a procesos activos en cuanto a que son éstos los que hacen peticiones de servicios a los servidores. Estos últimos tienen un carácter pasivo ya que esperan las peticiones de los clientes. No existe otra relación entre clientes y servidores que no sea la que se establece a través del intercambio de mensajes entre ambos. El mensaje es el mecanismo para la petición y entrega de solicitudes de servicio. El ambiente es heterogéneo. La plataforma de hardware y el sistema operativo del cliente y del servidor no son siempre la misma. Precisamente una de las principales ventajas de esta arquitectura es la posibilidad de conectar clientes y servidores independientemente de sus plataformas. El concepto de escalabilidad tanto horizontal como vertical es aplicable a cualquier sistema Cliente/Servidor. La escalabilidad horizontal permite agregar más estaciones de trabajo activas sin afectar significativamente el rendimiento. La escalabilidad vertical permite mejorar las características del servidor o agregar múltiples servidores. Arquitectura multicapaLa arquitectura multicapa es también conocida como arquitectura de procesamiento distribuido . En este caso, el sistema se descompone en varias capas, de ellas cada una lleva un tipo de procesamiento específico. Pondremos un ejemplo orientado a Bases de Datos, en la capa más cercana al usuario, se podría tener un programa con interfaces gráficas poderosas para facilitar la actuación de la información de la base de datos a través de ventanas. En la siguiente capa, se podría tener un servidor de internet que llevara el control de todas las páginas de internet que se mostrarían al usuario como interfaces de la aplicación. La tercera capa podría ser un servidor de aplicaciones que contendría las aplicaciones que implementan la lógica de la organización. Por último, una capa contendría al servidor de bases de datos. Esta arquitectura puede presentar muchas variantes, tanto en la lógica de procesamiento que puede existir en cada capa, como en la distribución que se puede hacer de los programas que implementan estas lógicas en diferentes equipos conectados a la red. La arquitectura más común en los sistemas de información abarca una interfaz para el usuario y el almacenamiento persistente de datos que se conoce con el nombre de arquitectura de tres capas. PRESENTACIÓN, LÓGICA y ALMACENAMIENTO. La calidad tan especial de la arquitectura de tres capas consiste en aislar la lógica de la aplicación y en convertirla en una capa intermedia bien definida y lógica del software. En la capa de presentación se realiza relativamente poco procesamiento de la aplicación. Las ventanas envían a la capa intermedia peticiones de trabajo y éste se comunica con la capa de almacenamiento del extremo posterior. Esta arquitectura constrasta con el diseño de dos capas, donde, por ejemplo, colocamos la lógica de aplicaciones dentro de las definiciones de ventana, que leen y escriben directamente en una base de datos; no hay una capa intermedia que separe la lógica. Arquitectura de Servicios Web (WS)SOA son las siglas de Service Oriented Architecture (Arquitectura Orientada a Servicios de cliente) y es el concepto de arquitectura de software que define la utilización de servicios para dar soporte a los requisitos del negocio. Ofrece una manera bien definida de exposición e invocación de servicios que normalmente se orientan a servicios web, lo cual facilita la interacción entre diferentes sistemas propios o de terceros. La W3C define “Servicio Web” como un sistema de software diseñado para permitir interoperabilidad máquina a máquina en la red. En general, los servicios web son sólo APIs (Interfaz de Programación de Aplicaciones) que pueden ser accedidas en una red, como internet, y ejecutadas en un sistema de hosting remoto. Un servicio web es cualquier sistema de software que sediseña para soportar interacción máquina a máquina sobre una red. Esta definición es muy amplia y abarca multitud de sistemas muy diferentes, pero en general “servicio web” se suele referir a clientes y servidores que se comunican usando mensajes XML que siguen el estándar SOAP. En definitiva, permite comunicación entre diferentes máquinas, con diferentes plataformas y entre programas distintos. Esta comunicación se consigue adoptando diversos estándares abiertos. Las organizaciones OASIS y W3C son los comités responsables de la arquitectura y reglamentación de los servicios web. Para mejorar la interoperabilidad entre distintas implementaciones de servicios web se ha creado el organismo WS-I, que se encarga de desarrollar diversos perfiles para definir de manera más exhaustiva estos estándares. Funcionamiento de los Servicios Web. Si nos fijamos en la imagen, un usuario (cliente), a través de una aplicación, pide información sobre un viaje que desea realizar haciendo una petición a una agencia de viajes que ofrece sus servicios a través de internet. La agencia de viajes ofrecerá a su cliente (usuario) la información requerida. Para proporcionar al cliente la información que necesita, esta agencia de viajes solicita a su vez información a otros recursos (otros Servicios Web) en relación con el hotel y la compañía aérea. La agencia de viajes obtendrá información de estos recursos, lo que la convierte a su vez en cliente de esos otros Servicios Web que le van a proporcionar la información solicitada sobre el hotel y la línea aérea. Por último, el usuario realizará el pago del viaje a través de la agencia de viajes que servirá de intermediario entre el usuario y el Servicio Web que gestionará el pago. Durante todo el proceso intervienen una serie de tecnologías que hacen posible esta circulación de información. Por un lado, estaría SOAP (Protocolo Simple de Acceso a Objetos). Este protocolo está basado en XML, y permite la interacción entre varios dispositivos además de ser capaz de transmitir información compleja.Los datos pueden ser transmitidos a través de HTTP, SMTP, etc. SOAP especifica el formato de los mensajes. El mensaje SOAP está compuesto por un envelope (sobre), cuya estructura está formada por los siguientes elementos: header (cabecera) y body (cuerpo). Bibliografía Escuela de Administración Pública de Castilla y León . El modelo TCP/IP y el modelo de referencia de interconexión de sistemas abiertos (OSI) de ISO: arquitectura, capas, interfaces, protocolos, direccionamiento y encaminamiento.El modelo OSI y los protocolos de redOSI, la pila teórica de protocolos de redA finales de la década de los setenta, la Organización Internacional para la Normalización (ISO) empezó a desarrollar un modelo conceptual para la conexión en red al que bautizó con el nombre de Open Systems Interconnection Reference Model o Modelo de Referencia de Interconexión de Sistemas Abiertos. En los entornos de trabajo con redes se les conoce más comúnmente como el modelo OSI. En 1984 este modelo pasó a ser el estándar internacional para las comunicaciones en red al ofrecer un marco de trabajo conceptual que permitía explicar el modo en que los datos se desplazan dentro de una red. El modelo OSI divide en siete capas el proceso de transmisión de la información entre equipos informáticos, donde cada capa se encarga de ejecutar una determinada parte del proceso global. Este marco de trabajo estructurado en capas, aun siendo puramente conceptual, puede utilizarse para describir y explicar el conjunto de protocolos reales que, como veremos, se utilizan para la conexión de sistemas. Por ejemplo, TCP/IP y AppleTalk son dos de las pilas de protoclos sque se utilizan en el mundo real para transmitir datos; los protocolos que, de hecho, sirven como capas o niveles dentro de un conjunto de protocolos como TCP/IP pueden, por tanto, explicarse de acuerdo con su correlación con el modelo teórico de capas o niveles de red que conforma OSI. ¿Qué es exactamente una pila de protocolos? Las pilas o suite (o capas) de protocolos no son más que una jerarquía de pequeños protocolos que trabajan juntos para llevar a cabo la transmisión de los datos de un nodo a otro de la red. Las pilas de protocolos se asemejan mucho a las carreras de relevos, pero, en vez de pasarse un testigo se transmiten paquetes de datos de un protocolo a otro hasta que éstos revisten la forma adecuada (una secuencia única de bits) para transmitirse por el entorno físico de la red. El modelo OSI abarca una serie de eventos importantes que se producen durante la comunicación entre sistemas. Proporciona las normal básicas empíricas para un serie de procesos distintos de conexión en red: El modo en que los datos se traducen a un formato apropiado para la arquitectura de red que se está utilizando. Cuando se envía un mensaje de correo electrónico o un archivo a otra computadora, se está trabajando, en realidad, con una determinada aplicación, como un cliente de correo electrónico o un cliente FTP. Los datos que se transmiten utilizando dicha aplicación tienen que convertirse a un formato más genérico si van a viajar por la red hasta llegar a su destino. El modo en que los PC u otro tipo de dispositivos de la red se comunican. Cuando se envían datos desde un PC, tiene que existir algún tipo de mecanismo que proporcione un canal de comunicación entre el remitente y el destinatario. Lo mismo que cuando se desea hablar por teléfono, para lo cual hay que descolgar el teléfono y marcar el número. El modo en que los datos se transmiten entre los distintos dispositivos y la forma en que se resuelve la secuenciación y comprobación de errores. Una vez establecida la sesión de comunicación entre dos computadoras, tiene que existir un conjunto de reglas que controlen la forma en que los datos van de una a otra. El modo en que el direccionamiento lógico de los paquetes pasa a convertirse en el direccionamiento físico que proporciona la red. Las redes informáticas utilizan esquemas de direccionamiento lógico, como direcciones IP. Por tanto, dichas direcciones lógicas tienen que convertirse en las direcciones relaes de hardware que determinan las NIC instaladas en las distintas computadoras. El modelo OSI ofrece los mecanismos y reglas que permiten resolver todas las cuestiones que acabamos de referir. Comprender las distintas capas del modelo OSI n o sólo permite internarse en los conjuntos de protocolos de red que actualmente se utilizan, sino que también proporciona un marco de trabajo conceptual del que puede servirse cualquiera para comprender el funcionamiento de dispositivos de red complejos, como conmutadores, puentes y routers. Las capas OSILas capas del modelo OSI describen el proceso de transmisión de los datos dentro de una red. Las dos únicas capas del modelo con las que, de hecho, interactúa el usuario son la primera capa, la capa Física, y la última capa, la capa de Aplicación. La capa física abarca los aspectos físicos de la red (es decir, los cables, hubs y el resto de dispositivos que conforman el entorno físico de la red). Seguramente ya habrá interactuado más de una vez con la capa Física, por ejemplo al ajustar un cable mal conectado. La capa de aplicación proporciona la interfaz que utiliza el usuario en su computadora para enviar mensajes de correo electrónico o ubicar un archivo en la red. La figura presenta la estructura de capas que conforman el modelo OSI de arriba abajo. La pirámide invertida es uno de los modos que mejor ilustran la estructura de este modelo, en el que los datos con un formato bastante complejo pasan a convertirse en una secuencia simple de bits cuando alcanzan el cable de la red. Como verá, las capas vienen numeradas de abajo arriba, cuando lo lógico sería que vinieran numeradas de arriba abajo. Éste es el sistema adoptado y, de hecho, muchas veces se alude al mismo para referirse a una de las capas de la red. Pero, tanto si se usa el nombre como el número, lo importante es que recuerde siempre el papel que desarrollan cada una de las capas en el proceso global de transmisión de los datos. El modelo OSI ofrece un modelo teórico que explica el modo en que se desplazan los datos desde una computadora emisora a otra computadora receptora. Antes de explicar cada una de las capas que componen la pila, conviene hacerse una idea general de lo que ocurre cuando los datos se mueven por el modelo OSI. Supongamos que un usuario decide enviar un mensaje de correo electrónico a otro usuario de la red. El usuario que envía el mensaje utilizará un cliente o programa de correo (como Outlook o Eudora) como herramienta de interfaz para escribir y enviar el mensaje. Esta actividad del usuario se produce en la capa de aplicación. Cuando los datos abandonan la capa de aplicación (la capa insertará un encabezado de capa de aplicación en el paquete de datos), éstos pasan por las restantes capas del modelo OSI. Cada capa proporcionará servicios específicos relacionados con el enlace de comunicación que debe establecerse, o bien formateará los datos de una determinada forma. Al margen de la función específica que tenga asignada cada capa, todas adjuntan un encabezado a los datos. Puesto que la capa física está integrada por dispositivos de hardware (un cable, por ejemplo) nunca añade un encabezado de datos. Los datos bajan por la pila OSI de la computadora emisora y suben por la pila OSI de la computadora receptora. Los datos llegan así a la capa física (en entorno tangible de la red, como los cables de par trenzado y hubs que conectan las computadoras entre sí) de la computadora del destinatario, desplazándose por el entorno físico de la red hasta alcanzar su destino final, el usuario al que iba dirigido el mensaje de correo electrónico. Los datos se reciben en la capa física de la computadora del destinatario y pasan a subir por la pila OSI. A medida que los datos van pasando por cada una de las capas, el encabezado pertinente se va suprimiendo de los datos. Cuando los datos finalmente alcanzan la capa de aplicación, el destinatario puede utilizar su cliente de correo electrónico para leer el mensaje que ha recibido. A continuación pasamos a explicar cada una de las capas que componen el modelo OSI, de arriba abajo (es decir, desde la capa de aplicación hasta la capa física). La capa de aplicación La capa de aplicación proporciona la interfaz y servicios que soportan las aplicaciones de usuario. También se encarga de ofrecer acceso general a la red. Esta capa suministra las herramientas que el usuario, de hecho ve. También ofrece los servicios de red relacionados con estas aplicaciones de usuario, como la gestión de mensajes, la transferencia de archivos y las consultas de bases de datos. La capa de aplicación suministra cada uno de estos servicios a los distintos programas de aplicación con los que cuenta el usuario en su computadora. Entre los servicios de intercambio de información que gestiona la capa de aplicación se encuentran la Web, los servicios de correo electrónico (como el Protocolo Simple de Transferencia de Correo, comúnmente conocido como SMTP – Simple Mail Transfer Protocol – incluido en TCP/IP), así como aplicaciones especiales de bases de datos clientes/servidor. La capa de presentación La capa de presentación puede considerarse el traductor del modelo OSI. Esta capa toma los paquetes (la creación del paquete para la transmisión de los datos por la red empieza en realidad en la capa de aplicación) de la capa de aplicación y los convierte a un formato genérico que pueden leer todas las computadoras. Por ejemplo, los datos escritos en caracteres ASCII se traducirán a un formato más básico y genérico. La capa de presentación también se encarga de cifrar los datos (si así lo requiere la aplicación utilizada en la capa de aplicación) así como de comprimirlos para reducir su tamaño. El paquete que crea la capa de presentación contiene los datos prácticamente con el formato con el que viajarán por las restantes capas de la pila OSI (aunque las capas siguientes irán añadiendo elementos al paquete, lo cual puede dividir los datos en paquetes más pequeños). La comunicación se produce directamente entre capas A medida que los datos bajan por la pila de protocolos de la computadora emisora (por ejemplo, un mensaje de correo electrónico) hasta llegar al cable físico y de ahí pasan a subir por la pila de protocolos de la computadora receptora, la comunicación entre ambas máquinas se está produciendo en realidad entre capas complementarias. Por ejemplo, cuando se envía un mensaje entre dos computadoras existe entre ellas una comunicación virtual en la capa de sesión. Lo cual es del todo lógico, ya que ésta es la capa que controla la comunicación entre ambas computadoras por el entorno físico de la red (ya sean cables coaxiales, de par trenzado o de fibra óptica). La capa de sesión La capa de sesión es la encargada de establecer el enlace de comunicación o sesión entre las computadoras emisora y receptora. Esta capa también gestiona la sesión que se establece entre ambos nodos. Una vez establecida la sesión entre los nodos participantes, la capa de sesión pasa a encargarse de ubicar puntos de control en la secuencia de datos. De esta forma, se proporciona cierta tolerancia a fallos dentro de la sesión de comunicación. Si una sesión falla y se pierde la comunicación entre los nodos, cuando después se restablezca la sesión sólo tendrán que volver a enviarse los datos situados detrás del último punto de control recibido. Así se evita el tener que enviar de nuevo todos los paquetes que incluía la sesión. Los protocolos que operan en la capa de sesión pueden proporcionar dos tipos distintos de enfoques para que los datos vayan del emisor al receptor: la comunicación orientada a la conexión y la comunicación sin conexión. Para comunicarse, los usuarios tienen que ejecutar el mismo conjunto de protocolos En el ejemplo anterior del envío y recepción de un mensaje de correo electrónico, dimos por sentado que tanto el remitente como el destinatario estaban ejecutando la misma pila de protocolos (la pila teórica OSI) en sus computadoras clientes. De hecho, las computadoras que ejecuten sistemas operativos distintos pueden comunicarse entre sí si utilizan el mismo conjunto de protocolos de red. Esto es lo que explica que una máquina UNIX, un Macintosh o un PC que esté ejecutando Windows utilicen el TCP/IP para comunicarse en Internet. Un ejemplo en el que dos computadoras no podrían comunicarse sería aquél en que una computadora ejecutara TCP/IP y la otra IPX/SPX. Estos dos protocolos de red del mundo real utilizan reglas distintas y formatos de datos diferentes que hacen que la comunicación resulte imposible. Los protocolos orientados a la conexión que operan en la capa de sesión proporcionan un entorno donde las computadoras conectadas se ponen de acuerdo sobre los parámetros relativos a la creación de los puntos de control en los datos, mantienen un diálogo durante la transferencia de los mismos, y después terminan de forma simultánea la sesión de transferencia. Los protocolos orientados a la conexión operan de forma parecida a una llamada telefónica: en este caso, la sesión se establece llamando a la persona con la que se desea hablar. La persona que llama y la que se encuentra al otro lado del teléfono mantienen una conexión directa. Y, cuando la conversación termina, ambos se ponen de acuerdo para dar por terminada la sesión y cuelgan el teléfono a la par. El funcionamiento de los protocolos sin conexión se parece más bien a un sistema de correo regular. Proporciona las direcciones pertinentes para el envío de los paquetes y éstos pasan a enviarse como si se echaran a un buzón de correos. Se supone que la dirección que incluyen permitirá que los paquetes lleguen a su destino, sin necesidad de un permiso previo de la computadora que va a recibirlos. La capa de transporte La capa de transporte es la encargada de controlar el flujo de datos entre los nodos que establecen una comunicación; los datos no sólo deben entregarse sin errores, sino además en la secuencia que proceda. La capa de transporte se ocupa también de evaluar el tamaño de los paquetes con el fin de que éstos tengan el tamaño requerido por las capas inferiores del conjunto de protocolos. El tamaño de los paquetes lo dicta la arquitectura de red que se utilice. La comunicación también se establece entre computadoras del mismo nivel (el emisor y el receptor); la aceptación por parte del nodo receptor se recibe cuando el nodo emisor ha enviado el número acordado de paquetes. Por ejemplo, el nodo emisor puede enviar de un solo golpe tres paquetes al nodo receptor y después recibir la aceptación por parte del nodo receptor. El emisor puede entonces volver a enviar otros tres paquetes de datos de una sola vez. Esta comunicación en la capa de transporte resulta muy útil cuando la computadora emisora manda demasiados datos a la computadora receptora. En este caso, el nodo receptor tomará todos los datos que pueda aceptar de una sola vez y pasará a enviar una señal de “ocupado” si se envían más datos. Una vez que la computadora receptora haya procesado los datos y esté lista para recibir más paquetes, enviará a la computadora emisora un mensaje de “luz verde” para que envíe los restantes. Cada capa ejecuta funciones de entrada y salida de datos No debe olvidarse que cada capa del modelo OSI (o de un conjunto real de protocolos de red, como IPX/SPX o TCP/IP) ejecutan funciones relativas a la entrada y salida de información. Cuando los datos bajan por la pila de protocolos en una computadora emisora, la capa de presentación convierte la información procedente de una determinada aplicación a un formato más genérico. En la computadora receptora, la capa de presentación se ocupará de tomar dicha información genérica y de convertirla al formato que utilice el programa que se esté ejecutando en la capa de aplicación de la computadora receptora. La capa de red La capa de red encamina los paquetes además de ocuparse de entregarlos. La determinación de la ruta que deben seguir los datos se produce en esta capa, lo mismo que el intercambio efectivo de los mismos dentro de dicha ruta. La Capa 3 es donde las direcciones lógicas (como las direcciones IP de una computadora de red) pasan a convertirse en direcciones físicas (las direcciones de hardware de la NIC, la Tarjeta de Interfaz para Red, para esa computadora específica). Los routers operan precisamente en la capa de red y utilizan los protocolos de encaminamiento de la Capa 3 para determinar la ruta que deben seguir los paquetes de datos. La capa de enlace de datos Cuando los paquetes de datos llegan a la capa de enlace de datos, éstos pasan a ubicarse en tramas (unidades de datos), que vienen definidas por la arquitectura de red que se está utilizando (como Ethernet, Token Ring, etc). La capa de enlace de datos se encarga de desplazar los datos por el enlace físico de comunicación hasta el nodo receptor, e identifica cada computadora incluida en la red de acuerdo con su dirección de hardware, que viene codificada en la NIC. Los protocolos reales utilizan ambos métodos de comunicación: sin conexión y orientados a la conexión En los conjuntos de protocolos de red, como TCP/IP e IPX/SPX, se utilizan ambas estrategias de comunicación, la que precisa de una conexión y la que no, para desplazar los datos por la red. Por lo general, en la capa de sesión opera más de un protocolo para gestionar estas estrategias distintas de comunicación. La información de encabezamiento se añade a cada trama que contenga las direcciones de envío y recepción. La capa de enlace de datos también se asegura de que las tramas enviadas por le enlace físico se reciben sin error alguno. Por ello, los protocolos que operan en esta capa adjuntarán un Chequeo de Redundancia Cíclica ( Cyclical Redundancy Check o CRC) al final de cada trama. El CRC es básicamente un valor que se calcula tanto en la computadora emisora como en la receptora. Si los dos valores CRC coinciden, significa que la trama se recibió correcta e íntegramente, y no sufrió error alguno durante su transferencia. Una vez más, y tal y como dijimos anteriormente, el tipo de trama que genera la capa de enlace de datos dependerá de la arquitectura de red que se esté utilizando, como Ethernet, Token Ring de IBM o FDDI. La siguiente figura muestra una trama Ethernet 802.2: Descripción de cada uno de los componentes: La trama se compone básicamente de un encabezado que la describe, de los datos que incluye, y de la información referente a la capa de enlace de datos (como los Puntos de Acceso al Servicio de Destino, Destination Service Access Points , y Puntos de Acceso al Servicio, Service Access Points ), que no sólo definen el tipo de trama de que se trata (en este caso, Ethernet), sino que también contribuyen a que la trama llegue a la computadora receptora. La capa de enlace de datos también controla la forma en que las computadoras acceden a las conexiones físicas de la red. La capa física En la capa física las tramas procedentes de la capa de enlace de datos se convierten en una secuencia única de bits que puede transmitirse por el entorno físico de la red. La capa física también determina los aspectos físicos sobre la forma en que el cableado está enganchado a la NIC de la computadora. En la computadora receptora de datos, la capa física es la encargada de recibir la secuencia única de bits (es decir, información formada por 1 y 0). Las subcapas del enlace de datosLa especificación IEEE 802 dividía la capa de enlace de datos en dos subcapas, el Control Lógico del Enlace ( Logical Link Control o LLC) y el Control de Acceso al Medio ( Media Access Control o MAC). La subcapa de Control Lógico del Enlace establece y mantiene el enlace entre las computadoras emisora y receptora cuando los datos se desplazan por el entorno físico de la red. La subcapa LLC también proporciona Puntos de Acceso al Servicio ( Service Access Point o SAP), que no son más que puntos de referencia a los que otras computadoras que envíen información pueden referirse y utilizar para comunicarse con las capas superiores del conjunto de protocolos OSI dentro de un determinado nodo receptor. La especificación IEEE que define la capa LLC es la 802.2. La subcapa de Control de Acceso al Medio determina la forma en que las computadoras se comunican dentro de la red, y cómo y dónde una computadora puede acceder, de hecho, al entorno físico de la red y enviar datos. La especificación 802 divide a su vez la subcapa MAC en una serie de categorías (que no son más que formas de acceder al entorno físico de la red), directamente relacionadas con la arquitectura específica de la red, como Ethernet y Token Ring. La capa de enlace de datos está compuesta por dos subcapas: la subcapa LLC y la subcapa MAC: Protocolos de red del mundo realDespués de repasar el modelo teórico que determina la forma en que los datos van de una computadora a otra dentro de una red, pasando por las distintas capas que conforman el modelo OSI, podemos pasar a explicar algunos de los conjuntos de protocolos de red más utilizados hoy en día y cotejar las capas que los integran con las del modelo OSI. De esta forma, lograremos una visión clara y sencilla del modo en que operan estas pilas de protocolos reales y de la forma en que transportan los datos por la red. También veremos qué protocolos de un determinado conjunto participan en la capa de red del modelo OSI. Estos protocolos son de suma importancia ya que contribuyen a encaminar los paquetes en una conexión entre redes. NetBEUINetBeUI ( NetBios Extended User Interface o Interfaz Ampliada de Usuario para NetBIOS) e sun protocolo de red rápido y sencillo que fue diseñado para se utilizado junto con el protocolo NetBIOS ( Network Basic Input Output System o Sistema Básico de Entrada/Salida para Red) desarrollado por Microsoft e IBM para redes pequeñas NetBEUI opera en las capas de transporte y red del modelo OSI. Puesto que NetBEUI sólo proporciona los servicios que se requieren en las capas de transporte y red de OSI, necesita funcionar con NetBIOS, que opera en la capa de sesión del modelo OSI, y se encarga de establecer la sesión de comunicación entre las dos computadoras conectadas a la red. Las redes Microsoft incluyen además otros dos componentes: el redirector y el Bloque de Mensajes del Servidor ( Server Message Block ). El redirector opera en la capa de aplicación y hace que una computadora cliente perciba todos los recursos de la red como si fueran locales. El Bloque de Mensajes del Servidor ( Server Message Block o SMB), por su parte, proporciona comunicación de mismo nivel entre los redirectores incluidos en las máquinas cliente y servidor de la red. El Bloque de Mensajes del Servidor opera en la capa de presentación del modelo OSI. Aunque resulta un excelente protocolo de transporte de bajo coste, NetBEUI no es un protocolo que pueda encaminarse por medio de routers, por lo que no puede utilizarse en las interconexiones de redes. Por tanto, si bien NetBEUI es una opción de protocolo de red para redes pequeñas y sencillas, no resulta válida para redes más amplias que requieren el uso de routers . Un apunte sobre direcciones hardware Las direcciones NIC de hardware también se denominan direcciones MAC . Esta sigla procede de la expresión ingles Media Access Control o Control de Acceso al Medio, y es una de las subcapas de la capa de enlace de datos. Las direcciones de hardware están grabadas en los chips de la memoria ROM en las tarjetas de interfaz para red y cada una de ellas proporciona una dirección única. El esquema de direccionamiento lo desarrolló en su día el Instituto de Ingenieros Eléctricos y Electrónicos (IEEE). De acuerdo con este esquema, cada dirección reviste la forma de una cadena de 48 bits escrita en formato hexadecimal. Un ejemplo de dirección MAC sería 00-00-B3-83-B3-3F. Tramas Ethernet La trama Ethernet que utilizaban las primeras versiones de NetWare de Novell (NetWare 2.x y 3.x) se creó antes de que el IEEE completara sus especificaciones. Esto hace que el tipo de trama Ethernet 802.3 no se ajuste estrictamente a las normas que ha dictado el IEEE. Las versiones más recientes de NetWare y de otros sistemas operativos de red utilizan la trama Ethernet 802.2, que cumple con todos los requisitos especificados por el IEEE. TCP/IPA menudo referido como el “protocolo de baja puja”, TCP/IP se ha convertido en el estándar de-facto para la conexión en red corporativa. Las redes TCP/IP son ampliamente escalables, por lo que TCP/IP puede utilizarse tanto para redes pequeñas como grandes. TCP/IP es un conjunto de protocolos encaminados que puede ejecutarse en distintas plataformas de software (Windows, UNIX, etc.) y casi todos los sistemas operativos de red lo soportan como protocolo de red predeterminado. TCP/IP consta de una serie de protocolos “miembro” que componen de hecho la pila TCP/IP. Y puesto que el conjunto de protocolos TCP/IP se desarrolló antes de que terminara de desarrollarse el modelo de referencia OSI, los protocolos que lo conforman no se corresponden perfectamente con las distintas capas del modelo. TCP/IP es un amplio conjunto de protocolos que utiliza una serie de protocolos miembro en varias de las capas del modelo OSI. Correlación entre el conjunto de protocolos TCP/IP y las capas OSI: La siguiente tabla describe los protocolos que aparecen en la figura: TCP/IP no sólo proporciona un amplio conjunto de características referidas a la conexión en red (lo cual significa que TCP/IP requiere de una gran carga general para ejecutarse) sino también un sistema de direccionamiento lógico y único. Cualquier usuario que se conecte a Internet estará familiarizado con las direcciones IP de 32 bits, que normalmente se escriben en 4 octetos (un octeto equivale a 8 bits de información). El formato de una dirección es del tipo 129.30.20.4, donde cada uno de los cuatro valores decimales separados por un punto representa 8 bits de información binaria. Especificaciones 802 del IEEE Las especificaciones IEEE 802 proporcionan categorías que definen la Capa del Enlace Lógico así como las distintas arquitecturas de red que puede utilizar la subcapa MAC. A continuación se incluye el listado de las categorías 802: Orígenes de TCP/IP TCP/IP lo desarrolló la Agencia de Defensa de Proyectos Avanzados de Investigación (DARPA) a petición del Departamento de Defensa de Estados Unidos. Dicho departamento necesitaba un conjunto de protocolos que pudieran utilizarse en cualquier sistema operativo, ya que no existía uniformidad alguna entre los sistemas informáticos de sus oficinas. Y ello por la forma misma en que funcionaba el Departamento, que licitaba todos sus proyectos y servicios. De ahí que de forma coloquial se conozca TCP/IP como protocolo de baja puja, ya que surgió a raíz de la práctica del gobierno estadounidense por pujar. IPX/SPXIPX/SPX ( Internetwork Packet Exchange/Sequenced Packet Exchange o Intercambio de Paquetes entre Redes/Intercambio Secuenciado de Paquetes) es un conjunto de protocolos de red desarrollado por Novell para ser utilizado en su sistema operativo de red NewWare. IPX/SPX agrupa menos protocolos que TCP/IP, por lo que no requiere la misma carga general que TCP/IP necesita. IPX/SPX puede utilizarse tanto en redes pequeñas como grandes y también permite el encaminamiento de datos. Correlación entre la pila IPX/SPX y las capas del modelo OSI: IPX/SPX es un conjunto de protocolos eficaz que se utiliza tanto en redes grandes como pequeñas. Descripción breve de cada uno de los protocolos que lo componen: Lo que más nos interesa acerca de IPX/SPX es la forma en que debe encaminarse este conjunto de protocolos dentro de una conexión entre redes. AppleTalkAunque muchos administradores de red no consideran AppleTalk un protocolo de red corporativo o de interconexión, AppleTalk permite el encaminamiento de datos mediante routers . De hecho, con el tipo apropiado de NIC (los Macintosh de Apple pueden conectarse a una red Ethernet si cuentan con tarjetas EtherTalk u otro tipo de adaptadores) AppleTalk puede soportar arquitecturas Ethernet, Token Ring y FDDI. Las computadoras Macintosh suelen utilizarse en los entornos empresariales para la manipulación de gráficos y otras tareas de tipo multimedia, por lo que no resulta nada descabellado incluir AppleTalk como otro protocolo encaminado a la red corporativa. AppleTalk es una arquitectura de red, pero lo cierto es que también se trata de un conjunto de protocolos. Correlación entre los protocolos que integra AppleTalk y las capas del modelo OSI: AppleTalk es un conjunto de protocolos encaminados para las redes Macintosh que pueden comunicarse con rede Ethernet, Token Ring y FDDI. La siguiente tabla describe brevemente cada uno de estos protocolos: Bibliografía ::blyx::Blog::Toni de la Fuente:: Lenguajes de marca o etiqueta. Características y funcionalidades. SGML, HTML, XML y sus derivaciones. Lenguajes de script.Lenguajes de Marca o EtiquetaLa idea de separar en un documento el contenido del formato apareció por primera vez en los años 60. Empieza a utilizarse el concepto de “marca” o “etiqueta” como texto que se añade a los datos y que proporciona información sobre ellos. Un lenguaje de marca seria un modo formalizado de proporcionar estas marcas. La definición de un lenguaje de marca debe proporcionar: Qué marcas están permitidas. Qué marcas son obligatorias. Como se distinguen las marcas del texto. El significado de cada marca. El primer modo de utilización de marcas que se exploró fueron las marcas procedurales. Se preocupaban sobre todo de la presentación y del formato del texto. Con las marcas procedurales se instruye al agente en qué hacer con el texto y especifica como procesarlo. Entre los lenguajes de marca procedurales se encuentran RTF y PostScript. A finales de los años 60 la GCA, Graphic Communication Association, crea un comité de estudio que llega a la conclusión de que las marcas deberían ser más descriptivas que procedurales y deberían tener en cuenta la estructura del documento. Las marcas descriptivas, también llamadas generalizadas, además de identificar la estructura del documento se centran en qué es el texto y no especifican los procedimientos que hay que aplicarle. A finales de los 60 IBM retoma los trabajos de la GCA y crea un lenguaje de marca al que llamaron GML (Generalized Markup Language, que además coincide con las iniciales de sus creadores Goldfarb, Mosher y Lorie). GML estaba basado en las ideas de codificación genérica, pero en vez de tener un simple esquema de etiquetas, introduce el concepto de tipo de documento formalmente definido con una estructura explícita de elementos anidados. A finales de los 70 ANSI crea un comité para elaborar una norma basada en el lenguaje GML. El primer borrador aparece en 1980. En 1984 ISO se une al grupo de trabajo de ANSI y en1986 SGML (Standard Generalized Markup Language) se convierte en estándar internacional (ISO 8879/1986). “SGML es el estándar internacional para la definición de métodos de representación de textos en forma electrónica, independientes del dispositivo e independientes del sistema”. SGML, más que un lenguaje, es un metalenguaje para la definición y la estandarización de la estructura de los documentos. Define una gramática con la que se pueden crear otros lenguajes de marca. Los lenguajes de marca específicos escritos de acuerdo con los requerimientos de SGML se llaman aplicaciones SGML. Uno de esos lenguajes derivados de SGML es HTML, Hypertext Markup Language. En 1989, Tim Berners-Lee creó una propuesta de un sistema de documentos hipertexto para ser usado en la comunidad del CERN (Conseil Européen pour la Recherche Nucléaire). Este sistema al que denominó posteriormente “The World Wide Web” perfilaba los componentes básicos necesarios que definen la WWW hoy: Ser independiente del sistema. Ser capaz de utilizar muchos de los recursos de información existentes y permitir añadir de una manera sencilla nueva información. Contar con un mecanismo de transporte de los documentos entre diferentes redes (HTTP). Contar con un esquema de identificación para el direccionamiento de documentos de hipertexto tanto local como remoto. Berners-Lee definió y desarrolló el lenguaje HTML, usando SGML, durante el desarrollo del primer navegador Web. En 1991 puso el código y las especificaciones del sistema, incluyendo HTML, en Internet. Rápidamente empezó a haber navegadores para una amplia variedad de plataformas y según crecía el número de implementaciones, así crecía la variedad de las mismas. El HTML especificado inicialmente (versión 1) se desarrolló más allá de su forma inicial sin que se hubiera creado todavía un estándar real. HTML 2.0 se convirtió en 1995 en el estándar oficial del lenguaje y fue considerado, en general, una decepción. En aquel año ya estaba en el mercado la versión 2.0 de Navigator, el navegador de Netscape, con implementaciones de marcos y referencias multimedia, muy por encima de las restricciones que imponía el estándar. La especificación estándar de HTML llegó a ser la versión 4. Sin embargo pronto se puso de manifiesto, sobre todo después de la guerra de etiquetas entre Microsoft y Netscape y con la llegada de las nuevas aplicaciones Web, que eran necesarios medios más flexibles y mejores que los proporcionados por HTML para describir los datos. En 1996, el W3C, patrocinó un grupo de expertos en SGML para definir un lenguaje de marca con la potencia de SGML y la simplicidad de HTML. Se eliminaron las partes más crípticas de SGML y lo considerado no esencial. El resultado fue la especificación de XML. XML es un subconjunto de SGML y al igual que él, más que un lenguaje de marca es un metalenguaje, es un instrumento para crear otros lenguajes de marca. Características y FuncionalidadesPodemos considerar dos grupos en los lenguajes de marca: los procedurales y los descriptivos o generalizados. Los lenguajes de marca procedurales se caracterizan por: Contener instrucciones claras para el programa visualizador o impresor sobre la forma en que se debe mostrar el contenido del documento, con un determinado estilo y formato. Dependencia de las instrucciones de formateado del medio de presentación, de forma que el documento final, el formado por el contenido original más las marcas, no es portable a otros medios. Por otra parte, los lenguajes de marca generalizados se caracterizan por: Marcar las estructuras del texto: las marcas se dirigen más a describir el contenido del documento, su estructura lógica, que a expresar instrucciones detalladas de formateado. Separar la estructura del documento de su aspecto. Ser independientes del software que procesa el documento. Facilitar el procesamiento automático de la extracción de la información contenida en los documentos. Facilitar la generación de visualizaciones y presentaciones del documento: permiten la presentación de diferentes vistas del documento dependiendo del dispositivo de visualización y de las preferencias del usuario. Utilizar hojas de estilo: la especificación de cómo presentar las estructuras lógicas, aunque a veces es implícita, generalmente es explícita mediante hojas de estilo del documento. Tener soporte para lenguajes de script: permiten el uso de lenguajes de script para añadir funcionalidades no proporcionadas por el lenguaje, generalmente comportamiento interactivo y dinámico. Proporcionar medios para determinar si un documento es válido o no, es decir si se ajusta a las reglas gramaticales del lenguaje. En cuanto a las funcionalidades de los lenguajes de marca generalizados se pueden citar: Permiten la publicación de documentos electrónicos en Web. Enlace de información a través de enlaces hipertexto, permiten recuperar la información on-line. Inclusión de otras aplicaciones, como hojas de cálculo, música, … etc en un documento. Diseño de formularios para transacciones electrónicas con otros servicios remotos. SGMLDos principios significativos que caracterizan a SGML son: Independencia de la representación de la información respecto de los programas y sistemas que la crean y procesan. De aquí su énfasis en las marcas generalizadas o descriptivas, y no procedurales. Con la utilización de marcas descriptivas el mismo documento se puede procesar por software diferente, aplicando diferentes instrucciones de procesamiento a cada una de las partes del documento o el mismo procesamiento a diferentes partes. Cubrir la necesidad de tener más de una representación para la misma información: una representación abstracta o lógica de la información y una representación de almacenamiento. SGML proporciona una herramienta conceptual para el modelado de información estructurada, el documento, y una notación para la representación de estos documentos. Los documentos se representan por medio de tres constructores básicos: Los elementos : son bloques estructurales, vistos desde un punto de vista lógico o abstracto, que forman parte del documento (por ejemplo cabeceras, párrafos, tablas, enlaces de hipertexto) y contienen datos, otros elementos subordinados o ambas cosas al mismo tiempo. Los atributos : son propiedades asociadas con un tipo de elemento dado cuyo valor describe a un elemento de ese tipo pero que no forman parte de su contenido (por ejemplo la longitud de una tabla). Las entidades : son unidades de almacenamiento virtual que contienen una parte del documento o varias partes o el documento completo. SGML define una entidad como una cadena de caracteres que puede ser referenciada como una unidad. Para obtener la representación abstracta de la información SGML permite definir la estructura de un documento basándose en la relación lógica entre sus partes. Para marcar esta estructura del documento, en vez de considerar un simple esquema de marcas, introduce los conceptos de tipo de documento y definición de tipo de documento (DTD). Lo mismo que cualquier otra clase de objeto procesada por un ordenador, SGML considera que los documentos tienen tipos. El tipo de documento se define formalmente por su estructura lógica, por las partes que lo constituyen. Una definición de tipo de documento es una especificación formal de la estructura de elementos anidados que componen un documento SGML. La distinción entre la representación lógica y la representación de almacenamiento de la información es inherente al procesamiento de la información (si un documento se representa mediante un único volumen o como varios más pequeños es independiente de su representación lógica; y por otra parte hay propiedades del documento, como su localización, que no se ven afectadas por un cambio en su estructura lógica). El proporcionar una representación de almacenamiento de la información entra en conflicto con la independencia respecto al sistema. Para resolver este conflicto SGML proporciona la estructura de entidades del documento como un sistema de almacenamiento virtual que interpone entre la estructura de elementos y el sistema real de almacenamiento (el sistema de ficheros, la BD o cualquiera que sea el sistema real). Como todo lenguaje SGML tiene una sintaxis. El estándar impone unas reglas formales para distinguir las marcas del contenido del documento, y para el uso y colocación de las marcas. Estas reglas constituyen la sintaxis de SGML. Pero el estándar no impone nombres particulares para los elementos del documento, sólo el concepto de elemento (en realidad se le denomina tipo de elemento), ni impone ninguna estructura particular, sólo una definición formal de la estructura cualquiera que sea ésta. Tipos de sintaxis de SGML. Sintaxis concreta de referencia.En SGML se han definido dos tipos de sintaxis: Una sintaxis abstracta , que en términos de conceptos abstractos, tales como los roles de los delimitadores y de los conjuntos de caracteres, define como se deben construir las marcas SGML. Una sintaxis concreta , que define como se han codificado estos conceptos abstractos en una clase específica de documentos SGML. Dentro del estándar ISO de SGML se ha establecido formalmente una sintaxis concreta particular, las sintaxis concreta de referencia. La definición formal es la siguiente: Contiene 8 subcláusulas que definen: Los números decimales de los códigos que se van a ignorar porque son caracteres de control (SUNCHAR). El conjunto de caracteres de la sintaxis, que consiste en la declaración del conjunto base de caracteres (BASESET), seguido de cómo se van a usar estos caracteres para definir la sintaxis correcta (DECSET). El conjunto base de caracteres es el definido en ISO 646 y el DECSET muestra que los 128 caracteres, empezando en la posición 0 de la lista, deberían asociarse a posiciones idénticas en la sintaxis concreta de referencia. Los códigos que representan caracteres de función requeridos por la sintaxis (FUNCTION). Se definen 4 códigos: el fin e inicio de registro, el espacio y el tabulador horizontal. Las reglas que se van a aplicar cuando se definen nombres de elementos, atributos o entidades (NAMING). La sintaxis concreta de referencia sólo permite utilizar los caracteres az, A-Z, 0-9, . (punto) y – (guión) para los nombres y tienen que comenzar con un carácter alfabético.Por defecto SGML suponer que los nombres empiezan con un carácter alfabético, seguido de caracteres alfanuméricos. Las en entradas en LCNMSTR y UCNMSTR permiten indicar qué otros caracteres se van a permitir como carácter inicial de los nombres y las entradas en LCNMCHAR y UCNMCHAR qué caracteres no alfanuméricos se pueden utilizar después del carácter inicial. Las entradas en NAMECASE indican que se permite la sustitución de caracteres en minúsculas por caracteres en mayúsculas en los nombres de elementos y marcas relacionadas, pero no en los nombres de entidad, es decir, los nombres de entidad son sensibles al uso de mayúsculas y minúsculas. Los delimitadores de marcas que se van a usar en el documento (DELIM), en este caso, al igual que en las dos subcláusulas siguientes, son los que proporciona SGML por defecto. Algunos de los delimitadores de marca generales proporcionados por defecto se muestran en la tabla siguiente. Las palabras reservadas utilizadas en las declaraciones de marcas (NAMES). El “conjunto cantidad” (quantity set) requerido por el documento (QUANTITY). Delimitadores de la sintaxis concreta de referencia: Estructura de elementosMarcado genérico. El rol de la DTD. El principio de marcado genérico o lógico consiste en marcar la estructura de un documento y comprende dos fases: La definición del conjunto de marcas que identifican todos los elementos de un documento, junto con la definición del conjunto de reglas que expresan las relaciones entre los elementos y su estructura (esto es el rol de la DTD). La introducción de las etiquetas en el contenido del documento de acuerdo a las reglas establecidas en la DTD. Varias instancias de documento pueden pertenecer a la misma clase de documento, es decir, tener la misma estructura lógica. Para describir la estructura formal de todas las instancias de documento de una determinada clase se construye la Definición de Tipo de Documento o DTD para esa determinada clase. SGML crea los mecanismos necesarios para describir la estructura de una clase de documentos usando unas declaraciones formales de marcas. Las declaraciones de marcas establecen las marcas que se pueden usar en el documento para delimitar de manera clara y sin ambigüedades su estructura. Las declaraciones SGML tienen en general la forma siguiente: &lt;!Palabra_clave Param Param_asociados&gt; “&lt;!” es el delimitador de apertura de la declaración de marca y “&gt;” es el delimitador de cierre. Un caso especial son las declaraciones de comentarios que son declaraciones que sólo contienen comentarios: Los dos guiones (–) que son los delimitadores de comentario deben ir inmediatamente después del delimitador de apertura de la declaración de marca e inmediatamente antes del delimitador de cierre, sin espacios en blanco entre ellos. Palabra_clave puede ser: DOCTYPE: mediante esta declaración se define el tipo de documento, que asigna el nombre en Param al conjunto de declaraciones. Este conjunto de declaraciones puede venir justo aquí a continuación encerrado entre [], o estar en otro fichero, que se identifica en Param_asociados, o en una combinación de ambos lugares. Este conjunto de declaraciones engloba a la DTD y puede incluir cualquier otra declaración de marcas que sea necesaria. ELEMENT: para declarar un tipo de elemento en la estructura lógica del documento. ATTLIST: para asociar un tipo de elemento con un conjunto de características. Estas características se pueden aplicar a una instancia específica de ese tipo de elemento. ENTITY: entre otras cosas permite declarar una cadena corta de texto que signifique otra cadena más larga, para sustituir la primera cadena por la segunda cada vez que sea referenciada en una instancia de documento. NOTATION: se utiliza para especificar cualquier técnica especial que se necesite cuando se procesa un documento que contiene datos no SGML como por ejemplo un fichero de gráficos. SHORTREF: da nombre a un conjunto de asociaciones entre cadenas cortas de caracteres y marcas. USEMAP: para activar el conjunto de SHORTREF nombrado en Param con los elementos nombrados en Param_asociados. Una DTD se expresa en el metalenguaje definido por el estándar SGML y en ella se define la estructura del documento en términos de los elementos que puede contener y el orden en el que estos elementos se pueden presentar. La DTD asigna un nombre a cada uno de esos elementos estructurales, el identificador genérico, por medio del cual se puede reconocer el rol del elemento. Cuando estos identificadores genéricos se colocan entre los delimitadores de marcas forman las etiquetas utilizadas para identificar el inicio y el final de cada elemento. Una vez definida la DTD se puede marcar la instancia de documento siguiendo las reglas definidas. Se podría marcar un documento sin una DTD formal, simplemente sería necesario un conjunto de etiquetas, pero no se puede comprobar la validez de un documento sin una DTD. Para poder validar un documento es necesario algún modo de asegurar que se ha marcado sin errores y que la estructura es coherente. Para poder cumplir estos objetivos, en la DTD a parte de dar el nombre de los elementos que se pueden utilizar, sus posibles atributos y valores por defecto para estos atributos, se define también el contenido de cada elemento, el orden en que deben ocurrir estos elementos y cuantas ocurrencias pueden tener, el nombre de las referencias de entidad que se pueden utilizar y si se pueden omitir las etiquetas de inicio o final. Los documentos SGML se pueden validar utilizando un software especial de análisis denominado parser SGML. Un parser SGML realiza tres tareas: Valida la estructura y la validez sintáctica de la propia DTD antes de analizar la instancia del documento para garantizar que no hay ambigüedades en la DTD. Analiza la estructura del documento. Comprueba si hay errores de marcado en el documento. Un documento sin errores se dice que ha sido validado. Elementos. Declaración de tipos de elementos. El rol de un elemento SGML depende del contexto en el que se encuentre. Tenemos dos roles principales: Elemento documento base. Es el primer elemento especificado en cualquier documento SGML. Este elemento debe ser formalmente declarado mediante una entrada de declaración de tipo de elemento que tenga el mismo nombre que el de la declaración de tipo de documento. Elementos anidados. Se pueden anidar otros elementos entre las dos etiquetas que identifican los límites del elemento documento base. El nivel de anidamientos que se pueden tener depende de la sintaxis concreta que se esté siguiendo. En la sintaxis concreta de referencia se pueden usar hasta 24 niveles de anidamiento. Una declaración de tipo de elemento tiene la forma: &lt;!ELEMENT nombre_elem n m modelo&gt; donde: nombre_elem: es el nombre del tipo de elemento (su identificador genérico) que lo identifica de manera única. n m: son los modificadores de minimización. Son reglas para incluir u omitir las etiquetas de inicio y fin respectivamente. Un – (guión) indica que la etiqueta (de inicio o fin) es obligatoria y una o (mayúscula o minúscula) que es opcional. modelo: es bien una declaración formal del tipo de datos que puede contener el elemento, bien un modelo de contenido, que muestra que subelementos pueden o deben formar parte del tipo de elemento que se está declarando. Cuando varios tipos de elementos comparten el mismo contenido y modificadores de minimización se puede sustituir el nombre del tipo de elemento por un grupo de nombres que es un conjunto de nombre de tipo de elemento conectados, generalmente por conectores OR, y encerrados entre paréntesis: &lt;!ELEMENT (nombre_elem1 | … | nombre_elemN) n m modelo&gt; Cuando un elemento puede contener subelementos se debe definir el modelo de contenido como un grupo de modelo. Los grupos de modelo son uno o más nombres de tipos de elementos unidos mediante conectores de ordenación y encerrados entre paréntesis. Los conectores de ordenación se utilizan para especificar un orden en un grupo y son: , (coma) conector de secuencia: especifica una secuencia estricta. &amp; conector AND: se permite cualquier orden. | conector OR: puede aparecer uno y sólo uno (o exclusivo). Los grupos de modelo utilizan además indicadores de ocurrencia. Los indicadores de ocurrencia modifican a un grupo o a un elemento individual y son: ?: opcional, 0 ó 1. +: requerido y puede repetirse, 1 ó más. *: opcional y puede repetirse, 0 ó más. La ausencia de uno de estos indicadores implica que el elemento, o grupo de elementos, tiene que ocurrir una vez y sólo una. Los indicadores de ocurrencia tienen una precedencia mayor que los conectores de ordenación. Para indicar en los modelos de grupo un punto en el cual el elemento puede contener texto se utiliza el indicador de nombre reservado, #, seguido de PCDATA (parsed character data). #PCDATA indica que en ese lugar del modelo el elemento contiene texto que ha sido comprobado por el parser SGML para asegurar que se han identificado todas las marcas y las referencias de entidad. Cuando un elemento no tiene subelementos, su contenido se puede declarar como uno de los siguientes tipos de contenidos declarados: RCDATA (replaceable character data): puede contener texto, referencias de carácter (una referencia que es reemplazada por un único carácter) o referencias de entidad que se resuelven en caracteres válidos SGML. CDATA (character data): contiene sólo caracteres válidos SGML. EMPTY: no tiene contenido. Para los elementos con contenido declarado, la palabra clave sustituye al grupo de modelo, incluyendo los paréntesis. Los grupos de modelo se pueden calificar añadiendo listas de excepciones. Hay dos tipos de excepciones: Excepciones de exclusión: identifican elementos que no se pueden utilizar mientras el elemento actual esté sin cerrar. Excepciones de inclusión: definen elementos que se pueden incluir en cualquier punto en el grupo de modelo. Atributos. Declaración de listas de definición de atributos. Un atributo es un parámetro nominado que se utiliza para calificar un elemento. Se introduce en la etiqueta de inicio del elemento. Hay dos partes en la especificación de un atributo, su nombre y su valor, que se unen mediante un indicador de valor (=): &lt;nom_elemen nom_atrib=”valor_atrib”&gt; Los atributos se declaran en declaraciones de listas de definición de atributos. Su formato es: &lt;!ATTLIST elem_asoc definicion_atrb1 …………… definicion_atrbN&gt; donde elem_asoc es el elemento al que se asocia la lista de atributos y cada definicion_atrib es la definición de un atributo. Cada definición de atributo consiste en un nombre de atributo, un valor declarado y un valor por defecto, separados entre ellos por carácter separador. Una valor declarado puede ser un nombre reservado que identifica el tipo de valores que se pueden introducir o una lista de valores de atributo encerrada entre paréntesis. Los nombres reservados que se pueden utilizar para los tipos de valores son: Un valor por defecto es o bien un valor específico o uno de los nombres reservados siguientes: Estructura de EntidadesLa estructura de entidades es un modelo virtual de almacenamiento que permite dividir un documento arbitrariamente para facilitar su gestión. Es virtual porque no hay relación uno a uno entre las entidades y los objetos reales de almacenamiento. La estructura de entidades es independiente de la estructura de elementos. Entidades. Tipos de entidades. SGML define una entidad como una cadena de caracteres que puede ser referenciada como una unidad. Un documento SGML completo es una entidad llamada entidad documento SGML y que pueden contener referencias a otras entidades. Cada entidad embebida en una entidad de documento SGML tiene dos componentes: una declaración de entidad y una o más referencias de entidad. La declaración de entidad define el nombre y el contenido de la entidad. Las referencias de entidad identifican los puntos en los que el contenido se va a introducir en el documento. Hay dos tipos principales de entidades: Entidades generales: contienen datos que van a formar parte de un documento. Se pueden referenciar en la instancia de documento o en el texto de sustitución de las declaraciones de otras entidades generales. Entidades de parámetro: contienen caracteres que se necesitan como parte de alguna declaración SGML. Se pueden referenciar sólo en las declaraciones de marcas. Ambas categorías se pueden subdividir en: Entidades externas: el texto de sustitución está definido por referencia a un identificador SYSTEM (datos que especifican el identificador del fichero, su localización y cualquier otra información que permitan localizar la entidad) o PUBLIC (un literal que identifica texto público, es decir, que es conocido más allá del contexto de un único documento o de un entorno de un sistema), es decir, el texto está almacenado en un fichero separado. Entidades internas: el texto de sustitución está definido dentro del prólogo. En la sintaxis concreta de referencia las referencias de entidad general tienen la forma &amp;nombre_entidad; o simplemente &amp;nombre_entidad si va seguida de un espacio en blanco de un código de final de registro; y las referencias de entidad de parámetro %nombre_entidad; o %nombre_entidad. Entidad documento SGML Una entidad documento SGML tiene tres secciones: Una declaración SGML (opcional). Es la parte de un documento SGML donde se define el esquema de codificación utilizado en su preparación. En la declaración se especifica, entre otras cosas, el conjunto de caracteres que se está utilizando y los caracteres usados para delimitar las marcas. Para los documentos que se transmiten sin la declaración SGML, llamados documentos básicos SGML, se asume que la declaración es la proporcionada por el estándar ISO de SGML para un documento básico típico. Un prólogo de documento . Contiene la definición de las estructuras del documento (la DTD) y otra información relacionada. La indicación de qué DTD se está usando en el documento se hace por medio de una declaración de tipo de documento (declaración DOCTYPE). Una instancia de documento . Es el contenido, más las marcas, del documento. Un ejemplo muy simple de documento SGML es el siguiente: Declaración de entidades Las declaraciones de entidades generales tienen la forma: &lt;!ENTITY nombre_entidad texto_de_sustitución&gt; El texto de sustitución puede tener códigos de marcas, referencias a otras entidades, referencias de carácter, etc. que se interpretan cuando se añade el texto de sustitución al documento. Hay variaciones de esta declaración básica, entre ellas las que permiten especificar: Entidades que contienen sólo datos de carácter que no deberían incluirse en un análisis de marcado. Esto se hace incluyendo la palabra clave CDATA entre el nombre de la entidad y su texto de sustitución. Esto significa que los caracteres dentro de la cadena de sustitución que posiblemente podrían ser interpretados como marcas serán ignorados. Entidades que contienen texto de sustitución específico del sistema. Para ello se incluye la palabra clave SDATA entre el nombre de la entidad y su texto de sustitución. Una entidad por defecto cuyo texto se usará cuando se referencia a una entidad cuyo nombre no se reconoce como una de las entidades declaradas. Esto se hace sustituyendo en la declaración nombre_entidad por la palabra reservada #DEFAULT. Las declaraciones de entidades de parámetro tienen la forma: &lt;!ENTITY % nombre_entidad texto_de_sustitución”&gt; Generalmente el texto de sustitución está formado por una serie de nombres de tipos de elemento separados por conectores de ordenación. Las entidades de parámetro tienen que declararse antes de que sean referenciadas. DSSSLDSSSL (Document Style Semantics and Specification Language) definido en el estándar ISO/IEC 10179:1996 proporciona un mecanismo de propósito general para transformar documentos y para asociar instrucciones de formateado a documentos codificados utilizando SGML, tanto on-line como off-line. DSSSL tiene dos componentes principales: Un lenguaje de transformación. Se utiliza para especificar transformaciones estructurales sobre ficheros fuente SGML, por ejemplo, como se fusionan dos o más documentos, como se generan índices o tablas de contenido, etc. Un lenguaje de estilo. Se utiliza para especificar el formato de una manera independiente de la plataforma. Para hacer posible una implementación limitada del lenguaje de estilo, dentro de él se creó CQL (Core Query Language) y CEL (Core Expression Language), que no contienen ciertas características del lenguaje de estilo designadas como opcionales. HTMLComo ya se ha comentado HTML es una aplicación de SGML que ha sido establecio como estándar internacional (ISO/EIC 15445:2000). HTML se utiliza para la publicación de documentos en Web y para definir la estructura de los elementos y los enlaces entre ellos. Entre las características y reglas básicas de HTML están: Los documentos tienen una estructura bien definida. Los documentos se estructuran utilizando elementos. La mayoría de los elementos son pares de etiquetas: una etiqueta de inicio y una de cierre. Algunos elementos tienen etiquetas de cierre opcionales y otros tienen una única etiqueta de apertura. Las etiquetas tienen la forma &lt;nom_elem&gt; o &lt;nom_elem&gt;&lt;/nom_elem&gt; Las etiquetas deben estar anidadas, no cruzadas. Los elementos pueden tener atributos: &lt;nom_elem nom_atributo=”valor”&gt;. El valor del atributo debería ir entre comillas. Los nombres de elementos no son sensibles al uso de mayúsculas o minúsculas pero sí podría serlo el valor de los atributos. Los navegadores ignoran los elementos o atributos desconocidos. Los navegadores colapsan los espacios en blanco a un único espacio, esto incluye a los tabuladores, saltos de línea y retorno de carro. Estructura de un documento HTMLDe la DTD de cualquier versión de HTML se puede derivar una plantilla básica para un documento HTML: La primera línea, la declaración DOCTYPE, muestra la DTD que se sigue en el documento. Dentro de la etiqueta se incluyen las dos secciones principales que tiene un documento HTML: la cabecera y el cuerpo. La cabecera contiene el título y otra información complementaria sobre el documento. En el cuerpo es donde va el contenido del documento con las marcas asociadas a su estructura y quizá presentación. Todo documento debe contener una etiquete de inicio de documento al principio, y una etiqueta de fin de documento, al final del documento. Contiene un único elemento HEAD y un único elemento BODY. Las etiquetas y marcan el inicio y el final de la cabecera. Las etiquetas y marcan el inicio y fin del cuerpo. El elemento body delimita el contenido del documento y sus atributos se utilizan para efectuar cambios que afecten al documento entero como por ejemplo establecer imágenes o colores de fondo. En el body están incluidos los elementos de bloque estructurado como encabezamientos, párrafos, listas y tablas. Además contiene también elementos de nivel de texto. Sin embargo, aunque la definición de la presentación física de esa estructura lógica debería estar definida en otro sitio (en hojas de estilo por ejemplo), HTML no es un lenguaje de marcado lógico puro y tiene etiquetas físicas que se pueden utilizar para formatear y dar un aspecto determinado al contenido del documento. Elementos de la cabecera del documentoSegún la especificación estricta, los elementos que pueden aparecer en la cabecera son: base, link, meta, script, style y title . El elemento title da un título al documento. Cuando se visualiza el documento en un navegador el título aparece en la barra del mismo. Su sintaxis es: titulo_documento. Todo documento debe tener exactamente un elemento tiltle en la cabecera. El título puede contener texto plano y referencias de carácter. No se permite ninguna otra marca. El elemento style embebe una hoja de estilo en el documento y la cabecera puede contener cualquier número de ellos. Tiene un atributo obligatorio, type, más otros opcionales. El atributo TYPE especifica el tipo de contenido del lenguaje de estilo. Por ejemplo, para CSS (Canscading Style Sheets) el valor del atributo debería ser “text/css”. Las reglas de estilo se especifican entre la etiqueta de inicio y la etiqueta de fin como se fuera una declaración de comentario: El elemento script incluye un script de cliente en el documento. La cabecera puede contener cualquier número de elementos script (se permiten también en el body). Tiene un atributo obligatorio, type , para especificar el tipo de contenido del lenguaje de script, por ejemplo “text/javascript”. Este atributo ha sustituido al atributo language en versiones anteriores, que se mantiene en esta versión como “desaprobado” (deprecated), donde se especifica el nombre del lenguaje. Un script embebido se da como el contenido del elemento script y debe ir como si fuera una declaración de comentario: Mediante el atributo opcional src se permite especificar la localización de un script externo, ya sea local o remoto. En este caso si hubiera un script embebido se ignora. El elemento meta especifica pares nombre-valor para proporcionar metainformación sobre el documento, link especifica una relación entre el documento actual y otros recursos y base proporciona una dirección base para interpretar URLs relativas. Marcas BásicasAntes de comenzar a examinar las principales marcas decir que en HTML 4 se han añadido tres conjuntos de atributos a prácticamente todos los elementos. Un primer conjunto de cuatro atributos, que se usan sobre todo en las hojas de estilo y lenguajes de script: id : asigna un nombre único en el documento al elemento. class : indica la clase o clases a las que pertenece el elemento (usado en hojas de estilo). style : añade información de hoja de estilo directamente en la etiqueta. title : se utiliza para dar un texto de aviso sobre un elemento o su contenido. Se ha añadido también un conjunto de atributos manejadores de eventos para poder añadir código de scripts que se definen en múltiples elementos. Los manejadores de eventos son: onclick, ondblclick, onkeydown, onkeypress, onkeyup, onmousedown, onmousemove, onmouseup, onmouseover y onmouseout . El último grupo está relacionado con los distintos idiomas. Los dos atributos que contempla son dir , que permite indicar la dirección del texto como ltr (de izquierda a derecha) o rtl (de derecha a izquierda); y lang que permite indicar el idioma utilizado. Elementos de nivel de bloque Párrafos y saltos de línea El elemento indica un párrafo. Generalmente el navegador incluye una línea antes y después del párrafo. Como los encabezamientos, tiene un atributo align para indicar el alineamiento. La etiqueta inserta un salto de línea en el documento. es un elemento vacío así que no tiene etiqueta de cierre. Divisiones y texto preformateado La etiqueta se utiliza para estructurar un documento en secciones o divisiones. Tiene el atributo align que puede valer right, left o centre para alinear el texto a la derecha a la izquierda o centrado. Existe una etiqueta que es un alias para el elemento div alineado en el centro: . La etiqueta se utiliza para indicar texto preformateado. El texto entre las etiquetas de inicio y fin de pre conservan todos los espacios en blanco, tabuladores y saltos de línea. Listas HTML tiene tres tipos de listas : Ordenadas : se marcan con y generalmente se presentan como un esquema numerado. No ordenadas : se marcan con . Los navegadores generalmente añaden algún símbolo a cada ítem (un círculo o un cuadrado) y añaden una sangría. De definición : se marcan con . Las listas de definición son listas de pares término-definición, es decir, un glosario. Las listas ordenadas y no ordenadas se pueden anidar y los elementos en la lista se definen utilizando la etiqueta . En las listas de definición cada término se marca con y cada definición con . No se requiere etiqueta de cierre para estos elementos. Las listas ordenadas tienen tres atributos: compact : sugiere al navegador que intente compactar la lista. type : establece el esquema de numeración para la lista. Toma los valores “a” para letras minúsculas, “A” para mayúsculas, “i” para números romanos en minúsculas, “I” para números romanos en mayúsculas y “1” para números (es el valor por defecto). start : indica el valor para comenzar la numeración de la lista. Este valor es numérico incluso aunque se hubiera indicado en type un numeral. Tablas Una tabla se marca con el par de etiquetas . Una tabla está compuesta de filas y las filas de celdas. Cada fila se marca con . Las celdas pueden ser de datos, , o cabeceras, . El número de filas de la tabla lo determina el número de elementos tr que contenga y el número de columnas el máximo entre los números de celdas de cada fila. Por medio de los atributos colspan y rowspan de los elementos de la tabla se pueden indicar celdas que se expandan al número de columnas o filas indicado. Se puede especificar la anchura de la tabla mediante el atributo width de table y el grosor del borde con border . Los atributos cellpadding y cellspacing controlan el espacio entre celdas y el espacio entre el borde de la celda y su contenido respectivamente. Elementos de nivel de texto Los elementos de nivel de texto pueden ser físicos y lógicos. Los físicos especifican como debería presentarse el texto, los lógicos se centran en lo qué es el texto y no en su aspecto. Elementos físicos típicos son: para negrita; para itálica, para subíndice. Elementos lógicos son: para citas (se suele mostrar en itálica&gt;; para énfasis (también se muestra en itálica); para mayor énfasis (se suele mostrar en negrita). Otros elementos y entidades Encabezados Los elementos de encabezamiento se utilizan para crear cabeceras en el contenido del documento. Hay seis niveles desde a . Su principal atributo es align que permite indicar los valores left (por defecto), right, center o justify para alinear el texto a la izquierda, a la derecha, centrado o justificado respectivamente. Imágenes Para insertar una imagen se utiliza la etiqueta con el URL de la imagen en el atributo src . Mediante el atributo alt se puede especificar un texto alternativo que se muestra mientras se está cargando la imagen o si esta no puede cargarse. Con los atributos height y width se dan la altura y la anchura a la imagen; y con hspace y vspace se especifica un espacio horizontal a la derecha y a la izquierda de la imagen, y un espacio vertical encima y debajo de la imagen. También se puede especificar como se alineará el texto alrededor de la imagen. Para esto se utiliza el atributo align con posibles valores top, middle, bottom, left y rigth. Entidades de carácter Para poner caracteres especiales en el texto (letras acentuadas, espacios en blanco que no se colapsen, etc) se utilizan las entidades de carácter. Se referencian mediante &codigo; donde código es un valor numérico o una palabra que indican el carácter real que se quiere poner en el contenido. En la siguiente tabla se dan algunas de estas entidades: FormulariosLos formularios en HTML están contenidos en un elemento form . Un formulario está compuesto de campos y de las marcas necesarias para estructurarlo y controlar su presentación. Cada uno de los campos se identifica de manera única dentro del formulario por el valor en el atributo name o id. Dentro de la etiqueta se deben identificar dos cosas: la dirección del programa que lo va a manejar, por medio del atributo action , y el método que se va a utilizar para pasar los datos a ese programa, mediante el atributo method . En action generalmente se especifica la URL del programa. El valor de method puede ser GET o POST (métodos de HTTP). Los principales campos del formulario incluyen: campos de texto, campos de password, campos de texto multilínea, radio buttons, check box, menús desplagables y botones. Los campos de texto se especifican por medio de una etiqueta (el elemento input no tiene etiqueta de cierre) y el atributo type igual a “text”. El tamaño del campo y el número máximo de caracteres que puede contener establecen con los atributos size y maxlength respectivamente. Además se puede establecer un valor por defecto para el campo con el atributo value . Si el atributo type se pone como “password” tenemos un campo de password en los que no se tiene eco de los caracteres tecleados. Los campos de texto multilínea se definen con el elemento textarea . El número de líneas lo determina el valor del atributo rows y el número de caracteres por línea el atributo cols . El texto por defecto es el contenido del elemento, es decir, el texto que va entre la etiqueta de apertura y de cierre , que debe ser texto sin incluir ninguna marca HTML. En este texto sí se conservan todos los espacios en blanco, saltos de línea y otros caracteres de control. Para crear un menú desplegable, que permitan seleccionar una opción entre varias posibles, se utiliza la etiqueta . Las opciones se indican con elementos option dentro del elemento select. El valor enviado cuando se envía el formulario es el valor entre las etiquetas de inicio y fin de option, pero si se incluye el atributo value , su valor será el que se transmita. El número de opciones mostradas se establece con el atributo size de select que por defecto vale 1. Incluyendo el atributo multiple en select convertimos al menú en una lista en la que se puede elegir más de una opción. La etiqueta input también se utiliza para crear check-boxex y radio buttons, estableciendo el valor del atributo type a checkbox o radio. En elcaso de los radio buttons, todos los que se quiera que estén dentro del mismo grupo, es decir, que cuando se seleccione uno del grupo se elimine la selección anterior dentro del grupo, deben tener el mismo valor en el atributo name. Los botones se definen con el elemento input y type=”button”. No tienen ninguna acción predefinida. Para definir una acción se utilizan los atributos manejadores de evento mencionados con anterioridad y los lenguajes de script. Por ejemplo: Dentro de un campo del formulario también es posible indicar el nombre de un fichero que se quiere cargar en el servidor. Para ello se utiliza el elemento input con type=”file”. Esto crea un campo de texto donde introducir el nombre del fichero y un botón a la derecha del campo generalmente etiquetado como examinar (o browse) que al pulsarlo permite examinar el sistema de archivos local para encontrar el fichero que se quiere enviar al servidor. Una vez que se ha rellenado el formulario necesitamos un medio para decir al navegador que lo envíe, este medio se proporciona de nuevo con el elemento input. Estableciendo type a submit, se crea un botón que al ser pinchado hace que el navegador envíe los datos a la dirección especificada en action. En el atributo value se establece el texto que va a aparecer en el botón y además se envía al servidor. Si el valor de type es reset se crea también un botón pero su función es limpiar el formulario y establecer los valores por defecto que se hayan podido especificar. Direccionamiento y enlacesEn HTML la forma de definir hiperenlaces es mediante la etiqueta (en hipertexto los extremos de los enlaces generalmente se les denomina anclas – anchors) que tiene un atributo href para indicar la dirección del recurso que se quiere enlazar. El contenido entre las etiquetas de inicio y fin del elemento es el hiperenlace que se activa al pinchar en él. El contenido puede ser texto o una imagen. Un posible destino del hiperenlace es una localización dentro del propio documento. Se necesita alguna forma de poder marcar esa localizavión para luego referenciarla en href. También se usa la etiqueta para definir estas localizaciones mediante un uso especial de la misma denominado establecimiento de identificador de fragmento o simplemente marcador. Para establecer un marcador se coloca una etiqueta en la localización y va a ser el valor del atributo name el que determine el nombre simbólico para poder referenciarla. La forma de referenciar esta localización es mediante el valor “#marcador” en el href. Por ejemplo: Esto es el inicio…..Ir a inicio XML y sus extensionesComo ya se ha comentado, XML (Extensible Markup Language) es una recomendación del W3C desde 1998, creada como subconjunto de SGML que aprovecha su potencia y flexibilidad eliminando parte de su complejidad. Como metalenguaje se puede utilizar para la definición de otros lenguajes de marcas. Por ejemplo se ha escrito HTML basándose en XML, dando lugar a la especificación XHTML. Otros lenguajes derivados de XML son WML (Wireless Markup Languages), SOAP (Simple Object Access Protocol) o MathML (Mathematical Markup Language). Los objetivos del diseño de XML, citados en la especificación, eran: Ser directamente utilizable en Internet. Soportar una amplia variedad de aplicaciones. Compatibilidad con SGML. Facilidad de creación de programas que procesen documentos XML. Mantener el número de características opcionales al mínimo, idealmente ninguna. Los documentos deberían ser legibles por humanos y razonablemente claros. El diseño debería ser preparado rápidamente. Diseño formal y conciso. Facilidad de creación de documentos XML. Importancia mínima de la concisión y refinamiento de las marcas. Cada subconjunto de SGML, XML no contempla ciertas características de SGML. Podemos citar entre las diferencias: Respecto a la sintaxis, en XML: Los nombres pueden utilizar caracteres Unicode y no se restringen a ASCII. Se permiten _ y : en los nombres. Los nombres son sensitivos al uso de mayúsculas y minúsculas. El delimitador de cierra de las instrucciones de procesamiento es ?&gt;. No se permiten marcas de inicio sin cerrar. Respecto a la declaración de elementos, atributos y entidades impone ciertas restricciones no presentes en SGML, como son: Las referencias de entidad y de carácter deben terminar en ;. No se permiten referencias a entidades externas de datos en el contenido. En las declaraciones de elementos no se permiten grupos de nombres, ni contenido declarado como CDATA o RDATA, si se pueden utilizar exclusiones o inclusiones. Además no se puede utilizar un grupo de nombres como tipo de elemento y en los modelos de contenido no pueden utilizar el conector AND. En las declaraciones de listas de definición de atributos no se permiten atributos CURRENT, no se puede utilizar un nombre de grupo como elemento asociado y los valores especificados por defecto deben ser literales. Hoy en día XML ha alcanzado un alto grado de utilización por los beneficios que ofrece, entre ellos: Simplicidad: la información codificada en XML el legible visualmente y puede ser procesada sin dificultad por los ordenadores. XML es fácil de entender, implementar y usar. Aceptabilidad de uso para transferencia de datos: XML no es un lenguaje de programación, es un forma estándar de poner la información en un formato que pueda ser procesado e intercambiado por diversos dispositivos de hardware, SO, aplicaciones de software y la Web. Uniformidad y conformidad: cuando se quiere integrar dos aplicaciones, la organización y los expertos técnicos deben decidir si se integran los sistemas o se modifica la arquitectura de las aplicaciones. Si los datos de ambas aplicaciones están de acuerdo en el formato y se pueden transformar fácilmente de uno a otro, los costes de desarrollo se pueden reducir. Si este formato común se puede basar en un estándar ampliamente aceptado entonces las interfaces entre aplicaciones se hacen menos costosas. Separación de los datos de su visualización: sin las separación de datos la reutilización de los mismos en múltiples intefaces de usuario sería difícil. Extensibilidad: como se deduce de su nombre, XML fue diseñado desde el principio para permitir extensiones. Podemos considerar dos ámbitos fundamentales de aplicación de XML: Compartir información: el problema principal a la hora de compartir información entre dos organizaciones cualesquiera es la interfaz entre ellas. Los beneficios de tener un formato común para compartir información entre dos organizaciones cualesquiera son obvios. Sobre XML se han construido tecnologías y estándares. Consorcios y organizaciones empresariales han desarrollado formatos y estandarizaciones XML. En diciembre de 2000 la UN/CEFACT y OASIS se unieron para iniciar un proyecto para estandarizar especificaciones XML para negocios. La iniciativa, llamada XML para negocio electrónico (Electronic Business XML, ebXML) desarrolló un marco técnico que permite utilizar XML para todo el intercambio de datos de negocio electrónico. Almacenamiento, transmisión e interpretación de los datos: si el almacenamiento de la información se puede adaptar a varios medios, éste se dirigirá al medio que tiene menor coste. XML está basado en texto, que obviamente es admitido por todos los medios de almacenamiento, y su coste de almacenamiento es barato comparado con los necesarios para el almacenamiento de los gráficos. Al igual que el coste de almacenamiento de estos objetos basados en texto es barato, también lo es su transmisión. Además como es comúnmente aceptado, al adherirse a estándares internacionales, puede ser fácilmente interpretado. La recomendación XML viene acompañada de otras especificaciones como son XLS, para la creación de hojas de estilo, XLink y XPointer para la creación de enlaces con otros recursos. Documentos XMLDocumentos válidos y documentos bien formados XML distingue entre documentos bien formados y válidos. Un documento es un documento XML bien formado si: Contiene uno o más elementos. Tiene exactamente un elemento raíz (o elemento documento) caracterizado por que ninguna de sus partes aparece en el contenido de ningún otro elemento, es decir, debe contener un par único de etiquetas de apertura y de cierre que contengan al documento completo. Para el resto de elementos si su etiqueta de inicio están en el contenido de otro elemento su etiqueta de cierre debe estar en el contenido del mismo, o lo que es lo mismo, los elementos deben estar anidados apropiadamente sin permitir cruces entre estos anidamientos. Obedece todas las restricciones de forma (web-formedness constraints) dadas en la especificación XML. El contenido de todas las entidades analizables (parsed entities) está bien formado. Una entidad analizable es aquella cuyo contenido se considera parte integrante del documento. Una entidad no analizable es un recurso cuyo contenido puede o no se texto, y si es texto puede ser otro que no sea XML. Algunas de las restricciones son las siguientes: Todos los elementos, menos los vacíos, deben llevar etiquetas de apertura y cierre. Los elementos vacíos, sin contenido, se marcarán con la etiqueta &lt;elem_sin_cont/&gt; Los valores de atributos van siempre entre comillas simples (‘) o dobles (“) Los nombres son sensibles al uso de mayúsculas y minúsculas. Deben comenzar por una letra, _ o :. Según se define en la especificación, un documento es documento XML válido si: Tiene asociada una declaración de tipo de documento. Cumple todas las restricciones expresada en la declaración de tipo de documento. Para comprobar la validez de los documentos, o si están bien formados, se utilizan los procesadores XML, que son módulos de software que leen el documento XML y proporcionan acceso a su contenido y estructura a la aplicación XML para la que el procesador está haciendo el análisis. En la especificación se distingue entre procesadores con validación y sin validación. Ambos deben informar de las violaciones a las restricciones de la especificación en la entidad documento y en cualquier otra entidad analizable que puedan leer. Los procesadores con validación deben además informar de las violaciones a las restricciones expresadas en las declaraciones de la DTD y a las restricciones de validación dadas en la especificación. Componentes de un documento XML Un documento XML consta de un prólogo que contiene toda la información relevante del documento que no sean marcas ni contenido, y el cuerpo del documento que contiene al menos el elemento raíz. El prólogo contiene: Una declaración XML. Es la sentencia que declara al documento como un documento XML. Una declaración de tipo de documento. Enlaza el documento con su DTD o el DTD puede estar incluido en la propia declaración o ambas cosas al mismo tiempo. Uno o más comentarios e instrucciones de procesamiento. Según la especificación el prólogo debe ir antes del primer elemento del documento pero puede estar vacío, es decir, todas las componentes nombradas anteriormente son opcionales, no son necesarias para tener un documento XML bien formado. La declaración de documento es una instrucción de procesamiento (las instrucciones de procesamiento permiten incluir en un documento instrucciones para aplicaciones propietarias; sus delimitadores de inicio y fin de etiqueta son &lt;? y ?&gt; respectivamente, tiene la forma: &lt;?xml version=”version_xml” encoding=”tipo_codificacion” standalone=”XX” ?&gt; Donde: version: describe la versión de XML que se está usando. Puede ser “1.0” o “1.1”. encoding (opcional): permite especificar la codificación de caracteres que se está usando. Por ejemplo UTF-8. standalone (opcional): le dice al procesador XML si el documento puede ser leído como un documento único o si es necesario buscar fuera del documento por otras reglas. Puede valer NO o YES. La declaración tipo de documento contiene un nombre que debe coincidir con el nombre del elemento raíz. Opcionalmente contiene una DTD interna, referencia a una DTD externa o ambas cosas al mismo tiempo: &lt;!DOCTYPE elemen_raiz declaracion_externa [dtd_interna]&gt; Las DTD externas pueden ser públicas o privadas. Las públicas se identifican por la palabra clave PUBLIC y tienen la forma siguiente: &lt;!DOCTYPE elemen_raiz PUBLIC “nombre_DTD” “localización_DTD”&gt; Las privadas se identifican por la palabra clave SYSTEM y tienen la forma: &lt;!DOCTYPE elemen_raiz SYSTEM “localización_DTD”&gt; Espacios de nombres en XMLNos podemos encontrar aplicaciones XML en las que un documento contiene elementos y atributos definidos en varios lenguajes. En varios contextos, sobre todo en la Web debido a su naturaleza distribuida, esto podría presentar un problema de colisión y reconocimiento de marcas para el software que los procesa. Se requieren entonces mecanismos que permitan tener con nombres universales cuyo alcance se extienda más allá de los documentos que los contienen. En XML este mecanismo son los espacios de nombres (XML namespaces). Un espacio de nombres XML es un conjunto de nombres referenciados por una referencia URI, que se usan en XML como nombres de elementos y atributos. Los nombres de elementos y atributos pueden aparecer entonces como nombres cualificados que consisten en un prefijo y una parte local separadas por : (dos puntos). El prefijo, que tiene una correspondencia con la referencia URI, selecciona un espacio de nombres. Un espacio de nombres se declara como un atributo. El valor del atributo, una referencia URI, es el nombre del espacio de nombres. El nombre del atributo debe comenzar por el prefijo xmlns: o xmlns. En este último caso el nombre del espacio de nombres es el del espacio de nombres por defecto que exista en el contexto del elemento donde se está incluyendo la declaración. Si el prefijo es xmlns: el nombre puede ser cualquier nombre válido XML. Un ejemplo de declaración que asocia el prefijo edi a http://ecomerce.org.schema es: &lt;nom_elem xmlns:edit=’http://ecomerce.org.schema&#39;&gt;……….&lt;/nom_elem&gt; XSLBásicamente XSL es un lenguaje para la expresión de hojas de estilo. Una hoja de estilo XSL es un fichero que describe como mostrar un documento XML de un tipo dado. Se basa en especificaciones anteriores como DSSSL y CSS, aunque es más sofisticado. XSL Contiene tres especificaciones: XSLT, XSL Transformation, un lenguaje de transformación de documentos XML. Su intención inicial era permitir operaciones de estilo más complejas que las contempladas inicialmente tales como la generación de tablas de contenido o índices, pero ahora se utiliza como un lenguaje de procesamiento de XML de propósito general. Las hojas de estilo XSLT se utilizan por ejemplo para la generación de páginas (X)HTML a partir de datos XML. XSL-FO, XSL Formating Objects, un vocabulario para la especificación de formateado. XPath, XML Path Language, un lenguaje utilizado por XSLT para referenciar o acceder a partes de un documento XML. También se usa en la recomendación de XLing, XML Linking Language. XSL especifica como dar estilo a un documento XML utilizando XSLT para describir como el documento se transforma en otro documento XML que utiliza el vocabulario de formateado. XSLT Cada documento bien formado XML se puede considerar un árbol cuyos nodos son los elementos y su contenido. Para los propósitos de XSL también deben considerarse nodos los atributos, las estructuras de procesamiento y los comentarios. Básicamente un procesador XML/XSLT toma como fuente un documento XML y construye un árbol denominado árbol fuente y construye otro árbol, el árbol resultado, a partir del árbol fuente. En este proceso puede reordenar nodos, duplicarlos y añadir nuevos objetos de elementos o texto. Los nodos del árbol resultado son objetos de flujo a los que se les ha aplicado estilo. Después se interpreta el árbol resultado para obtener los resultados formateados para su presentación. Una transformación expresada en XSLT describe las reglas para transformar un árbol fuente en uno resultado y se la llama hoja de estilo pues cuando XSLT realiza una transformación al vocabulario de formateado de XSL (XSL-FO), la transformación funciona como una hoja de estilo. Una hoja de estilo contiene un conjunto de reglas de plantilla , que dan las reglas para la transformación. Una regla de plantilla tiene dos partes; un patrón, que se compara con los nodos del árbol fuente y una plantilla de la que se crea un ejemplar para que forme parte del árbol resultado. Los patrones son expresiones XSL, que utilizan la sintaxis de XPath, cuya evaluación va a dar un conjunto de condiciones. Los nodos que cumplen las condiciones son los que se eligen del árbol de entrada. Las plantillas pueden contener elementos que especifican un literal para incluirlo en el árbol resultado o elementos XSLT que son instrucciones que crearán un fragmento del árbol. Cuando se crea un ejemplar de una plantilla cada instrucción es ejecutada y reemplazada por el fragmento de árbol resultado que crea. Hay elementos que seleccionan y procesan los elementos del árbol fuente que son descendientes del elemento actual (xsl:apply-templates/). La construcción del árbol resultado comienza encontrando la regla de plantilla para el nodo raíz y creando una instancia de su plantilla. Un ejemplo de regla de plantilla es el siguiente: &lt;xsl:template match=”/“&gt; xsl:apply-templates/&lt;/xsl:template&gt; El valor de match es el patrón de la regla de pantilla (/ se corresponde con el nodo raíz) y lo que va entre la apertura y cierre de template es la plantilla. Una hoja de estilo se representa en un documento XML por un elemento stylesheet que contienen a los elementos template que especifican las reglas de plantilla, además de otros elementos. El elemento stylesheet debe tener un atributo versión y puede tener como atributos prefijos de extensiones de elementos, como por ejemplo los del espacio de nombres. Por ejemplo: &lt;xsl:stylesheet version=”1.0” xmlns:xls=http://www.w3.org/1999/XLS/Transform&gt;………&lt;/xsl:stylesheet&gt; Algunos de los elementos XSLT son: xsl:apply-templates: mediante esta instrucción se le dice al procesador que compare cada elemento hijo del nodo actual con el patrón de las reglas de plantilla en la hoja de estilo y si encuentra uno que se ajuste cree un ejemplar de la plantilla de la regla. Puede llevar un atributo select para seleccionar por medio de una expresión los nodos que se van a procesar. xsl:value-of: tiene un atributo select donde se incluye una expresión. La instrucción crea un nodo de texto en el árbol resultado con la cadena de texto que se obtiene al evaluar la expresión de select y convertir el objeto resultante a una cadena. xsl:if: el contenido del elemento es una plantilla y tiene un atributo test con una expresión que se evalúa y se convierte el resultado a booleano. Si es cierto se crea una instancia de la plantilla en el contenido, en otro caso no se hace nada. xsl:element: permite crear un elemento. Tiene un atributo nombre donde se especifica el nombre del elemento. El contenido es una plantilla para los atributos y los hijos del elemento creado. XSL-FO XSL-FO especifica un vocabulario para el formateado a través de los llamados objetos de formateado. Utiliza y extiende el conjunto de propiedades comunes de formateado desarrollado conjuntamente con el grupo de trabajo CSS&amp;FP (Cascading Style Sheet and Formating Property). Cuando un árbol de resultado utiliza este conjunto estandarizado de objetos de formateado, un procesador XSL procesa el resultado y obtiene la salida especificada. Algunos objetos de formateado comunes son: Page-sequence: una parte principal en la que el esquema de diseño (layout) de la página básica puede ser diferente del de otras páginas. Flow: una división en una secuencia de página como puede ser un capítulo o una sección. Block: como por ejemplo un título, un párrafo, etc. Inline: por ejemplo un cambio de fuente en un párrafo. Wrapper: un objeto intercambiable, puede utilizarse como block o como inline. Se utiliza sólo para manejar propiedades heredables de los objetos. Graphic: referencia un objeto gráfico externo. Table-FO: similares a los modelos de tablas HTML. List-FO: son listas de objetos. Por ejemplo list-item, list-block, list-item-body. Algunas propiedades básicas de formateado son: Propiedades relativas a márgenes y espaciado. Propiedades de fuentes. Sangría. Justificación. XPath El principal objetivo de XPath es el poder dirigirse a partes de un documento XML. Además también se ha diseñado para que pueda utilizarse para comprobar si un nodo se ajusta a un patrón. El constructor sintáctico básico de XPath es la expresión, que al evaluarse proporciona un objeto de uno de los siguientes tipos básicos: Un conjunto de nodos. Un valor booleano. Un número (de coma flotante). Una cadena. Una clase de expresión es un path de localización. El resultado de evaluar la expresión es un conjunto de nodos que contiene los nodos seleccionados por el path de localización. Hay dos tipos de paths de localización: Relativos: consisten en uno o más pasos de localización separados por /. Los pasos se componen de izquierda a derecha. Por ejemplo child::div/child::xxx selecciona el elemento hijo xxx del elemento hijo div del nodo de contexto. Absolutos: consisten en una / seguida, opcionalmente, de un path relativo. Algunos ejemplos de paths de localización son los siguientes: /: selecciona el documento raíz. /descendat::xxx: selecciona todos los elementos xxx en el mismo documento que el nodo de contexto. child::xxx[position()=1]: selecciona el primer hijo xxx del nodo de contexto. child::*: selecciona todos los hijos del nodo de contexto. XLink y XPointer XLink, XML Link Language, permite insertar elementos en un documento XML que crean y describen enlaces entre recursos. Su uso más común es para la creación de hiperenlaces. XPointer especifica un modo fácil de entender y conveniente para la localización de documentos XML. XLink proporciona funcionalidades de enlace avanzadas como: Enlaces multidireccionales, se dirigen a varios recursos. Asociar metadatos con los enlaces. Expresar enlaces que residen en una localización separada de los recursos enlazados. Los enlaces se implementan por medio de atributos y no por elementos. XPointer proporciona mejores especificaciones de localización: Enlaces que apuntan a un punto específico dentro de un documento, incluso aunque no exista un ID justo en ese punto. Granularidad fina que permite apuntar a elementos, cadenas de texto. Sintaxis clara para tratar las localizaciones y las relaciones en jerarquías, de forma que es legible por las personas. Lenguajes de ScriptAunque muchos lenguajes son calificados como lenguajes de script, no hay un consenso sobre el significado exacto del término ni ha sido formalmente definido. Se llaman lenguajes de script a lenguajes como awk, sh de UNIX, Javascript, VBScript, Perl, … Los lenguajes de script se diseñaron para realizar tareas diferentes de las que realizan otros tipos de lenguajes de programación, como C o C++, y esto lleva a diferencias fundamentales entre los dos tipos de lenguajes. Los lenguajes de programación tradicionales se diseñaron para construir estructuras de datos y algoritmos empezando desde el principio, desde elementos de computación como las palabras de memoria. Para manejar esta complejidad suelen ser fuertemente tipados. Frente a esto, los lenguajes de script se diseñaron para unir componentes . Asumen la existencia de un conjunto de potentes componentes y se centran generalmente en poner juntas estas componentes. Además suelen ser débilmente tipados, el tipo se determina en tiempo de ejecución, para simplificar la conexión entre componentes. Otra diferencia entre los dos grupos de lenguajes, los de script frente a los que no reciben esta denominación, es que los lenguajes de script son generalmente interpretados en vez de compilados. Los lenguajes interpretados eliminan los tiempos de compilación durante el desarrollo y hacen a las aplicaciones más flexibles al permitir que se genere código fácilmente en tiempo de ejecución. Estas características producen, generalmente, código que es menos eficiente. Sin embargo, el rendimiento no suele ser un aspecto fundamental en un lenguaje de script. Los lenguajes de script se dirigen fundamentalmente a áreas donde la flexibilidad y un desarrollo rápido son mucho más importantes que el puro rendimiento. Las aplicaciones en lenguajes de script suelen ser más pequeñas que las aplicaciones en otros lenguajes de programación y el rendimiento de una aplicación de scripts está dominado por el rendimiento de sus componentes, que generalmente están implementadas en algún otro tipo de lenguaje de programación. Los dos grupos de lenguajes son complementarios, y las principales plataformas de computación han proporcionado tanto lenguajes de programación como de scripts desde los años 60. Mientras que los lenguajes de programación que podríamos denominar de sistemas o tradicionales son buenos para producir implementaciones eficientes de funcionalidades críticas con respecto al tiempo, los lenguajes de script son buenos para poner juntas componentes de distinta funcionalidad. Sin embargo la existencia de máquinas cada vez más rápidas, la existencia de mejores lenguajes de scripts, la importancia creciente de las interfaces gráficas de usuario y de las arquitecturas de componentes, y sobre todo el crecimiento de Internet, han incrementado enormemente la aplicabilidad de los lenguajes de script. Podemos citar entre las características típicas de los lenguajes de script, aunque no siempre las tienen todas: Interpretados. Débilmente tipados. Asignación dinámica de memoria con liberación automática de la misma. Potentes capacidades para la manipulación de cadenas (expresiones regulares). Procedurales y con extensiones de orientación a objetos. No es necesario que el usuario del lenguaje defina clases o métodos. Utilizan un método de invocación de métodos muy simple, no es necesario especificar los paths de los objetos para acceder a los métodos. Entre los lenguajes de script más comunes podemos citar Javascript, Perl, VBScript, … Aplicación de los lenguajes de script a internetLos lenguajes de script se han utilizado en Internet para un aspecto fundamental, añadir interactividad y dinamismo a las páginas web. El método más antiguo para añadir esta interactividad es mediante programas que utilizan la interfaz CGI (Common Gateway Interface). CGI es un protocolo estándar que especifica como pasar información desde una página web, a través de un servidor web, a un programa y devolver información desde el programa a la página en el formato apropiado. Los lenguajes de script, sobre todo Perl, son los más comúnmente utilizados para escribir estos programas CGI. Pero CGI presenta ciertos problemas de ineficiencia. Por un lado los programas CGI se ejecutan fuera del servidor web y además están diseñados para manejar sólo una petición y terminar su ejecución después que han devuelto sus resultados al servidor. Hoy en día se utiliza sistemas que toman la forma de componentes que se apoyan en APIs específicas del servidor para interactuar directamente con el proceso del servidor. Conectándose como un subproceso al servidor Web se evita mucha de la sobrecarga de los programas CGI convencionales. La generación de contenido dinámico del lado del servidor requiere que el servidor procese las peticiones en tiempo de ejecución para dar una respuesta apropiada a la petición, así que se necesitan instrucciones para ejecutar este procesamiento y por tanto alguna forma de programarlas. Otra forma de añadir interactividad y dinamismo es mediante scripts del lado del cliente, fundamentalmente usando Javascript. El código de script se introduce directamente o se referencia en el documento HTML y se ejecuta cuando se carga la página o como respuesta a otro tipo de eventos. Un ejemplo de uso de scripts en Web es la validación de formularios (uno de los motivos originales para el desarrollo de Javascript). La validación de formularios es el proceso de comprobar la validez de los datos introducidos por el usuario antes de enviarlos al servidor. Con la aparición de la generación 4.x de los navegadores se introdujo el concepto DHTML, HTML dinámico, que describe la capacidad de manipular dinámicamente los elementos de una página a través de la interacción: HTML: la versión 4 introdujo dos cosas importantes, CSS y DOM. CSS: con CSS se tiene un modelo de estilo y layout para documentos HTML. DOM (Document Object Model): proporciona un modelo del contenido para documentos HTML. Javascript (y VBScript): permite codificar scripts que controlen los elementos HTML. Bibliografía Scribd (Ibiza Ales) Análisis y gestión de riesgos de los sistemas de información. La metodología MAGERIT: método, elementos, técnicas.Planes de Seguridad, Contingencias y RecuperaciónLa Información y los sistemas e instalaciones que la sustentan, son en la actualidad un activo fundamental para toda organización. Este activo está expuesto a una serie de amenazas más o menos destructivas (intrusiones, virus, fuego, desastres naturales, etc.), lo que hace necesario un instrumento capaz de gestionar su seguridad. La seguridad de la información se caracteriza por la preservación de la: Confidencialidad : Asegurando que la información es accesible sólo a aquellos que tienen autorización para ello. Integridad : Salvaguardando la completitud y precisión de la información y de los métodos de procesamiento. Disponibilidad : Asegurando que los usuarios autorizados tienen acceso a la información en el momento que este acceso es requerido. La gestión de la seguridad de los sistemas de información incluye el análisis de los requerimientos de seguridad, el establecimiento de un plan para satisfacer estos requerimientos, la implementación de este plan y el mantenimiento y administración de la seguridad implementada. Este plan es el Plan de Seguridad . El plan de seguridad se diseña para asegurar que se ha establecido de forma correcta el contexto y el alcance de la gestión de la seguridad, que se ha identificado y evaluado todos los riesgos de seguridad y que se han desarrollado estrategias apropiadas para el tratamiento de estos riesgos. Pero incluso en las organizaciones bien preparadas, con buenos programas de seguridad, suceden incidentes, emergencias o desastres que afectan a la continuidad del servicio. Debido al avance de las tecnologías de la información, las organizaciones actuales dependen en gran medida de ellas. Con la aparición del e-business muchas organizaciones no pueden sobrevivir sin un modo de operación 24x7x365 (24 horas al día, 7 días a la semana, 365 días al año). El Plan de Seguridad debe incluir tanto una estrategia proactiva como una reactiva: La estrategia proactiva es una estrategia de prevención que comprende el conjunto de pasos que ayudan a minimizar los puntos vulnerables de los activos (recursos de los sistemas de información). La estrategia reactiva es una estrategia de corrección que comprende aquellas medidas orientadas a reducir los daños ocasionados una vez que se ha producido un ataque o incidente. Así que son necesarios planes extensos y rigurosos para alcanzar el estado de continuidad de las operaciones, en el que los sistemas críticos y las telecomunicaciones estén continuamente disponibles. La descripción de las acciones y decisiones necesarias para asegurar la continuidad de las operaciones de la Organización en el caso de un incidente, una emergencia o de un desastre se recogen en el Plan de Contingencias . Mediante el Plan de Contingencias se diseñan y formulan los modos de actuación alternativos que van a permitir la operación de la Organización en condiciones adversas, la recuperación de sus funciones de la forma más rápida posible, y que van a proporcionar continuidad a los procesos críticos del negocio. Como parte del Plan de Contingencias está el Plan de Recuperación de Desastres, en el que se desarrollan todas las normas de actuación para reiniciar las actividades de proceso, bien sea en el propio centro de proceso de datos o en un lugar alternativo (Centro de Respaldo) en el caso de producirse un desastre. Gestión de la Seguridad. El Plan de SeguridadLa gestión de la seguridad es un proceso que comprende varias etapas. Comienza estableciendo los objetivos y las estrategias de seguridad y desarrollando una política corporativa de seguridad de tecnologías de la información. Como parte de esta política de seguridad corporativa está la creación de un estructura organizativa apropiada para asegurar que los objetivos definidos se pueden alcanzar. La siguiente etapa sería el análisis y gestión de los riesgos, que incluye las siguientes actividades: Realización de un análisis y evaluación de riesgos. Selección de las salvaguardas para los sistemas de información basada en el resultado del análisis y evaluación de riesgos. Formulación de políticas de seguridad de sistemas de información. Elaboración del plan de seguridad de sistemas de información, basado en las políticas de seguridad anteriores, para implementar las salvaguarda. Las dos últimas etapas son la implementación del plan de seguridad y su mantenimiento. El propósito del Plan de Seguridad es dar una visión general de los requerimientos de seguridad del sistema de información y describir los controles necesarios para alcanzar esos requerimientos. Además delimita las responsabilidades y el comportamiento esperado de toda persona que acceda al sistema. Mediante el Plan de Seguridad se establecen las salvaguardas necesarias para proteger los recursos de tecnologías de la información, ya sean salvaguardas de gestión, técnicas u operativas. Objetivos, Estrategias y Política de Seguridad CorporativaEl primer paso en el proceso de la gestión de la seguridad debería ser determinar cual es el nivel de riesgo aceptable para la Organización y de aquí cual es el nivel apropiado de seguridad. El nivel de seguridad apropiado está determinado por los objetivos o requerimientos de seguridad que la Organización necesita cumplir. Es esencial que una Organización identifique sus requerimientos de seguridad. Hay tres fuentes principales para esta indentificación: El análisis y evaluación de riesgos: mediante éste se identifican las amenazas sobre los activos y las vulnerabilidades de éstos. Además se evalúa la posibilidad de ocurrencia de estas amenazas y se estima su impacto. El marco legal: en nuestro caso se deben tener en cuenta los requisitos establecidos por la Ley Orgánica 15/1999 de Protección de datos de carácter personal y el Real Decreto 994/1999, por el que se aprueba el Reglamento de medidas de seguridad de los ficheros automatizados que contengan datos de carácter personal. Para las Organizaciones de la Administración Pública se tiene además el Real Decreto 263/1996 por el que se regula la utilización de técnicas electrónicas, informáticas y telemáticas por la Administración General del Estado, modificado por el Real Decreto 209/2003 por el que se regulan los registros y las notificaciones telemáticas, así como la utilización de medios telemáticos para la sustitución de la aportación de certificados por los ciudadanos. El conjunto de requerimientos, objetivos y principios particulares para el procesamiento de la información que la Organización ha desarrollado para dar soporte a sus operaciones. Dependiendo de los objetivos de seguridad se eligen las estrategias para alcanzar estos objetivos. Cuatro son las estrategias básicas usadas para controlar el riesgo que resulta de las vulnerabilidades: Evitar el riesgo,mediante la aplicación de salvaguardas que eliminan o reducen los riesgos que permanecen para la vulnerabilidad considerada. Transferir el riesgo a otras áreas o a entidades externas, por ejemplo mediante la contratación de un seguro. Mitigar el riesgo, reduciendo los impactos en caso de que se explote la vulnerabilidad. La forma de mitigar el riesgo es mediante la Planificación de la Contingencia. Aceptar el riesgo sin control o mitigación, bien sea por su poca posibilidad de ocurrencia o por su bajo impacto. Es importante el compromiso de la dirección y los altos niveles de gestión de la Organización con la seguridad de los sistemas de información. Este compromiso debería resultar en una política corporativa de seguridad de las tecnologías de la información formalmente consensuada y documentada. Esta política debería cubrir, como mínimo, los siguientes aspectos: Los objetivos de seguridad. Los aspectos organizativos de la seguridad, desde el punto de vista de las infraestructuras y de la asignación de roles y responsabilidades. Integración de la seguridad dentro del ciclo de desarrollo de sistemas. Directivas y procedimientos generales de seguridad. Definición de clases para la clasificación de la información y de los sistemas. Políticas de salvaguarda. Planificación de la contingencia. Consideraciones legales. Análisis y Evaluación de RiesgosComo ya se ha comentado el análisis y evaluación de riesgos (AER) es una de las fuentes fundamentales para la identificación de los requerimientos de seguridad. Los resultados de esta evaluación van a servir de guía para determinar las acciones de gestión adecuadas y las prioridades en el tratamiento de los riesgos de seguridad. Además van a ayudar a seleccionar e implementar las salvaguardas para protegerse de esos riesgos. Mediante el AER se identifican las amenazas que soportan los activos del Sistema de Información (o los relacionados con él) y se determina cual es la vulnerabilidad de esos activos frente a esas amenazas. Con esto se estima el grado perjuicio (impacto) que pueden tener estas amenazas y vulnerabilidades y se calcula el riesgo que se corre. Existen dos aproximaciones al Análisis de Riesgos, una cuantitativa y otra cualitativa. La primera se basa en dos parámetros fundamentales: la probabilidad de que ocurra un suceso y una estimación de las pérdidas en caso de que así sea. El producto de estos dos factores es el denominado Coste Anual Estimado (EAC, Estimated Annual Cost) y aunque teóricamente es posible calcularlo para cualquier evento y tomar decisiones basadas en su valor, en la práctica la dificultad de la estimación o la inexactitud en el cálculo de parámetros hace que esta aproximación sea la menos usada. La segunda aproximación, la cualitativa, es mucho más sencilla e intuitiva que la anterior, y es la más extendida hoy en día. Se basa en una simple estimación de pérdidas potenciales (ya no interviene el cálculo de probabilidades exactas). Para ello se interrelacionan cuatro elementos: las amenazas, las vulnerabilidades, los impactos asociados a las amenazas y las salvaguardas que se toman para minimizar los impactos o las vulnerabilidades. Con estos elementos se obtiene un indicador cualitativo del riesgo asociado a un activo, visto como la posibilidad de que una amenaza se materialice en un activo y produzca un impacto. Los riesgos se pueden clasificar entonces según su nivel en: No aceptable: el riesgo pone en peligro los objetivos fijados en el proyecto de seguridad, causa un daño o pérdida irreparable. Se deben tomar acciones inmediatas para reducir su nivel. Crítico: el riesgo puede afectar los objetivos del proyecto o causar un daño o pérdida importante. El riesgo debe mitigarse. Mayor: el riesgo produce un impacto significativo en el proyecto (afecta a costos, calidad u a otras áreas). Se deben ejecutar acciones que reduzcan el impacto. Menor: No causa problemas significativos. Se debe monitorear de una manera regular para asegurar que no sube de nivel. Selección de SalvaguardasSe deben identificar y seleccionar las salvaguardas apropiadas para reducir el riesgo evaluado hasta niveles aceptables. Para seleccionar las salvaguardas se tienen que considerar: el resultado del análisis y evaluación de riesgos, las salvaguardas ya existentes o planificadas y las restricciones de varios tipos que puedan existir. Mediante el análisis y evaluación de riesgos se han detectado las vulnerabilidades de los sistemas con amenazas asociadas que pueden explotar estas vulnerabilidades y cual es el impacto que pueden causar. Así que los resultados de esta evaluación nos van a indicar dónde es necesario una protección adicional y de qué tipo. Se deben tener también en cuenta las salvaguardas ya existentes o planificadas para evitar trabajo o costes innecesarios o para determinar si las nuevas salvaguardas son compatibles con aquellas. Además se tienen que examinar en términos de comparaciones de coste con vistas a eliminarlas (o no implementarlas) o mejorarlas en caso de que no sean suficientemente efectivas. Hay varios tipos de restricciones que deben ser tenidos en cuenta a la hora de seleccionar las salvaguardas, entre ellos: Restricciones de tiempo: por ejemplo hay que considerar si el tiempo que va a tomar implementar la salvaguarda es un periodo de tiempo aceptable para dejar el sistema expuesto a un determinado riesgo. Restricciones financieras: el coste de la salvaguarda no debe ser mayor que el valor de los activos que va a proteger. Restricciones técnicas: como la compatibilidad de software o hardware. Para la selección de salvaguardas se debe tener en cuenta que exista un equilibrio entre las salvaguardas de gestión, las salvaguardas operacionales y las salvaguardas técnicas. Las salvaguardas de gestión, que se centran en la gestión de la seguridad del sistema, incluyen: Reglas de comportamiento: delimitan las responsabilidades y el comportamiento esperado de toda persona que acceda al sistema. Revisión de las salvaguardas: la seguridad de los sistemas puede degradarse con el tiempo, debido a los cambios tecnológicos, a la propia evolución del sistema o cambios en los procedimientos. Se deben realizar revisiones periódicas para garantizar que los controles funcionan de una manera efectiva y proporcionan los niveles adecuados de protección. Planificación de la seguridad durante todo el ciclo de vida del sistema: se deben tener en cuenta los requisitos de seguridad en todas las fases del ciclo de vida del sistema. Las salvaguardas operacionales, salvaguardas no técnicas que proporcionan seguridad física, de personal y administrativa, incluyen: Controles de seguridad física y del entorno: entre ellas se encuentran el establecimiento de áreas seguras y equipamiento de seguridad. Controles de seguridad del personal: cubren comprobaciones en el reclutamiento del personal, especialmente del personal de confianza, programas de educación, entrenamiento y de compromiso con la seguridad. Controles del mantenimiento de aplicaciones de software: se utilizan para controlar la instalación o la actualización de aplicaciones de software. Entre ellos están el control de versiones, la gestión del cambio o la gestión de la configuración. Controles de validación e integridad de datos: se utilizan para proteger a los datos de alteraciones accidentales o malintencionadas, o para evitar su destrucción. Planificación de la contingencia que se puede considerar también como una salvaguarda. Las salvaguardas técnicas, enfocadas a los controles que ejecutan los sistemas de computación, incluyen: Identificación y autenticación. Controles lógicos de acceso (autorización y controles de acceso). Registros de auditoría. Política de Seguridad de Sistemas de InformaciónLa política de seguridad del sistema de información contiene los detalles de las salvaguardas necesarias y el por qué son necesarias. Muchos sistemas, que introducen consideraciones especiales no contempladas en otros sistemas de la Organización, necesitan sus propias políticas de seguridad. Estas políticas deben ser compatibles con la política corporativa de seguridad de tecnologías de la información. La política debe estar basada en los resultados del análisis de riesgos y debe contener las salvaguardas necesarias para alcanzar el nivel de seguridad apropiado para el sistema considerado. En concreto la política debe contener: Una descripción del sistema y de sus componentes. Una descripción de los servicios y funciones de negocio cubiertos por el sistema. Una identificación de los objetivos de seguridad para el sistema. El grado de dependencia de la Organización respecto al sistema. Los activos del sistema que se quieren proteger y una valoración de estos activos. Las vulnerabilidades del sistema y las amenazas a las que se enfrenta, incluyendo la relación entre los activos y las amenazas y la probabilidad de que éstas se materialicen. Los riesgos de seguridad del sistema resultantes de la combinación de la probabilidad de que se materialicen las amenazas, la facilidad de explotación de las vulnerabilidades y el impacto sobre el negocio. La lista de salvaguardas identificadas para proteger el sistema. El coste estimado de seguridad. Elaboración e Implementación del Plan de SeguridadEl Plan de Seguridad del Sistema debe estar basado en la política de seguridad del sistema y describe las acciones que se van a tomar para mantener el nivel apropiado de seguridad y para implementar las salvaguardas requeridas para el sistema. Debe asegurar que las salvaguardas se implementan en el tiempo planificado, de acuerdo con las prioridades que se derivan del análisis de riesgos. Se debe incluir: Los objetivos de seguridad en términos de confidencialidad, integridad, disponibilidad, autenticidad y fiabilidad. Una evaluación del riesgo residual aceptado después de implementar las salvaguardas identificadas. Una lista de las salvaguardas seleccionadas que se van a implementar, así como una lista de las salvaguardas ya existentes. Una identificación y definición de las acciones que se van a tomar, con sus prioridades, para implementar las salvaguardas. Una estimación de los recursos necesarios para la implementación de las salvaguardas y de los costes asociados. Un plan detallado de implementación del Plan. Después de haber elaborado el plan de seguridad es necesario implementarlo. A la vez que se implementan las salvaguardas se debe desarrollar un programa para conseguir que todo el personal de tecnología de la información y los usuarios finales tengan un conocimiento suficiente de los sistemas y que entiendes porqué las salvaguardas son necesarias y como utilizarlas correctamente. Durante la implementación deben tenerse en cuenta los siguientes puntos: El coste de las salvaguardas permanece dentro del rango aprobado. Las salvaguardas se implementan correctamente según lo establecido en el Plan de Seguridad. Mantenimiento del Plan de SeguridadEl mantenimiento del Plan comprende la revisión y el análisis de las salvaguardas implementadas, la gestión del cambio y monitorización de la seguridad. El objetivo de la revisión y análisis de las salvaguardas es comprobar que las salvaguardas establecidas en el plan están implementadas y que se usan correctamente. Mediante la gestión del cambio se va a determinar que impacto tiene en la seguridad del sistema los cambios que se realizan, o está planificado realizar, en el mismo. Por monitorización del Plan se entienden las actividades de comprobación de si el sistema, su entorno y sus usuarios mantienen el nivel de seguridad establecido en el Plan de Seguridad. La monitorización es un modo de detectar cambios relevantes en la seguridad. Planificación de la Contingencia. El Plan de ContingenciasEn el Plan de Contingencias se detalla la forma en que debe reaccionar la Organización y qué acciones emprender para garantizar la recuperación y continuidad y de las operaciones críticas, frente a determinados eventos (ya sean accidentales o malintencionados) que pueden provocar una interrupción del servicio. En el plan se tiene que incluir quien es la autoridad para iniciar las actividades descritas, cuales son las operaciones a las que se debe garantizar la continuidad y como restablecer dichas operaciones. Un plan de Contingencias debería cubrir la ocurrencia de eventos como: Fallos del hardware. Fallos del software. Interrupción del suministro eléctrico o de los servicios de telecomunicaciones. Errores humanos o sabotajes. Ataques de software malicioso. Fuego. Desastres naturales. La planificación de la contingencia es un proceso que, siguiendo la Guía para la Planificación de la Contingencia del NIST (National Institute of Standars and Technology), comprende los siguientes pasos: Desarrollar una política de planificación de la contingencia. Realizar un Análisis de Impacto en el Negocio. Desarrollar estrategias de recuperación. Desarrollar el Plan de Contingencias. Realizar pruebas y entrenamientos. Revisión y mantenimiento del plan. Vamos a desarrollar a continuación cada una de estas etapas. Desarrollo de Políticas de Planificación de ContingenciasPara que un plan de contingencias sea efectivo debe basarse en políticas claramente definidas que aseguren que todo el personal entiende de una forma completa los requerimientos de planificación de contingencias de la Organización. Estas políticas definen los objetivos y las responsabilidades, y establecen un marco para la planificación de la contingencia de los sistemas de información. Análisis de Impacto en el NegocioUna vez definidas las políticas, la primera fase en el desarrollo de un proceso de planificación de contingencias es realizar un análisis de impacto en el negocio (BIA, Business Impact Analysis). Un BIA es una investigación y una evaluación del impacto que pueden provocar en un sistema los diversos ataques potenciales a los que se enfrenta. Con él se establece el alcance del Plan y se determina cuales son los recursos críticos y los tiempos de recuperación aceptables. El BIA arranca donde termina el Análisis y Gestión de Riesgos (AGR). Se asume que los controles que se establecieron como resultado del AGR han fallado, han conseguido ser sobrepasados o simplemente no han sido efectivos, y que el ataque ha tenido éxito. Los tres pasos para realizar un BIA son: identificación de los recursos críticos del sistema, identificación de los impactos y de los tiempos asumibles de las interrupciones, y establecimiento de las prioridades de recuperación. Los sistemas de información pueden ser extremadamente complejos y realizar múltiples servicios a través de numerosos componentes, procesos e interfaces. El primer paso del BIA es evaluar el sistema para determinar cuales son las funciones críticas y cuales son los recursos necesarios para realizarlas. En el siguiente paso se analizan los recursos críticos identificados en la etapa anterior y se determina el impacto de un daño o de una interrupción en cada uno de dichos recursos. Los efectos de estos daños o interrupciones se deben estudiar teniendo en cuenta: Su duración en el tiempo, para poder identificar cual es el tiempo máximo que se puede asumir que un recurso estará fuera de servicio antes de que se produzca la negación de una función esencial. Los recursos relacionados y los sistemas dependientes, para identificar cualquier efecto en cascada que pudiera ocurrir. Se debe determinar cual es el punto óptimo de recuperación del sistema teniendo en cuenta que tiene que haber un equilibrio entre el coste de la no operatividad del sistema y el coste de los recursos necesarios para su restauración. Considerando la gráfica del coste de interrupción y recuperación como función del tiempo, como se muestra en la siguiente figura, el punto óptimo de recuperación es el punto en el que se cortan ambas funciones. El último paso, establecimiento de las prioridades de recuperación, se basa en los resultados de la etapa anterior para priorizar las estrategias de recuperación que se establecerán durante la activación del Plan de Contingencias. Mediante esta priorización se pueden tomar decisiones bien fundadas respecto a la asignación de recursos y gastos de recuperación, ahorrando esfuerzos, tiempo y dinero. Desarrollo de Estrategias de RecuperaciónLas estrategias de recuperación proporcionan los medios para restaurar el sistema de una manera rápida y efectiva después de una interrupción de las operaciones. Una estrategia básica e imprescindible la constituyen los respaldos o copias de seguridad (backups). Para conseguir una política de respaldo efectiva es necesario determinar aspectos como: Qué ficheros o sistemas de ficheros respaldar y dónde se encuentran. El tipo de backup que se va a realizar. Tenemos tres tipos de backups: totales, incrementales y diferenciales. En el total se copia el conjunto total de los datos que queremos respaldar. En el incremental se salvan sólo aquellos ficheros que han cambiado desde la última copia de seguridad, sea esta del tipo que sea. En el diferencial se salvan sólo aquellos ficheros que han cambiado desde el último backup total. Cada uno de los tipos tiene sus ventajas y sus inconvenientes y son más o menos efectivos dependiendo de las circunstancias. La mayoría de las estrategias de respaldo utilizan una combinación de backups totales con backups incrementales o diferenciales. La frecuencia con la que se va a realizar. La frecuencia de realización de los backups depende de dos factores claves: la tasa de cambio de los datos y de la sensibilidad de los mismos para la organización. Un caso típico es realizar backups completos una vez a la semana y backup incrementales o diferenciales diariamente. Cual es el tiempo de retención de las copias. Se determina el periodo de tiempo durante el cual se tienen que mantener archivadas las copias de seguridad antiguas. Qué medios de almacenamiento se van a utilizar, cintas, discos magnéticos, discos ópticos, y cual va a ser la estrategia de rotación de los conjuntos de medios que se utilicen. Un esquema de rotación típico es la rotación “abuelo-padre-hijo” (GFS, Grandfather-Father-Son). Este esquema utiliza conjuntos de medios diarios (hijo), semanales (padre) y mensuales (abuelo). Una implementación típica del esquema GFS es considerar un periodo de rotación de tres meses y 12 cintas: Hijo: se asignan 4 cintas para el backup incremental diario [1-4] Padre: se asignan 5 cintas para el backup total semanal [5-9] Abuelo: se asignan 3 cintas para el backuptotal mensual [10-12] Notas: Sobre el último día hábil del mes Para cuando hay un quinto viernes en el mes Aunque interrupciones muy graves, con efectos a largo plazo, pueden ser que ocurran raramente se deben considerar en el plan de contingencia. El plan debe incluir entonces una estrategia de recuperación de las operaciones del sistema en instalaciones alternativas para operar en ellas durante un periodo de tiempo. Se pueden clasificar estos lugares alternativos en: “Cold site”: está localizado en el exterior de la Organización con toda la infraestructura necesaria preparada para la instalación en el caso de que ocurra un desastre, pero es la organización que utiliza el sitio frío la que debe instalar el equipamiento necesario. En un centro frío llevaría más de un día reiniciar las operaciones después del desastre. “Mutual Backup”: dos Organizaciones con una configuración similar de los sistemas se ponen de acuerdo para servir una a la otra como lugar de respaldo. “Hot site”: un lugar con instalaciones de telecomunicaciones, software y hardware compatibles con el lugar de producción. En un centro frío con ese equipamiento tomaría menos de un día el reiniciar las operaciones después del desastre. “Remote Journaling”: se refiere a transmisiones on-line de las transacciones de datos para salvaguardar de manera periódica el sistema de forma que se minimice la pérdida de datos y el tiempo de recuperación. “Mirrores Site”: un lugar equipado con un sistema idéntico al de producción con instalaciones de mirroring. Los datos se duplican en el sistema de salvaguarda de manera inmediata de forma que la recuperación es transparente a los usuarios. Desarrollo del Plan de ContingenciasPodemos considerar cuatro componentes primarios en un Plan de Contingencias: Información de soporte. Fase de notificación/activación. Fase de recuperación. Fase de restauración. Apéndices del plan. En la primera sección, información de soporte, generalmente se incluye cual es el propósito y los objetivos del Plan, y se define su alcance, identificando los sistemas y las localizaciones que van a estar cubiertos por el plan. Se describe además la estructura de los equipos de contingencias, incluyendo su jerarquía y mecanismos de coordinación, con sus roles y responsabilidades en una situación de contingencia. En la sección fase de notificación/activación se definen las acciones que se van a tomar una vez que se detecta una situación de emergencia o una interrupción del servicio. Estas acciones incluyen la notificación al personal de recuperación, la evaluación de daños y activación del Plan. Los procedimientos de notificación, que describen los métodos para comunicarse con el equipo de recuperación, tienen que estar documentados en el Plan. La activación del Plan se realiza sólo cuando el análisis de daños indica que se cumplen uno o más de los criterios de activación que se han establecido en la declaración de la política de la planificación de la contingencia (fase 1 de la planificación). Una vez que se ha caracterizado el daño y establecido que se debe activar el Plan de Contingencias, el coordinador del plan puede seleccionar una estrategia de recuperación adecuada y se realiza la notificación al equipo de recuperación asociada con ella. En la sección fase de recuperación se describen las actividades que se centran en las medidas para ejecutar las capacidades de procesamiento de una manera temporal y reparar el daño y recuperar las capacidades operativas del sistema original (o de uno nuevo si este fuera irrecuperable). Los procedimientos de recuperación deben reflejar las prioridades identificadas en el Análisis de Impacto en el Negocio y estar documentados en un formato secuencial, de paso a paso, de forma que los distintos componentes del sistema se puedan restaurar de una manera lógica. En la fase de restauración las operaciones de recuperación han terminado y se transfieren de nuevo las operaciones normales al sistema original (o uno nuevo en el caso de que fuera irrecuperable). Las siguientes operaciones tienen lugar en esta fase: Asegurarse que las infraestructuras adecuadas, tales como energía eléctrica o telecomunicaciones están operativas. Instalar el hardware y el software del sistema. Establecer las conexiones y las interfaces con la red y con los sistemas externos. Probar las operaciones del sistema para probar su completa funcionalidad. Copiar los datos del sistema de contingencia y cargarlos en el sistema restaurado. Dar por terminadas las operaciones de contingencias. Pruebas y EntrenamientoLas pruebas del plan es un elemento crítico que va a permitir detectar sus deficiencias y evaluar la efectividad de cada procedimiento de recuperación y del plan como un todo. Entre las pruebas a realizar están: Recuperación a partir de los backups. Restauración de las operaciones normales. Rendimiento del sistema al utilizar equipamientos alternativos. Los procedimientos de notificación de contingencias. Para complementar las pruebas el personal debe entrenarse en la ejecución de los procedimientos bajo su responsabilidad con el objetivo de que sean capaces de realizarlos sin necesidad de la ayuda del documento real del plan de contingencias. Mantenimiento del PlanLos sistemas de información sufren cambios debidos a las necesidades cambiantes de la Organización, a los cambios tecnológicos o a nuevas políticas internas o externas. Para ser efectivo el Plan de Contingencias debe mantenerse para que se ajuste a los nuevos requerimientos, procedimientos o políticas. El plan debería revisarse con una base periódica, por ejemplo una vez al año, y siempre que se efectúen cambios significativos en cualquier elemento del plan. Los cambios en el Plan deberían documentarse por medio de un registro de cambios en el que se especifique la fecha del cambio, la parte a la que afecta y un comentario sobre el mismo. Plan de Recuperación de DesastresEl Plan de Recuperación de Desastres se refiere al plan enfocado a las tecnologías de la información diseñado para restaurar la operativa del sistema, de las aplicaciones o de las instalaciones de computación, generalmente en un lugar alternativo después de una emergencia o incidente grave. Es la parte del Plan de Contingencias que debe incluir todas las acciones necesarias que van a permitir a la Organización operar durante el periodo de tiempo del desastre. Se definen en él los recursos, las acciones y los datos requeridos para reiniciar los procesos críticos del negocio, ya sea en el CPD o en Centro de Respaldo. En el Plan de Recuperación de Desastres se incluyen: Los criterios usados para identificar cuando debería activarse el plan, quien es el responsable de la activación del plan y como se va a comunicar ésta. Tareas y responsabilidades específicas de los miembros del equipo de recuperación de desastres. Procedimientos para evaluar los daños provocados por el desastre. Uso de soportes de procesamiento alternativo. Acciones de mantenimiento o sustitución de equipos dañados. Uso de copias de seguridad. Para la recuperación en un Centro de Respaldo se debe especificar además la política de traslados y vuelta al centro inicial. Una guía de las secciones que se incluyen en un Plan de Recuperación de Desastres es la siguiente: Propósito: incluye la declaración de los objetivos del plan. Alcance: identifica los sistemas críticos y las localizaciones específicas a las que se les aplica el plan. Bases: identifica las bases en las que se apoya el plan. Equipo: identifica al equipo que lidera las actividades del plan, así como los otros miembros involucrados en el proceso. Notificación: establece los métodos formales de comunicación para alertar al equipo de recuperación de la ocurrencia del incidente o desastre. Evaluación de daños: describe los procesos de análisis del alcance del daño. Activación: describe las situaciones y procesos que van a iniciar las actividades de recuperación. Operaciones de recuperación: describe todos los pasos para la recuperación de los sistemas y aplicaciones críticos, bien el propio centro o en Centro de Respaldo (cuando sea aplicable). Aquí se debería incluir la información sobre como recuperar los datos basándose en las copias de seguridad. Retorno a las operaciones normales: los procedimientos que debe seguir el equipo para la completa recuperación de todos los datos y la vuelta al procesamiento normal de todas las operaciones del negocio. Políticas de SalvaguardaLas políticas de salvaguarda son las que van a determinar como se van a seleccionar los controles y medidas de protección para alcanzar los requerimientos de seguridad que se hayan especificado para los sistemas. Basándose en el resultado de revisiones de alto nivel en cuanto a seguridad, se pueden considerar tres opciones a la hora de hacer esta selección: Aproximación de línea base. Basada en un análisis de riesgos detallado. Basada en una combinación de las dos opciones anteriores. Con la primera opción se selecciona un conjunto de salvaguardas para alcanzar un nivel básico de protección para todos los sistemas. Para establecer este conjunto de salvaguardas un buen punto de partida es considerar las que se basan en requerimientos legislativos o las consideradas como “buenas prácticas comunes” para la seguridad de la información. Salvaguardas consideradas esenciales para una organización desde el punto de vista legislativo incluyen: Las salvaguardas para la protección y privacidad de los datos de carácter personal. Las salvaguardas para la protección de los derechos de la propiedad intelectual. Las salvaguardas para los registros de la organización que deben mantenerse por requerimientos normativos o legales. Salvaguardas consideradas como “buenas prácticas” incluyen: Documentación de las políticas de seguridad. Asignación de responsabilidades de seguridad de la información. Educación y entrenamiento al personal en materia de seguridad. Información de incidentes de seguridad. Gestión de la contingencia. El método MAGERIT de gestión de la seguridad, identifica 10 salvaguardas mínimas de seguridad en su guía de aproximación: Documentación de políticas de seguridad de la información. Como ya se ha visto en la sección del plan de seguridad, la organización debe publicar las normas de seguridad de los sistemas de información de una manera sencilla y comprensible para todos los empleados de la organización. Asignación de funciones y responsabilidades de seguridad. Se deben indicar de forma explícita las funciones y las responsabilidades de seguridad en la Organización. Para cada activo sujeto a medidas de seguridad deben identificarse los roles que actúan sobre ellos, y las personas que los ejercen. Responsabilidades del usuario en el acceso al sistema. De manera especial los usuarios deben conocer su responsabilidad en cuanto a los medios que se ponen a su cargo y a los controles de acceso para que éstos puedan mantener su eficacia. Educación y formación en la seguridad de la información. Para garantizar que los usuarios están preparados para seguir los procedimientos de seguridad de la organización y se minimizan posibles riesgos, deben recibir formación sobre los requerimientos de seguridad y sobre el uso correcto de los sistemas de información. Comportamiento ante incidentes de seguridad. Se deben establecer procedimientos, que deben ser conocidos por todos los empleados de la organización, para realizar y remitir informes sobre los diferentes tipos de incidentes, amenazas, vulnerabilidades o mal funcionamiento del hardware o del software. Controles físicos de seguridad. Deben existir controles que sólo permitan el acceso al personal autorizado a cada área de seguridad. Las autorizaciones de acceso se deben dar para propósitos específicos y controlados. Gestión de la seguridad del Equipamiento. Los aspectos que se consideran en la gestión de la seguridad del equipamiento son: Instalación y protección del equipamiento. El equipamiento debe estar físicamente protegido para salvaguardarlo de pérdidas o daños. Suministro eléctrico. Se debe garantizar un suministro eléctrico adecuado que cumpla las especificaciones de los fabricantes de equipos. Para los equipos que soporten operaciones críticas se recomienda tener un sistema de alimentación ininterrumpida. Mantenimiento de equipos. Los equipos deben mantenerse en buen estado siguiendo las recomendaciones y especificaciones de los proveedores. Deben registrarse documentalmente los fallos identificados o las sospechas de fallos. Movimientos de activos fuera de la organización. Cuando sea necesario sacar de la Organización equipos, datos o software se hará siempre con la correspondiente autorización. Cumplimiento de las obligaciones y restricciones jurídicas vigentes. Protección, transporte y destrucción de la Información. La Organización debe controlar de manera especial los intercambios de información con otras organizaciones para prevenir la modificación, pérdida o mal uso de la información en tránsito. Cuando se transmita información mediante medios de comunicación inalámbricos que utilicen radiofrecuencias de información debe ir cifrada. Gestión Externa de servicios. Ante una propuesta para cambiar a una gestión externa de servicios se deben incluir los controles de seguridad apropiados para las nuevas implicaciones de seguridad. En concreto deben estudiarse: Identificación de las aplicaciones que se van a retener en la Organización por su criticidad o sensibilidad. Implicaciones para los planes de contingencia de la Organización. Especificación de las nuevas Normas de seguridad y del proceso de control de su cumplimiento. Entre las ventajas de esta primera opción, aproximación de línea base, está que no se necesitan recursos para hacer un análisis de riesgos detallados y que el tiempo y esfuerzo utilizado en la selección de salvaguardas es reducido. Además se pueden usar las mismas salvaguardas básicas para varios sistemas sin grandes esfuerzos. Entre sus desventajas está que si este nivel de línea base es demasiado alto, podría proporcionar una seguridad demasiado restrictiva para algunos sistemas o demasiado cara, y si el nivel es demasiado bajo, para algunos sistemas podría no proporcionar una seguridad suficiente. La segunda opción es realizar un análisis de riesgos detallado para cada sistema de la Organización. Basándose en los resultados obtenidos se identifican y se seleccionan las salvaguardas que reduzcan los riesgos identificados hasta los niveles considerados como aceptables. La principal ventaja de esta opción es que se identifica un nivel de seguridad apropiado para las necesidades de cada sistema, pero tiene como desventaja que para obtener resultados viables se necesita una cantidad de tiempo y esfuerzo considerable, además de tener que ser llevada a cabo por equipos expertos. La tercera opción es considerar una combinación de las dos anteriores. Se analizan los sistemas de la Organización utilizando un análisis de alto nivel para identificar aquellos sistemas que sustentan operaciones críticas para el negocio o que tienen un alto riesgo. Se clasifican entonces los sistemas en dos categorías: los que necesitan un análisis de riesgos detallado para alcanzar la protección adecuada, y los que es suficiente que tengan una protección basada en la aproximación de línea base. Entre las ventajas de esta opción están que el esfuerzo y el dinero se aplicarán primero sobre los sistemas que tienen un riesgo mayor y donde es más beneficioso. Como desventaja está que si ese primer análisis de alto nivel produce resultados poco exactos,algunos sistema que necesitan un análisis de riesgo detallado podrían pasarse por alto. El Método MAGERIT de Gestión de la SeguridadMAGERIT, Metodología de Análisis y Gestión de Riesgos de los Sistemas de Información de las Administraciones Públicas, es un método formal para investigar los riesgos que afectan a los sistemas de información y para recomendar las medidas de control de esos riesgos. Es una metodología de carácter público (su utilización no requiere autorización previa) elaborada por el Comité Técnico de Seguridad de los Sistemas de Información y Tratamiento Automatizado de Datos Personales, SSITAD, del Consejo Superior de Informática. Los objetivos de MAGERIT son: Estudiar los riesgos que soporta un sistema de información y su entorno asociado (análisis de los riesgos). Recomendar las medidas que deben tomarse para prevenir, impedir, reducir o controlar esos riesgos (gestión de los riesgos). Es decir MAGERIT se ocupa del Análisis y de la Gestión de Riesgos, punto de arranque del ciclo de gestión de seguridad. MAGERIT se presenta como un conjunto de guías más unas herramientas de apoyo. Las guías son: Guía de Aproximación. En ella se presentan los conceptos básicos de seguridad de los sistemas de información y una introducción al núcleo de MAGERIT, formado por la Guía de Procedimientos y la Guía de Técnicas. Guía de Procedimientos. Su finalidad es servir de marco de referencia para la identificación y la valoración de los activos y de las amenazas, para la evaluación de las vulnerabilidades y de los impactos, y como preparación para la toma de decisiones sobre políticas de salvaguardas a partir del cálculo de riesgos. Guía de Técnicas. Dan las bases para comprender y seleccionar las técnicas más apropiadas para los procedimientos de análisis y gestión de riesgos. Guía para Responsables del Dominio protegible. Determina cual es la participación de los directivos en la realización del Análisis y Gestión de Riesgos de los sistemas de información relacionados con su dominio funcional dentro de la organización. Guía para Desarrolladores de aplicaciones. Está diseñada para que pueda utilizarse como documento de referencia por los desarrolladores en la preparación de los mecanismos de seguridad adecuados durante el desarrollo de nuevas aplicaciones. Arquitectura de la información y especificaciones de la interfaz para el intercambio de datos. La arquitectura de la información permite a los técnicos informáticos estructurar la información que se ha de cargar en todo producto informatizado que sea semejante o esté relacionado con las herramientas de apoyo de MAGERIT. La interfaz para el intercambio de datos posibilita a los usuarios de MAGERIT establecer la comunicación con otras aplicaciones, facilitando la incorporación de otros productos a las herramientas de apoyo de MAGERIT o a la inversa. Referencia de normas legales y técnicas. Sirve como referencia a la documentación normativa en cuestiones de seguridad (como referencia a fecha 31/12/1996, pues no se han publicado actualizaciones de la guía que recojan la nueva normativa) y como instrumento para homogeneizar el intercambio de información entre las distintas entidades y personas implicadas en la seguridad de los sistemas de información. Las herramientas de apoyo, que vienen junto con sus correspondientes guías de uso, son: Herramienta 1 Introductoria. Permite una primera aproximación al Análisis y Gestión de Riesgos. Es un apoyo para la identificación de: Riesgos menores, a los que sencillamente se les puede aplicar medidas básicas de seguridad de una forma global. Riesgos mayores, a cada uno de los cuales hay que aplicar un Análisis y Gestión de Riesgos más detallado. Herramienta 2 Avanzada. Permite realizar un Análisis y Gestión de Riesgos detallado con los que afrontar proyectos de complejidad media o alta en materia de seguridad. Utiliza técnicas algorítmicas, técnicas matriciales y técnicas de lógica difusa. Existe además otra herramienta, “chinchón version 1.3”, para analizar cuantitativamente el riesgo de un sistema de información siguiendo la metodología de MAGERIT. El modelo de MAGERIT está formado por tres submodelos: Submodelo de elementos: son los componentes del modelo. Submodelo de eventos: relaciona los componentes entre sí y con el tiempo. Submodelo de procesos: es la descripción funcional del proyecto de seguridad a construir. Vamos a ver a continuación cada uno de estos submodelos. Submodelo de ElementosSe basa en seis entidades que están relacionadas entre sí y cada una de ellas dotada de ciertos atributos. Estas seis entidades son: Activos, Amenazas, Vulnerabilidades, Impactos, Riesgos y Salvaguardas. Activos Los activos se definen en la Guía de Procedimientos como “ los recursos del sistema de información o relacionados con éste, necesarios para que la organización funcione correctamente y alcance los objetivos propuestos por su dirección “. MAGERIT considera los activos clasificados en cinco grandes categorías: Entorno del Sistema de Información. Se incluye aquí cualquier elemento del entorno necesario para las siguientes categorías de activos. Por ejemplo, equipamientos y suministros, personal, instalación física. Sistema de Información. Incluye hardware, software, comunicaciones. Información. Datos, meta-información, soportes. Funcionalidades de la organización. Se refiere a las funcionalidades que justifican la existencia de los Sistemas de Información y les dan finalidad. Incluye: Objetivos y misión de la organización. Bienes y servicios producidos. Personal usuario/destinatario de los bienes o servicios producidos. Otros activos. Cualquier activo no incluido en las categorías anteriores. Por ejemplo, credibilidad, conocimiento acumulado, independencia de criterio, etc. Las tres primeras categorías constituyen el dominio estricto de todo proyecto de Seguridad de Sistemas de Información. El fallo de un Activo de una categoría puede provocar cadenas de fallos: fallos en Activos del Entorno (1) generarían otros fallos en el Sistema de Información (2), que repercutirían en la Información (3) y así sucesivamente. Cada activo tiene como atributos esenciales dos indicadores sobre dos tipos de valoraciones: La valoración intrínseca del activo: este atributo permite determinar para qué sirve un activo. En él se basa la clasificación dada anteriormente de los tipos de activos. Se centra en el aspecto cualitativo del activo como valor de uso. La valoración del estado de seguridad del activo: cada activo se caracteriza por su estado de seguridad. Este estado es el resultado de la estimación de cuatro subestados definidos por MAGERIT: Autenticación, Confidencialidad, Integridad y Disponibilidad. Amenazas Se definen en MAGERIT como “ los eventos que pueden desencadenar un incidente en la organización, produciendo daños materiales o pérdidas inmateriales en sus activos “. Se ven las amenazas bajo un enfoque dinámico, como un evento de tipo potencial que si se materializa en una agresión hace pasar al activo de un estado de seguridad antes del evento a otro posterior. Se consideran cuatro tipos de amenazas: Grupo A de Accidentes: son amenazas no humanas. Grupo E de Errores: amenazas humanas pero involuntarias. Grupo P de Amenazas Intencionales Presenciales: amenazas humanas intencionales que necesitan presencia física. Grupo T de Amenazas Intencionales de Origen Remoto: amenazas humanas intencionales que proceden de un origen remoto. Vulnerabilidades Se define la vulnerabilidad como la “ potencialidad o posibilidad de ocurrencia de la materialización de una Amenaza sobre un Activo “. La Vulnerabilidad tiene dos aspectos: el estático, realiza una función de mediación entre un Activo, como objeto de cambio del estado de seguridad, y una Amenaza como acción. el dinámico, es el mecanismo que convierte a la Amenaza en una agresión materializada. Mediante el término Vulnerabilidad se cubren dos acepciones distintas: Vulnerabilidad intrínseca del Activo respecto al tipo de Amenaza, en la que sólo se tienen en cuenta el Activo y la Amenaza. Vulnerabilidad efectiva, en la que se tienen también en cuenta las Salvaguardas aplicadas en cada momento al Activo. Siempre que sea factible se mide la Vulnerabilidad por la posibilidad de la materialización de la Amenaza en agresión o por su frecuencia histórica. Impactos Según MAGERIT “ el Impacto en un Activo es la consecuencia sobre éste de la materialización de una Amenaza “. El impacto mide la diferencia entre el estado de seguridad del activo antes de la materialización de la amenaza y el estado posterior a dicha materialización. Por tanto una simple disfunción en un activo no constituye necesariamente un Impacto, a no ser que se produzca un perjuicio apreciable como cambio del estado de seguridad. MAGERIT considera tres grandes grupos de Impactos: Cuantitativos: si representan pérdidas que se pueden cuantificar monetáriamente, ya sea directa o indirectamente. Cualitativos con pérdidas funcionales: si son reductoras directamente de los subestados de seguridad (Autenticación, Confidencialidad, Integridad y Disponibilidad). Cualitativos con pérdidas orgánicas: como la pérdida de fondos patrimoniales intangibles, el daño a personas, la responsabilidad penal por incumplimiento de obligaciones legales, etc. Riesgos El riesgo es “ la posibilidad de que se produzca un Impacto determinado en un Activo, en un Dominio o en toda la Organización “. MAGERIT distingue entre dos tipos de riesgo: Riesgo intrínseco: el que se define o calcula antes de aplicar Salvaguardas. Riesgo residual: el que seda después de aplicar las salvaguardas dispuestas. Además define el umbral de riesgo como el valor establecido como base para decidir, por comparación, si el riesgo calculado es asumible. Salvaguardas Se define en MAGERIT la Función o Servicio de Salvaguarda como “ la acción que reduce el riesgo “. Se hace una distinción entre la Función o Servicio de Salvaguarda que acabamos de definir y el mecanismo de salvaguarda que es el “ dispositivo físico o lógico capaz de reducir el riesgo “. Una Función o Servicio de Salvaguarda representa por tanto una actuación para reducir un riesgo, actuación que se concreta en un mecanismo de salvaguarda. Tipificando las salvaguardas según su forma de actuación, distingue entre: Preventivas: actúan sobre la Vulnerabilidad y reducen la posibilidad de materialización de la Amenaza, es decir, actúan con anterioridad a la agresión. Entre ellas están la formación, información y concienciación del personal, la disuasión, la detección preventiva, etc. Curativas (o restablecedoras): actúan sobre el Impacto reduciendo su gravedad, es decir, actúan con posterioridad a la agresión. Entre ellas están la corrección, la recuperación, etc. Submodelo de EventosPara presentar este modelo de forma intuitiva se ha utilizado la metáfora de la “ciudad amurallada”: Las salvaguardas son la muralla y las vulnerabilidades las brechas de la muralla. Las amenazas son el enemigo exterior y los activos quedan dentro del recinto amurallado. Esta visión estática de la seguridad se ha comprobado que es ineficiente en los Sistemas de Información actuales que son cada vez más “ciudades abiertas” sujetas a nuevos tipos de amenazas intencionales. MAGERIT ofrece un submodelo de eventos de la metodología con tres puntos de vista: Vista estática relacional. En esta vista se ponen de manifiesto las relaciones existentes entre las entidades vistas en el Submodelo de Elementos. Es necesaria para establecer el Modelo Lógico de Datos que requerirán toda herramienta de apoyo de MAGERIT. Vista dinámica de tipo organizativo. Recoge de forma detallada la forma que tienen de interactuar entre sí las entidades del Submodelo de Elementos. Es necesaria para la construcción de herramientas de apoyo de MAGERIT, para determinar las técnicas de cálculo de riesgos y de selección de salvaguardas, y como soporte al Submodelo Lógico de Procesos. Vista dinámica de tipo físico. Recoge otra forma de funcionamiento de la interactuación entre los Elementos, con un nivel intermedio de detalle entre las dos vistas anteriores. Esta visión no es imprescindible para la comprensión de la metodología y será necesaria sólo en determinadas situaciones. Es útil para dar apoyo a ciertas técnicas de cálculo de riesgos y de selección de salvaguardas como las ténicas de simulación. Vista Estática Relacional La vista estática relacional recoge las relaciones directas entre las entidades del Submodelo de Elementos. El esquema básico se muestra en la siguiente figura: El Activo se relaciona directamente con las entidades: Activo: un activo puede ser componente de otro activo o depender de él. Vulnerabilidad: un activo puede tener vulnerabilidades respecto a diversas amenazas. Impacto: un activo se puede ver afectado por varios impactos procedentes de diversas amenazas. Salvaguarda: una salvaguarda puede proteger un activo. La Amenaza (si se materializa) se relaciona con: Amenaza: una amenaza puede ser componente de otra amenaza o depender de ella. Vulnerabilidad: una amenaza tiene que aprovecharse de una vulnerabilidad para afectar a un activo. Impacto: una amenaza puede causar un impacto sobre un activo. El Impacto se relaciona con: Activo: un impacto se produce sobre un activo. Amenaza: un impacto puede causarlo la materialización de una amenaza. Salvaguarda: un impacto puede ser influenciado por un servicio de salvaguarda. La Vulnerabilidad se relaciona con: Activo: una vulnerabilidad se produce sobre un activo. Amenaza: una amenaza puede sacar provecho de una vulnerabilidad. Salvaguarda: una vulnerabilidad puede ser influenciada por un servicio de salvaguarda. Riesgo: una vulnerabilidad contribuye al riesgo. El riesgo se relaciona con: Vulnerabilidad: un riesgo se refiere a la vulnerabilidad de un activo. Impacto: un riesgo se refiere al impacto propiciado por la vulnerabilidad de un activo. Salvaguarda: un riesgo puede ser influenciado por una salvaguarda. La salvaguarda se relaciona con: Activo: una salvaguarda protege un activo. Impacto: una salvaguarda puede afectar al impacto de una amenaza. Vulnerabilidad: una salvaguarda puede afectar a la vulnerabilidad una amenaza. Riesgo: una salvaguarda influye en un riesgo. Vista Dinámica Organizativa En esta vista se parte primero de una distinción entre el Dominio de Información que MAGERIT está ayudando a analizar dentro de la Organización, y el Entrono del Dominio delimitado. Dentro de este último se diferencia entre una parte interna a la Organización y otra externa. El dominio está formado por activos y tiene como sujeto principal al Responsable del Dominio o de los Activos protegibles, que es quien establece su valor y las necesidades de seguridad mediante objetivos y decisiones. El Dominio, definido en cada momento por su estado de seguridad, está sujeto a cambios en su estado por las acciones de las amenazas materializables y también por las salvaguardas. A partir de esta materialización desencadenante, el Submodelo de Eventos funciona dinámicamente como un escenario en el que se establecen acciones para obtener la seguridad. Se distinguen dos subescenarios: Subescenario de análisis de las amenazas (o de ataque): es el análisis de riesgos. Parte de un ataque y analiza las consecuencias que tiene sobre los activos del dominio. Subescenario de análisis y gestión de salvaguardas (o de defensa): es la gestión de riesgos. Describe para cada ataque la gestión de las funciones de salvaguarda que sean apropiadas. El subescenario de análisis de las amenazas considera que cada Activo del dominio, si la Amenaza se ha materializado en agresión aprovechando la Vulnerabilidad del activo con respecto a esa Amenaza, la Vulnerabilidad y su Impacto sobre el Activo determinan el Riesgo calculado . Su contribución es asimétrica, es decir, se considera que el Impacto influye más en el nivel de Riesgo que la Vulnerabilidad. La determinación del Riesgo calculado cierra este subescenario de ataque y abre el subescenario de defensa. El subescenario de análisis y gestión de las salvaguardas se caracteriza por la toma de decisiones. Se comienza por comparar el riesgo calculado con el umbral de riesgo asumible. Tras la implantación de las salvaguardas se recalcula el riesgo calculado obteniéndose el llamado Riesgo residual . Si este es inferior al umbral de riesgo asumible, se considera que el subescenario de defensa ha cumplido su objetivo y se puede pasar a otras fases de Gestión de Seguridad de los Sistemas de Información. Si el riesgo residual es superior al umbral de riesgo asumible, el subescenario de defensa no ha cumplido su objetivo y MAGERIT propone incorporar nuevas salvaguardas y repetir todo el proceso hasta que el riesgo residual sea menor al umbral de riesgo aceptable. Las salvaguardas se incorporan en distintos momentos y con distintas formas. Unas son anteriores a la materialización de la amenaza y otras posteriores. En el subescenario de defensa se incorporan las salvaguardas en dos niveles: Incorporación funcional de Servicios de Salvaguarda. Incorporación material de Mecanismos de Salvaguarda. Vista Dinámica Física Esta vista puede resulta útil para los especialistas en seguridad que utilizan MAGERIT en sistemas críticos que necesitan modelos específicos de simulación de procesos con el propósito de observar el funcionamiento y determinar los parámetros óptimos a efectos de seguridad. También es útil para dar soporte a herramientas sofisticadas de selección de salvaguardas y cálculo de riesgos. Las Entidades de Seguridad pueden identificarse con las variables y parámetros de nivel (de estado) y de flujo (de acción) que caracterizan la simulación de los modelos de sistemas dinámicos. Bajo este enfoque, cada agresión se muestra como un flujo de la acción que modifica el nivel de seguridad de un activo, es decir, es una acción que lo hace pasar de un estado de seguridad a otro. El impacto es la medida del resultado de la agresión sobre el activo, es decir, es la medida del flujo de la acción de cambio de nivel de seguridad del activo. El Riesgo es un indicador del nivel de seguridad. Submodelo de ProcesosEn el submodelo de procesos se ordenan las acciones que se van a realizar durante un proyecto de Análisis y Gestión de Riesgos. Tiene una estructura jerárquica de tres niveles: etapas, actividades y tareas. Las etapas se componen de actividades y éstas de tareas. Las tareas tienen la misma acepción que en METRICA V3. Para describirlas se especifican los siguientes puntos: Acciones que se van a realizar. Actores que intervienen en la tarea o que están afectados por ésta. Productos y documentos que se van a obtener. Validaciones que se van a realizar de los resultados obtenidos. Técnicas que se van a utilizar. Las actividades agrupan a un conjunto de tareas siguiendo generalmente criterios funcionales. Tienen la misma acepción que en METRICA V3. Las etapas agrupan a un conjunto de actividades y se corresponden con los procesos de METRICA V3 (fases en METRICA V2.1). Las etapas marcan los puntos de toma de decisiones y entrega de productos. Es necesario que al final de cada etapa haya una aceptación formal de sus resultados ya que se utiliza el producto final de cada etapa como inicio de la siguiente. MAGERIT propone las cuatro etapas siguientes: Etapa 1. Planificación del Análisis y Gestión de Riesgos. Su objetivo es definir el marco de referencia para la realización del proyecto de Análisis y Gestión de Riesgos. Etapa 2. Análisis de Riesgos. Se identifican las entidades y se valoran, obteniendo una evaluación del riesgo y una estimación del umbral de riesgo aceptable. Etapa 3. Gestión de Riesgos. Se identifican las funciones de salvaguarda para los riesgos detectados en la etapa anterior y se seleccionan las que son aceptables basándose en las ya existentes y en las restricciones. Etapa 4. Selección de salvaguardas. Se seleccionan los mecanismos de salvaguardas que se van a implantar y los procedimientos de seguimiento de dicha implantación. MAGERIT tiene interfaces de enlace con METRICA V3, enlazando con los procesos de Planificación, Análisis, Diseño, Construcción e Implementación. En cada enlace se recogen los productos, los trata con procedimientos de aseguramiento y devuelve las salvaguardas. Etapa 1. Planificación del Análisis y Gestión de Riesgos En esta etapa se establecen las condiciones necesarias para iniciar el proyecto de Análisis y Gestión de Riesgos: se definen los objetivos del proyecto y el ámbito de actuación (dominio), se planifican los medios materiales y humanos para su ejecución y se particularizan las técnicas que se van a utilizar en las actividades del proyecto. MAGERIT incluye en esta etapa las siguientes actividades: Oportunidad de realización. Definición de dominio y objetivos. Organización y planificación del proyecto. Lanzamiento del proyecto. La primera actividad, oportunidad de realización, tiene por objetivo despertar el interés de la Dirección en el proyecto de Análisis y Gestión de Riesgos y consta de una única tarea, clarificar la oportunidad de realización. La iniciativa de la realización del proyecto parte de un promotor (interno o externo a la organización) que es consciente de los problemas de seguridad que existen. Este sujeto elabora un cuestionario sobre aspectos de seguridad para ser respondido tanto por los responsables de informática como del resto de unidades de la Organización con el objetivo de recabar información y sensibilizar a la Organización sobre las cuestiones de seguridad. Con estos elementos realiza un informe preliminar en el que se incluyen los argumentos básicos para la realización del proyecto de Análisis y Gestión de Riesgos y una primera aproximación del Dominio del proyecto y de los medios materiales y humanos necesarios para hacerlo. En la segunda actividad, definición de dominio y objetivos, se definen los límites del Dominio y los objetivos finales del proyecto. Además se identifican las restricciones que hay que considerar y las personas de las que se va a recoger información. Consta de las siguientes tareas: Especificar los objetivos del proyecto. Definir el dominio y los límites del proyecto. Identificar el entorno y las restricciones generales. Estimar dimensión, coste y retornos del proyecto. En la tercera actividad, organización y planificación del proyecto, se calcula la carga de trabajo que va a suponer el proyecto y se establece el grupo y plan de trabajo. Las tareas de que consta son: Evaluar cargas y planificar entrevistas. Organizar a los participantes. Planificar el trabajo. En la cuarta actividad, lanzamiento del proyecto, se asignan los recursos requeridos para el comienzo del proyecto y se eligen las técnicas primordiales de Análisis y Gestión de Riesgos. Incluye las siguientes tareas: Adaptar los cuestionarios. Seleccionar criterios de evaluación y técnicas para el proyecto. Asignar los recursos necesarios. Sensibilizar (campaña informativa). Etapa 2. Análisis de Riesgos Esta etapa constituye el núcleo de MAGERIT, la utilidad y validez de todo el proyecto depende de su correcta aplicación. El objetivo principal es la evaluación del riesgo del sistema en estudio, tanto del intrínseco como del efectivo, al que se añaden la presentación al Comité de Dirección de las áreas de mayor riesgo y la aprobación de los umbrales de riesgo asumibles. La estimación de los Impactos y Vulnerabilidades es una tarea compleja y con un cierto grado de incertidumbre, lo que unido a su enorme influencia en la determinación del Riesgo trae consigo que deba ser dirigida por un grupo de expertos. La etapa de Análisis de Riesgos comprende las siguientes actividades: Recogida de información. Identificación y agrupación de activos. Identificación y evaluación de amenazas. Identificación y estimación de vulnerabilidades. Identificación y valoración de impactos. Evaluación del Riesgo. Durante la primera actividad, recogida de información, se recoge la información del sistema y de los factores que influyen en la seguridad. Comprende las siguientes tareas: Preparar la información. Realizar las entrevistas. Analizar la información recogida. En la segunda actividad, identificación y agrupación de activos, se identifican los activos y sus relaciones, y se profundiza en sus características a partir de la información recogida en la actividad anterior. Incluye las tareas: Identificar y agrupar activos. Identificar los mecanismos de salvaguarda existentes. Valorar activos. En la tercera actividad, identificación y evaluación de amenazas, se incluyen las tareas: Identificar y agrupar amenazas. Establecer los árboles de fallos generados por amenazas. La cuarta actividad, identificación y estimación de vulnerabilidades, incluye las tareas: Identificar vulnerabilidades. Estimar vulnerabilidades. La quinta actividad, identificación y valoración de impactos, incluye las tareas: Identificar impactos. Tipificar impactos. Valorar impactos. En la sexta etapa, evaluación del riesgo, se incluyen las tareas: Evaluar el riesgo intrínseco. Analizar las funciones de salvaguarda existentes. Evaluar el riesgo efectivo. Etapa 3. Gestión del Riesgo En esta etapa se seleccionan los servicios de salvaguarda más apropiados con el objetivo de reducir el riesgo hasta niveles que se consideren aceptables. Comprende las siguientes actividades: Interpretación del riesgo. Identificación de servicios de salvaguarda y estimación de su efectividad. Selección de los servicios de salvaguarda. Cumplimiento de objetivos. La primera actividad, interpretación del riesgo, tiene una única tarea, interpretar y manejar los riesgos. El riesgo efectivo calculado en la etapa anterior se compara con el umbral de riesgo para determinar si es asumible. Si no lo es, MAGERIT propone continuar con las dos siguientes actividades de la etapa: identificación de servicios de salvaguarda y estimación de su efectividad, y selección de los servicios de salvaguarda. La segunda actividad, Identificación de servicios de salvaguarda y estimación de su efectividad, comprende las tareas: Identificar funciones y servicios de salvaguarda. Estimar la efectividad de las funciones y servicios de salvaguarda. La tercera actividad, selección de servicios de salvaguarda, incluye las tareas: Aplicar los parámetros de selección (ordena la lista de funciones desalvaguarda según su efectividad para reducir el riesgo). Reevaluar el riesgo. La última actividad, cumplimiento de objetivos, tiene una única tarea: determinar el cumplimiento de objetivos. Si los riesgos efectivos calculados en la tarea anterior no están por debajo de los umbrales de riesgo fijados se conservan provisionalmente los resultados parciales alcanzados y se vuelve a repetir toda la actividad de selección de los servicios de salvaguarda. Etapa 4. Selección de Salvaguardas En esta etapa se eligen los mecanismos de salvaguarda que materializan las funciones y servicios de salvaguarda seleccionados previamente, tomando como base su efectividad. Se estudian sus tipos, costes y relaciones y se analiza si existen contraindicaciones en su aplicación. Finalmente se establece un orden para su implantación. Las actividades de esta etapa, y las tareas de cada una de ellas, son: Identificación de los mecanismos de salvaguarda. Identificar posibles mecanismos. Estudiar mecanismos implantados. Incorporar restricciones. Selección de los mecanismos. Identificar mecanismos a implantar. Evaluar el riesgo con los mecanismos elegidos. Seleccionar mecanismos a implantar. Especificación de los mecanismos a implantar. Especificar los mecanismos a implantar. Orientación a la planificación de la implantación. Priorizar mecanismos. Evaluar los recursos necesarios. Evaluar cronogramas tentativos. Integración de resultados. Integrar los resultados. Bibliografía Scribd (Ibiza Ales) Auditoría Informática: objetivos, alcance y metodología. Técnicas y herramientas. Normas y estándares. Auditoría del ENS y de protección de datos. Auditoría de seguridad física.Auditoria InformáticaLa auditoria informática es un proceso llevado a cabo por profesionales especialmente capacitados para el efecto, y que consiste en recoger, agrupar y evaluar evidencias para determinar si un sistema de información salvaguarda el activo empresarial, mantiene la integridad de los datos, lleva a cabo eficazmente los fines de la organización, utiliza eficientemente los recursos, y cumple con las leyes y regulaciones establecidas. Permiten detectar de forma sistemática el uso de los recursos y los flujos de información dentro de una organización y determinar qué información es crítica para el cumplimiento de su misión y objetivos, identificando necesidades, duplicidades, costes, valor y barreras, que obstaculizan flujos de información eficientes. La auditoria informática es el proceso de recoger, agrupar y evaluar evidencias para determinar si un sistema de información salvaguarda el activo empresarial, mantiene la integridad de los datos, lleva a cabo eficazmente los fines de la organización y utiliza eficientemente los recursos. Auditar consiste principalmente en estudiar los mecanismos de control que están implantados en una empresa u organización, determinando si los mismos son adecuados y cumplen unos determinados objetivos o estrategias, estableciendo los cambios que se deberían realizar para la consecución de los mismos. Los mecanismos de control pueden ser directivos, preventivos, de detección, correctivos o de recuperación ante una contingencia. Los objetivos de la auditoria Informática son: El control de la función informática. El análisis de la eficiencia de los Sistemas Informáticos. La verificación del cumplimiento de la Normativa en este ámbito. La revisión de la eficaz gestión de los recursos informáticos. La auditoria informática sirve para mejorar ciertas características en la empresa como: Desempeño Fiabilidad Eficacia Rentabilidad Seguridad Privacidad Generalmente se puede desarrollar en alguna o combinación de las siguientes áreas: Gobierno corporativo Administración del Ciclo de vida de los sistemas Servicio de Entrega y Soporte Protección y Seguridad Planes de continuidad y Recuperación de desastres La necesidad de contar con lineamientos y herramientas estándar para el ejercicio de la auditoria informática ha promovido la creación y desarrollo de mejores prácticas como COBIT , COSO e ITIL . Actualmente la certificación de ISACA para ser CISA Certified Information Systems Auditor es una de las más reconocidas y avaladas por los estándares internacionales ya que el proceso de selección consta de un examen inicial bastante extenso y la necesidad de mantenerse actualizado acumulando horas (puntos) para no perder la certificación. Tipos de Auditoria InformáticaDentro de la auditoria informática destacan los siguientes tipos (entre otros): Auditoria de la gestión : La contratación de bienes y servicios, documentación de los programas, etc. Auditoria legal del Reglamento de Protección de Datos : Cumplimiento legal de las medidas de seguridad exigidas por el Reglamento de desarrollo de la Ley Orgánica de Protección de Datos. Auditoria de los datos : Clasificación de los datos, estudio de las aplicaciones y análisis de los flujogramas. Auditoria de las bases de datos : Controles de acceso, de actualización, de integridad y calidad de los datos. Auditoria de la seguridad : Referidos a datos e información verificando disponibilidad, integridad, confidencialidad, autenticación y no repudio. Auditoria de la seguridad física : Referido a la ubicación de la organización, evitando ubicaciones de riesgo, y en algunos casos no revelando la situación física de esta. También está referida a las protecciones externas (arcos de seguridad, CCTV, vigilantes, etc) y protecciones del entorno. Auditoria de la seguridad lógica : Comprende los métodos de autenticación de los sistemas de información. Auditoria de las comunicaciones : Se refiere a la auditoria de los procesos de autenticación en los sistemas de comunicación. Auditoria de la seguridad en producción : Frente a errores, accidentes y fraudes. Principales pruebas y herramientas para efectuar una auditoria informáticaEn la realización de una auditoria informática el auditor puede realizar las siguientes pruebas: Pruebas sustantivas : Verifican el grado de confiabilidad del SI del organismo. Se suelen obtener mediante observación, cálculos, muestreos, entrevistas, técnicas de examen analítico, revisiones y conciliaciones. Verifican asimismo la exactitud, integridad y validez de la información. Pruebas de cumplimiento : Verifican el grado de cumplimiento de lo revelado mediante el análisis de la muestra. Proporciona evidencias de que los controles claves existen y que son aplicables efectiva y uniformemente. Las principales herramientas de las que dispone un auditor informáticos son: Observación Realización de cuetionarios Entrevistas a auditados y no auditados Muestreo estadístico (Trazas y/o huellas) Flujogramas Listas de chequeo (checklist) Mapas conceptuales Inventario Fases Auditoria Informática Fase I: Conocimientos del Sistema Aspectos Legales y Políticas Internas Características del Sistema Operativo Características de la aplicación de computadora Fase II: Análisis de transacciones y recursos Definición de transacciones Análisis de las transacciones Análisis de los recursos Relación entre transacciones y recursos Fase III: Análisis de riesgos y amenazas Identificación de riesgos Identificación de amenazas Relación entre recursos/amenazas/riesgos Fase IV: Análisis de controles Codificación de controles Relación entre controles Análisis de cobertura de los controles requeridos Fase V: Evaluación de controles Objetivos de la evaluación Plan de pruebas de los controles Pruebas de controles Análisis de resultados de las pruebas Fase VI: El informe de auditoria Informe detallado de recomendaciones Evaluación de las respuestas Informe resumen para la alta gerencia Fase VII: Seguimiento de las Recomendaciones Informes de seguimiento Evaluación de los controles implantados Normas, técnicas y procedimientos de auditoria en informáticaEl desarrollo de una auditoria se basa en la aplicación de normas, técnicas y procedimientos de auditoria. Es fundamental mencionar que para el auditor en informática conocer los productos de software que han sido creados para apoyar su función aparte de los componentes de la propia computadora resulta esencial, esto por razones económicas y para facilitar el manejo de la información. El auditor desempeña sus labores mediante la aplicación de una serie de conocimientos especializados que vienen a formar el cuerpo técnico de su actividad. El auditor adquiere responsabilidades, no solamente con la persona que directamente contratan sus servicios, sino con un número de personas desconocidas para él que van a utilizar el resultado de su trabajo como base para tomar decisiones. La auditoria no es una actividad meramente mecánica, que implique la aplicación de ciertos procedimientos cuyos resultados, una vez llevados a cabo son de carácter indudable. La auditoria requiere el ejercicio de un juicio profesional, sólido, maduro, para juzgar los procedimientos que deben seguirse y estimar los resultados obtenidos. NormasLas normas de auditoria son los requisitos mínimos de calidad relativos a la personalidad del auditor, al trabajo que desempeña y a la información que rinde como resultado de este trabajo. Las normas de auditoria se clasifican en: Normas personales : son cualidades que el auditor debe tener para ejercer sin dolo una auditoria, basados en sus conocimientos profesionales así como en un entrenamiento técnico, que le permita ser imparcial a la hora de dar sus sugerencias. Normas de ejecución del trabajo : son la planificación de los métodos y procedimientos, tanto como papeles de trabajo a aplicar dentro de la auditoria. Normas de información : son el resultado que el auditor debe entregar a los interesados para que se den cuenta de su trabajo, también es conocido como informe o dictamen. TécnicasSe define a las técnicas de auditoria como “los métodos prácticos de investigación y prueba que utiliza el auditor para obtener la evidencia necesaria que fundamente sus opiniones y conclusiones, su empleo se basa en su criterio o juicio, según las circunstancias”. Al aplicar su conocimiento y experiencia el auditor, podrá conocer los datos de la empresa u organización a ser auditada, que pudieran necesitar una mayor atención. Las técnicas procedimientos están estrechamente relacionados, si las técnicas no son elegidas adecuadamente, la auditoria no alcanzará las normas aceptadas de ejecución, por lo cual las técnicas así como los procedimiento de auditoria tienen una gran importancia para el auditor. Según el IMCP en su libro Normas y procedimientos de auditoria las técnicas se clasifican generalmente con base en la acción que se va a efectuar, estas acciones pueden ser oculares, verbales, por escrito, por revisión del contenido de documentos y por examen físico. Siguiendo esta clasificación las técnicas de auditoria se agrupan específicamente de la siguiente manera: Estudio General Análisis Inspección Confirmación Investigación Declaración Certificación Observación Cálculo ProcedimientosAl conjunto de técnicas de investigación aplicables a un grupo de hechos o circunstancias que nos sirven para fundamentar la opinión del auditor dentro de una auditoria, se les dan el nombre de procedimientos de auditoria informática. La combinación de dos o más procedimientos, derivan en programas de auditoria, y al conjunto de programas de auditoria se le denomina plan de auditoria, el cual servirá al auditor para llevar una estrategia y organización de la propia auditoria. El auditor no puede obtener el conocimiento que necesita para sustentar su opinión en una sola prueba, es necesario examinar los hechos, mediante varias técnicas de aplicación simultánea. En General los procedimientos de auditoria permiten: Obtener conocimientos del control interno. Analizar las características del control interno. Verificar los resultados de control interno. Fundamentar conclusiones de la auditoria. Por esta razón el auditor deberá aplicar su experiencia y decidir cual técnica o procedimiento de auditoria serán los más indicados para obtener su opinión. Análisis de datos Dentro de este trabajo, desarrollaremos diversos tipos de técnicas y procedimientos de auditoria, de los cuales destacan el análisis de datos, ya que para las organizaciones el conjunto de datos o información son de tal importancia que es necesario verificarlos y comprobarlos, así también tiene la misma importancia para el auditor ya que debe de utilizar diversas técnicas para el análisis de los datos, los cuales se describen a continuación: Comparación de programas : esta técnica se emplea para efectuar una comparación de código (fuente, objeto o comandos de proceso) entre la versión de un programa en ejecución y la versión de un programa piloto que ha sido modificado en forma indebida, para encontrar diferencias. Mapeo y rastreo de programas : esta técnica emplea un software especializado que permite analizar los programas en ejecución, indicando el número de veces que cada línea de código es procesada y las de las variables de memoria que estuvieron presentes. Análisis de código de programas : se emplea para analizar los programas de una aplicación. El análisis puede efectuarse en forma manual (en cuyo caso sólo se podría analizar el código ejecutable). Datos de prueba : se emplea para verificar que los procedimientos de control incluidos en los programas de una aplicación funcionen correctamente. Los datos de prueba consisten en la preparación de una serie de transacciones que contienen tanto datos correctos como datos erróneos predeterminados. Datos de prueba integrados : técnica muy similar a la anterior, con la diferencia de que en ésta se debe crear una entidad falsa dentro de los sistemas de información. Análisis de bitácoras : existen varios tipos de bitácoras que pueden ser analizadas por el auditor, ya sea en forma manual o por medio de programas especializados, tales como bitácoras de fallas del equipo, bitácoras de accesos no autorizados, bitácoras de uso de recursos, bitácoras de procesos ejecutados. Simulación paralela : técnica muy utilizada que consiste en desarrollar programas o módulos que simulen a los programas de un sistema en producción. El objetivo es procesar los dos programas o módulos de forma paralela e identificar diferencias entre los resultados de ambos. Monitoreo Dentro de las organizaciones todos los procesos necesitan ser evaluados a través del tiempo para verificar su calidad en cuanto a las necesidades de control, integridad y confidencialidad, este es precisamente el ámbito de esta técnica, a continuación se muestran los procesos de monitoreo: M1 Monitoreo del proceso. M2 Evaluar lo adecuado del control Interno. M3 Obtención de aseguramiento independiente. M4 Proveer auditoria independiente. Análisis de bitácoras Hoy en día los sistema de cómputo se encuentran expuestos a distintas amenazas, las vulnerabilidades de los sistemas aumentan, al mismo tiempo que se hacen más complejos, el número de ataques también aumenta, por lo anterior, las organizaciones deben reconocer la importancia y utilidad de la información contenida en las bitácoras de los sistemas de computo así como mostrar algunas herramientas que ayuden a automatizar el proceso de análisis de las mismas. El crecimiento de Internet enfatiza esta problemática, los sistemas de cómputo generan una gran cantidad de información, conocidas como bitácoras o archivos logs, que pueden ser de gran ayuda ante un incidente de seguridad, así como para el auditor. Una bitácora puede registrar mucha información acerca de eventos relacionados con el sistema que la genera, los cuales pueden ser: Fecha y hora Direcciones IP origen y destino Dirección IP que genera la bitácora Usuarios Errores La importancia de las bitácoras es la de recuperar información ante incidentes de seguridad, detección de comportamiento inusual, información para resolver problemas, evidencia legal, es de gran ayuda en las tareas de cómputo forense. Las Herramientas de análisis de bitácoras más conocidas son las siguientes: Para UNIX: Logcheck, Swatch Para Windows: LogAgent Las bitácoras contienen información crítica, es por ello que deben ser analizadas, ya que están teniendo mucha relevancia, como evidencia en aspectos legales. El uso de herramientas automatizadas es de mucha utilidad para el análisis de bitácoras, es importante registrar todas las bitácoras necesarias de todos los sistemas de cómputo para mantener un control de las mismas. Técnicas de auditoria asistida por computadora La utilización de equipos de computación en las organizaciones, ha tenido una repercusión importante en el trabajo del auditor, no sólo en lo que se refiere a los sistemas de información, sino también al uso de las computadoras en la auditoria. Al llevar a cabo auditorias donde existen sistemas computarizados, el auditor se enfrenta a muchos problemas de muy diversa condición, uno de ellos, es la revisión de los procedimientos administrativos de control interno establecidos en la empresa que es auditada. La utilización de paquetes de programas generalizados de auditoria ayuda en gran medida a la realización de pruebas de auditoria, a la elaboración de evidencias plasmadas en los papeles de trabajo. Según las técnicas de auditoria Asistidas por Computadora (CAAT) son la utilización de determinados paquetes de programas que actúan sobre los datos, llevando a cabo con más frecuencia los trabajos siguientes: Selección e impresión de muestras de auditorias sobre bases estadísticas o no estadísticas, a lo que agregamos, sobre la base de los conocimientos adquiridos por los auditores. Verificación matemática de sumas, multiplicaciones y otros cálculos en los archivos del sistema auditado. Realización de funciones de revisión analítica, al establecer comparaciones, calcular razones, identificar fluctuaciones y llevar a cabo cálculos de regresión múltiple. Manipulación de la información al calcular subtotales, sumar y clasificar la información, volver a ordenar en serie la información, etc. Examen de registros de acuerdo con los criterios especificados. Búsqueda de alguna información en particular, la cual cumpla ciertos criterios, que se encuentra dentro de las bases de datos del sistema que se audita. Evaluación del control interno En un ambiente de evolución permanente, determinado por las actuales tendencias mundiales, las cuales se centran en el plano económico soportadas por la evolución tecnológica, surge la necesidad de que la función de auditoria pretenda el mejoramiento de su gestión. La práctica de nuevas técnicas para evaluar el control interno a través de las cuales, la función de auditoria informática pretende mejorar la efectividad de su función y con ello ofrecer servicios más eficientes y con un valor agregado. La evolución de la teoría del control interno se definió en base a los principios de los controles como mecanismos o prácticas para prevenir, identificar actividades no autorizadas, más tarde se incluyó el concepto de lograr que las cosas se hagan; la corriente actual define al control como cualquier esfuerzo que se realice para aumentar las posibilidades de que se logren los objetivos de la organización. En este proceso evolutivo se considera actualmente, y en muchas organizaciones que el director de finanzas o al director de auditoria como los responsables principales del correcto diseño y adecuado funcionamiento de los controles internos. Benchmarking Las empresas u organizaciones deben buscar formas o fórmulas que las dirijan hacia una mayor calidad, para poder ser competitivos, una de estas herramientas o fórmulas es el Benchmarking. Benchmarking es el proceso continuo de medir productos, servicios y prácticas contra los competidores o aquellas compañías reconocidas como líderes en la industria. Esta definición presenta aspectos importantes tales como el concepto de continuidad, ya que benchmarking no sólo es un proceso que se hace una vez y se olvida, sino que es un proceso continuo y constante. Según la definición anterior podemos deducir que se puede aplicar benchmarking a todas las facetas de las organizaciones, y finalmente la definición implica que el benchmarking se debe dirigir hacia aquellas organizaciones y funciones de negocios dentro de las organizaciones que son reconocidas como las mejores. Otra definición puede ser: “benchmarking es un proceso sistemático y continuo para comparar nuestra propia eficiencia en términos de productividad, calidad y prácticas con aquellas compañías y organizaciones que representan la excelencia. Dentro del benchmarking existen los siguientes tipos: Benchmarking interno Benchmarking competitivo Benchmarking genérico Auditoria Informática como objeto de estudioLa auditoría en informática es la revisión y la evaluación de los controles, sistemas, procedimientos de informática; de los equipos de cómputo, su utilización, eficiencia y seguridad, de la organización que participan en el procesamiento de la información, a fin de que por medio del señalamiento de cursos alternativos se logre una utilización más eficiente y segura de la información que servirá para una adecuada toma de decisiones. Los factores que pueden influir en una organización a través del control y la auditoría en informática,son: Necesidad de controlar el uso evolucionado de las computadoras. Controlar el uso de la computadora, que cada día se vuelve más importante y costosa. Altos costos que producen los errores en una organización. Abuzo en las computadoras. Posibilidad de pérdida de capacidades de procesamiento de datos. Posibilidad de decisiones incorrectas. Valor del hardware, software y personal. Necesidad de mantener la privacidad individual. Posibilidad de pérdida de información o de mal uso de la misma. Necesidad de mantener la privacidad de la organización. La información es un factor importante y cada día cobra más valor para una empresa u organización para la continuidad de las operaciones, ya que la imagen de su ambiente depende de la situación actual, su desarrollo y competitividad dependen del ambiente pasado y futuro, ya que tomar una decisión incorrecta mediante datos erróneos proporcionados por los sistemas trae como consecuencia efectos significativos que afectan directamente a la organización. Objetivo fundamental de la auditoria InformáticaLa operatividad en una empresa es el punto más importante, es encargada de vigilar el funcionamiento de mínimos consistentes de la organización y las máquinas a nivel global como parcial. La auditoria se debe realizar en el momento en que la maquinaria informática está en funcionamiento, con el fin de identificar falencias que obstruyan la operatividad de las mismas, con el objeto de corregir o buscar alternativas de solución a tiempo, sin tener que parar el trabajo. La operatividad de los sistemas ha de constituir entonces la principal preocupación del auditor informático. Para conseguirla hay que acudir a la realización de Controles Técnicos Generales de Operatividad y Controles Técnicos Específicos de Operatividad, previos a cualquier actividad de aquel. Los Controles Técnicos Generales son importantes en las instalaciones de empresas grandes, ya que se realizan para verificar la compatibilidad de funcionamiento simultáneo del SO y el software de base con todos los subsistemas existentes, como también la compatibilidad del hardware y del software instalado. En una empresa existen diferentes entornos de trabajo que conlleva a la contratación de productos de software básico, así como software especial para algunos departamentos, con el riesgo de abonar más de una vez el mismo producto o desaprovechar el software instalado, así mismo puede existir software desarrollado por personal de sistemas de la misma empresa que hagan mal uso y que no se aprovechen todos los recursos de este, sobre todo cuando los diversos equipos están ubicados en Centros de Proceso de datos geográficamente alejados. Lo negativo de esta situación es que puede producir la inoperatividad del conjunto. Cada Centro de Proceso de Datos tal vez sea operativo trabajando independientemente, pero no será posible la interconexión e intercomunicación de todos los centros de proceso de datos si no existen productos comunes y compatibles. Los controles técnicos específicos, menos evidentes, son también necesarios para lograr la operatividad de los sistemas. Es decir por más pequeña que sea la aplicación que se deba ejecutar, esta debe funcionar al máximo, evitando así la inoperatividad, bien sea en hardware como en software. Una vez conseguida la operatividad de los sistemas, el segundo objetivo de la auditoria es la verificación de la observación de las normas teóricamente existentes en el departamento de informática y su coherencia con las del resto de la empresa. Para ello, habrán de revisarse sucesivamente y en este orden: Las normas generales de la instalación informática. Se realiza una revisión inicial sencilla, verificando la aplicación de las normas pero también registrando las áreas que no cumplan o que no las apliquen, sin olvidar que esta normativa no está en contradicción con alguna norma no informática de la empresa. Los procedimientos generales informáticos. Se verificará su existencia, al menos en los sectores más importantes. Por ejemplo, la recepción definitiva de las máquinas deberá estar firmada por la persona responsable de este cargo. Tampoco el alta de una nueva aplicación podría producirse si no existieran los Procedimientos de Backup y recuperación correspondientes. Los procedimientos específicos informáticos. Igualmente, se revisara su existencia en las áreas fundamentales. Así, explotación no debería explotar una aplicación sin haber exigido a desarrollo la pertinente documentación. Del mismo modo, deberá comprobarse que los procedimientos específicos no se opongan a los procedimientos generales. En todos los casos anteriores, a su vez, deberá verificarse que no existe contradicción alguna con la normativa y los procedimientos generales de la propia empresa, a los que la informática debe estar sometida. Características de la auditoria informáticaLa información de la empresa y para la empresa, siempre importante, se ha convertido en un activo real de la misma, como sus Stocks o materias primas si las hay. Por ende, han de realizarse inversiones informáticas, materia de la que se ocupa la auditoria de Inversión informática. Del mismo modo, los sistemas informáticos han de protegerse de modo global y particular: a ello se debe la existencia de la auditoria de seguridad informática en general, o a la auditoria de seguridad de alguna de sus áreas, como pudieran ser desarrollo o técnicas de sistemas. Cuando se producen cambios estructurales en la informática, se reorganiza de alguna forma su función: se está en el campo de la auditoria de organización informática. Estos tres tipos de auditorias engloban a las actividades auditoras que se realizan en una auditoria parcial. De otra manera: cuando se realiza una auditoria del área de desarrollo de proyectos de la informática de una empresa, es porque en ese desarrollo existen, además de ineficiencias, debilidades de organización, o de inversiones, o de seguridad, o alguna mezcla de ellas. Teniendo en cuenta lo anterior y partiendo de las diferentes actividades de sistemas que cada empresa tiene dentro de su organización dentro de las áreas generales, se establecen las siguientes divisiones de auditoria informática de Explotación, de sistemas, de comunicaciones y de desarrollo de proyectos. Estas son las áreas específicas de la auditoria informática más importantes. Cada área específica puede ser auditada desde los siguientes criterios generales, que pueden modificarse según sea el tipo de empresa a auditar: Desde su propio funcionamiento interno. Desde el apoyo que recibe de la dirección y, en sentido ascendente, del grado de cumplimiento de las directrices de ésta. Desde la perspectiva de los usuarios, destinatarios reales de la informática. Desde el punto de vista de la seguridad que ofrece la informática en general o la rama auditada. Clasificación de la auditoria informáticaAuditoria informática de explotación Explotación informática se encarga de obtener resultados informáticos, como son: listados impresos, ficheros soportados magnéticamente, órdenes automatizadas para lanzar o modificar procesos industriales, entre otras. Los datos es la materia prima que hay que transformar por medio del proceso informático (gobernado por programas), bajo el criterio de integridad y control de calidad y así lleguen finalmente al usuario. Para auditar explotación hay que auditar las sesiones que la componen y sus interrelaciones. Auditoria informática de sistemas Encargada de analizar todo lo concerniente a técnica de sistemas en todas sus facetas, teniendo como resultado en la actualidad que todo lo que forme el entorno general de sistemas, como son las comunicaciones, líneas y redes de las instalaciones informáticas, se auditen por separado. Dentro de la auditoria informática de sistemas se evalúa lo siguiente: Sistemas operativos : debe verificarse en primer lugar que los sistemas estén actualizados con las últimas versiones del fabricante, indagando las causas de las omisiones si las hubiera. El análisis de las versiones de los SO permite descubrir las posibles incompatibilidades entre otros productos de software básico adquiridos por la instalación y determinadas versiones de aquellas. Software básico : es fundamental para el auditor conocer los productos de software básico que han sido adquiridos aparte de la propia computadora. Esto, por razones económicas y por razones de comprobación de que la computadora podría funcionar sin el producto adquirido por el cliente. En cuanto al software desarrollado por el personal informático de la empresa, el auditor debe verificar que éste no agreda ni condiciona al Sistema. Igualmente, debe considerar el esfuerzo realizado en términos de costes, por si hubiera alternativas más económicas. Tunning : es el conjunto de técnicas de observación y de medidas encaminadas a la evaluación del comportamiento de los Subsistemas y del sistema en su conjunto. Las acciones de tunning deben diferenciarse de los controles habituales que realiza el personal de técnica de sistemas. El tunning posee una naturaleza más revisora, estableciéndose previamente planes y programas de actuación según los síntomas observados. Optimización de los sistemas y subsistemas : la técnica de sistemas debe realizar acciones permanentes de optimización como consecuencia de la realización de tunnings pre-programados o específicos. El auditor verificará que las acciones de optimización fueron efectivas y no comprometieron la operatividad de los sistemas ni el plan crítico de producción diaria de explotación. Administración de base de datos : el diseño de las BD, sean relacionales o jerárquicas, se ha convertido en una actividad muy compleja y sofisticada, por lo general desarrollada en el ámbito de técnica de sistemas, y de acuerdo con las áreas de desarrollo y usuarios de la empresa. Al conocer el diseño y arquitectura de éstas por parte de Sistemas, se les encomienda también su administración. Al auditor de BD analizará los sistemas de salvaguarda existentes, revisará finalmente la integridad y consistencia de los datos, así como la ausencia de redundancias entre ellos. Investigación y desarrollo : como empresas que utilizan y necesitan de informáticas desarrolladas, saben que sus propios efectivos están desarrollando aplicaciones y utilidades que, concebidas inicialmente para su uso interno, pueden ser susceptibles de adquisición por otras empresas, haciendo competencia las compañías del mismo campo. La auditoria informática deberá cuidar de que la actividad de investigación y desarrollo no interfiera ni dificulte las tareas fundamentales internas. La propia existencia de aplicativos para la obtención de estadísticas desarrollados por los técnicos de sistemas de la empresa auditada, y su calidad, proporcionan al auditor experto una visión bastante exacta de la eficiencia y estado de desarrollo de los sistemas. Auditoria informática de comunicaciones y redes Para el informático y para el auditor informático, el entramado que constituyen las redes nodales, líneas, concentradores, multiplexores, redes locales, etc. no son sino el soporte físico-lógico del tiempo real. El auditor tropieza con la dificultad técnica del entorno, pues ha de analizar situaciones y hechos alejados entre sí, y está condicionado a la participación del monopolio telefónico que presta el soporte. Como en otros casos, la auditoría de este sector requiere un equipo de especialistas, expertos simultáneamente en comunicaciones y en redes locales (no hay que olvidarse que en entornos geográficos reducidos, algunas empresas optan por el uso interno de redes locales, diseñadas y cableadas con recursos propios). El auditor de comunicaciones deberá inquirir sobre los índices de utilización de las líneas contratadas con información abundante sobre tiempos de desuso. Deberá proveerse de la topología de la red de comunicaciones, actualizada, ya que la desactualización de esta documentación significaría una grave debilidad. La inexistencia de datos sobre las líneas existen, cómo son y donde están instaladas, supondría que se bordea la inoperatividad informática. Sin embargo, las debilidades más frecuentes o importantes se encuentran en las disfunciones organizativas. La contratación e instalación de líneas va asociada a la instalación de los puestos de trabajo correspondientes (Pantallas, Servidores de redes locales, computadoras con tarjetas de comunicaciones, impresoras, etc). Todas estas actividades deben estar muy coordinadas y de ser posible, dependientes de una sola organización. Auditoria informática de desarrollo de proyectos o aplicaciones El desarrollo es una evolución del llamado análisis y programación de sistemas y aplicaciones, que a su vez, engloba muchas áreas que tiene la empresa. Una aplicación recorre las siguientes fases: Pre-requisitos del usuario (único o plural) y del entorno Análisis funcional Diseño Análisis orgánico (pre-programación y programación) Pruebas Entrega a explotación y alta para el proceso Estas fases deben estar sometidas a un exigente control interno, caso contrario, además del disparo de los costes, podrá producirse la insatisfacción del usuario. Finalmente, la auditoria deberá comprobar la seguridad de los programas en el sentido de garantizar que los ejecutados por la máquina sean exactamente los previstos y no otros. Auditoria de la seguridad informática La computadora es un instrumento que estructura gran cantidad de información, la cual puede ser confidencial para individuos, empresas o instituciones, y puede ser mal utilizada o divulgada a personas que hagan mal uso de esta. También pueden ocurrir robos, fraudes o sabotajes, virus, etc, que provoquen la destrucción total o parcial de la actividad computacional. Esta información puede ser de suma importancia, y el no tenerla en el momento preciso puede provocar retrasos sumamente costosos. Al auditar los sistemas se debe tener cuidado que no se tengan copias “piratas” o bien que, al conectarse en red con otras computadoras, no exista la posibilidad de transmisión del virus. El uso inadecuado de la computadora comienza desde la utilización de tiempo de máquina para usos ajenos de la organización, la copia de programas para fines de comercialización sin reportar los derechos de autor hasta el acceso por vía telefónica a BD a fin de modificar la información con propósitos fraudulentos. La seguridad en la informática abarca los conceptos de seguridad física y seguridad lógica. Metodología de auditoria informáticaComo auditor se debe recolectar toda la información general, que permita así mismo definir un juicio global objetivo siempre amparadas en pruebas o hechos demostrables. Dar como resultado un informe claro, conciso y a la vez preciso depende del análisis y experiencia del auditor, frente a diferentes entornos a evaluar, dependiendo de las debilidades y fortalezas encontradas en dicha empresa auditada. La recolección de información, el análisis, la aplicación de diferentes normas de acuerdo al tipo de auditoria, los hallazgos encontrados y pruebas que avalen estos resultados son indispensables en la realización de una auditoria. Para llegar al resultado hay que seguir una serie de pasos que permiten tener claridad y orden de la auditoria a aplicar. El método de trabajo del auditor pasa por las siguientes etapas: Alcance y objetivos de la auditoria informática. Estudio inicial del entorno auditable. Determinación de los recursos necesarios para realizar la auditoria. Elaboración del plan y de los programas de trabajo. Actividades propiamente dichas de la auditoria. Confección y redacción del informe final. Redacción de la carta de introducción o carta de presentación del informe final. Alcance y objetivos de la auditoria informática El alcance de la auditoria expresa los límites de la misma. Debe existir un acuerdo muy preciso entre auditores y clientes sobre las funciones, las materias y las organizaciones a auditar. A los efectos de acotar el trabajo, resulta muy beneficioso para ambas partes expresar las excepciones de alcance de la auditoria, es decir cuales materias, funciones u organizaciones no van a ser auditadas. Tanto los alcances como las excepciones deben figurar al comienzo del informe final. Estudio inicial del entorno auditable Esta etapa es una de las más importantes en el desarrollo de la auditoria, ya que el auditor debe conocer todos los procesos desarrollados, relacionado con el área tomada como caso de estudio. Para realizar dicho estudio ha de examinarse las funciones y actividades generales de la informática. Para su realización el auditor debe conocer lo siguiente: Organización: para el auditor, el conocimiento de quién ordena, quién diseña y quién ejecuta es fundamental. Para realizarlo el auditor deberá fijarse en: Organigrama : el organigrama expresa la estructura oficial de la organización a auditar. Permite identificar las jerarquías, dependencias y direcciones entre las áreas existentes. Departamentos: se entiende como departamento a los órganos que siguen inmediatamente a la Dirección. El equipo auditor describirá brevemente las funciones de cada uno de ellos. Relaciones Jerárquicas y funcionales entre órganos de la Organización: el auditor verificará si se cumplen las relaciones funcionales y jerárquicas previstas por el organigrama, o por el contrario detectará, por ejemplo, si algún empleado tiene dos jefes. Las de jerarquía implican la correspondiente subordinación. Las funcionales por el contrario, indican relaciones no estrictamente subordinables. Flujos de información: además de las corrientes verticales intra-departamentales, la estructura organizativa cualquiera que sea, produce corrientes de información horizontales y oblicuas extra-departamentales. Número de puestos de trabajo: el equipo auditor comprobará que los nombres de los puestos de trabajo de la organización corresponden a las funciones reales distintas. Número de personas por puesto de trabajo: es un parámetro que los auditores informáticos deben tener en cuenta ya que la inadecuación del personal determina que el número de personas que realizan las mismas funciones rara vez coincida con la estructura oficial de la organización. Entorno operacional : el auditor informático debe tener una referencia del entorno en el que va a desenvolverse y se obtiene determinando lo siguiente: Situación geográfica de los sistemas: se determinará la ubicación geográfica de los distintos centros de proceso de datos en la empresa, continuando con la verificación de la existencia de responsables en cada uno de ellos, así como el uso de los mismos estándares de trabajo. Arquitectura y configuración de hardware y software: cuando existen varios equipos, es fundamental la configuración elegida para cada uno de ellos, ya que los mismos deben constituir un sistema compatible e intercomunicado. La configuración de los sistemas está muy ligada a las políticas de seguridad lógica de las compañías, para esto es importante que los auditores, en su estudio inicial, tengan en su poder la distribución e interconexión de los equipos. Inventario de hardware y software: el auditor recabará información escrita, en donde figuren todos los elementos físicos y lógicos de la instalación. En cuanto al hardware figurarán las CPU’s, unidades de control local y remotas, perfiéricos de todo tipo, etc. El inventario de software debe contener todos los productos lógicos del sistema, desde el software básico hasta los programas de utilidad adquiridos o desarrollados internamente. Suele ser habitual clasificarlos en facturables y no facturables. Comunicación y redes de comunicación: al realizar el estudio inicial los auditores dispondrán del número, situación y características principales de las líneas, así como de los accesos a la red pública de comunicaciones, igualmente, poseerán información de las redes locales de la Empresa y todo lo que tenga que ver con la red de comunicaciones. Determinación de los recursos necesarios para realizar la auditoria Mediante los resultados del estudio inicial realizado se procede a determinar los recursos humanos y materiales que han de emplearse en la auditoria. Recursos humanos : la cantidad de recursos depende del volumen auditable. Las características y perfiles del personal seleccionado dependen de la materia auditable. Es igualmente señalable que la auditoria en general suele ser ejercida por profesionales universitarios y por otras personas de probada experiencia multidisciplinaria. Recursos materiales : los recursos materiales del auditor son de dos tipos: Recursos software como son, cantidad y complejidad de BD y ficheros, que son programas propios de la auditoria, son muy potentes y flexibles. Recursos materiales hardware: los recursos hardware que el auditor necesita son proporcionados por el cliente. Los procesos de control deben efectuarse necesariamente en las computadoras del auditado. Por lo cual habrá de convenir, tiempo de máquina, espacio de disco, impresoras ocupadas, scanner, etc. Elaboración del plan y de los programas de trabajo Una vez asignados los recursos, el responsable de la auditoria y sus colaboradores establecen un plan de trabajo y así, se procede a la programación del mismo. El plan se elabora teniendo en cuenta, entre otros criterios, los siguientes: Si la revisión debe realizarse por áreas generales o áreas específicas. Si la auditoria es global, de toda la informática, o parcial. El volumen determina no solamente el número de auditores necesarios, sino las especialidades necesarias del personal. En el Plan no se consideran calendarios, porque se manejan recursos genéricos y no específicos. En el Plan se establecen los recursos y esfuerzos globales que van a ser necesarios. En el Plan se establecen las prioridades de materias auditables, de acuerdo siempre con las prioridades del cliente. El Plan establece disponibilidad futura de los recursos durante la revisión. El Plan estructura las tareas a realizar por cada integrante del grupo. En el Plan se expresan todas las ayudas que el auditor ha de recibir del auditado. Una vez elaborado el plan, se procede a la programación de actividades, esta ha de ser lo suficientemente flexible como para permitir modificaciones a lo largo del proyecto. Actividades propiamente dichas de la auditoria informática La auditoria informática general se realiza por áreas generales o por áreas específicas. Si se examina por grandes temas, resulta evidente la mayor calidad y el empleo de más tiempo total y mayores recursos. Cuando la auditoria se realiza por áreas específicas, se abarcan de una vez todas las peculiaridades que afectan a la misma, de forma que el resultado se obtiene más rápidamente y con menor calidad. Existen técnicas que hacen que el auditor las aplique de acuerdo a su juicio y al tipo de auditoria a ejecutar son: Técnicas de Trabajo : Análisis de la información obtenida del auditado. Análisis de la información propia. Cruzamiento de las informaciones anteriores. Entrevistas. Simulación. Muestreos. Inspección. Confirmación. Investigación. Certificación. Observación. Herramientas : Cuestionario general inicial. Cuestionario Checklist. Estándares. Monitores. Simuladores (Generadores de datos). Paquetes de auditoria (Generadores de programas) Matrices de riesgo. Confección y redacción del informe final La función de la auditoria se materializa exclusivamente por escrito. Por lo tanto, la elaboración final es el exponente de su calidad. Resulta evidente la necesidad de redactar borradores e informes parciales previos al informe final, los que son elementos de contraste entre opinión entre auditor y auditado y que pueden descubrir fallos de apreciación en el auditor. Redacción de la carta de introducción o carta de presentación del informe final La carta de introducción tiene especial importancia porque en ella ha de resumirse la auditoria realizada. Se destina exclusivamente al responsable máximo de la empresa, o a la persona concreta que encargó o contrató la auditoria. Así como pueden existir tantas copias del informe final como solicite el cliente, la auditoria no hará copias de la citada carta de introducción. La carta de introducción poseerá los siguientes atributos: Tendrá como máximo 4 folios. Incluirá fecha, naturaleza, objetivos y alcance. Cuantificará la importancia de las áreas analizadas. Proporcionará una conclusión general, concretando las áreas de gran debilidad. Presentará las debilidades en orden de importancia y gravedad. En la carta de introducción no se escribirán nunca recomendaciones. Estructura del informe final : el informe comienza con la fecha de comienzo de la auditoria y la fecha de redacción del mismo. Se incluyen los nombres del equipo auditor y los nombres de todas las personas entrevistadas, con indicación de la jefatura, responsabilidad y puesto de trabajo que ostente. Siguiendo los siguientes pasos: Definición de objetivos y alcance de la auditoria. Enumeración de temas considerados. Cuerpos expositivo. Para cada tema, se seguirá el siguiente orden: Situación actual. Cuando se trate de una revisión periódica, en la que se analiza no solamente una situación sino además su evolución en el tiempo, se expondrá la situación prevista y la situación real. Tendencias. Se tratarán de hallar parámetros que permitan establecer tendencias futuras. Puntos débiles y amenazas. Recomendaciones y planes de acción. Constituyen junto con la exposición de puntos débiles, el verdadero objetivo de la auditoria informática. Redacción posterior de la carta de introducción o presentación. Modelo conceptual de la exposición del informe final: El informe debe incluir solamente hechos importantes. La inclusión de hechos poco relevantes o accesorios desvía la atención del lector. El informe debe consolidar los hechos que se describen en el mismo. El término de “hechos consolidados” adquiere un especial significado de verificación objetiva y de estar documentalmente probados y soportados. La consolidación de los hechos debe satisfacer, al menos los siguientes criterios: El hecho debe poder ser sometido a cambios. Las ventajas del cambio deben superar los inconvenientes derivados de mantener la situación. No deben existir alternativas viables que superen al cambio propuesto. La recomendación del auditor sobre el hecho, debe mantener o mejorar las normas y estándares existentes en la instalación. La aparición de un hecho en un informe de auditoria implica necesariamente la existencia de una debilidad que ha de ser corregida. Flujo del hecho o debilidad: Hecho encontrado. A de ser relevante para el auditor y para el cliente. A de ser exacto, y además convincente. No deben existir hechos repetidos. Consecuencias del hecho : las consecuencias deben redactarse de modo que sean directamente deducibles del hecho. Repercusión del hecho : se redactará las influencias directas que el hecho pueda tener sobre otros aspectos informáticos u otros ámbitos de la empresa. Conclusión del hecho : no deben redactarse conclusiones más que en los casos en que la exposición haya sido muy extensa o compleja. Recomendación del auditor informático Deberá entenderse por sí sola, por simple lectura. Deberá estar suficientemente soportada en el propio texto. Deberá ser concreta y exacta en el tiempo, para que pueda ser verificada su implementación. La recomendación se redactará de forma que vaya dirigida expresamente a la persona o personas que puedan implementarla. Herramientas y técnicas para la auditoria informáticaCuestionarios Las auditorias informáticas se materializan recabando información y documentación de todo tipo. Los informes finales de los auditores dependen de sus capacidades para analizar las situaciones de debilidad o fortaleza de los diferentes entornos. El trabajo de campo del auditor consiste en lograr toda la información necesaria para la emisión de un juicio global objetivo, siempre amparado en hechos demostrables, llamados también evidencias. Para esto, suele ser habitual comenzar solicitando la cumplimentación de cuestionarios pre-impresos que se envían a las personas concretas que el auditor cree adecuadas, sin que sea obligatorio que dichas personas sean las responsables oficiales de las diversas áreas a auditar. Estos cuestionarios no pueden ni deben ser repetidos para instalaciones distintas, sino diferentes y muy específicos para cada situación, y muy cuidados en su fondo y su forma. Sobre esta base, se estudia y analiza la documentación recibida, de modo que tal análisis determine a su vez la información que deberá elaborar el propio autor. El cruzamiento de ambos tipos de información es una de las bases fundamentales de la auditoria. Cabe aclarar, que esta primera fase puede omitirse cuando los auditores hayan adquirido por otros medios la información que aquellos pre-impresos hubiesen proporcionado. Entrevistas El auditor comienza a continuación las relaciones personales con el auditado. Lo hace de tres formas: Mediante la petición de documentación concreta sobre alguna materia de su responsabilidad. Mediante “entrevistas” en las que no se sigue un plan determinado ni un método estricto de sometimiento a un cuestionario. Por medio de entrevistas en las que el auditor sigue un método preestablecido de antemano y busca unas finalidades concretas. La entrevista es una de las actividades personales más importantes del auditor; en ellas, éste recoge más información, y mejor matizada, que la proporcionada por medios propios puramente técnicos o por las respuestas escritas a cuestionarios. Aparte de algunas cuestiones menos importantes, la entrevista entre auditor y auditado se basa fundamentalmente en el concepto de interrogatorio; es lo que hace un auditor, interroga y se interroga a sí mismo. El auditor informático experto entrevista al auditado siguiendo un cuidadoso sistema previamente establecido, consistente en que bajo la forma de una conversación correcta y lo menos tensa posible, el auditado conteste sencillamente y con pulcritud a una serie de preguntas variadas, también sencillas. Sin embargo, esta sencillez es solo aparente. Tras ella debe existir una preparación muy elaborada y sistematizada, y que es diferente para cada caso particular. Checklist El auditor profesional y experto es aquél que reelabora muchas veces sus cuestionarios en función de los escenarios auditados. Tiene claro lo que necesita saber, y por qué. Sus cuestionarios son vitales para el trabajo de análisis, cruzamiento y síntesis posterior, lo cual no quiere decir que haya que someter al auditado a unas preguntas estereotipadas que no conducen a nada. Muy por el contrario, el auditor conversará y hará preguntas “normales”, que en realidad servirán para la cumplimentación sistemática de sus cuestionarios, de sus checklists. Hay opiniones que descalifican el uso de las checklists, ya que consideran que leerle una pila de preguntas recitadas de memoria o leídas en voz alta descalifica al auditor informático. Pero esto no es usar checklists, es una evidente falta de profesionalismo. El profesionalismo pasa por un procesamiento interno de información a fin de obtener respuestas coherentes que permitan una correcta descripción de puntos débiles y fuertes. El profesionalismo pasa por poseer preguntas muy estudiadas que han de formularse flexiblemente. El conjunto de estas preguntas recibe el nombre de checklist. Salvo excepciones, las checklists deben ser contestadas oralmente, ya que superan en riqueza y generalización a cualquier otra forma. Según la claridad de las preguntas y el talante del auditor, el auditado responderá desde posiciones muy distintas y con disposición muy variable. El auditado, habitualmente informático de profesión, percibe con cierta facilidad el perfil técnico y los conocimientos del auditor, precisamente a través de las preguntas que éste le formula. Esta percepción configura el principio de autoridad y prestigio que el auditor debe poseer. Por ello, aun siendo importante tener elaboradas listas de preguntas muy sistematizadas, coherentes y clasificadas por materias, todavía lo es más el modo y el orden de su formulación. Las empresas externas de auditoria informática guardan sus checklists, pero de poco sirven si el auditor no las utiliza adecuada y oportunamente. No debe olvidarse que la función auditora se ejerce sobre bases de autoridad, prestigio y ética. El auditor deberá aplicar la checklist de modo que el auditado responda clara y concisamente. Se deberá interrumpir lo menos posible a éste, y solamente en los casos en que las respuestas se aparten sustancialmente de la pregunta. En algunas ocasiones, se hará necesario invitar a aquél a que exponga con mayor amplitud un tema concreto, y en cualquier caso, se deberá evitar absolutamente la presión sobre el mismo. Algunas de las preguntas de las checklists utilizadas para cada sector, deben ser repetidas. En efecto, bajo apariencia distinta, el auditor formulará preguntas equivalentes a las mismas o a distintas personas, en las mismas fechas, o en fechas diferentes. De este modo, se podrán descubrir con mayor facilidad los puntos contradictorios; el auditor deberá analizar los matices de las respuestas y reelaborar preguntas complementarias cuando hayan existido contradicciones, hasta conseguir la homogeneidad. El entrevistado no debe percibir un excesivo formalismo en las preguntas. El auditor, por su parte, tomará las notas imprescindibles en presencia del auditado, y nunca escribirá cruces ni marcará cuestionarios en su presencia. Los cuestionarios o checklists responden fundamentalmente a dos tipos de “filosofía” de calificación o evaluación: Checklist de rango Contiene preguntas que el auditor debe puntuar dentro de un rango preestablecido (por ejemplo, de 1 a 5, siendo 1 la respuesta más negativa y el 5 el valor más positivo). Ejemplo: Se supone que se está realizando una auditoria sobre la seguridad física de una instalación y, dentro de ella, se analiza el control de los accesos de personas y cosas al Centro de Cálculo. Podrían formularse las preguntas que figuran a continuación, en donde las respuestas tienen los siguientes significados: Muy deficiente. Deficiente. Mejorable. Aceptable. Correcto. Se figuran posibles respuestas de los auditados. Las preguntas deben sucederse sin que parezcan encorsetadas ni clasificadas previamente. Basta conque el auditor lleve un pequeño guión. La cumplimentación de la checklist no debe realizarse en presencia del auditado. Ejemplo: ¿Existe personal específico de vigilancia externa al edificio? Respuesta: No, solamente un guarda por la noche que atiende además otra instalación adyacente. Puntuación: 1 ¿Para la vigilancia interna del edificio, ¿Hay al menos un vigilante por turno en los aledaños del centro de cálculo? Respuesta: Si, pero sube a las otras 4 plantas cuando se le necesita. Puntuación: 2 ¿Hay salida de emergencia además de la habilitada para la entrada y salida de máquinas? Respuesta: Si, pero existen cajas apiladas en dicha puerta. Algunas veces la quitan. Puntuación: 3 El personal de comunicaciones, ¿Puede entrar directamente en la Sala de Computadoras? Respuesta: No, solo tiene tarjeta el Jefe de Comunicaciones. No se la da a su gente más que por causa muy justificada, y avisando casi siempre al jefe de explotación. Puntuación: 4 El resultado sería el promedio de las puntuaciones: (1 + 2 + 3 + 4 ) / 4 = 2,25 Checklist binaria Es la constituida por preguntas con respuesta única y excluyente: Si o No. Aritméticamente, equivalen a 1 (uno) o 0 (cero), respectivamente. Ejemplo: Se supone que se está realizando una revisión de los métodos de pruebas de programas en el ámbito de desarrollo de proyectos. ¿Existe Normativa de que el usuario final compruebe los resultados finales de los programas? Puntuación: 1 ¿Conoce el personal de desarrollo la existencia de la anterior normativa? Puntuación: 1 ¿Se aplica dicha norma en todos los casos? Puntuación: 0 ¿Existe una norma por la cual las pruebas han de realizarse con juegos de ensayo o copias de BD reales? Puntuación: 0 Obsérvese como en este caso están contestadas las siguientes preguntas: ¿Se conoce la norma anterior? Puntuación: 0 ¿Se aplica en todos los casos? Puntuación: 0 Las checklist de rango son adecuadas si el equipo auditor no es muy grande y mantiene criterios uniformes y equivalentes en las valoraciones. Permiten una mayor precisión en la evaluación que en la checklist binaria. Sin embargo, la bondad del método depende excesivamente de la formación y competencia del equipo auditor. Las checklists binarias siguen una elaboración inicial mucho más ardua y compleja. Deben ser de gran precisión, como corresponde a la suma precisión de la respuesta. Una vez construidas, tienen la ventaja de exigir menos uniformidad del equipo auditor y el inconveniente genérico del frente a la mayor riqueza del intervalo. No existen checklists estándar para todas y cada una de las instalaciones informáticas a auditar. Cada una de ellas posee peculiaridades que hacen necesarios los retoques de adaptación correspondientes en las preguntas a realizar. Trazas y/o huellas Con frecuencia, el auditor informático debe verificar que los programas, tanto de los sistemas como de usuario, realizan exactamente las funciones previstas, y no otras. Para ello se apoya en productos Software muy potentes y modulares que, entre otras funciones, rastrean los caminos que siguen los datos a través del programa. Muy especialmente, estas “Trazas” se utilizan para comprobar la ejecución de las validaciones de datos previstas. Las mencionadas trazas no deben modificar en absoluto el sistema. Si la herramienta auditora produce incrementos apreciables de carga, se convendrá de antemano las fechas y horas más adecuadas para su empleo. Por lo que se refiere al análisis del sistema, los auditores informáticos emplean productos que comprueban los valores asignados por técnica de sistemas a cada uno de los parámetros variables de las Librerías más importantes del mismo. Estos parámetros variables deben estar dentro de un intervalo marcado por el fabricante. A modo de ejemplo, algunas instalaciones descompensan el número de iniciadores de trabajos de determinados entornos o toman criterios especialmente restrictivos o permisivos en la asignación de unidades de servicio para según cuales tipos carga. Estas actuaciones, en principio útiles, pueden resultar contraproducentes si se traspasan los límites. Observación La observación es una de las técnicas más utilizadas en la recolección de información para aplicación de una auditoria, ya que a través de diferentes técnicas y métodos de observación permite recolectar directamente la información necesaria sobre el comportamiento del sistema, del área de sistemas, de las funciones, actividades y operaciones del equipo procesador o de cualquier otro hecho, acción o fenómeno del ámbito de sistemas. Existen diferentes tipos de observación, entre las cuales están: Observación directa. Observación indirecta. Observación oculta. Observación participativa. Observación no participativa. Introspección. Estrospección. Observación histórica. Observación controlada. Observación natural. Inventarios Esta forma de recopilación de información consiste en hacer un recuento físico de lo que se está auditando, consiste propiamente en comparar las cantidades reales existentes con las que debería haber para comprobar que sean iguales o, en caso contrario, para resaltar las posibles diferencias e investigar sus causas. Los principales tipos de inventarios aplicables en el ambiente de sistemas computacionales, son: Inventario de software. Inventario de hardware. Inventario de documentos. Inventario de documentos administrativos. Manuales de la organización. Manuales de procedimientos administrativos. Manuales de perfil de puestos. Otros manuales administrativos. Inventario de documentos técnicos para el sistema. Manuales e instructivos técnico del hardware, periféricos y componentes del sistema. Manuales e instructivos de mantenimiento físico del sistema (hardware), entre otros. Estándares de AuditoriaPara la realización y ejecución de una auditoria se hace necesario aplicar normas o estándares bajo los cuales las empresas deben regirse, de allí la importancia de identificar los estándares internacionales que en este caso, son: Directrices gerenciales de COBIT, desarrollado por la Information Systems Audit and Control Association (ISACA) Asociación de auditoria y control de los sistemas de información: las directrices generenciales son un marco internacional de referencias que abordan las mejores prácticas de auditoria y control de sistemas de información. Permiten que la gerencia incluya, comprenda y administre los riesgos relacionados con la tecnología de información y establezca el enlace entre los procesos de administración, aspectos técnicos, la necesidad de controles y los riesgos asociados. Uno de los objetivos de ISACA es promover estándares aplicables internacionalmente para cumplir con su visión. La estructura para los estándares de auditoria de SI brinda múltiples niveles de asesoría, como: Los auditores de SI respecto al nivel mínimo de desempeño aceptable requerido para cumplir con las responsabilidades profesionales indicadas en el código de ética Profesional de ISACA. La dirección y otras partes interesadas en las expectativas de la profesión con respecto al trabajo de sus profesionales. Los poseedores de la designación de auditor certificado de sistemas de información (Certified Information Systems Auditor, CISA) respecto a los requisitos que deben cumplir. El incumplimiento de estos estándares puede resultar en una investigación de la conducta del poseedor del certificado CISA por parte de la junta de directores de ISACA o del comité apropiado de ISACA y, en última instancia, en sanciones disciplinarias, así: The Management of the Control of data Infromation Technology, desarrollado por el Instituto Canadiense de Contadores Certificados (CICA): Este modelo está basado en el concepto de roles y establece responsabilidades relacionadas con seguridad y los controles correspondientes. Dichos roles están clasificados con base en siete grupos: administración general, gerentes de sistemas, dueños, agentes, usuarios de sistemas de información, así como proveedores de servicios, desarrollo y operaciones de servicios y soporte de sistemas. Además, hace distinción entre los conceptos de autoridad, responsabilidad y responsabilidad respecto a control y riesgo previo al establecimiento del control, en términos de objetivos, estándares y técnicas mínimas a considerar. Administración de la inversión de tecnología de Inversión: un marco para la evaluación y mejora del proceso de madurez, desarrollado por la oficina de contabilidad general de los Estados Unidos (GAO): Este modelo identifica los procesos críticos, asegurando el éxito de las inversiones de tecnología de información y comunicación electrónicas. Además los organiza en cinco niveles de madurez, similar al modelo CMM. Estándares de administración de calidad y aseguramiento de calidad ISO 9000, desarrollados por la Organización Internacional de Estándares (ISO): La colección ISO 9000 es un conjunto de estándares y directrices que apoyan a las organizaciones a implementar sistemas de calidad efectivos, para el tipo de trabajo que ellos realizan. SysTrust – Principios y criterios de confiabilidad de sistemas, desarrollados por la Asociación de Contadores Públicos (AICPA) y el CICA: Este servicio pretende incrementar la confianza de la alta gerencia, clientes y socios, con respecto a la confiabilidad en los sistemas por una empresa o actividad en particular. Este modelo incluye elementos como: infraestructura, software de cualquier naturaleza, personal especializado y usuarios, procesos manuales y automatizados, y datos. El modelo persigue determinar si un sistema de información es confiable, (si un sistema funciona sin errores significativos, o fallas durante un periodo de tiempo determinado bajo un ambiente dado). Modelo de evolución de capacidades de software (CMM), desarrollado por el Instituto de Ingeniería de software (SEI): Este modelo hace posible evaluar las capacidades o habilidades para ejecutar, de una organización, con respecto al desarrollo y mantenimiento de sistemas de información. Consiste en 18 sectores clave, agrupados alrededor de cinco niveles de madurez. Se puede considerar que CMM es la base de los principios de evaluación recomendados por COBIT, así como para algunos de los procesos de administración de COBIT. Administración de sistemas de información: una herramienta de evaluación práctica, desarrollado por la directiva de recursos de tecnología de información (ITRB): Este es una herramienta de evaluación que permite a entidades gubernamentales, comprender la implementación estratégica de tecnología de información y comunicación electrónica que puede apoyar su misión e incrementar sus productos y servicios. Guía para el cuerpo de conocimientos de administración de proyectos, desarrollado por el comité de estándares del instituto de administración de proyectos: esta guía está enfocada en las mejores prácticas sobre administración de proyectos. Se refiere a aspectos sobre los diferentes elementos necesarios para una administración exitosa de proyectos de cualquier naturaleza. En forma precisa, este documento identifica y describe las prácticas generalmente aceptadas de administración de proyectos que pueden ser implementadas en las organizaciones. Ingeniería de seguridad de sistemas – Modelo de madurez de capacidades (SSE – CMM), desarrollado por la agencia de seguridad nacional (NSA) con el apoyo de la Universidad de Carnegie Mellon: Este modelo describe las características esenciales de una arquitectura de seguridad organizacional para tecnología de información y comunicación electrónica, de acuerdo con las prácticas generalmente aceptadas observadas en las organizaciones. Administración de seguridad de información: aprendiendo de organizaciones líderes, desarrollado por la oficina de contabilidad general de los Estados Unidos (GAO): este modelo considera ocho organizaciones privadas reconocidas como líderes respecto a seguridad en cómputo. Este trabajo hace posible la identificación de 16 prácticas necesarias para asegurar una adecuada administración de la seguridad de cómputo, las cuáles deben ser suficientes para incrementar significativamente el nivel de administración de seguridad en tecnología de información y comunicación electrónica. El Modelo COBIT para auditoria y control de sistemas de información La evaluación de los requerimientos del negocio, los recursos y procesos IT, son puntos bastante importantes para el buen funcionamiento de una compañía y para el aseguramiento de su supervivencia en el mercado. COBIT es precisamente un modelo para auditar la gestión y control de los sistemas de información y tecnología, orientado a todos los sectores de una organización, es decir, administradores IT, usuarios y por supuesto, los auditores involucrados en el proceso. Las siglas COBIT significan Objetivos de Control para Tecnología de Información (Control Objetives for Information Systems and related Tecnology). El modelo es el resultado de una investigación con expertos de varios países, desarrollado por ISACA (Information Systems Audit and Control Association). COBIT, lanzado en 1996, es una herramienta de gobierno de TI que ha cambiado la forma en que trabajan los profesionales de tecnología. Vinculando tecnología informática y prácticas de control, el modelo COBIT consolida y armoniza estándares de fuentes globales prominentes en un recurso crítico para la gerencia, los profesionales de control y los auditores. La estructura del modelo COBIT propone un marco de acción donde se evalúan los criterios de información, como por ejemplo la seguridad y calidad, se auditan los recursos que comprenden la tecnología de información, como por ejemplo el recurso humano, instalaciones, sistemas, entre otros, y finalmente se realiza una evaluación sobre los procesos involucrados en la organización. “La adecuada implementación de COBIT en una organización, provee una herramienta automatizada, para evaluar de manera ágil y consistente el cumplimiento de los objetivos de control y controles detallados, que aseguran que los procesos y recursos de información y tecnología contribuyen al logro de los objetivos del negocio en un mercado cada vez más exigente, complejo y diversificado. Cualquier tipo de empresa puede adoptar una metodología COBIT, como parte de un proceso de reingeniería en aras de reducir los índices de incertidumbre sobre vulnerabilidades y riesgos de los recursos IT y consecuentemente, sobre la posibilidad de evaluar el logro de los objetivos del negocio apalancado en procesos tecnológicos”, señaló un informe de ETEK. COBIT se aplica a los sistemas de información de toda la empresa, incluyendo los computadores personales y las redes. Está basado en la filosofía de que los recursos TI necesitan ser administrados por un conjunto de procesos naturalmente agrupados para proveer la información pertinente y confiable que requiere una organización para lograr sus objetivos. Criterios de información de COBIT Para satisfacer los objetivos del negocio, la información necesita adaptarse a ciertos criterios de control, los cuales son referidos en COBIT como requerimientos de información del negocio. Con base en los requerimientos más amplios de calidad, fiduciarios y de seguridad, se definieron los siguientes siente criterios de información: La efectividad tienen que ver con que la información sea relevante y pertinente a los procesos del negocio, y se propone de una manera oportuna, correcta, consistente y utilizable. La eficiencia consiste en que la información sea generada con el óptimo (más productivo y económico) uso de los recursos. La confidencialidad se refiere a la protección de información sensitiva contra revelación no autorizada. La integridad está relacionada con la precisión y completitud de la información, así como con su validez de acuerdo a los valores y expectativas del negocio. La disponibilidad se refiere a que la información esté disponible cuando sea requerida por los procesos del negocio en cualquier momento. También concierne a la protección de los recursos y las capacidades necesarias asociadas. El cumplimiento tienen que ver con acatar aquellas leyes, reglamentos y acuerdos contractuales a los cuales está sujeto el proceso de negocios, es decir, criterios de negocios impuestos externamente, así como políticas internas. La confiabilidad se refiere a proporcionar la información apropiada para que la gerencia administre la entidad y ejerza sus responsabilidades fiduciarias y de gobierno. Los productos COBIT se han organizado en tres niveles, diseñados para dar soporte a lo siguiente: Administración y consejos ejecutivos. Administración del negocio y de TI. Profesionales en Gobierno, aseguramiento, control y seguridad. Figura de Diagrama de Contenido del COBIT: El diagrama de contenido de COBIT mostrado presenta las audiencias principales, sus preguntas sobre gobierno TI y los productos que generalmente les aplican para proporcionar las respuestas. También hay productos derivados para propósitos específicos, para dominios tales como seguridad o empresas especificas. El conjunto de lineamientos y estándares internacionales conocidos como COBIT, define un marco de referencia que clasifica los procesos de las unidades de tecnología de información de las organizaciones en cuatro “dominios” principales, a saber: Planear y organizar (PO) : Proporciona dirección para la entrega de soluciones (AI) y la entrega de servicios (DS). Adquirir e implementar (AI) : Proporciona las soluciones y las pasa para convertirlas en servicios. Entregar y dar soporte (DS) : Recibe las soluciones y las hace utilizables por los usuarios finales. Monitorear y evaluar (ME) : Monitorear todos los procesos para asegurar que se sigue la dirección provista. Planificación y organización PO PO1 Definir un plan estratégico de TI PO2 Definir la arquitectura de la información PO3 Determinar la dirección tecnológica PO4 Definir los procesos, organización y relaciones de TI PO5 Administrar la inversión en TI PO6 Comunicar las aspiraciones y la dirección de la gerencia PO7 Administrar recursos humanos de TI PO8 Administrar la calidad PO9 Evaluar y administrar los riesgos de TI PO10 Administrar proyectos Adquisición e implantación AI AI1 Identificar soluciones automatizadas AI2 Adquirir y mantener software aplicativo AI3 Adquirir y mantener infraestructura tecnológica AI4 Facilitar la operación y el uso AI5 Adquirir recursos de TI AI6 Administrar cambios AI7 Instalar y acreditar soluciones y cambios Soporte y Servicios DS DS1 Definir y administrar los niveles de servicio DS2 Administrar los servicios de terceros DS3 Administrar el desempeño y la capacidad DS4 Garantizar la continuidad del servicio DS5 Garantizar la seguridad de los sistemas DS6 Identificar y asignar costos DS7 Educar y entrenar a los usuarios DS8 Administrar la mesa de servicio y los incidentes DS9 Administrar la configuración DS10 Administrar los problemas DS11 Administrar los datos DS12 Administrar el ambiente físico DS13 Administrar las operaciones Monitoreo y evaluación ME ME1 Monitorear y evaluar el desempeño de TI ME2 Monitorear y evaluar el control interno ME3 Garantizar el cumplimiento regulatorio ME4 Proporcionar gobierno de TI Estos dominios agrupan objetivos de control de alto nivel que cubren tanto los aspectos de información, como de la tecnología que la respalda. Estos dominios y objetivos de control facilitan que la generación y procesamiento de la información cumplan con las características de efectividad, eficiencia, confidencialidad, integridad, disponibilidad, cumplimiento y confiabilidad. El cubo de COBIT: Asimismo, se debe tomar encuenta los recursos que proporciona la tecnología de información, tales como: datos, aplicaciones, plataformas tecnológicas, instalaciones y recurso humano. Dominios de COBIT Entendiéndose como dominio, la agrupación natural de procesos, normalmente corresponden a un dominio o una responsabilidad organizacional, los procesos a su vez son conjuntos o series de actividades unidas con delimitación o cortes de control y las actividades son acciones requeridas para lograr un resultado medible. COBIT proporciona una lista completa de procesos que puede ser utilizada para verificar que se completan las actividades y responsabilidades; sin embargo, no es necesario que apliquen todas, y, aun más, se pueden combinar como se necesite por cada empresa. Dominio: Planificación y organización (PO) Este dominio cubre las estrategias y tácticas, y se refiere a la identificación de la forma en que la tecnología de información puede contribuir de la mejor manera al logro de los objetivos de negocio. Además, la consecución de la visión estratégica necesita ser planeada, comunicada y administrada desde diferentes perspectivas. Finalmente, deberán establecerse una organización y una infraestructura tecnológica apropiadas. Procesos: PO1 Definición de un plan estratégico Objetivo: Lograr un balance óptimo entre las oportunidades de tecnología de información y los requerimientos de TI de negocio, para asegurar sus logros futuros. Su realización se concreta a través de un proceso de planeación estratégica emprendido en intervalos regulares dando lugar a planes a largo plazo, los que deberán ser traducidos periódicamente en planes operacionales estableciendo metas claras y concretas a corto plazo, teniendo en cuenta: La definición de objetivos de negocio y necesidades de TI, la alta gerencia será la responsable de desarrollar e implementar planes a largo y corto plazo que satisfagan la misión y las metas generales de la organización. El inventario de soluciones tecnológicas e infraestructura actual, deberá evaluar los sistemas existentes en términos de: nivel de automatización de negocio, funcionalidad, estabilidad, complejidad, costo y fortalezas y debilidades, con el propósito de determinar el nivel de soporte que reciben los requerimientos del negocio de los sistemas existentes. Los cambios organizacionales, se deberá asegurar que se establezca un proceso para modificar oportunamente y con precisión el plan a largo plazo de tecnología de información con el fin de adaptar los cambios al plan a largo plazo de la organización y los cambios en las condiciones de la TI. Estudios de factibilidad oportunos, para que se puedan obtener resultados efectivos. PO2 Definición de la arquitectura de información Objetivo: satisfacer los requerimientos de negocio, organizando de la mejor manera posible los sistemas de información, a través de la creación y mantenimiento de un modelo de información de negocio, asegurándose que se definan los sistemas apropiados para optimizar la utilización de esta información, tomando en consideración: La documentación deberá conservar consistencia con las necesidades permitiendo a los responsables llevar a cabo sus tareas eficiente y oportunamente. El diccionario de datos, el cual incorpora las reglas de sintaxis de datos de la organización deberá ser continuamente actualizado. La propiedad de la información y la clasificación de severidad con el que se se establecerá un marco de referencia de clasificación general relativo a la ubicación de datos en clases de información. PO3 Determinación de la dirección tecnológica Objetivo: aprovechar al máximo la tecnología disponible o tecnología emergente, satisfaciendo los requerimientos de negocio, a través de la creación y mantenimiento de un plan de infraestructura tecnológica, tomando en consideración: La capacidad de adecuación y evolución de la infraestructura actual, que deberá concordar con los planes a largo y corto plazo de tecnología de información y debiendo abarcar aspectos tales como arquitectura de sistemas, dirección tecnológica y estrategias de migración. El monitoreo de desarrollos tecnológicos que serán tomados en consideración durante el desarrollo y mantenimiento del plan de infraestructura tecnológica. Las contingencias (por ejemplo, redundancia, resistencia, capacidad de adecuación y evolución de la infraestructura), con lo que se evaluará sistemáticamente el plan de infraestructura tecnológica. Planes de adquisición, los cuales deberán reflejar las necesidades identificadas en el plan de infraestructura tecnológica. PO4 Definición de la organización y de las relaciones de TI Objetivo: prestación de servicios de TI. Esto se realiza por medio de una organización conveniente en número y habilidades, con tareas y responsabilidades definidas y comunicadas, teniendo en cuenta: El comité de dirección el cual se encargará de vigilar la función de servicios de información y sus actividades. Propiedad, custodia, la gerencia deberá crear una estructura para designar formalmente a los propietarios y custodios de los datos. Sus funciones y responsabilidades deberán estar claramente definidas. Supervisión, para asegurar que las funciones y responsabilidades sean llevadas a cabo apropiadamente. Segregación de funciones, con la que se evitará la posibilidad de que un solo individuo resuelva un proceso crítico. Los roles y responsabilidades, la gerencia deberá asegurarse de que todo el personal deberá conocer y contar con la autoridad suficiente para llevar a cabo las funciones y responsabilidades que le hayan sido asignadas. La descripción de puestos, deberá delinear claramente tanto la responsabilidad como la autoridad, incluyendo las definiciones de las habilidades y la experiencia necesarias para el puesto, y ser adecuadas para su utilización en evaluaciones de desempeño. Los niveles de asignación de personal, deberán hacerse evaluaciones de requerimientos regularmente para asegurar una asignación de personal adecuada en el presente y en el futuro. El personal clave, la gerencia deberá definir e identificar al personal clave de tecnología de información. PO5 Manejo de la inversión Objetivo: tiene como finalidad la satisfacción de los requerimientos de negocio, asegurando el financiamiento y el control de desembolsos de recursos financieros. Su realización se concreta a través de presupuestos periódicos sobre inversiones y operaciones establecidas y aprobados por el negocio, teniendo en cuenta: Las alternativas de financiamiento, se deberán investigar diferentes alternativas de financiamiento. El control del gasto real, se deberá tomar como base el sistema de contabilidad de la organización, mismo que deberá registrar, procesar y reportar rutinariamente los costos asociados con las actividades de la función de servicios de información. La justificación de costos y beneficios, deberá establecerse un control gerencial que garantice que la prestación de servicios por parte de la función de servicios de información se justifique en cuanto a costos. Los beneficios derivados de las actividades de TI deberán ser analizados en forma similar. PO6 Comunicación de la dirección y aspiraciones de la gerencia Objetivo: Asegura el conocimiento y comprensión de los usuarios sobre las aspiraciones de alto nivel (gerencia), se concreta a través de políticas establecidas y transmitidas a la comunidad de usuarios, necesitándose para esto estándares para traducir las opciones estratégicas en regalas de usuario prácticas y utilizables. Toma en cuenta: Los código de ética/conducta, el cumplimiento de las reglas de ética, conducta, seguridad y estándares de control interno deberá ser establecido y promovido por la Alta Gerencia. Las directrices tecnológicas. El cumplimiento, la Gerencia deberá también asegurar y monitorear la duración de la implementación de sus políticas. El compromiso con calidad, la gerencia de la función de servicios de información deberá definir, documentar y mantener una filosofía de calidad, debiendo ser comprendidos, implementados y mantenidos por todos los niveles de la función de servicios de información. Las políticas de seguridad y control interno, la alta gerencia deberá asegurar que esta política de seguridad y de control interno especifique el propósito y los objetivos, la estructura gerencial, el alcance dentro de la organización, la definición y asignación de responsabilidades para su implementación a todos los niveles y la definición de multas y de acciones disciplinarias asociadas con la falta de cumplimiento de estas políticas. PO7 Administración de recursos humanos Objetivo: Maximizar las contribuciones del personal a los procesos de TI, satisfaciendo así los requerimientos de negocio, a través de técnicas sólidas para administración de personal, tomando en consideración: El reclutamiento y promoción, deberá tener como base criterios objetivos, considerando factores como la educación, la experiencia y la responsabilidad. Los requerimientos de calificaciones, el personal deberá estar calificado, tomando como base una educación, entrenamiento y o experiencia apropiados, según se requiera. La capacitación, los programas de educación y entrenamiento estarán dirigidos a incrementar los niveles de habilidad técnica y administrativa del personal. La evaluación objetiva y medible del desempeño, se deberá asegurar que dichas evaluaciones sean llevada a cabo regularmente según los estándares establecidos y las responsabilidades específicas del puesto. Los empleados deberán recibir asesoría sobre su desempeño o su conducta cuando esto sea apropiado. PO8 Asegurar el cumplimiento con los requerimientos externos Objetivo: Cumplir con obligaciones legales, regulatorias y contractuales. Para ello se realiza una identificación y análisis de los requerimientos externos en cuanto a su impacto en TI, llevando a cabo las medidas apropiadas para cumplir con ellos y se toma en consideración: Definición y mantenimiento de procedimientos para la revisión de requerimientos externos, para la coordinación de estas actividades y para el cumplimiento continuo de los mismos. Leyes, regulaciones y contratos. Revisiones regulares en cuanto a cambios. Búsqueda de asistencia legal y modificaciones. Seguridad y ergonomía con respecto al ambiente de trabajo de los usuarios y el personal de la función de servicios de información. Privacidad. Propiedad intelectual. Flujo de datos externos y criptografía. PO9 Evaluación de riesgos Objetivo: Asegurar el logro de los objetivos de TI y responder a las amenazas hacia la provisión de servicios de TI. Para ello se logra la participación de la propia organización en la identificación de riesgos de TI y en el análisis de impacto, tomando medidas económicas para mitigar los riesgos y se toma en consideración: Identificación, definición y actualización regular de los diferentes tipos de riesgos de TI (por ej: tecnológicos, de seguridad, etc.) de manera que se pueda determinar la manera en la que los riesgos deben ser manejados a un nivel aceptable. Definición de alcances, límites de los riesgos y la metodología para las evaluaciones de los riesgos. Actualización de evaluación de riesgos. Metodología de evaluación de riesgos. Medición de riesgos cualitativos y/o cuantitativos. Definición de un plan de acción contra los riesgos para asegurar que existan controles y medidas de seguridad económicas que mitiguen los riesgos en forma continua. Aceptación de riesgos dependiendo de la identificación y la medición del riesgo, de la política organizacional, de la incertidumbre incorporada al enfoque de evaluación de riesgos y de que tan económico resulte implementar protecciones y controles. PO10 Administración de proyectos Objetivo: Establecer prioridades y entregar servicios oportunamente y de acuerdo al presupuesto de inversión. Para ello se realiza una identificación y priorización de los proyectos en línea con el plan operacional por parte de la misma organización. Además, la organización deberá adoptar y aplicar sólidas técnicas de administración de proyectos para cada proyecto emprendido y se toma en consideración: Definición de un marco de referencia general para la administración de proyectos que defina el alcance y los límites del mismo, así como la metodología de administración de proyectos a ser adoptada y aplicada para cada proyecto emprendido. La metodología deberá cubrir, como mínimo, la asignación de responsabilidades, la determinación de tareas, la realización de presupuestos de tiempo y recursos, los avances, los puntos de revisión y las aprobaciones. El involucramiento de los usuarios en el desarrollo, implementación o modificación de los proyectos. Asignación de responsabilidades y autoridades a los miembros del personal asignados al proyecto. Aprobación de fases de proyecto por parte de los usuarios antes de pasar a la siguiente fase. Presupuestos de costos y horas hombre. Planes y metodologías de aseguramiento de calidad que sean revisados y acordados por las partes interesadas. Plan de administración de riesgos para eliminar o minimizar los riesgos. Planes de prueba, entrenamiento, revisión post-implementación. PO11 Administración de calidad Objetivo: satisfacer los requerimientos del cliente. Para ello se realiza una planeación, implementación y mantenimiento de estándares y sistemas de administración de calidad por parte de la organización y se toma en consideración: Definición y mantenimiento regular del plan de calidad, el cual deberá promover la filosofía de mejora continua y contestar a las preguntas básicas de qué, quién y cómo. Responsabilidades de aseguramiento de calidad que determine los tipos de actividades de aseguramiento de calidad tales como revisiones, auditorias, inspecciones, etc. que deben realizarse para alcanzar los objetivos del plan general de calidad. Metodologías del ciclo de vida de desarrollo de sistemas que rija el proceso de desarrollo, adquisión, implementación y mantenimiento de sistemas de información. Documentación de pruebas de sistemas y programas. Revisiones y reportes de aseguramiento de calidad. Dominio: Adquisión e implementación (AI) Para llevar a cabo la estrategia de TI, las soluciones de TI deben ser identificadas, desarrolladas o adquiridas, así como implementadas e integradas dentro del proceso del negocio. Además, este dominio cubre los cambios y el mantenimiento realizados a sistemas existentes. Procesos: AI1 Identificación de soluciones automatizadas Objetivo: Asegurar el mejor enfoque para cumplir con los requerimientos del usuario. Para ello se realiza un análisis claro de las oportunidades alternativas comparadas contra los requerimientos de los usuarios y toma en consideración: Definición de requerimientos de información para poder aprobar un proyecto de desarrollo. Estudios de factibilidad con la finalidad de satisfacer los requerimientos del negocio establecidos para el desarrollo de un proyecto. Arquitectura de información para tener en consideración el modelo de datos al definir soluciones y analizar la factibilidad de las mismas. Seguridad con relación de costo-beneficio favorable para controlar que los costos no excedan los beneficios. Pistas de auditoria para ello deben existir mecanismos adecuados. Dichos mecanismos deben proporcionar la capacidad de proteger datos sensitivos (ej. Identificación de usuarios contra divulgación o mal uso). Contratación de terceros con el objeto de adquirir productos con buena calidad y excelente estado. Aceptación de instalaciones y tecnología a través del contrato con el proveedor donde se acuerda un plan de aceptación para las instalaciones y tecnología especifica a ser proporcionada. AI2 Adquisición y mantenimiento del software aplicativo Objetivo: Proporcionar funciones automatizadas que soporten efectivamente al negocio. Para ello se definen declaraciones específicas sobre requerimientos funcionales y operacionales y una implementación estructurada con entregables claros y se toma en consideración: Requerimientos de usuarios, para realizar un correcto análisis y obtener un software claro y fácil de usar. Requerimientos de archivo, entrada, proceso y salida. Interface usuario-máquina asegurando que el software sea fácil de utilizar y que sea capaz de auto documentarse. Personalización de paquetes. Realizar pruebas funcionales (unitarias, de aplicación, de integración y de carga y estrés), de acuerdo con el plan de prueba del proyecto y con los estándares establecidos antes de ser aprobado por los usuarios. Controles de aplicación y requerimientos funcionales. Documentación (materiales de consulta y soporte para usuarios) con el objeto de que los usuarios puedan aprender a utilizar el sistema o puedan sacarse todas aquellas inquietudes que se les puedan presentar. AI3 Adquisición y mantenimiento de la infraestructura tecnológica Objetivo: proporcionar las plataformas apropiadas para soportar aplicaciones de negocios. Para ello se realizará una evaluación del desempeño del hardware y software, la provisión de mantenimiento preventivo de hardware y la instalación, seguridad y control del software del sistema y toma en consideración: Evaluación de tecnología para identificar el impacto del nuevo hardware o software sobre el rendimiento del sistema general. Mantenimiento preventivo del hardware con el objeto de reducir la frecuencia y el impacto de fallas de rendimiento. Seguridad del software de sistema, instalación y mantenimiento para no arriesgar la seguridad de los datos y programas ya almacenados en el mismo. AI4 Desarrollo y mantenimiento de procedimientos Objetivo: Asegurar el uso apropiado de las aplicaciones y de las soluciones tecnológicas establecidas. Para ello se realiza un enfoque estructurado del desarrollo de manuales de procedimientos de operaciones para usuarios, requerimientos de servicio y material de entrenamiento y toma en consideración: Manuales de procedimientos de usuarios y controles, de manera que los mismos permanezcan en permanente actualización para el mejor desempeño y control de los usuarios. Manuales de operaciones y controles, de manera que estén en permanente actualización. Materiales de entrenamiento enfocados al suso del sistema en la práctica diaria. AI5 Instalación y aceptación de los sistemas Objetivo: Verificar y confirmar que la solución sea adecuada para el propósito deseado. Para ello se realiza una migración de instalación, conversión y plan de aceptaciones adecuadamente formalizadas y toma en consideración: Capacitación del personal de acuerdo al plan de entrenamiento definido y los materiales relacionados. Conversión / carga de datos, de manera que los elementos necesarios del sistema anterior sean convertidos al sistema nuevo. Pruebas específicas (cambios, desempeño, aceptación final, operacional) con el objeto de obtener un producto satisfactorio. Acreditación de manera que la Gerencia de operaciones y usuaria acepten los resultados de las pruebas y el nivel de seguridad para los sistemas, junto con el riesgo residual existente. Revisiones post implementación con el objeto de reportar si el sistema proporcionó los beneficios esperados de la manera más económica. AI6 Administración de los cambios Objetivo: minimizar la probabilidad de interrupciones, alteraciones no autorizadas y errores. Esto se hace posible a través de un sistema de administración que permita el análisis, implementación y seguimiento de todos los cambios requeridos y llevados a cabo a la infraestructura de TI actual y toma en consideración: Identificación de cambios tanto internos como por parte de proveedores. Procedimientos de categorización, priorización y emergencia de solicitudes de cambios. Evaluación del impacto que provocaran los cambios. Autorización de cambios. Manejo deliberación de manera que la liberación de software esté regida por procedimientos formales asegurando aprobación, empaque, pruebas de regresión, entrega, etc. Distribución de software, estableciendo medidas de control especificas para asegurar la distribución de software correcto al lugar correcto, con integridad y de manera oportuna. Dominio: Entregar y dar soporte (DS) En este dominio se hace referencia a la entrega de los servicios requeridos, que abarca desde las operaciones tradicionales hasta el entrenamiento, pasando por seguridad y aspectos de continuidad. Con el fin de proveer servicios, deberán establecerse los procesos de soporte necesarios. Este dominio incluye el procesamiento de los datos por sistemas de aplicación, frecuentemente clasificados como controles de aplicación. Procesos: DS1 Definición de niveles de servicio Objetivo: Establecer una comprensión común del nivel de servicio requerido. Para ello se establecen convenios de niveles de servicio que formalicen los criterios de desempeño contra los cuales se medirá la cantidad y la calidad del servicio y se toma en consideración: Convenios formales que determinen la disponibilidad, confiabilidad, desempeño, capacidad de crecimiento, niveles de soporte proporcionados al usuario, plan de contingencia / recuperación, nivel mínimo aceptable de funcionalidad del sistema satisfactoriamente liberado, restricciones (límites en la cantidad de trabajo), cargos por servicio, instalaciones de impresión central (disponibilidad), distribución de impresión central y procedimiento de cambio. Definición de las responsabilidades de los usuarios y de la función de servicios de información. Procedimientos de desempeño que aseguren que la manera y las responsabilidades sobre las relaciones que rigen el desempeño entre todas las partes involucradas sean establecidas, coordinadas, mantenidas y comunicadas a todos los departamentos afectados. Definición de dependencias asignando un Gerente de nivel de Servicio que sea responsable de monitorear y reportar los alcances de los criterios de desempeño del servicio especificado y todos los problemas encontrados durante el procesamiento. Provisiones para elementos sujetos a cargos en los acuerdos de niveles de servicio para hacer posibles comparaciones y decisiones de niveles de servicios contra su costo. Garantías de integridad. Convenios de confidencialidad. Implementación de un programa de mejoramiento del servicio. DS2 Administración de servicios prestados por terceros Objetivo: Asegurar que las tareas y responsabilidades de las terceras partes estén claramente definidas, que cumplan y continúen satisfaciendo los requerimientos. Para ello se establecen medidas de control dirigidas a la revisión y monitoreo de contratos y procedimientos existentes, en cuanto a su efectividad y suficiencia, con respecto a las políticas de la organización y toma en consideración: Acuerdos de servicios con terceras partes a través de contratos entre la organización y el proveedor de las administración de instalaciones esté basado en niveles de procesamiento requeridos, seguridad, monitoreo y requerimientos de contingencia, así como en otras estipulaciones según sea apropiado. Acuerdos de confidencialidad. Además, se deberá calificar a los terceros y el contrato deberá definirse y acordarse para cada relación de servicio con un proveedor. Requerimientos legales regulatorios de manera de asegurar que estos concuerde con los acuerdos de seguridad identificados, declarados y acordados. Monitoreo de la entrega de servicio con el fin de asegurar el cumplimiento de los acuerdos del contrato. DS3 Administración de desempeño y capacidad Objetivo: Asegurar que la capacidad adecuada está disponible y que se esté haciendo el mejor uso de ella para alcanzar el desempeño deseado. Para ello se realizan controles de manejo de capacidad y desempeño que recopilen datos y reporten acerca del manejo de cargas de trabajo, tamaño de aplicaciones, manejo y demanda de recursos y toma en consideración: Requerimientos de disponibilidad y desempeño de los servicios de sistemas de información. Monitoreo y reporte de los recursos de tecnología de información. Utilizar herramientas de modelado apropiadas para producir un modelo del sistema actual para apoyar el pronóstico de los requerimientos de capacidad, confiabilidad de configuración, desempeño y disponibilidad. Administración de capacidad estableciendo un proceso de planeación para la revisión del desempeño y capacidad de hardware con el fin de asegurar que siempre exista una capacidad justificable económicamente para procesar cargas de trabajo con cantidad y calidad de desempeño. Prevenir que se pierda la disponibilidad de recursos mediante la implementación de mecanismos de tolerancia de fallas, de asignación de recursos y de prioridad de tareas. DS4 Asegurar el servicio continuo Objetivo: mantener el servicio disponible de acuerdo con los requerimientos y continuar su provisión en caso de interrupciones. Para ello se tiene un plan de continuidad probado y funcional, que esté alineado con el plan de continuidad del negocio y relacionado con los requerimientos de negocio y toma en consideración: Planificación de severidad. Plan documentado. Procedimientos alternativos. Respaldo y recuperación. Pruebas y entrenamiento sistemático y singulares. DS5 Garantizar la seguridad de sistemas Objetivo: salvaguardar la información contra uso no autorizados, divulgación, modificación, daño o pérdida. Para ello se realizan controles de acceso lógico que aseguren que el acceso a sistemas, datos y programas está restringido a usuarios autorizados y toma en consideración: Autorización, autenticación y el acceso lógico junto con el uso de los recursos de TI deberá restringirse a través de la instrumentación de mecanismos de autenticación de usuarios identificados y recursos asociados con las reglas de acceso. Perfiles e identificación de usuarios estableciendo procedimientos para asegurar acciones oportunas relacionadas con la requisición, establecimiento, emisión, suspensión y suspensión de cuentas de usuario. Administración de llaves criptográficas definiendo e implementando procedimientos y protocolos a ser utilizados en la generación, distribución, certificación, almacenamiento, entrada, utilización y archivo de llaves criptográficas con el fin de asegurar la protección de las mismas. Manejo,reporte y seguimiento de incidentes implementando capacidad para la atención de los mismos. Prevención y detección de virus tales como Caballos de Troya, estableciendo adecuadas medidas de control preventivas, detectivas y correctivas. Utilización de Firewalls si existe una conexión con Internet u otras redes públicas en la organización. DS6 Educación y entrenamiento de usuarios Objetivo: Asegurar que los usuarios estén haciendo un uso efectivo de la tecnología y estén conscientes de los riesgos y responsabilidades involucrados. Para ello se realiza un plan completo de entrenamiento y desarrollo y se toma en consideración: Currículo de entrenamiento estableciendo y manteniendo procedimientos para identificar y documentar las necesidades de entrenamiento de todo el personal que haga uso de los servicios de información. Campañas de concienciación, definiendo los grupos objetivos, identificar y asignar entrenadores y organizar oportunamente las sesiones de entrenamiento. Técnicas de concienciación proporcionando un programa de educación y entrenamiento que incluya conducta ética de la función de servicios de información. DS7 Identificación y asignación de costos Objetivo: asegurar un conocimiento concreto de los costos atribuibles a los servicios de TI. Para ello se realiza un sistema de contabilidad de costos que asegure que éstos sean registrados, calculados y asignados a los niveles de detalle requeridos y toma en consideración: Los elementos sujetos a cargos deben ser recursos identificables, medibles y predecibles para los usuarios. Procedimientos y políticas de cargo que fomenten el uso apropiado de los recursos de cómputo y aseguren el trato justo de los departamentos usuarios y sus necesidades. Tarifas definiendo e implementando procedimientos de costeo de prestar servicios, para ser analizados, monitoreados, evaluados asegurando al mismo tiempo la economía. DS8 Apoyo y asistencia a los clientes de TI Objetivo: asegurar que cualquier problema experimentado por los usuarios sea atendido apropiadamente. Para ello se realiza un buró de ayuda que proporcione soporte y asesoría de primera línea y toma en consideración: Consultas de usuarios y respuesta a problemas estableciendo un soporte de una función de buró de ayuda. Monitoreo de consultas y despacho estableciendo procedimientos que aseguren que las preguntas de los clientes que pueden ser resueltas sean reasignadas al nivel adecuado para atenderlas. Análisis y reporte de tendencias adecuado de las preguntas de los clientes y su solución, de los tiempos de respuesta y la identificación de tendencias. DS9 Administración de la configuración Objetivo: dar cuenta de todos los componentes de TI, prevenir alteraciones no autorizadas, verificar la existencia física y proporcionar una base para el sano manejo de cambios. Para ello se realizan controles que identifiquen y registren todos los activos de TI así como su localización física y un programa regular de verificación que confirme su existencia y toma en consideración: Registro de activos estableciendo procedimientos para asegurar que sean registrados únicamente elementos de configuración autorizados e identificables en el inventario, al momento de adquisición. Administración de cambios en la configuración asegurando que los registros de configuración reflejen el status real de todos los elementos de la configuración. Chequeo de software no autorizado revisando periódicamente las computadoras personales de la organización. Controles de almacenamiento de software definiendo un área de almacenamiento de archivos para todos los elementos de software válidos en las fases del ciclo de vida de desarrollo de sistemas. DS10 Administración de problemas Objetivo: Asegurar que los problemas e incidentes sean resueltos y que sus causas sean investigadas para prevenir que vuelvan a suceder. Para ello se necesita un sistema de manejo de problemas que registre y dé seguimiento a todos los incidentes, además de un conjunto de procedimientos de escalamiento de problemas para resolver de la manera más eficiente los problemas identificados. Este sistema de administración de problemas deberá también realizar un seguimiento de las causas a partir de un incidente dado. DS11 Administración de datos Objetivo: Asegurar que los datos permanezcan completos, precisos y válidos durante su entrada, actualización, salida y almacenamiento. Lo cual se logra a través de una combinación efectiva de controles generales y de aplicación sobre las operaciones de TI. Para tal fin, la gerencia deberá diseñar formatos de entrada de datos para los usuarios de manera que se minimicen los errores y las omisiones durante la creación de los datos. Este proceso deberá controlar los documentos fuentes (de donde se extraen los datos), de manera que estén completos, sean precisos y se registren apropiadamente. Se deberán crear también procedimientos que validen los datos de entrada y corrijan o detecten los datos erróneos, como así también procedimientos de validación para transacciones erróneas, de manera que éstas no sean procesadas. Cabe destacar la importancia de crear procedimientos para el almacenamiento, respaldo y recuperación de datos, teniendo un registro físico (discos, disquetes, CD y cintas magnéticas) de todas las transacciones y datos manejados por la organización, albergados tanto dentro como fuera de la empresa. La gerencia deberá asegurar también la integridad, autenticidad y confidencialidad de los datos almacenados, definiendo e implementando procedimientos para tal fin. DS12 Administración de las instalaciones Objetivo: proporcionar un ambiente físico conveniente que proteja al equipo y al personal de TI contra peligros naturales (fuego, polvo, calor excesivos) o fallas humanas lo cual se hace posible con la instalación de controles físicos y ambientales adecuados que sean revisados regularmente para su funcionamiento apropiado definiendo procedimientos que provean control de acceso del personal a las instalaciones y contemplen su seguridad física. DS13 Administración de la operación Objetivo: asegurar que las funciones importantes de soporte de TI estén siendo llevadas a cabo regularmente y de una manera ordenada. Esto se logra a través de una calendarización de actividades de soporte que sea registrada y completada en cuanto al logro de todas las actividades. Para ello, la gerencia deberá establecer y documentar procedimientos para las operaciones de tecnología de información (incluyendo operaciones de red), los cuales deberán ser revisados periódicamente para garantizar su eficiencia y cumplimiento. Dominio: monitoreo y evaluación (ME) Todos los procesos de una organización necesitan ser evaluados regularmente a través del tiempo para verificar su calidad y suficiencia en cuanto a los requerimientos de control, integridad y confidencialidad. Este es, precisamente, el ámbito de este dominio. Procesos: M1 Monitoreo del proceso Objetivo: Asegurar el logro de los objetivos establecidos para los procesos de TI. Lo cual se logra definiendo por parte de la gerencia reportes e indicadores de desempeño gerenciales y la implementación de sistemas de soporte así como la atención regular a los reportes emitidos. Para ello la gerencia podrá definir indicadores claves de desempeño y/o factores críticos de éxito y compararlos con los niveles objetivos propuestos para evaluar el desempeño de los procesos de la organización. La gerencia deberá también medir el grado de satisfacción de los clientes con respecto a los servicios de información proporcionados para identificar deficiencias en los niveles de servicio y establecer objetivos de mejoramiento, confeccionando informes que indiquen el avance de la organización hacia los objetivos propuestos. M2 Monitorear y evaluar el control interno Objetivo: Asegurar el logro de los objetivos de control interno establecidos para los procesos de TI. Para ello la gerencia es la encargada de monitorear la efectividad de los controles internos a través de actividades administrativas y de supervisión, comparaciones, reconciliaciones y otras acciones rutinarias, evaluar su efectividad y emitir reportes sobre ellos en forma regular. Estas actividades de monitoreo continuo por parte de la Gerencia deberán revisar la existencia de puntos vulnerables y problemas de seguridad. M3 Garantizar el cumplimiento con requerimientos Objetivo: Incrementar los niveles de confianza entre la organización, clientes y proveedores externos. Este proceso se lleva a cabo a intervalos regulares de tiempo. Para ello la gerencia deberá obtener una certificación o acreditación independientes de seguridad y control interno antes de implementar nuevos servicios de tecnología de información que resulten críticos, como así también para trabajar con nuevos proveedores de servicios de tecnología de información. Luego la gerencia deberá adoptar como trabajo rutinario tanto hacer evaluaciones periódicas sobre la efectividad de los servicios de tecnología de información y de los proveedores de estos servicios como así también asegurarse el cumplimiento de los compromisos contractuales de los servicios de tecnología de información y de los proveedores de estos servicios. M4 Proporcionar gobierno de TI Objetivo: Incrementar los niveles de confianza y beneficiarse de recomendaciones basadas en mejores prácticas de su implementación, lo que se logra con el uso de auditorias independientes desarrolladas a intervalos regulares de tiempo. Para ello la gerencia deberá establecer los estatutos para la función de auditoria, destacando en este documento la responsabilidad, autoridad y obligaciones de la auditoria. El auditor deberá ser independiente del auditado, esto significa que los auditores no deberán estar relacionados con la sección o departamento que esté siendo auditado y en lo posible deberá ser independiente de la propia empresa. Esta auditoria deberá respetar la ética y los estándares profesionales, seleccionando para ello auditores que sean técnicamente competentes, es decir que cuenten con habilidades y conocimientos que aseguren tareas efectivas y eficientes de auditoria. La función de auditoria deberá proporcionar un reporte que muestre los objetivos de la auditoria, período de cobertura, naturaleza y trabajo de auditoria realizado, así como también la organización, conclusión y recomendaciones relacionadas con el trabajo de auditoria llevado a cabo. Los 34 procesos propuestos se concretan en 32 objetivos de control detallados anteriormente. Un control se define como “las normas, estándares, procedimientos, usos y costumbres y las estructuras organizativas, diseñadas para proporcionar garantía razonable de que los objetivos empresariales se alcanzarán y que los eventos no deseados se preverán o se detectarán, y corregirán”. Un objetivo de control se define como “la declaración del resultado deseado o propuesto que se ha de alcanzar mediante la aplicación de procedimientos de control en cualquier actividad de TI”. En resumen, la estructura conceptual se puede enfocar desde tres puntos de vista: Los recursos de las TI. Los criterios empresariales que deben satisfacer la información. Los procesos de TI. Para cada uno de estos 34 procesos, tiene un enlace a las metas de negocio y TI que soporta. Información de cómo se pueden medir las metas, también se proporcionan cuáles son sus actividades clave y entregables principales, y quién es el responsable de ellas. Glosario Amenaza : Según [ISO/IEC 13335-1:2004], causa potencial de un incidente no deseado, el cual puede causar el daño a un sistema o la organización. Análisis de riesgos : Según [ISO/IEC Guía 73:2002], uso sistemático de la información para identificar fuentes y estimar el riesgo. Análisis de riesgos cualitativo : Análisis de riesgos en el que se usa una escala de puntuaciones para situar la gravedad del impacto. Análisis de riesgos cuantitativo : Análisis de riesgos en función de las pérdidas financieras que causaría el impacto. Auditoría : Proceso planificado y sistemático en el cual un auditor obtiene evidencias objetivas que le permitan emitir un juicio informado sobre el estado y efectividad del SGSI de una organización. Auditor : Persona encargada de verificar, de manera independiente, la calidad e integridad del trabajo que se ha realizado en un área particular. Autenticación : Proceso que tiene por objetivo asegurar la identificación de una persona o sistema. Backup : Acción de copiar archivos o datos de forma que estén disponibles en caso de que un fallo produzca la pérdida de los originales. Esta sencilla acción evita numerosos, y a veces irremediables problemas si se realiza de forma habitual y periódica. Centro de cómputo : Es un área de trabajo cuya función es la de concentrar, almacenar y procesar los datos y funciones operativas de una empresa de manera sistematizada. Checklist : Lista de apoyo para el auditor con los puntos a auditar, que ayuda a mantener claros los objetivos de la auditoría, sirve de evidencia del plan de auditoría, asegura su continuidad y profundidad y reduce los prejuicios del auditor y su carga de trabajo. Este tipo de listas también se pueden utilizar durante la implantación del SGSI para facilitar su desarrollo. Cliente : Cliente o ‘programa cliente’ es aquel programa que permite conectarse a un determinado sistema, servicio o red. COBIT : (Control Objetives for Information and related Technology) Objetivos de Control para la información y tecnología relacionadas. Publicados y mantenidos por ISACA. Su misión es investigar, desarrollar, publicar y promover un conjunto de objetivos de control de tecnología de información, aceptados para ser empleados por gerentes de empresas y auditores. Control : Las políticas, los procedimientos, las prácticas y las estructuras organizativas concebidas para mantener los riesgos de seguridad de la información por debajo del nivel de riesgo asumido. Datos : Término general para la información procesada por un ordenador. Desastre : Cualquier evento accidental, natural o malintencionado que interrumpe las operaciones o servicios habituales de una organización durante el tiempo suficiente como para verse la misma afectada de manera significativa. Dominio : Agrupación de objetivos de control de etapas lógicas en el ciclo de vida de inversión de TI. Evaluación de riesgos : Según [ISO/IEC Guía 73:2002]: proceso de comparar el riesgo estimado contra un criterio de riesgo dado con el objeto de determinar la importancia del riesgo. Gestión de riesgos : Proceso de identificación, control y minimización o eliminación, a un coste aceptable, de los riesgos que afecten a la información de la organización. Incluye la valoración de riesgos y el tratamiento de riesgos. Según [ISO/IEC Guía 73:2002]: actividades coordinadas para dirigir y controlar una organización con respecto al riesgo. Hardware : Conjunto de dispositivos de los que consiste un sistema. Comprende componentes tales como el teclado, el Mouse, las unidades de disco y el monitor. Impacto : El coste para la empresa de un incidente de la escala que sea, que puede o no ser medido en términos estrictamente financieros ej,. pérdida de reputación, implicaciones legales, etc. Información : En sentido general, es todo lo que reduce la incertidumbre y sirve para realizar acciones y tomar decisiones. Integridad : Mantenimiento de la exactitud y completitud de la información y sus métodos de proceso. Según [ISO/IEC 13335-1:2004]: propiedad/característica de salvaguardar la exactitud y completitud de los activos. Infraestructura : La tecnología, los recursos humanos y las instalaciones que permiten el procesamiento de las aplicaciones. Internet : Interconexión de redes informáticas que permite a las computadoras conectadas comunicarse directamente. Inventario de activos : Lista de todos aquellos recursos (físicos, de información, software, documentos, servicios, personas, reputación de la organización, etc.) dentro del alcance del SGSI, que tengan valor para la organización y necesiten por tanto ser protegidos de potenciales riesgos. ISACA : (Information Systems Audit and Control Association) Asociación de Auditoría y Control de los Sistemas de Información. Publica COBIT y emite diversas acreditaciones en el ámbito de la seguridad de la información. ISO : (International Organization form Standardization) Organización Internacional para la Normalización. Organización de carácter voluntario fundada en 1946 que es responsable de la creación de estándares internacionales en muchas áreas, incluyendo la informática y las comunicaciones. Mantenimiento Correctivo : Medida de tipo reactivo orientada a eliminar la causa de una no-conformidad, con el fin de prevenir su repetición. Mantenimiento Preventivo : Medida de tipo pro-activo orientada a prevenir potenciales no-conformidades. Norma : Principio que se impone o se adopta para dirigir la conducta o la correcta realización de una acción o el correcto desarrollo de una actividad. Objetivo : Declaración del resultado o fin que se desea lograr mediante la implementación de procedimientos de control en una actividad de TI determinada. Organización : Conjunto de personas e instalaciones con una disposición de responsabilidades, autoridades y relaciones. Una organización puede ser pública o privada. Políticas de seguridad : Según [ISO/IEC 27002:2005]: intención y dirección general expresada formalmente por la Dirección. Procedimiento : Forma especificada para llevar a cabo una actividad o un proceso. Proceso : Por lo general, un conjunto de procedimientos influenciados por las políticas y estándares de la organización, que toman las entradas provenientes de un número de fuentes, incluyendo otros procesos, manipula las entradas, y genera salidas, incluyendo a otros procesos, para los clientes de los procesos. Los procesos tienen razones claras de negocio para existir, propietarios responsables, rol claro y responsabilidades alrededor de la ejecución del proceso, así como los medios para medir el desempeño. Red : Servicio de comunicación de datos entre ordenadores. Conocido también por su denominación inglesa: ‘network’. Se dice que una red está débilmente conectada cuando la red no mantiene conexiones permanentes entre los ordenadores que la forman. Esta estructura es propia de redes no profesionales con el fin de abaratar su mantenimiento. Riesgo : Según [ISO Guía 73:2002]: combinación de la probabilidad de un evento y sus consecuencias. Riesgo residual : Según [ISO/IEC Guía 73:2002] El riesgo que permanece tras el tratamiento del riesgo. Seguridad de la información : Según [ISO/IEC 27002:2005]: Preservación de la confidencialidad, integridad y disponibilidad de la información; además, otras propiedades como autenticidad, responsabilidad, no repudio y fiabilidad pueden ser también consideradas. Servidor : Ordenador que ejecuta uno o más programas simultáneamente con el fin de distribuir información a los ordenadores que se conecten con él para dicho fin. Vocablo más conocido bajo su denominación inglesa ‘server’. Software : Componentes inmateriales del ordenador: programas, SO, etc. TI : Tecnologías de Información. Tratamiento de riesgos : Según [ISO/IEC Guía 73:2002]: Proceso de selección e implementación de medidas para modificar el riesgo. Usuario : Una persona o una entidad externa o interna que recibe los servicios empresariales de TI. Valoración de riesgos : Según [ISO/IEC Guía 73:2002]: Proceso completo de análisis y evaluación de riesgos. Vulnerabilidad : Según [ISO/IEC 13335-1:2004]: debilidad de un activo o conjunto de activos que puede ser explotado por una amenaza. Bibliografía Scribd (Jecazagu) Biblioteca udenar Gestión de la atención a clientes y usuarios: centros de contacto, CRM. Arquitectura multicanal. Sistemas de respuesta de voz interactiva (IVR). Voice XML.Gestión de la atención a clientes y usuarios: centros de contacto, CRM.IntroducciónLa customer relationship management , más conocida por sus siglas CRM , puede tener varios significados: Administración basada en la relación con los clientes , un modelo de gestión de toda la organización, basada en la satisfacción del cliente (u orientación al mercado según otros autores). El concepto más cercano es marketing relacional (según se usa en España) y tiene mucha relación con otros conceptos como: clienting, marketing 1×1, marketing directo de base de datos , etc. Software para la administración de la relación con los cliente . Sistemas informáticos de apoyo a la gestión de las relaciones con los clientes, a la venta y al marketing, y que se integran en los llamados Sistemas de Gestión Empresarial (SGE ), y que incluyen CRM, ERP, PLM, SCM, y SRM . El software de CRM puede comprender varias funcionalidades para gestionar las ventas y los clientes de la empresa: automatización y promoción de ventas, tecnologías “data warehouse” (Almacén de datos) para agregar la información transaccional y proporcionar capa de reporting, dashboards e indicadores claves de negocio, funcionalidades para seguimiento de campañas de marketing y gestión de oportunidades de negocio, capacidades predictivas y de proyección de ventas. Customer relationship management (CRM) es un enfoque para gestionar la interacción de una empresa con sus clientes actuales y potenciales. Utiliza el análisis de datos de la historia de los clientes con la empresa y para mejorar las relaciones comerciales con dichos clientes, centrándose específicamente en la retención de los mismos y, en última instancia, impulsando el crecimiento de las ventas. Un aspecto importante del enfoque de CRM son los sistemas informáticos de CRM que recopilan datos de una variedad de canales de comunicación diferentes, incluidos el sitio web, el teléfono, el correo electrónico, el chat en vivo, los materiales de marketing y, más reciéntemente, las redes sociales de la compañía. A través del enfoque de CRM y los sistemas utilizados para facilitarlo, las empresas aprenden más sobre sus audiencias objetivo y cómo atender mejor sus necesidades. Sin embargo, la adopción del enfoque de CRM también puede ocasionalmente generar favoritismo entre una audiencia de consumidores, lo que resulta en insatisfacción entre los clientes y en derrotar el propósito de CRM. FuncionesCuando el software CRM está separado para gestionar el negocio, la gestión del ciclo de vida de las ventas y clientes es difícil o imposible. Y la gestión de ciclo de vida es muy importante ya que muchas empresas hoy en día interactúan con el cliente mucho tiempo después de que se realizó la venta, colaborando con ellos en la ingeniería bajo pedido (ETO), configurar a pedido (CTO) o procesos de gestión de servicios. Cada vez más el CRM debe ser extensible para apoyar a la planificación de recursos empresariales funcionalidades como la ingeniería, fabricación, compras, finanzas y gestión de servicios. Debido a que el CRM de empresa – o el CRM estratégico – es una parte integral del ERP, aporta información completa del cliente sobre el proyecto, las facturas, inventario, etc. CRM como modelo de gestiónDe acuerdo con Peppers y Rogers, “una empresa que se vuelca a sus clientes es una empresa que utiliza la información para obtener una ventaja competitiva y alcanzar el crecimiento y la rentabilidad. En su forma más generalizada, CRM puede ser considerado un conjunto de prácticas diseñadas, simplemente, para poner a una empresa en un contacto mucho más cercano con sus clientes. De este modo, aprender más acerca de cada uno, con el objetivo más amplio de que cada uno sea más valioso incrementando el valor de la empresa”. CRM socialCRM es una forma de pensar y actuar de una empresa hacia los clientes/consumidores. A partir de la formación de grandes corporaciones, el contacto 1 a 1 se va perdiendo y se despersonaliza cualquier transacción, dejando de lado la relación de los clientes con la marca. El CRM, y especialmente el CRM Social nacen de la necesidad de recuperar los vínculos personales con los clientes, especialmente en la era de las Redes Sociales , en donde cada opinión se multiplica de forma viral y afecta significativamente la imagen de la marca. Por eso el Social CRM difiere del tradicional agregando la posibilidad de intercambio y conversación con los clientes. Mediante la conexión constante y el registro de la información de la actividad, la empresa lleva un seguimiento de cada uno de sus contactos. Se les provee de información y soporte, se les avisa de nuevas activaciones y propuestas, y se les recompensa por producir contenido positivo. Esto conduce a una constante realimentación, pues los clientes tienen la posibilidad de opinar y compartir mediante redes sociales como Facebook y Twitter , que también permiten identificar prospectos y conocer sus gustos y preferencias. Así la producción de contenidos se vuelve cada vez más personalizada y relevante, profundizando la relación. Un CRM abarca a los sistema que mantienen datos específicos con el fin de mantener la relación de los clientes con la empresa en todo momento. Módulo de ventasAutomatización de la parte o eslabón final: entre el cliente y el punto de venta. Un módulo de ventas es incluido en la mayoría de los CRM para poder tomar estas acciones dentro del almacén de datos. Por medio de esto se asigna oportunidades potenciales y tareas de manera automática según reglas predefinidas analizadas por medio de la información recaudada por los puntos de ventas automatizados. Módulo de mercadoCRM que sea flexible, fácil de usar y que esté diseñada para la empresa. Transforma cada punto de contacto en una oportunidad de marketing y aprovecha el potencial oculto dentro de la base de datos de los clientes. Con las capacidades de marketing familiares y afines pueden comercializar productos de manera más eficaz, mejorar la productividad y obtener conocimientos accionables con los esfuerzos de marketing. Señala esfuerzos de marketing. Amplía la captura de pantalla. Usa consultas en idioma natural para segmentar de manera instantánea clientes o clientes potenciales. Crear listas altamente dirigidas y asociarlas con campañas y compañías. Configurar vistas personales o públicas para reutilización. Compartir fácilmente listas dirigidas con colegas y proveedores. Exportar listas en varios formatos para comunicaciones por correo electrónico masivo o correo directo. Planear actividades, tareas, presupuestos y detalles para cada actividad de marketing, y realizar su seguimiento. Coordinar de mejor manera las ventas al hacer un seguimiento de las oportunidades potenciales en un sistema centralizado. Asignar o clasificar oportunidades potenciales de manera automática según los flujos de trabajo predefinidos. El cliente o consumidor es el corazón de toda organización. Características de programaciónLos sistemas CRM tienen distintos módulos y categorías de programación, Plugins que funcionan de manera sincrónica realizando acciones durante la pre y post creación y actualización de registros y workflow que realiza tareas de manera asincrónica. Software CRMSe muestran algunos programas CRM: CiviCRM Cligraphcrm screenshots Hypos CRM IDempiere Microsoft Dynamics CRM Odoo OpenERP OpenZ Salesforce.com SAP CRM SugarCRM Sistemas de respuesta de voz interactiva (IVR)IntroducciónLa respuesta de voz interactiva o IVR ( Interactive Voice Response ) consiste en un sistema telefónico que es capaz de recibir una llamada e interactuar con el humano a través de grabaciones de voz y el reconocimiento de respuestas simples, como “sí”, “no” u otras. Es un sistema automatizado de respuesta interactiva, orientado a entregar o capturar información a través del teléfono, permitiendo el acceso a servicios de información u otras operaciones. Los sistemas de IVR implementados en la red tienen capacidad para administrar grandes volúmenes de llamadas y también se usan para llamadas salientes, ya que estos sistema son más inteligentes que muchos sistemas de marcación predictiva. IVR se puede utilizar para compras con dispositivos móviles, pagos y servicios bancarios, pedidos minoristas, servicios públicos, información sobre viajes y condiciones meteorológicas. Un concepto erróneo común hace referencia a un asistente automático como un sistema de IVR. Los términos son distintos y significan cosas diferentes para los profesionales tradicionales del ámbito de las telecomunicaciones: el propósito de un sistema de IVR es tomar la entrada, procesarla y devolver un resultado, mientras que la tarea de un asistente automático consiste en redirigir las llamadas. En ocasiones, también se utiliza el término unidad de respuesta de voz o VRU ( Voice Response Unit ). HistoriaA pesar del crecimiento de la tecnología IVR durante la década de 1970, la tecnología se consideraba compleja y costosa para la automatización de tareas en los centros de llamadas. Los primeros sistemas de respuesta de voz se basaban en la tecnología DSP y estaban limitados a vocabularios reducidos. A principios de 1980, Perception Technology de Leon Ferber se convirtió en el primer competidor del mercado principal, después de que la tecnología de discos duros (del acceso aleatorio de lectura/escritura a datos de voz digitalizados) alcanzara un nivel de precio rentable. En aquel momento, un sistema podía guardar palabras digitalizadas en un disco, reproducir el mensaje hablado apropiado y procesar la respuesta DTMF humana. Se utilizan dos variantes principales del reconocimiento de voz en IVR: una basada en gramáticas predefinidas (utilizadas en diálogos “dirigidos”) y otra basada en modelos de lenguaje preparados estadísticamente (utilizada en diálogos de “lenguaje natural”). Los diálogos dirigidos presentan a la persona que llama preguntas u opciones específicas. Los diálogos de lenguaje natural utilizan preguntas abiertas (por ejemplo, “¿En qué puedo ayudarlo?”), son más coloquiales y pueden interpretar respuestas de formato libre. A menudo, un distribuidor automático de llamadas o ACD (Automatic Call Distributor) es el primer punto de contacto al llamar a muchas grandes empresas. Un ACD utiliza dispositivos de almacenamiento digital para reproducir saludos o anuncios, pero comúnmente redirige a la persona que llama sin solicitar ninguna entrada. Un sistema de IVR puede reproducir anuncios y solicitar una entrada a la persona que llama. Esta información se puede usar para obtener el perfil de la persona que llama y redirigir la llamada a un agente con un conjunto de aptitudes específico. (Un conjunto de aptitudes es una función que se aplica a un grupo de agentes de un centro de llamadas con una habilidad en concreto). La respuesta de voz interactiva se puede utilizar para establecer la interfaz inicial de las operaciones de un centro de llamadas mediante la identificación de las necesidades de la persona que llama. Se puede obtener información de la persona que llama, como por ejemplo, un número de cuenta. Asimismo, se pueden brindar respuestas a preguntas simples, como saldos de cuentas o información previamente grabada, sin necesidad de que intervenga el operador. Con frecuencia, los números de cuenta obtenidos del sistema de IVR se comparan con los datos de identificación de la llamada por cuestiones de seguridad y, si la identificación de la llamada no coincide con el registro de la cuenta, se requieren respuestas de IVR adicionales. ServiciosEl IVR se implementa habitualmente en empresas o entidades que reciben gran cantidad de llamadas, a fin de reducir la necesidad de personal y los costes que el servicio ofrecido representan para dicha entidad. Entre otras, podemos mencionar a las bancas telefónicas. Las empresas suelen usar la tecnología IVR para dirigir una llamada entrante hacia un departamento u otro, sin la necesidad de intervención humana, así reduciendo el tiempo de espera de sus clientes. En los centros de atención telefónica al cliente, se utiliza la tecnología IVR para dirigir las llamadas hacia los agentes con mayor conocimiento de una materia específica, reduciendo así el tiempo de la llamada y evitando la necesidad de hacer transferencias entre agentes. Se está implementando también en empresas de taxis, en las que la identificación del número que llama permite conocer dónde se encuentra el pasajero y generar el viaje rápidamente sin la intervención de un telefonista físico. Puede combinarse con SMS para prestar cualquier clase de servicio: televotación, encuestas, sorteos, acceso a bases de datos, servicios informativos, etc. UsoLos sistemas de IVR se utilizan para atender gran cantidad de llamadas, reducir los costos y mejorar la experiencia del cliente. El uso de IVR y la automatización de voz permiten resolver las consultas de quienes llaman sin necesidad de colocar las llamadas en cola ni incurrir en el costo de un agente real. Si las personas que llaman no encuentran la información que necesitan o requieren asistencia adicional, las llamadas se suelen transferir a un agente. Esto da lugar a un sistema eficiente, que permite a los agentes tener más tiempo para abordar interacciones complejas. Cuando un sistema de IVR responde llamadas de varios números de teléfono, el uso del Servicio de identificación de número marcado (DNIS) garantiza la ejecución de la aplicación y el idioma correcto. Un único sistema de IVR grande puede atender llamadas para miles de aplicaciones, cada una con sus propios números de teléfono y guiones. Sector bancarioLas instituciones bancarias dependen de los sistema de IVR para mantener las relaciones con los clientes y ampliar el horario comercial para ofrecer servicios las 24 horas de los 7 días de la semana. El servicio de Banca telefónica permite a los clientes consultar saldos e historiales de transacciones, así como realizar pagos y transferencias. A medida que han surgido los canales en línea, la satisfacción del cliente bancario ha disminuido. Sector médicoLas empresas farmacéuticas y las organizaciones de investigación por contrato utilizan los sistemas de IVR para realizar ensayos clínicos y administrar el gran volumen de datos que se genera. La persona que llama responde a las preguntas en el idioma de su preferencia y las respuestas se ingresan en una base de datos y, posiblemente, se registran al mismo tiempo para confirmar la autenticidad. Entre las aplicaciones se incluyen la asignación aleatoria de pacientes y la gestión del suministro de medicamentos. También se emplean para registrar los diarios y cuestionarios del paciente. Los sistemas de IVR permiten a quienes llaman obtener datos de manera relativamente anónima. En hospitales y clínicas, los sistemas de IVR se han utilizado para que quienes llaman puedan obtener acceso anónimo a los resultados de las pruebas. Este tipo de información podría ser fácilmente gestionada por una persona, pero se emplea el sistema de IVR para preservar la privacidad y evitar la posible incomodidad que puede suponer la información sensible o los resultados de las pruebas. Los usuarios reciben una clave de ingreso para poder acceder a sus resultados. EncuestasAlguna de las mayores plataformas de IVR instaladas se utilizan para la “televotación” en concursos televisivos, como Pop Idol y Gran Hermano, que pueden generar enormes picos de llamada. A menudo, el proveedor de red implementará un método de bloqueo de destinos (call gapping) en la red telefónica pública conmutada (PSTN) para evitar la saturación de la red. Las organizaciones de sondeo también pueden usar sistemas de IVR para formular preguntas más sensibles cuando los investigadores tienen que un participante podría sentirse incómodo al dar sus respuestas a un interlocutor humano (por ejemplo, preguntas sobre consumo de drogas o comportamiento sexual). En algunos casos, se puede usar un sistema de IVR en la misma encuesta junto con un encuestador humano. Tecnología involucradaEl IVR para brindar mejores servicios involucra otras tecnologías como, por ejemplo: DTMF (Dual Tone Multi Frecuency) : Propia de la telefonía, es la tecnología de tonos utilizada para el marcado. TTS (Text To Speech) : Iniciada en la informática, le da capacidad de transformar texto a audio que escucha el operador. ASR (Reconocimiento de Voz) : Iniciada por la informática. Le da la capacidad de reconocer palabras del usuario y aceptarlas como órdenes. Voice XMLIntroducciónMuchos usuarios encuentran mucho más práctico los servicios automatizados por voz, y en la constante evolución de la web no ha podido faltar VoiceXML, que se ha convertido en un estandard W3C capaz de darnos la oportunidad de navegar interactuando con el ordenador utilizando exclusivamente nuestra voz, se deja de lado los periféricos como el ratón y el teclado para dar lugar al micrófono. Como si se tratara de una conversación se establecen los roles de emisor y receptor, alternándose entre ordenador y usuario. HistoriaAT&amp;T, IBM, Lucent, y Motorola creó el foro de VoiceXML en 1999, antes de septiembre de 1999 el foro lanzó VoiceXML 0.9 y en 2000 publicaron VoiceXML 1.0. El W3C lo aceptó como “estándard” en marzo de 2004 en su versión 2.0, algo más tarde surgió la 2.1 añadiendo algunas pequeñas mejoras, las cuales se convirtieron en recomendación W3C en 2007. Actualmente se está trabajando en VoiceXML 3.0, el cual utilizará un nuevo idioma descriptivo del statechart de XML llamado SCXML. ¿Qué es?VoiceXML, es un lenguaje destinado al manejo y creación de aplicaciones de voz, que son empleadas para navegar, de forma auditiva en vez de utilizar la forma visual, más convencional y extendida hasta el momento. VoiceXML es un lenguaje de etiquetas basado en XML que permite describir servicios de voz con independencia de la aplicación en la que corran. De esta manera no es necesario conocer detalles específicos de una plataforma para entender el funcionamiento del sistema de diálogo. Los documentos que origina, son los llamados XML (eXtensible Markup Languaje), que admiten y poseen las características necesarias para dar lugar a la reproducción de sonidos digitales y sintetizados. Posee un tipo de arquitectura no delimitada y de alto nivel de compatibilidades con respecto a las distintas salidas o recursos de la informática e internet. El lenguaje VoiceXML describe la interacción hombre-máquina a partir de los siguientes elementos: Salida de texto-a-voz Salida de audio grabado Reconocimiento de entrada hablada Reconocimiento de tonos DTMF Grabación de entrada hablada Control de flujo de diálogo Funciones de telefonía (transferencia de llamada, desconexión, ect). ComponentesLas aplicaciones de VoiceXML, contienen ciertos componentes, normalmente comunes entre ellos como: El Servidor de aplicaciones que es el encargado al igual que cualquier función de un servidor, de proporcionar y almacenar datos de las aplicaciones e interfaces, para poder facilitarlas a otras externas. Por otra parte, el Servidor de VoiceXML de Telefonía que es una plataforma que actúa como cliente frente al servidor de aplicaciones acabado de mencionar. Éste controla los diálogos producidos en VoiceXML, y los entiende para su control del habla y los diferentes recursos que posee (como ADR o TTS). También posee una red de paquetes TCP/IP basada en la conexión del servidor de aplicaciones y el servidor de telefonía a través de protocolos HTTP. Y a su vez, contiene una red telefónica comúnmente pública (PSTN), aunque no descarta la posibilidad de ser privada (PBX). FuncionamientoEl usuario utiliza su voz para empezar a dar órdenes, de modo VoiceXML pone en marcha su ASR (un sistema encargado de reconocer la voz humana) transformando así la voz en una señal digital formada por 0’s y 1’s. Una vez se procesa y si es necesario, la máquina puede contestar también mediante voz al usuario, poniendo en marcha el TTS y mediante éste se crean los documentos XML nombrados con anterioridad. Para la creación de estos documentos, se utiliza ésta tecnología específica denominada TTS, que es referente a tecnologías de síntesis de voz. Y la síntesis de voz consiste en la reproducción de manera no natural, es decir, artificial, del lenguaje natural y su origen proviene de las señales de voz que son generadas por el ordenador, que da lugar a un proceso inverso al ASR, es decir, transforma la señal digital que crea (respuesta) en voz entendible para el usuario. AplicacionesVoiceXML está en expansión, y seguramente tenga cabida en multitud de entornos, actualmente es más usado en servicios telefónicos, un ejemplo claro lo encontramos cuando hacemos llamadas a nuestro operador telefónico, donde una voz nos va pidiendo datos para poder emparejarnos después con una persona real. Otra aplicación importante es en los sistemas de información, incluso en el ámbito turístico, dando la opción de comunicarse con la máquina en múltiples idiomas. Pero además de la comodidad que nos puede proporcionar una navegación mediante VoiceXML nos encontramos con una muy buena opción para dotar a cualquier página web de más usabilidad para gente con problemas de movilidad, incapaces de moverse con la soltura necesaria mediante los periféricos como el ratón y el teclado. Ejemplo de sintaxisComo ya sabemos, una de las primeras pruebas a la hora de empezar con un lenguaje es el famoso “Hola mundo”. Así quedaría en VXML (es la extensión de los ficheros VoiceXML): &lt;?xml version=”1.0” encoding=”iso-8859-1”?&gt; ¡Hola mundo! ConclusionesVoiceXML, utilizado de manera conjunta con otros estándares, proporciona una base sólida para la definición de sistemas de voz. Las constantes revisiones y ampliaciones del estándar aseguran su continuidad y progresiva incorporación en las herramientas para la construcción de aplicaciones de voz. El aumento de los sistemas de telefonía a través de Internet y los progresos en los campos del reconocimiento de voz y las herramientas digitales para la lecturas así como la progresiva incorporación de los estándares propuestos por el W3C, señalan la gran importancia que sin duda VoiceXML y el resto de estándares de voz tendrán en un futuro muy próximo. Bibliografía Wikipedia (CRM) Wikipedia (IVR) Scribd VoiceXML (Alex Ice) Universidad de Oviedo Seguridad física y lógica de un sistema de información. Herramientas en ciberseguridad. Gestión de incidentes. Informática forense.Seguridad Lógica de un Sistema de InformaciónIntroducción. Concepto de Seguridad InformáticaPodemos entender como seguridad una característica de cualquier sistema (informático o no) que nos indica que ese sistema está libre de todo peligro, daño o riesgo, y que es, en cierta manera, infalible. Como esta característica, particularizando para el caso de SO o redes de ordenadores, es muy difícil de conseguir (según la mayoría de expertos, imposible), se suaviza la definición de seguridad y se pasa a hablar de fiabilidad (probabilidad de que un sistema se comporte tal y como se espera de él) más que de seguridad. Se entiende que mantener un sistema seguro (o fiable) consiste básicamente en garantizar tres aspectos: Confidencialidad o Privacidad . Es la necesidad de que la información sólo sea conocida por personas autorizadas no convirtiendo esa información en disponible para otras entidades. En casos de falta de confidencialidad, la información puede provocar daños a sus dueños o volverse obsoleta. Integridad . Es la característica que hace que el contenido de la información permanezca inalterado a menos que sea modificado por personal autorizado, y esta modificación sea registrada para posteriores controles o auditorias. Disponibilidad u Operatividad . Es la capacidad de que la información esté siempre disponible para ser procesada por personal autorizado. Esto requiere que dicha información se mantenga correctamente almacenada con el hardware y el software funcionando perfectamente y que se respeten los formatos para su recuperación en forma satisfactoria. Generalmente tienen que existir los tres aspectos descritos para que haya seguridad. Los tres elementos principales a proteger en cualquier sistema informático son el software, el hardware y los datos. Habitualmente los datos son el principal elemento, ya que es el más amenazado y el más difícil de recuperar.Así, por ejemplo en una máquina Unix tenemos un hardware ubicado en un lugar de acceso físico restringido, o al menos controlado, en caso de pérdida de una aplicación ( o un programa de sistema, o el propio núcleo de Unix) este software se puede restaurar sin problemas desde su medio original (por ejemplo, el CD-ROM con el SO que se utilizó para su instalación). Sin embargo, en caso de pérdida de una BD o de un proyecto de un usuario, no tenemos un medio ‘original’ desde el que restaurar: hemos de pasar obligatoriamente por un sistema de copias de seguridad, y a menos que la política de copias sea muy estricta, es difícil devolver los datos al estado en que se encontraban antes de la pérdida. Se deben conocer las amenazas a que los datos están sometidos y las vulnerabilidades de los sistemas en los que residen, así como los principales tipos de ataques para crear una buena política de seguridad que los proteja. Seguridad Física y Seguridad LógicaEl estudio de la seguridad puede estudiarse dependiendo de las fuentes de las amenazas a los sistemas, lo que da lugar a hablar de seguridad física y seguridad lógica. La seguridad física trata de la protección de los sistemas ante amenazas físicas. Consiste en la aplicación de barreras físicas y procedimientos de control, como medidas de prevención y contramedidas, ante amenazas a los recursos e informaciones confidenciales. Desastres naturales, sabotajes internos o externos, etc, forman parte de este tipo de seguridad. La seguridad lógica protege la información dentro de su propio medio mediante el uso de herramientas de seguridad. Se puede definir como conjunto de operaciones y técnicas orientadas a la protección de la información contra la destrucción, la modificación, la divulgación indebida o el retraso en su gestación. El activo más importante de una organización es la información por lo que es necesario técnicas que vayan más allá de la seguridad física para proteger dicha información. Estas técnicas las brinda la seguridad lógica. Seguridad LógicaLa Seguridad Lógica consiste en la “ aplicación de barreras y procedimientos que resguarden el acceso a los datos y sólo permitan acceder a ellos a las personas autorizadas para hacerlo “. Los objetivos que se plantean serán: Restringir el acceso a los programas y archivos. Asegurar que los operadores puedan trabajar sin una supervisión minuciosa y no puedan modificar los programas ni los archivos que no les correspondan. Asegurar que se estén utilizando los datos, archivos y programas correctos por el procedimiento correcto. Que la información transmitida sea recibida por el destinatario al que ha sido enviada y no a otro. Que la información recibida sea la misma que ha sido transmitida. Que existan sistemas alternativos secundarios de transmisión entre diferentes puntos. Que se disponga de pasos alternativos de emergencia para la transmisión de información La seguridad lógica está estandarizada de acuerdo a unos niveles de seguridad. El estándar más utilizado internacionalmente es el TCSEC (Trusted Computer System Evaluation) Orange Book desarrollado en 1982 de acuerdo a las normas de seguridad de ordenadores del Departamento de Defensa de la Estados Unidos. Los niveles describen diferentes tipos de seguridad del SO y se enumeran desde el mínimo grado de seguridad al máximo. Estos niveles han sido la base de desarrollo de estándares europeos (ITSEC/ITSEM, Information Technology Security Evaluation Criteria / Methodology) y luego internacionales (ISO/IEC). Cada nivel requiere todos los niveles definidos anteriormente: así el subnivel B2 abarca los subniveles B1, C2, C1 y D. Nivel D Reservado para sistemas que no cumplen con ninguna especificación de seguridad. Sin sistemas fiables, no hay protección para el hardware, el SO es inestable y no hay autenticación con respecto a usuarios y sus derechos de acceso a la información. Un SO en este niveles por ejemplo MS-DOS. Nivel C1: Protección Discrecional El acceso a distinta información se realiza mediante identificación de usuarios. Cada usuario maneja su información privada y distingue entre usuarios y el administrador del sistema, quien tiene control total de acceso. Muchas de las tareas cotidianas de la administración sólo pueden ser realizadas por este “super usuario” quien tiene gran responsabilidad en la seguridad. Con la actual descentralización de los sistemas, no es raro que en una organización encontremos dos o tres personas cumpliendo este papel. Los requerimientos mínimos que debe cumplir la clase C1 son: Acceso de control discrecional: distinción entre usuario y recursos. Se podrán definir grupos de usuarios y grupos de objetos sobre los cuales podrán actuar usuarios o grupos de ellos. Identificación y Autenticación: un usuario debe identificarse antes de comenzar a ejecutar acciones sobre el sistema. El dato de un usuario no podrá ser accedido por un usuario sin autorización o identificación. Nivel C2: Protección de Acceso controlado Cuenta con características adicionales al C1 que crean un ambiente de acceso controlado. Se debe llevar una auditoria de accesos e intentos fallidos de acceso a objetos. Tiene la capacidad de restringir aún más el que los usuarios ejecuten ciertos comandos o tengan accesos a ciertos archivos, permite o deniega datos a usuarios concretos en base no sólo a los permisos sino también a los niveles de autorización. Requiere que se audite el sistema. Esta auditoria es utilizada para llevar registros de todas las acciones relacionadas con la seguridad, como las actividades efectuadas por el administrador y sus usuarios. La auditoria requiere de autenticación adicional para estar seguro de que la persona que ejecuta el comando es quien dice ser. Su mayor desventaja reside en los recursos adicionales requeridos. Los usuarios de un sistema C2 tienen la autorización para realizar algunas tareas de administración del sistema sin necesidad de ser administradores. Permite llevar mejor cuenta de las tareas relacionadas con la administración del sistema, ya que es cada usuario quién ejecuta y no el administrador. Nivel B1: Seguridad Etiquetada A cada objeto del sistema (usuario, dato, etc) se le asigna una etiqueta con nivel de seguridad jerárquico (alto secreto, secreto, reservado, etc) y con unas categorías (contabilidad, nóminas, ventas, etc). Cada usuario que accede a un objeto debe poseer un permiso expreso para hacerlo y viceversa. Es decir que cada usuario tiene sus objetos asociados. También se establecen controles para limitar la propagación de derecho de accesos a los distintos objetos. Nivel B2: Protección Estructurada Requiere que se etiquete cada objeto de nivel superior por ser padre de un objeto inferior. La protección estructurada es la primera que empieza a referirse al problema de un objeto a un nivel más elevado de seguridad y comunicación con otro objeto a un nivel inferior. Así un disco duro será etiquetado por almacenar archivos que son accedidos por distintos usuarios. El sistema es capaz de alertar a los usuarios si sus condiciones de accesibilidad y seguridad son modificadas; y el administrador es el encargado de fijar los canales de almacenamiento y ancho de banda a utilizar por los demás usuarios. Nivel B3: Dominios de Seguridad Refuerza a los dominios con las instalación de hardware: por ejemplo, el hardware de administración de memoria se usa para proteger el dominio de seguridad de acceso no autorizado a la modificación de objetos de diferentes dominios de seguridad. Existe un monitor de referencia que recibe las peticiones de acceso de cada usuario y las permite o las deniega según las políticas de acceso que se hayan definido. Todas las estructuras de seguridad deben ser lo suficientemente pequeñas como para permitir análisis y comprobaciones ante posibles violaciones. Este nivel requiere que el terminal del usuario se conecte al sistema por medio de una conexión segura. Cada usuario tiene asignado los lugares y objetos a los que puede acceder. Nivel A: Protección Verificada Es el nivel más elevado, incluye un proceso de diseño, control y verificación, mediante métodos formales (matemáticos) para asegurar todos los procesos que realiza un usuario sobre el sistema. Para llegar a este nivel todos los componentes de niveles inferiores deben incluirse. El diseño requiere ser verificado de forma matemática y se deben realizar análisis de canales encubiertos y de distribución fiable. El software y el hardware son protegidos para evitar infiltraciones ante traslados o movimientos de equipamiento. Amenazas, Riesgos, Vulnerabilidades y AtaquesAmenazaBajo la etiqueta de ‘amenazas lógicas’ encontramos todo tipo de programas que de una forma u otra pueden dañar a nuestro sistema, creados de forma intencionada (software malicioso, también conocido como malware) o simplemente un error (bugs o agujeros). Esto es, una amenaza es la posibilidad de la ocurrencia de algún evento que afecte el buen funcionamiento de un sistema, es decir, cualquier elemento que comprometa el sistema. Las amenazas pueden ser analizadas en tres momentos: antes del ataque, durante y después del mismo, por lo que son necesarios mecanismos que garanticen la seguridad para cada momento. Estos son: La prevención (antes): mecanismos que aumentan la seguridad (fiabilidad) de un sistema durante su funcionamiento normal. Por ejemplo, el cifrado de información. La detección (durante): mecanismos orientados a revelar violaciones a la seguridad. Generalmente son programas de auditoria. La recuperación (después): mecanismos que se aplican cuando la violación del sistema ya se ha detectado, para retornar éste a su funcionamiento normal. Por ejemplo, recuperación desde las copias de seguridad realizadas previamente. La identificación de las amenazas requiere conocer los tipos de ataques, el tipo de acceso, método de trabajo y los objetivos del atacante. RiesgoProximidad o posibilidad de daño sobre un bien. Ya se trate de actos naturales, errores u omisiones humanas y actos intencionados, cada riesgo debería ser considerado de las siguientes maneras: Minimizando la posibilidad de que ocurra. Reduciendo al mínimo el perjuicio producido si no ha podido evitarse que ocurriera. Diseñando métodos para la más rápida recuperación de los daños experimentados. Corrigiendo las medidas de seguridad en función de la experiencia recogida. VulnerabilidadCaracterística del sistema o del medio ambiente que facilita que la amenaza tenga lugar. Son las debilidades del sistema que pueden ser empleadas por la amenaza para comprometerlo. AtaqueEvento, exitoso o no, que atenta contra el buen funcionamiento de un sistema, sea intencionado o accidental. Las consecuencias de los ataques se podrían clasificar en: Corrupción de Datos: la información que no contenía defectos pasa a tenerlos. Denegación de Servicio: servicios que deberían estar disponibles no lo están. Filtrado: los datos llegan a destinos a los que no deberían llegar. Los ataques de una forma general se clasifican en: Ataques Pasivos El atacante no altera la comunicación sino que únicamente la escucha o monitoriza para obtener información que está siendo transmitida. Sus objetivos son la interceptación de datos y el análisis de tráfico. Se suelen emplear para: Obtención del origen y destinatario de la comunicación, a través de la lectura de las cabeceras de los paquetes monitorizados. Control del volumen de tráfico intercambiando entre las entidades monitorizadas, obteniendo así información acerca de actividad o inactividad inusuales. Control de las horas habituales de intercambio de datos entre las entidades de la comunicación, para extraer información acerca de los periodos de actividad. El cifrado de información, por ejemplo, puede evitar el éxito, si bien no el ataque. Ataques Activos Estos ataques implican algún tipo de modificación del flujo de datos transmitidos o la creación de un falso flujo de datos. Generalmente son realizados por hackers, piratas informáticos o intrusos remunerados y se los pueden subdividir en varias categorías: Interrupción : Un ataque se clasifica como interrupción si hace que un objeto del sistema se pierda, quede inutilizable o no disponible. Interceptación : Se tratará de una interceptación si un elemento no autorizado consigue un acceso a un determinado objeto del sistema. Modificación : Si además de conseguir el acceso consigue modificar el objeto. Fabricación : Se dice que un ataque es una fabricación si se trata de una modificación destinada a conseguir un objeto similar al atacado de forma que sea difícil distinguir entre el objeto original y el ‘fabricado’. Destrucción : Algunos autores consideran un caso especial de la modificación la destrucción, entendiéndola como una modificación que inutiliza al objeto afectado. Tipos de AtaquesIngeniería Social Es la manipulación de las personas para convencerlas de que ejecuten acciones o actos que normalmente no realizan para que revele todo lo necesario para superar las barreras de seguridad. Esta técnica es una de las más usadas y efectivas a la hora de averiguar nombres de usuarios y passwords. Para evitar situaciones de Ingeniería Social es conveniente tener en cuenta estas recomendaciones: Tener servicio técnico propio o de confianza. Instruir a los usuarios para que no respondan ninguna pregunta sobre cualquier característica del sistema y deriven la inquietud a los responsables que tenga la competencia para dar esa información. Asegurarse que las personas que llaman por teléfono son quien dicen ser. Ingeniería Social Inversa En este caso el intruso da a conocer de alguna manera que es capaz de brindar ayuda a los usuarios y estos llaman ante algún imprevisto. El intruso aprovechará la oportunidad para pedir información necesaria para solucionar el problema consiguiendo información útil. Trashing Generalmente un usuario anota su login y password en un papelito y luego, cuando lo recuerda, lo arroja a la basura. Este procedimiento por más inocente que parezca esel que puede aprovechar un atacante para hacerse de una llave para entrar en el sistema. El Trashing puede ser físico (como el caso descrito) o lógico, como analizar buffers de impresora y memoria, bloques de discos, etc. Ataques de Monitorización Este tipo de ataque se realiza para observar a la víctima y su sistema, con el objetivo de obtener información y establecer sus vulnerabilidades y posibles formas de acceso futuro. Shoulder Surfing : Consiste en espiar físicamente a los usuarios para obtener su login y password correspondiente. Decoy : Son programas diseñados con la misma interfaz que el original. En ellos se imita la solicitud de login y el usuario desprevenido lo hace. Luego el programa guardará esa información y dejará paso a las actividades normales del sistema. La información recopilada será utilizada por el atacante para futuras “visitas”. Una técnica semejante es aquella, que mediante un programa se guardan todas las teclas presionadas durante una sesión. Scanning : La idea es recorrer (escanear) tantos puertos de escucha como sea posible, y guardar información de aquellos que sean receptivos o de utilidad para cada necesidad en particular. Muchas utilidades de auditoria también se basan en este paradigma. Hay varios tipos de scanning entre los que destaca TCP Connect Scanning que es la forma básica de escaneo de puertos TCP. Si el puerto está escuchando devolverá una respuesta de éxito; cualquier otro caso significará que el puerto no está abierto o que no se puede establecer conexión con él. Su ventaja es que es rápido y no necesita privilegios especiales. Su desventaja es que es fácilmente detectable por el administrador del sistema. Eavestropping-Packet Sniffing : Es la interceptación del tráfico de red. Esto se realiza con Packet Sniffers, son programas que controlan los paquetes que circulan por una red. Los sniffers pueden ser colocados tanto en estaciones de trabajo conectadas a la red como a un equipo Router o un Gateway de Internet y esto puede ser realizado por un usuario con legítimo acceso o por un intruso que ha ingresado por otras vías. Cada máquina conectada a la red (mediante una placa con una dirección única) verifica la dirección destino de los paquetes TCP. Si estas direcciones son iguales asume que el paquete enviado es para ella, caso contrario libera el paquete para que otras placas lo analicen. Un sniffer consiste en colocar a la placa de red en un modo llamado promiscuo, el cual desactiva el filtro de verificación de direcciones y por tanto todos los paquetes enviados a la red llegan a esta placa. Actualmente existen sniffers para capturar cualquier tipo de información específica. Por ejemplo, passwords de un recurso compartido o de acceso a una cuenta, que generalmente viajan sin encriptar. También son útiles para capturar números de tarjetas de crédito y direcciones de correo electrónico entrantes y salientes. Snooping-Downloading : Además de interceptar el tráfico de red, el atacante ingresa a los documentos, mensajes de correo electrónico y otra información guardada realizando una copia de sus documentos (downloading) a su propio ordenador para luego hacer un análisis exhaustivo de la misma. Ataques de Autenticación Este tipo de ataque tiene como objetivo engañar al sistema de la víctima para entrar en este. Generalmente se realiza tomando las sesiones ya establecidas por la víctima u obteniendo su nombre de usuario y password. Spoofing-Looping : Spoofing puede traducirse como “hacerse pasar por otro”. Una forma común de Spoofing es conseguir el nombre y password para una vez ingresado al sistema, tomar acciones en nombre de él. El intruso usualmente utiliza un sistema para obtener información e ingresar en otro y luego utiliza este para entrar en otro y así sucesivamente. Este proceso llamado Looping, tiene la finalidad de ocultar la identificación y ubicación del atacante. IP Spoofing : El atacante genera paquetes de Internet con una dirección de red falsa en el origen, pero que es aceptada por el destinatario del paquete. Su utilización más común es enviar los paquetes con la dirección de un tercero de forma que la víctima “ve” un ataque proveniente de esa tercera red y no la dirección real del intruso. DNS Spoofing : Se manipulan paquetes UDP pudiéndose comprometer el servidor de nombres del Dominio (Domain Name Server DNS). Web Spoofing : El atacante crea un sitio web completo (falso) similar al que la víctima desea entrar. Los accesos a este sitio están dirigidos por el atacante permitiéndole controlar todas las acciones de la víctima, desde sus datos hasta las passwords, número de tarjetas de crédito, etc. El atacante también es libre de modificar cualquier dato que se esté transmitiendo entre el servidor original y la víctima o viceversa. IP Splicing-Hijacking : Se produce cuando un atacante consigue interceptar una sesión ya establecida. El atacante espera a que la víctima se identifique ante el sistema y tras ello le suplanta. Para entender el procedimiento supongamos la siguiente situación: IP Cliente: IP 195.1.1.1IP Servidor: IP 195.1.1.2IP Atacante: IP 195.1.1.3 El cliente establece una conexión con su servidor enviando un paquete que contendrá la dirección origen, destino, número de secuencia y un número de autenticación utilizado por el servidor para reconocer el paquete siguiente en la secuencia. Supongamos que este paquete contiene: IP Origen: 195.1.1.1 Puerto 1025IP Destino: 195.1.1.2 Puerto 23SEQ=3DF45ADAACK=F454FDF5Datos: Solicitud El servidor luego de recibir el primer paquete contesta al cliente con paquete Echo (recibido). IP Origen: 195.1.1.2 Puerto 1025IP Destino: 195.1.1.1 Puerto 23SEQ=F454FDF5 (ACK enviado por el cliente)ACK=3DF454E4Datos: Recepción OK (Echo) El cliente envía un paquete ACK al servidor, sin datos, en donde le comunica lo “perfecto de la comunicación. IP Origen: 195.1.1.1 Puerto 1025IP Destino: 195.1.1.2 Puerto 23SEQ=3DF454E4 (ACK enviado por el servidor)ACK=F454FDFFDatos: Confirmación de Recepción (ACK) El atacante que ha visto, mediante un Sniffer, los paquetes que circularon por la red calcula el número de secuencia siguiente: el actual + tamaño del campo de datos. Para calcular el tamaño de este campo: 1º paquete ACK Cliente=F454FDF52º paquete ACK Cliente=F454FDFFTamaño del campo de datos = F454FDFF - F454FD5 = 0A Hecho esto el atacante envía un paquete con el siguiente aspecto: IP Origen: 195.1.1.1 (IP del Cliente por el atacante)IP Destino: 195.1.1.2 (IP del servidor)SEQ=3DF454E4 (Último ACK enviado por el cliente)ACK=F454FE09 (F454FDFF + 0A) El servidor al recibir estos datos no detectará el cambio de origen ya que los campos que ha recibido como secuencia y ACK son los que esperaba recibir. El cliente, a su vez, quedará esperando datos como si su conexión estuviera establecida pero sin enviar datos y el atacante podrá seguir enviando datos mediante el procedimiento descrito. Utilización de puertas traseras (Backdoors) : Las puertas traseras son trozos de código en un programa que permiten saltarse los métodos usuales de autenticación para realizar ciertas tareas. Habitualmente son insertados por los programadores del sistema para agilizar la tarea de probar el código durante la fase de desarrollo. Esta situación se convierte en una falla de seguridad si se mantiene, involuntaria o intencionalmente, una vez terminado el producto ya que cualquiera que conozca el agujero o lo encuentre en su código podrá saltarse los mecanismos de control normales. Utilización de Exploits : Es frecuente entrar en un sistema explotando agujeros en los algoritmos de encriptación usados, en la administración de claves por parte de la empresa o encontrando un error en los programas utilizados. Los programas para explotar estos “agujeros” reciben el nombre de Exploits y lo que hacen es aprovechar la debilidad, fallo o error hallado en el sistema (hardware o software) para entrar al mismo. Nuevos agujeros se publican cada día por lo que mantenerse informado de los mismos y de las herramientas para combatirlos es de vital importancia. Obtención de Passwords : Este método comprende la obtención por “Fuerza Bruta” de aquellas claves que permiten entrar en los sistemas, aplicaciones, cuentas, etc, atacados. Muchas passwords de acceso son obtenidas fácilmente porque involucran el nombre u otro dato familiar del usuario, y además, nunca (o rara vez) se cambia. En este caso el ataque se simplifica e involucra algún tiempo de prueba y error. Otras veces se realizan ataques sistemáticos con la ayuda de programas especiales y diccionarios que prueban millones de posibles claves hasta encontrar la password correcta. El programa encargado encripta cada una de ellas, mediante el algoritmo utilizado por el sistema atacado y compara la palabra encriptada contra el archivo de passwords del sistema atacado, previamente obtenido. Si coinciden se ha encontrado la clave de acceso. Denegación de servicio (DoS – Denial of Service) Los protocolos existentes actualmente fueron diseñados para ser empleados en una comunidad abierta y con una relación de confianza mutua. La realidad indica que es más fácil desorganizar el funcionamiento de un sistema que acceder al mismo; así los ataques de negación de servicio tienen como objetivo saturar los recursos de las víctimas de forma que se inhabiliten los servicios brindados por la misma. Algunas razones por las que son útiles para un atacante son: Se ha instalado un troyano y se necesita que la víctima reinicie la máquina para que surta efecto. Se necesita cubrir inmediatamente sus acciones o un uso abusivo de CPU. El intruso cree que actúa bien al dejar fuera de servicio algún sitio web que le disgusta. El administrador de sistema quiere comprobar que sus instalaciones no son vulnerables a estos ataques. El administrador del sistema tiene un proceso que no puede “matar” en su servidor y debido a este no puede acceder al sistema. Para ello, lanza contra sí mismo un ataque DoS deteniendo los servicios. Entre los distintos tipos de DoS tenemos: Jamming o Flooding : Este tipo de ataques desactivan o saturan los recursos del sistema. Por ejemplo, se puede consumir toda la memoria o espacio disponible en disco, así como enviar tanto tráfico a la red que nadie puede utilizarla. El atacante satura el sistema con mensajes que requieren establecer conexión. Sin embargo, en vez de proveer la dirección IP del emisor, el mensaje contiene falsas direcciones IP usando “Spoofing” y “Looping”. El sistema responde al mensaje pero al no recibir respuesta acumula buffers con información de las conexiones abiertas, no dejando lugar a las conexiones legítimas. Connection Flood : La mayoría de las empresas que brindan servicios de Internet tienen un límite máximo en el número de conexiones simultáneas. Una vez que se alcanza este límite, no se admitirán conexiones nuevas. Así,por ejemplo, un servidor Web puede tener, capacidad para atender a mil usuarios simultáneos. Si un atacante establece mil conexiones y no realiza ninguna petición sobre ellas, monopolizará la capacidad del servidor. Las conexiones van caducando por inactividad poco a poco, pero el atacante sólo necesita intentar nuevas conexiones para mantener fuera de servicio al servidor. Net Flood : El atacante envía tantos paquetes de solicitud de conexión que las conexiones auténticas no pueden competir. En casos así el primer paso a realizar es ponerse en contacto con el Proveedor del servicio para que intente determinar la fuente del ataque, y como medida provisional, filtre el ataque en su extremo de la línea. El siguiente paso consiste en localizar las fuentes del ataque e informar a sus administradores, ya que seguramente se estarán usando sus recursos sin su conocimiento y consentimiento. Land Attack : Este ataque se basa en un error de la pila TCP/IP de las plataformas Windows. Consiste en mandar a algún puerto abierto de un servidor un paquete con la dirección y puerto origen igual que la dirección y puerto destino. El resultado es que después de cierta cantidad de mensajes enviados-recibidos la máquina termina colgándose. OOB (Out Of Band), Supernuke o winnuke : Es un ataque característico en los equipos Windows que hace que los equipos que escuchan por el puerto NETBios sobre TCP/UDP 137 a 139 queden fuera de servicio o disminuyan su rendimiento al enviarle paquetes UDP manipulados. Generalmente se envían fragmentos de paquetes OOB que la máquina víctima detecta como inválidos pasando a un estado inestable. OOB es el término normal pero realmente consiste en configurar el bit Urgente (URG) en los indicadores del encabezamiento TCP. E-Mail Bombing – Spamming : Consiste en enviar muchas veces un mensaje idéntico a una misma dirección saturando así el buzón de correo del destinatario. El spamming, en cambio, se refiere a enviar un e-mail a miles de usuarios, hayan estos solicitados el mensaje o no. Es muy utilizado por las empresas para hacer publicidad de sus productos. Ataques de Modificación-Daño Tampering o Data Diddling : Se refiere a la modificación desautorizada de los datos o el software instalado en el sistema víctima incluyendo borrado de archivos. Son particularmente serios cuando el que lo realiza ha obtenido derechos de administrador, con la capacidad de ejecutar cualquier comando. Ejemplos típicos: empleados bancarios (o externos) que crean falsas cuentas para derivar fondos de otras, estudiantes que modifican calificaciones de exámenes, et. Múltiples sitios web han sido víctimas del cambio en sus páginas por imágenes o manifiestos. Otras veces se reemplazan versiones de software por otros con el mismo nombre pero que incorporan código malicioso (virus, troyanos, etc). La utilización de virus y troyanos está dentro de esta categoría y se le dedicará un apartado especial. Borrado de Huellas : El borrado de huellas es una de las tareas más importantes que debe realizar el intruso después de ingresar en un sistema, ya que si se detecta su ingreso, el administrador buscará como conseguir “tapar el hueco” de seguridad evitando ataques futuros o incluso rastrear al atacante. Las huellas son todas las operaciones que realizó el intruso en el sistema y, por lo general, son almacenadas en Logs (archivos que guardan la información de lo que se realiza en el sistema) por el SO. Los archivos de Logs son una de las principales herramientas con las que cuenta un administrador para conocer los detalles de las tareas realizadas en el sistema y para la detección de intrusos. Ataque mediante ActiveX : ActiveX es una de las tecnologías de Microsoft que permite reutilizar código, descargar código totalmente funcional de un sitio remoto, etc. ActiveX soluciona los problemas de seguridad mediante certificados y firmas digitales. Una Autoridad Certificadora (AC) expende un certificado que acompaña a los controles activos y a una firma digital del programador. Cuando un usuario descarga una página con un control, se le preguntará si confía en la AC que expedió el certificado y en el control ActiveX. Si el usuario acepta el control, éste puede pasar a ejecutarse sin ningún tipo de restricciones salvo las propias que tenga el usuario en el SO por lo que la seguridad del sistema se deja en manos del usuario, característica que es utilizada para realizar ataques. Vulnerabilidad en los Navegadores : Un fallo común ha sido el denominado “Buffer Overflow” que consisten en explotar una debilidad relacionada con los buffers que la aplicación usa para almacenar las entradas de usuario. Por ejemplo, cuando el usuario escribe una dirección en formato URL ésta se guarda en un buffer para luego procesarla. Si no se realizan las oportunas operaciones de comprobación, un usuario podría manipular estas direcciones. Errores de Diseño, Implementación y de Operación Muchos sistemas están expuestos a “agujeros” de seguridad que son explotados para acceder a archivos, obtener privilegios o realizar sabotaje. Estas vulnerabilidades ocurren por varias razones y miles de “puertas invisibles” son descubiertas cada día en SO, aplicaciones de software, protocolos de red, exploradores de internet, correo electrónico y toda clase de servicios informáticos disponibles. Los SO abiertos como Unix o Linux tienen agujeros más conocidos y controlados que aquellos que son cerrados, por ejemplo Windows. La importancia y ventaja del código abierto radica en miles de usuarios que analizan dicho código y buscan posibles errores y ayudan a obtener soluciones de forma inmediata. Virus Informáticos Un virus informático es un pequeño programa invisible para el usuario de funcionamiento específico y subrepticio cuyo código incluye información suficiente y necesaria para que utilizando los mecanismos de ejecución que le ofrecen otros programas puedan reproducirse formando réplicas de sí mismos susceptibles de mutar, resultando de dicho proceso la modificación, alteración y/o destrucción de los programas, información y/o hardware afectado. Un virus informático es un ataque de tipo “Tampering” que puede ser ingresado al sistema por un dispositivo externo o a través de la red (emails u otros protocolos) sin intervención directa del atacante. Existen distintos tipos de virus, como aquellos que infectan archivos ejecutables, los sectores de arranque y la tabla de partición de los discos, etc. Los que causan mayores problemas suelen ser las macros y virus scripts que están ocultos en simples documentos, plantillas de cálculo, correo electrónico y aplicaciones que utiliza cualquier usuario de PC. La difusión se potencia con la posibilidad de su transmisión de un continente a otro a través de cualquier red o Internet y además son multiplataformas. Las técnicas de propagación más comunes son: Disquetes y otros medios: A la posibilidad de que un disquete contenga un archivo infectado se une el peligro de que integre un virus de sector de arranque (Boot). En este segundo caso, y si el usuario lo deja en la disquetera infectará el ordenador al encenderlo ya que el sistema arrancará desde el disquete. Correo electrónico: el usuario no necesita hacer nada para recibir mensajes que en muchos casos ni siquiera ha solicitado y que pueden llegar de cualquier lugar del mundo. Los mensajes de correo electrónico pueden incluir archivos, documentos o cualquier objeto Active-X-java infectado que al ejecutarse contagian la computadora del usuario. En las últimas generaciones de virus se envían emails sin mensajes pero con archivos adjuntos que al abrirlos proceden a su ejecución y posterior infección ya que se envían automáticamente a los contactos de la libreta de direcciones del sistema infectado. IRC o chat: Las aplicaciones de mensajería instantánea proporcionan un medio de comunicación anónimo, rápido, eficiente, cómodo y barato. Sin embargo, son peligrosas ya que los entornos chat ofrecen generalmente facilidades para transmisión de archivos con un gran riesgo en un entorno de red. Páginas web y transferencias vía FTP: los archivos que se descargan desde Internet pueden estar infectados y pueden provocar acciones dañinas en el sistema en el que se ejecutan. Grupos de noticias: Sus mensajes e información pueden estar infectados y por lo tanto contagiar al equipo del usuario que participe en ellos. Los tipo se virus más habituales son: Archivos Ejecutables (Virus ExeVir) : El virus se adosa a un archivo ejecutable y desvía el flujo de ejecución a su código, para luego retornar al huésped y ejecutar acciones esperadas por el usuario. Al realizarse esta acción el usuario no se percata de lo sucedido. Una vez que el virus es ejecutado se aloja en memoria y puede infectar otros archivos ejecutables que sen abiertos en esa máquina. Virus en el Sector de Arranque (Virus Anterior a la Carga del SO) : En los primeros 512 bytes de un disquete formateado están las rutinas necesarias para la carga y reconocimiento de dicho disquete. Entre ellas se encuentra la función invocada si no se encuentra el SO. Se guarda la zona de arranque original en otro sector del disco. Luego el virus carga la antigua zona de arranque. Al arrancar el disquete ejecuta el virus (que obligatoriamente debe tener 512 bytes o menos) quedando residente en memoria, luego ejecuta la zona de arranque original, salvada anteriormente. Virus Residente : El objetivo de residir en memoria es controlar los accesos a disco realizados por el usuario y el SO. Cada vez que se produce un acceso, el virus verifica si el disco o archivo objeto al que se accede está infectado y si no lo está procede a almacenar su propio código en el mismo. Este código se almacenará en un archivo, tabla de partición o en el sector de arranque dependiendo del tipo de virus de que se trate. Virus de Macros : Estos virus infectan archivos de información generados por aplicaciones de oficina que cuentan con lenguajes de programación de macros. Su funcionamiento consiste en que si una aplicación abre un archivo infectado, la aplicación (o parte de ella) se infecta y cada vez que se genera un nuevo archivo o se modifique uno existente contendrá el virus de macros. Virus de Mail : Su modo de actuar se basa en la confianza excesiva por parte del usuario, a este le lleva vía mail un mensaje con un archivo comprimido, el usuario lo descomprime y al terminar esta acción, el contenido del archivo se ejecuta y comienza el daño. Este tipo de virus tomó relevancia con la explosión masiva de Internet y virus tipo Melissa y I Love You. Generalmente estos virus se auto-envían a algunas de las direcciones de la libreta. Hoax, los Virus Fantasmas : No es un virus realmente. El auge del correo electrónico generó la posibilidad de transmitir mensajes de alerta de seguridad. Así comenzaron a circular mensajes de distinta índole de casos inexistentes. Los objetivos de estas alertas pueden causar alarma, pérdida de tiempo, robo de direcciones de correo y saturación de los servidores. Gusanos : Un gusano es un programa capaz de ejecutarse y propagarse por sí mismo a través de redes, en ocasiones portando virus o aprovechando errores de los sistemas a los que se conecta para dañarlos. Al ser difíciles de programar, su número no es muy elevado, pero el daño que pueden causar es muy grande. Hemos de pensar que un gusano puede automatizar y ejecutar en unos segundos todos los pasos que seguiría un atacante humano para acceder a nuestro sistema, mientras que una persona, por muchos conocimientos y medios que posea, tardaría como mínimo horas en controlar nuestra red completa (un tiempo más que razonable para detectarlo), un gusano puede hacer eso mismo en pocos minutos: de ahí su enorme peligro y sus devastadores efectos. Caballos de Troya : Es un programa que aparentemente realiza una función útil a la vez que una operación que el usuario desconoce y que generalmente beneficia al autor del troyano o daña el sistema huésped. Ejemplos conocidos son el Back Oriffice y el Net Bus que son utilizados como poderosas armas para tomar el control del ordenador infectado. Estos programas pueden ser utilizados para la administración total del sistema atacado por parte de un tercero, con los mismos permisos y restricciones que el usuario de la misma. Modelo de Virus Informático Un virus está compuesto por su propio entorno dentro del que pueden distinguirse tres módulos principales: Módulo de Reproducción encargado de manejar las rutinas de parasitación de entidades ejecutables con el fin de que el virus pueda ejecutarse subrepticiamente. Módulo de Ataque que maneja las rutinas de daño adicional al virus y se activan cuando el sistema cumple cierta condición (por ejemplo una fecha). Módulo de Defensa con la misión de proteger al virus para evitar su detección o eliminación. Medidas de Protección y AseguramientoHoy es imposible hablar de un sistema cien por cien seguro porque el coste de la seguridad total es muy alto. Sin embargo, sí se pueden aplicar una serie de medidas de protección para controlar todo un conjunto de vulnerabilidades aunque no se logre la seguridad total. En este sentido las Políticas de Seguridad Informática (PSI) surgen como una herramienta de organización para concienciar a los miembros de una organización sobre la importancia y sensibilidad de la información y servicios críticos. Entre las medidas de protección más comunes tenemos las que se exponen a continuación. Controles de accesoEstos controles pueden implementarse en el SO, sobre los sistemas de aplicación, en BD, en un paquete específico de seguridad o en cualquier otro sistema que se utilice. Constituyen una importante ayuda para proteger al SO de la red, al sistema de aplicación y demás software de la utilización o modificación no autorizadas para mantener la integridad de la información y para resguardar la información confidencial del acceso no autorizado. Asimismo es conveniente tener en cuenta otras consideraciones referidas a la seguridad lógica como por ejemplo las relacionadas al procedimiento que se lleva a cabo para determinar si corresponde un permiso de acceso (solicitado por un usuario) a un determinado recurso. Al respecto el NIST (National Institute for Standards and Technology) ha resumido los siguientes estándares de seguridad que se refieren a los requisitos mínimos de seguridad en cualquier sistema: Identificación y Autenticación Se denomina identificación el momento en que el usuario se da a conocer en el sistema mientras que autenticación es la verificación que realiza el sistema sobre esta identificación. Existen cuatro tipos de técnicas que permiten realizar la autenticación de la identidad del usuario, que pueden llevarse a cabo individual o combinadamente: Algo que solamente el individuo conoce: una clave secreta o password, clave criptológica o un número de identificación personal (PIN). Algo que la persona posee: por ejemplo una tarjeta magnética. Algo que el individuo es y que lo identifica unívocamente: las huellas digitales o la voz. Algo que el individuo es capaz de hacer: por ejemplo los patrones de escritura. En los dos primeros casos es frecuente que las claves se olviden o que las tarjetas o dispositivos se pierdan mientras que por otro lado los controles de autenticación biométricos serían más apropiados y fáciles de administrar resultado ser también los más costosos por lo dificultoso de su implementación eficiente. Desde el punto de vista de la eficiencia es conveniente que los usuarios sean identificados y autenticados solamente una vez, pudiendo acceder a partir de allí a todas las aplicaciones y datos a los que su perfil les permita tanto en sistema locales como en sistemas a los que debe acceder en forma remota. Esto se denomina “single log-in” o sincronización de passwords. Una de las posibles técnicas para implementar esta única identificación de usuarios sería la utilización de un servidor de autenticaciones sobre el cual los usuarios se identifican y que se encarga luego de autenticar al usuario sobre los restantes equipos a los que éste pueda acceder. Este servidor de autenticaciones no debe ser necesariamente un equipo independiente y puede tener sus funciones distribuidas tanto geográfica como lógicamente, de acuerdo con los requerimientos de carga de tareas. La seguridad informática se basa en gran medida en la efectiva administración de los permisos de acceso a los recursos informáticos, basados en la identificación, autenticación y autorización de accesos. Esta administración abarca: Proceso de solicitud, establecimiento, manejo y seguimiento y cierre de las cuentas de usuarios. Es preciso considerar que la solicitud de habilitación de un permiso de acceso para un usuario determinado, debe provenir de su superior y de acuerdo con sus requerimientos específicos de acceso debe crearse el perfil en el sistema de seguridad, en el SO o en la aplicación según corresponda. Además, la identificación de usuarios debe definirse según una norma homogénea para toda la organización. Revisiones periódicas sobre la administración de las cuentas y los permisos de acceso establecidos. Las mismas deben encararse desde el punto de vista del SO, y aplicación por aplicación, pudiendo ser llevadas a cabo por personal de auditoria o por la gerencia propietaria del sistema; siempre sobre la base de que cada usuario disponga del mínimo permiso que requiera de acuerdo con sus funciones. Las revisiones deben orientarse a verificar la adecuación de los permisos de acceso de cada individuo de acuerdo con sus necesidades operativas, la actividad de las cuentas de usuarios o la autorización de cada habilitación de acceso. Detección de actividades no autorizadas. Además de realizar auditorias o efectuar el seguimiento de los registros de transacciones (pistas) hay otras medidas que ayudan a detectar actividades no autorizadas. Algunas se basan en evitar la dependencia de personas determinadas, estableciendo la obligatoriedad de tomar vacaciones o efectuar rotaciones periódicas a las funciones asignadas a cada una de ellas. Nuevas consideraciones sobre cambios en la asignación de funciones del empleado. Para implantar la rotación de funciones o en caso de reasignar funciones por ausencias temporales de algunos empleados, es necesario considerar la importancia de mantener actualizados los permisos de acceso. Procedimientos en caso de desvinculaciones de personal con la organización, amistosas o no. Para evitar estas situaciones es recomendable anular los permisos de acceso a las personas que se desvincularán de la organización lo antes posible. En caso de despido, el permiso de acceso debería anularse previamente a la notificación de la persona sobre la situación. Roles El acceso a la información también puede controlarse a través de la función o rol del usuario que requiere dicho acceso. Algunos ejemplos de roles son: programador, jefe de proyecto, gerente, administrador de sistema, etc. En este caso los derechos de acceso se pueden agrupar de acuerdo con el rol. Limitaciones a los servicios Estos controles se refieren a las restricciones que dependen de parámetros propios de la utilización de la aplicación o preestablecidos por el administrador del sistema. Un ejemplo podría ser que en la organización se disponga de licencias para la utilización simultánea de un determinado producto de software para cinco personas en donde exista un control a nivel de sistema que no permita la utilización del producto a un sexto usuario. Modalidad de Acceso Se refiere al modo de acceso que se permite al usuario sobre los recursos y a la información. Puede ser: Lectura: el usuario puede únicamente leer o visualizar la información pero no puede alterarla. La información puede ser copiada o impresa. Escritura: se permite agregar datos, modificar o borrar información. Ejecución: este acceso otorga al usuario el privilegio de ejecutar programas. Borrado: permite al usuario eliminar recursos del sistema. Todas las anteriores. Ubicación y Horario El acceso a determinados recursos del sistema puede estar basado en la ubicación física o lógica de los datos o personas. En cuanto a horarios, este tipo de controles permite limitar el acceso de los usuarios a determinadas horas del día o días de la semana. Así se mantiene un control más restringido de los usuarios y zonas de ingreso. Control de Acceso Interno Palabras claves (passwords) Generalmente se utilizan para realizar la autenticación del usuario y sirven para proteger los datos y aplicaciones. Los controles independientes a través de la utilización de palabras claves resulta de muy bajo costo. Sin embargo, cuando el usuario se ve en la necesidad de utilizar varias palabras clave para acceder a diversos sistemas encuentra dificultoso recordarlas y probablemente las escriba o elija palabras fácilmente deducibles con lo que se ve disminuida de esta técnica. Sincronización de passwords: consiste en permitir que un usuario acceda con la misma password a diferentes sistemas interrelacionados y su actualización automática en todos ellos en caso de ser modificada. Caducidad y control: este mecanismo controla cuando pueden y deben cambiar sus passwords los usuarios. Se define el periodo mínimo que debe pasar para que los usuarios puedan cambiar sus passwords y un periodo máximo que puede transcurrir para que estas caduquen. Encriptación La información encriptada solamente puede ser desencriptada por quienes posean la clave apropiada. Listas de control de accesos Se refiere a un registro donde se encuentran los nombres de los usuarios que obtuvieron el permiso de acceso a un determinado recurso del sistema así como la modalidad de acceso permitido. Este tipo de listas varían considerablemente en su capacidad y flexibilidad. Límites sobre la interfaz de usuario Estos límites generalmente utilizados en conjunto con las listas de control de acceso restringen a los usuarios a realizar funciones específicas. Básicamente pueden ser de 3 tipos: menús, vistas sobre la BD y límites físicos sobre la interfaz de usuario. Por ejemplo, los cajeros automáticos donde el usuario sólo puede ejecutar ciertas funciones presionando teclas específicas. Etiqueta de seguridad Consiste en designaciones otorgadas a los recursos (por ejemplo, un archivo) que pueden utilizarse para varios propósitos como control de accesos, especificación de medidas de protección, etc. Estas etiquetas no son modificables. Control de Acceso Externo Dispositivos de control de puertos Estos dispositivos autorizan el acceso a un puerto determinado y pueden estar físicamente separados o incluidos en otro dispositivo de comunicaciones, como por ejemplo un módem. Firewalls o puertas de seguridad Permiten bloquear o filtrar el acceso entre dos redes, usualmente una privada y otra externa (por ejemplo Internet). Los firewalls permiten que los usuarios internos se conecten a la red exterior al tiempo que previenen la intromisión de atacantes o virus a los sistemas de la organización. Acceso de personal contratado o consultores Debido a que este tipo de personal en general presta servicios temporales, debe ponerse especial consideración en la política y administración de sus perfiles de acceso. Accesos públicos Para los sistemas de información consultados por el público en general, o los utilizados para distribuir o recibir información computerizada (mediante, por ejemplo, la distribución y recepción de formularios en soporte magnético o la consulta y recepción de información a través del correo electrónico) deben tenerse en cuenta medidas especiales de seguridad, ya que se incrementa el riesgo y se dificulta su administración. Debe considerarse para estos casos de sistemas públicos, que un ataque externo o interno puede acarrear un impacto negativo en la imagen de la organización. Administración Una vez establecidos los controles de acceso sobre los sistemas y la aplicación, es necesario realizar una eficiente administración de estas medidas de seguridad lógica, lo que involucra la implantación, seguimiento, pruebas y modificaciones sobre los accesos de los usuarios de los sistemas. La política de seguridad que se desarrolle respecto a la seguridad lógica debe guiar a las decisiones referidas a la determinación de los controles de accesos y especificando las consideraciones necesarias para el establecimiento de perfiles de usuarios. La definición de los permisos de acceso requiere determinar cual será el nivel de seguridad necesario sobre los datos, por lo que es imprescindible clasificar la información, determinando el riesgo que produciría una eventual exposición de la misma a usuarios no autorizados. Así los diversos niveles de la información requerirán diferentes medidas y niveles de seguridad. Para empezar la implantación, es conveniente comenzar definiendo las medidas de seguridad sobre la información más sensible o las aplicaciones más críticas y avanzar de acuerdo a un orden de prioridad descendente, establecido alrededor de las aplicaciones. Una vez clasificados los datos, deberán establecerse las medidas de seguridad para cada uno de los niveles. Un programa específico para la administración de los usuarios informáticos desarrollado sobre la base de las consideraciones expuestas, puede constituir un compromiso vacío, si no existe una conciencia de la seguridad en la organización por parte de todos los empleados. Esta conciencia de la seguridad puede alcanzarse mediante el ejemplo del personal directivo en el cumplimiento de las políticas y el establecimiento de compromisos firmados por el personal, donde se especifique la responsabilidad de cada uno. Pero además de este compromiso debe existir una concienciación por parte de la administración hacia el personal en donde se remarque la importancia de la información y las consecuencias de su pérdida o apropiación de la misma por agentes extraños a la organización. Administración del personal y usuarios Lleva generalmente cuatro pasos: Definición de puestos: debe contemplarse la máxima separación de funciones posibles y el otorgamiento del mínimo permiso de acceso requerido por cada puesto para la ejecución de las tareas asignadas. Determinación de la sensibilidad del puesto: es necesario determinar si la función requiere permisos rigurosos que le permitan alterar procesos, perpetrar fraudes o visualizar información confidencial. Elección de la persona para cada puesto: requiere considerar los requerimientos de experiencia y conocimientos técnicos necesarios para cada puesto. Asimismo,para los puestos definidos como críticos puede requerirse una verificación de antecedentes personales. Formación inicial y continua del empleado: cuando la persona seleccionada entra en la organización además de sus responsabilidades individuales para la ejecución de las tareas que se asignen, debe comunicársele las políticas de organizaciones haciendo hincapié en la política de seguridad. El individuo debe conocer las disposiciones de la organización, su responsabilidad en cuanto a la seguridad informática y lo que se espera de él. Defensa de los AtaquesLa mayoría de los ataques mencionados se basan en fallo de diseño inherentes a Internet (o sus protocolos) y a los SO utilizados, por lo que no son “solucionables” en un plazo breve de tiempo. La solución inmediata en cada caso es mantenerse informado sobre todos los tipos de ataques existentes y las actualizaciones que permanentemente lanzan las empresas de software, principalmente en SO. Las siguientes medidas preventivas son: Mantener las máquinas actualizadas y seguras físicamente. Mantener personal especializado en cuestiones de seguridad. Aunque una máquina no contenga información valiosa hay que tener en cuenta que puede resulta útil para un atacante, a la hora de ser empleada en una denegación de servicio (DoS) coordinado o para ocultar su verdadera dirección. No permitir el tráfico “broadcast” desde fuera de nuestra red. De esta forma evitamos ser empleados como “multiplicadores”. Auditorias de seguridad y sistemas de detección. Mantenerse informado constantemente sobre cada una de las vulnerabilidades encontradas y parches instalados. Para esto es recomendable estar subscrito a listas que brinden este tipo de información. La información continua del usuario. Política de Seguridad InformáticaSe define (RFC 1244) como “una declaración de intenciones de alto nivel que cubre la seguridad de los sistemas informáticos y que proporciona las bases para definir y delimitar responsabilidades para las diversas actuaciones técnicas y organizativas que se requerirán”. La política se refleja en una serie de normas, reglamentos y protocolos a seguir donde se definen las medidas a tomar para proteger la seguridad del sistema. Deben tener las siguientes características: Cubrir todos los aspectos relacionados con la seguridad. Adecuarse a las necesidades y recursos. Definir estrategias y criterios generales y adoptar en distintas funciones y actividades las alternativas ante circunstancias que se puedan dar repetidas veces. Evaluación de RiesgosEl análisis de riesgos supone, además de calcular la posibilidad de que ocurran cosas negativas, los siguientes puntos: Poder obtener una evaluación económica del impacto de estos sucesos. Tener en cuenta la probabilidad de que sucedan cada uno de los problemas posibles. Conocer qué se quiere proteger, dónde y cómo, asegurando que con los costes en que se incurre se obtengan beneficios efectivos. Para ello se deberá identificar los recursos (hardware, software, información, personal, accesorios, etc) con que se cuenta y las amenazas a las que se está expuesto. Los riesgos se suelen clasificar por su nivel de importancia y por la severidad de su pérdida: Estimación del riesgo de pérdida del recurso (Ri) Estimación de la importancia del recurso (Ii) Para la cuantificación del riesgo de perder un recurso, es posible asignar un valor numérico de 0 a 10 tanto a la importancia del recurso (10 es el recurso de mayor importancia) como al riesgo de perderlo (10 es el riesgo más alto). El riesgo del recurso será el producto de su importancia por el riesgo de perderlo: Luego, con la siguiente fórmula es posible calcular el riesgo general de los recursos de la red: Otros factores que debe considerar para el análisis de riesgo de un recurso de red son su disponibilidad, su integración y su carácter confidencial, los cuales se pueden incorporar a la fórmula para ser evaluados. Identificación de una AmenazaUna vez conocido los riesgos, los recursos que se deben proteger y como su daño o falta pueden influir en la organización es necesario identificar cada una de las amenazas y vulnerabilidades que pueden causar estas bajas en los recursos. Se suele dividir las amenazas existentes según su ámbito de actuación: Desastre del entorno (Seguridad física) Amenazas del sistema (Seguridad lógica) Amenazas en la red (Comunicaciones) Amenazas de personas Estrategia de SeguridadPara establecer una estrategia adecuada es conveniente pensar una política de protección en los distintos niveles que esta debe abarcar: Física, Lógica, Humana y la interacción que existe entre estos factores. En cada caso considerado, el plan de seguridad debe incluir tanto una estrategia Proactiva (proteger o proceder) o de previsión de ataques para minimizar los puntos vulnerables existentes en la directiva de seguridad y desarrollar planes de contingencias, como una estrategia Reactiva (perseguir y procesar) posterior al ataque que ayuda al personal de seguridad a evaluar el daño causado o a implantar un plan de contingencia adecuado. Auditoria y ControlSe estudiarán aparte en el último punto de este tema, dada su gran importancia. Plan de ContingenciaLos planes de contingencia se elaboran como respuesta a la acción de los diferentes riesgos y tienen los siguientes objetivos fundamentales: Minimizar las interrupciones en la operación normal. Limitar la extensión de las interrupciones y de los daños que originen Posibilitar una vuelta al servicio rápida y sencilla Ofrecer al personal unas normas de actuación frente a emergencias Dotar de medios alternativos de proceso en caso de catástrofe Para garantizar la validez del Plan y que no quede obsoleto, debe ser revisado periódicamente. El Plan de Contingencia recoge los siguientes planes como respuesta a los problemas: Plan de Emergencia: normas de actuación durante o inmediatamente después de cada fallo o daño. Plan de Recuperación: normas para reiniciar todas las actividades del proceso en el Centro. Plan de Respaldo: especifica todos los elementos y procedimientos precisos para mantener la seguridad de la información, como configuración del equipo, comunicaciones, SO y opciones, etc. Equipos de Respuesta a IncidenciasEs aconsejable formar un equipo de respuestas a incidencias implicado en los trabajos del profesional de seguridad que incluye: Desarrollo de instrucciones para controlar incidencias. Creación del sector o determinación del responsable. Identificación de las herramientas de software para responder a incidentes y eventos. Investigación y desarrollo de otras herramientas de seguridad informática. Realización de actividades formativas y de motivación. Realización de investigaciones acerca de virus. Ejecución de estudios relativos a ataques al sistema. Estos trabajos proporcionarán los conocimientos que la organización puede utilizar y la información que hay que distribuir antes y durante los incidentes. Una vez que el Administrador de seguridad y el equipo de respuestas a incidentes han analizado la incidencia, el Administrador debe delegar la responsabilidad del control de incidentes en el equipo de respuesta que será el responsable de responder a incidentes como virus, gusanos o cualquier otro código dañino, engaños y ataques de personal interno. Copias de Respaldo (Backups)El backup de archivos permite tener disponible e íntegra la información para cuando sucedan los accidentes. Sin un backup es imposible devolver la información al estado anterior al desastre. Es necesario realizar un análisis coste/beneficio para determinar qué información será almacenada, espacios de almacenamiento destinados a tal fin, la forma de realización, las estaciones de trabajo que cubrirá el backup, etc. Para una correcta realización y seguridad de backups se deberán tener en cuenta estos puntos: Se debe contar con un procedimiento de respaldo de los SO y de la información de usuario para poder reinstalar fácilmente en caso de sufrir un accidente. Se debe determinar el medio y las herramientas correctas para realizar las copias, basándose en análisis de espacios, tiempos de lectura/escritura, tipos de backup a realizar, etc. El almacenamiento de los backups debe realizarse en locales diferentes de donde reside la información primaria. De este modo se evita la pérdida si el desastre alcanza a todo el edificio o local. Se debe verificar, periódicamente la integridad de los respaldos que se están almacenando. Se debe contar con un procedimiento para garantizar la integridad física de los respaldos en previsión de robos o destrucción. Se debe contar con una política para garantizar la privacidad de la información que se respalda en medios de almacenamiento secundarios. Se debe contar con un procedimiento para borrar físicamente la información de los medios de almacenamiento antes de desecharlos. Programas AntivirusUn antivirus es una gran BD con la huella digital de todos los virus conocidos para identificarlos y con las pautas que más contienen los virus. Los fabricantes de antivirus avanzan tecnológicamente en la misma medida que lo hacen los creadores de virus. Esto sirve para combatirlos aunque no par aprevenir la creación e infección de otros nuevos. Las funciones presentes en un antivirus son: Detección: Se debe poder afirmar la presencia y/o ejecución de un virus en un ordenador. Adicionalmente puede brindar módulos de identificación, erradicación del virus o eliminación de la entidad infectada. Identificación de un virus: Existes diversas técnicas para realizar esta acción: Scanning: Técnica que consiste en revisar el código de los archivos (fundamentalmente ejecutables y documentos) en busca de pequeñas porciones de código que puedan pertenecer a un virus almacenados en una BD del antivirus. Su principal ventaja reside en la rápida y exacta que resulta la identificación del virus. Sus desventajas con que brinda una solución a posteriori y es necesario que el virus alcance un grado de dispersión considerable para que llegue a mano de los investigadores y estos lo incorporen a su BD lo que no es una solución definitiva. Heurística: Búsqueda de acciones potencialmente dañinas pertenecientes a un virus informático. No identifica con certeza al virus sino que rastrea rutinas de alteración de información y zonas generalmente no controlada por el usuario (Sector de arranque, FAT, etc). Su principal ventaja reside en que es capaz de detectar virus que no han sido agregados a las BD de los antivirus. Su desventaja radica en que puede sospechar de demasiadas cosas y el usuario debe ser medianamente capaz de identificar falsas alarmas. Comprobadores de Integridad. Controlan la actividad del PC señalando si algún proceso intenta modificar sectores críticos de la misma. Su ventaja residen en la prevención aunque a veces pueden ser vulnerados por los virus y ser desactivados por ellos, haciendo que el usuario se crea protegido, no siendo así. Es importante diferenciar los términos “detectar”: determinación de la presencia de un virus e “identificar”: determinación de qué virus fue el detectado. Lo importante es la detección del virus y luego, si es posible su identificación y erradicación. MODELO DE UN ANTIVIRUS Un antivirus puede estar constituido por dos módulos principales y cada uno de ellos contener otros módulos: Módulo de Control : Posee la técnica de verificación de Integridad que posibilita el registro de posibles cambios en las zonas y archivos considerados de riesgo. Módulo de Respuesta : La función de “alarma” se encuentra en todos los antivirus y consiste en detener la ejecución de todos los programas e informar al usuario de la posible existencia de un virus. La mayoría ofrecen la posibilidad de su erradicación si la identificación ha sido positiva. Herramientas de SeguridadCualquier herramienta de seguridad representa un arma de doble filo: de la misma forma que un administrador las utiliza para detectar y solucionar fallos en sus sistemas o en la subred completa, un potencial intruso las puede utilizar para detectar esos mismos fallos y aprovecharlos para atacar los equipos. Herramientas como NESSUS, SAINT o SATAN pasan de ser útiles a ser peligrosas cuando las utilizan crackers que buscan información sobre las vulnerabilidades de un equipo o de una red completa. La conveniencia de diseñar y distribuir libremente herramientas que puedan facilitar un ataque es un tema peliagudo; incluso expertos reconocidos como Alec Muffet (autor del adivinador de contraseñas Crack) han recibido enormes críticas por diseñar determinadas herramientas de seguridad para Unix. Tras numerosos debates sobre el tema, ha quedado claro que no se puede basar la seguridad de un sistema en el supuesto desconocimiento de sus problemas por parte de los atacantes: esta política, denominada “Security through obscurity”, se ha demostrado inservible en múltiples ocasiones. Si los administradores no utilizan herramientas de seguridad que muestren las debilidades de los sistemas (para corregirlas), un atacante no va a dudar en utilizar tales herramientas (para explotar las debilidades encontradas). Sistemas de Protección a la Integridad y Privacidad de la InformaciónHerramientas que utilizan criptografía para asegurar que la información sólo sea visible para quien tiene autorización. Su aplicación se realiza principalmente en las comunicaciones entre dos entidades. Dentro de este tipo de herramientas se pueden citar Pretty Good Privacy (PGP), Secure Sockets Layer (SSL) y los Certificados Digitales. También protocolos como Message Digest (MD5) o Secure Hash Algorithm (SHA) tratan de asegurar que no ha habido alteraciones indeseadas en la información que se intenta proteger. Auditoria de Seguridad LógicaLa auditoria consiste en contar con los mecanismos para determinar qué sucede en el sistema, qué hace cada uno y cuando lo hace. Mediante una auditoria de seguridad informática, se pueden identificar los puntos fuertes y débiles de una organización con respecto al manejo de la seguridad de su información y se pueden definir claramente los pasos a seguir para lograr un perfeccionamiento de la misma. Ventajas de una auditoria: Mejora sustancialmente la eficiencia en la seguridad de la información. Minimiza el riesgo de intrusión en sus sistemas, robos, uso indebido o alteración de información, abuso de privilegios o interrupción de los servicios ofrecidos. Elimina riesgos innecesarios. Posibilita la toma de decisiones sobre la seguridad de sus sistemas basándose en la información más completa. Posibilita la definición de responsabilidades bien diferenciadas. Brinda protección para toda la organización. Premisas fundamentales: El nivel de seguridad satisfactorio “ahora”, es mejor que un nivel de seguridad perfecto “a largo plazo”. Es imprescindible conocer los propios puntos débiles y evitar riesgos imposibles de cuantificar. Realizar y exigir auditorias periódicas mejoran la salud de los sistemas y previenen ataques e incidentes. En una organización la seguridad es tan fuerte como el punto más débil de la misma, por lo tanto, interesa concentrarse (en un principio al menos) en estos últimos. Un 75% de las agresiones intencionadas son internas a las organizaciones. Lo más productivo es concentrarse en amenazas factibles y conocidas. Las áreas principales a centrarse son las siguientes: Política de seguridad : Se necesita una política que refleje las expectativas de la organización en materia de seguridad a fin de suministrar administración con dirección y soporte. La política también se puede utilizar como base para el estudio y evaluación en curso. Organización de la seguridad : Sugiere diseñar una estructura de administración dentro de la organización que establezca la responsabilidad de los grupos en ciertas áreas de la seguridad y un proceso para el manejo de respuestas ante incidentes. Control y clasificación de los recursos de información : Necesita un inventario de los recursos de información de la organización y con base en éste conocimiento, debe asegurar que se brinde un nivel adecuado de protección. Seguridad del personal : Establece la necesidad de educar e informar a los empleados sobre lo que se espera de ellos en materia de seguridad y confidencialidad. También determina cómo incide el papel que desempeñan los empleados en materia de seguridad en el funcionamiento general de la compañía. Se debe implantar un plan para reportar los eventuales incidentes. Seguridad física y ambiental : Responde a la necesidad de proteger las áreas, el equipo y los controles generales. Manejo de las comunicaciones y las operaciones : Los objetivos de ésta sección son: Asegurar el funcionamiento correcto y seguro de las instalaciones de proceso de datos. Minimizar el riesgo de falla de los sistemas. Proteger la integridad del software y la información. Conservar la integridad y disponibilidad del procesamiento y la comunicación de la información. Garantizar la protección de la información en las redes y de la infraestructura de soporte. Evitar daños a los recursos de información e interrupciones en las actividades de la compañía. Evitar la pérdida, modificación o uso indebido de la información que intercambian las organizaciones. Control de acceso : Establece la importancia de controlar el acceso a la red y los recursos de aplicación para proteger contra los abusos internos e intrusos externos. Desarrollo y mantenimiento de los sistemas : Recuerda que en toda labor de tecnología de la información, se debe implantar y mantener la seguridad mediante controles de seguridad en todas las etapas del proceso. Manejo de la continuidad de la empresa : Aconseja estar preparado para contrarrestar las interrupciones en las actividades de la empresa y para proteger los procesos importantes de la misma en caso de un fallo grave o un suceso fortuito. Una auditoria de seguridad lógica debe centrarse en los siguientes aspectos: Contraseñas de acceso. Control de errores. Garantías de una transmisión para que sólo sea recibida por el destinatario. Para esto, regularmente se cambia la ruta de acceso de la información a la red. Registros de las actividades de los usuarios en la red. Encriptación de la información pertinente. Si se evita la importación y exportación de datos. Que el sistema pida el nombre de usuario y la contraseña para cada sesión: Que en cada sesión de usuario, se revise que no accede a ningún sistema sin autorización y que si un usuario introduce incorrectamente su clave un número establecido de veces, su cuenta queda deshabilitada. Si se obliga a los usuarios a cambiar su contraseña regularmente, y si las contraseñas son mostradas en pantalla cuando se introducen. Si por cada usuario, se da información sobre su última conexión a fin de evitar suplantaciones. Si el software o hardware con acceso libre está inhabilitado. Generación de estadísticas de las tasas de errores y transmisión. Creación de protocolos con detección de errores. Mensajes lógicos de transmisión que han de llevar origen, fecha, hora y receptor. Software de comunicación, que ha de tener procedimientos correctivos y de control ante mensajes duplicados, fuera de orden, perdidos o retrasados. Datos importantes que sólo pueden ser impresos en una impresora especificada y ser vistos desde un terminal debidamente autorizado. Análisis del riesgo de aplicaciones en los procesos. Análisis de la conveniencia de cifrar los canales de transmisión entre diferentes organizaciones. Cifrados de los datos que viajan por internet. Si en la LAN hay equipos con módem entonces se debe revisar el control de seguridad asociado para impedir el acceso de equipos fuera de la red. Existencia de políticas que prohíben la instalación de programas o equipos personales en la red. Inhabilitación de los accesos a servidores remotos. Si la propia empresa genera ataques propios para probar la solidez de la red y encontrar posibles fallos en cada una de las siguientes facetas: Servidores = Desde dentro del servidor y de la red interna. Servidores web. Intranet = Desde dentro. Firewall = Desde dentro. Accesos del exterior y/o Internet. Bibliografía Scribd (Ibiza Ales) Técnicas de evaluación de alternativas y análisis de viabilidad. Personal, procedimientos, datos, software, hardware. Presupuestación y control de costes de un proyecto informático.IntroducciónMientras que el Plan de Sistemas de Información tiene como objetivo proporcionar un marco estratégico que sirva de referencia para los Sistemas de Información de un ámbito concreto de una organización, el objetivo del Estudio de Viabilidad del Sistema es el análisis de un conjunto concreto de necesidades para proponer una solución a corto plazo, que tenga en cuenta restricciones económicas, técnicas, legales y operativas. La solución obtenida como resultado del estudio puede ser la definición de uno o varios proyectos que afecten a uno o varios sistemas de información ya existentes o nuevos. Para ello, se identifican los requisitos que se ha de satisfacer y se estudia, si procede, la situación actual. A partir del estado inicial, la situación actual y los requisitos planteados, se estudian las alternativas de solución. Dichas alternativas pueden incluir soluciones que impliquen desarrollos a medida, soluciones basadas en la adquisición de productos software del mercado o soluciones mixtas. Se describe cada una de las alternativas, indicando los requisitos que cubre. Una vez descritas cada una de las alternativas planteadas, se valora el impacto en la organización, la inversión a realizar en cada caso y los riesgos asociados. Esta información se analiza con el fin de evaluar las distintas alternativas y seleccionar la más adecuada, definiendo y estableciendo su planificación. Análisis de ViabilidadEl propósito de este proceso es analizar un conjunto concreto de necesidades, con la idea de proponer una solución a corto plazo. Los criterios con los que se hace esta propuesta no serán estratégicos sino tácticos y relacionados con aspectos económicos, técnicos, legales y operativos. Los resultados del Estudio de Viabilidad del Sistema constituirán la base para tomar la decisión de seguir adelante o abandonar. Si se decide seguir adelante pueden surgir uno o varios proyectos que afecten a uno o varios sistemas de información. Dichos sistemas se desarrollarán según el resultado obtenido en el estudio de viabilidad y teniendo en cuenta la cartera de proyectos para la estrategia de implantación del sistema global. Se ha considerado que este proceso es obligatorio, aunque el nivel de profundidad con el que se lleve a cabo dependerá de cada caso. La conveniencia de la realización del estudio de la situación actual depende del valor añadido previsto para la especificación de requisitos y para el planteamiento de alternativas de solución. En las alternativas se considerarán soluciones “a medida”, soluciones basadas en la adquisición de productos software del mercado o soluciones mixtas. Para valorar las alternativas planteadas y determinar una única solución, se estudiará el impacto en la organización de cada una de ellas, la inversión y los riesgos asociados. El resultado final de este proceso son los productos relacionados con la solución que se propone para cubrir la necesidad concreta que se planteó en el proceso, y depende de si la solución conlleva desarrollo a medida o no: Contexto del sistema (con la definición de las interfaces en función de la solución). Impacto en la organización. Coste / beneficio de la solución. Valoración de riesgos de la solución. Enfoque del plan de trabajo de la solución. Planificación de la solución. Solución propuesta: Descripción de la solución. Modelo de descomposición en subsistemas. Matriz de procesos / localización geográfica. Matriz de datos / localización geográfica. Entorno tecnológico y comunicaciones. Estrategia de implantación global del sistema. Descripción de los procesos manuales. Si la alternativa incluye desarrollo: Modelo abstracto de datos / modelo de procesos. Modelo de negocio / modelo de dominio. Si la alternativa incluye un producto software estándar de mercado: Descripción del producto. Evolución del producto. Costes ocasionados por el producto. Estándares del producto. Descripción de adaptación si es necesaria. Si en la organización se ha realizado con anterioridad un Plan de Sistemas de Información que afecte al sistema objeto de este estudio, se dispondrá de un conjunto de productos que proporcionarán información a tener en cuenta en todo el proceso. Las actividades que engloba este proceso se recogen en la siguiente figura, en la que se indican las actividades que pueden ejecutarse en paralelo y las que precisan para su realización resultados originados en actividades anteriores. Actividad EVS 1: Establecimiento del alcance del sistemaEn esta actividad se estudia el alcance de la necesidad planteada por el cliente o usuario, o como consecuencia de la realización de un PSI, realizando una descripción general de la misma. Se determinan los objetivos, se inicia el estudio de los requisitos y se identifican las unidades organizativas afectadas estableciendo su estructura. Se analizan las posibles restricciones, tanto generales como específicas, que puedan condicionar el estudio y la planificación de las alternativas de solución que se propongan. Si la justificación económica es obvia, el riesgo técnico bajo, se esperan pocos problemas legales y no existe ninguna alternativa razonable, no es necesario profundizar en el estudio de viabilidad del sistema, analizando posibles alternativas y realizando una valoración y evaluación de las mismas, sino que éste se orientará a la especificación de requisitos, descripción del nuevo sistema y planificación. Se detalla la composición del equipo de trabajo necesario para este proceso y su planificación. Finalmente, con el fin de facilitar la implicación activa de los usuarios en la definición del sistema, se identifican sus perfiles, dejando claras sus tareas y responsabilidades. Tarea EVS 1.1: Estudio de la solicitud Se realiza una descripción general de la necesidad planteada por el usuario, y se estudian las posibles restricciones de carácter económico, técnico, operativo y legal que puedan afectar al sistema. Antes de iniciar el estudio de los requisitos del sistema se establecen los objetivos generales del Estudio de Viabilidad, teniendo en cuenta las restricciones identificadas anteriormente. Si el sistema objeto de estudio se encuentra en el ámbito de un Plan de Sistemas de Información vigente, se debe tomar como referencia el catálogo de requisitos y la arquitectura de información resultante del mismo, como información adicional para la descripción general del sistema y determinación de los requisitos iniciales. Productos De entrada Catálogo de Requisitos del PSI (PSI 9.2) Arquitectura de Información (PSI 9.2) Solicitud (externo) De salida Descripción General del Sistema Catálogo de Objetivos del EVS Catálogo de Requisitos Prácticas Catalogación Sesiones de trabajo Participantes Comité de Dirección Jefe de Proyecto Analistas Tarea EVS 1.2: Identificación del alcance del sistema Se analiza el alcance de la necesidad planteada y se identifican las restricciones relativas a la sincronización con otros proyectos, que puedan interferir en la planificación y futura puesta a punto del sistema objeto del estudio. Esta información se recoge en el catálogo de requisitos. Si el sistema pertenece al ámbito de un Plan de Sistemas de Información, se debe tener en cuenta la arquitectura de información propuesta para analizar el alcance del sistema e identificar los sistemas de información que quedan fuera del ámbito del estudio. Además, se estudia el plan de proyectos, para determinar las posibles dependencias con otros proyectos. Una vez establecido el alcance, se identifican las unidades organizativas afectadas por el sistema, así como su estructura y responsables de las mismas. Para determinar los responsables se tiene en cuenta a quiénes afecta directamente y quiénes pueden influir en el éxito o fracaso del mismo. Productos De entrada Plan de Proyectos (PSI 9.2) Arquitectura de Información (PSI 9.2) Descripción General del Sistema (EVS 1.1) Catálogo de Objetivos del EVS (EVS 1.1) Catálogo de Requisitos (EVS 1.1) De salida Descripción General del Sistema: Contexto del Sistema Estructura Organizativa Catálogo de Requisitos: Requisitos Relativos a Restricciones o Dependencias con Otros Proyectos Catálogo de Usuarios Técnicas Diagrama de Flujo de Datos Diagrama de Descomposición Funcional Prácticas Catalogación Sesiones de trabajo Participantes Comité de Dirección Jefe de Proyecto Analistas Tarea EVS 1.3: Especificación del alcance del EVS En función del alcance del sistema y los objetivos del Estudio de Viabilidad del Sistema, se determinan las actividades y tareas a realizar. En particular, hay que decidir si se realiza o no el estudio de la situación actual y, en el caso de considerarlo necesario, con qué objetivo. Si el sistema pertenece al ámbito de un Plan de Sistemas de Información, los criterios que pueden orientar sobre la necesidad de llevar a cabo el estudio de la situación actual dependen de la arquitectura de información propuesta, en cuanto a la identificación de los sistemas de información actuales, implicados en el ámbito de este estudio, que se haya decidido conservar. Se identifican los usuarios participantes de las distintas unidades organizativas afectadas para la realización del Estudio de Viabilidad del Sistema, determinando previamente sus perfiles y responsabilidades. Debe comunicarse el plan de trabajo a los usuarios identificados como implicados en el Estudio de Viabilidad, solicitando su aceptación y esperando su confirmación. Productos De entrada Arquitectura de Información (PSI 9.2) Catálogo de Objetivos del EVS (EVS 1.1) Descripción General del Sistema (EVS 1.2) Catálogo de Usuarios (EVS 1.2) De salida Catálogo de Objetivos del EVS: Objetivos del Estudio de la Situación Actual Catálogo de Usuarios Plan de Trabajo Prácticas Catalogación Sesiones de trabajo Participantes Comité de Dirección Jefe de Proyecto Analistas Actividad EVS 2: Estudio de la situación actualLa situación actual es el estado en el que se encuentran los sistemas de información existentes en el momento en el que se inicia su estudio. Teniendo en cuenta el objetivo del estudio de la situación actual, se realiza una valoración de la información existente acerca de los sistemas de información afectados. En función de dicha valoración, se especifica el nivel de detalle con que se debe llevar a cabo el estudio. Si es necesario, se constituye un equipo de trabajo específico para su realización y se identifican los usuarios participantes en el mismo. Si se decide documentar la situación actual, normalmente es conveniente dividir el sistema actual en subsistemas. Si es posible se describirá cada uno de los subsistemas, valorando qué información puede ser relevante para la descripción. Como resultado de esta actividad se genera un diagnóstico, estimando la eficiencia de los sistemas de información existentes e identificando los posibles problemas y las mejoras. Tarea EVS 2.1: Valoración del estudio de la situación actual En función de los objetivos establecidos para el estudio de la situación actual, y considerando el contexto del sistema especializado en la descripción general del mismo, se identifican los sistemas de información existentes que es necesario analizar con el fin de determinar el alcance del sistema actual. Asimismo, se decide el nivel de detalle con el que se va a llevar a cabo el estudio de cada uno de los sistemas de información implicados. En el caso de haber realizado un Plan de Sistemas de Información que afecte a dicho sistema, se toma como punto de partida para este análisis la arquitectura de información propuesta. Para poder abordar el estudio, se realiza previamente una valoración de la información existente acerca de los sistemas de información afectados por el EVS. Se debe decidir si se realizan o no los modelos lógicos del sistema actual o si se describe el modelo físico, en función de los siguientes criterios: Si existen los modelos lógicos, y son fiables, se utilizan en la tarea Descripción de los Sistemas de Información Existentes (EVS 2.3). Si no existen dichos modelos, o no son fiables, se considera el tiempo de vida estimado para el sistema de información en función de la antigüedad, la obsolescencia de la tecnología o la falta de adecuación funcional para determinar si se obtienen los modelos lógicos y físicos del sistema actual o por el contrario no se elabora ningún modelo. La información relativa a los sistemas de información que se decida analizar, se obtiene mediante sesiones de trabajo con los Directores de Usuarios y el apoyo de los profesionales de Sistemas y Tecnologías de la Información y Comunicaciones (STIC) que se considere necesario. Productos De entrada Información Existente del Sistema Actual (externo) Arquitectura de Información (PSI 9.2) Catálogo de Objetivos del EVS (EVS 1.3) Descripción General del Sistema (EVS 1.2) De salida Descripción de la Situación Actual: Contexto del Sistema Actual Descripción de los Sistemas de Información Actuales Técnicas Diagrama de Flujo de Datos Prácticas Diagrama de Representación Sesiones de Trabajo Participantes Jefe de Proyecto Analistas Directores de Usuarios Tarea EVS 2.2: Identificación de usuarios participantes en el estudio de la situación actual En función del nivel de detalle establecido para el estudio de la situación actual, se identifican los usuarios participantes de cada una de las unidades organizativas afectadas por dicho estudio. Se informa a los usuarios implicados en el Estudio de la Situación Actual, se solicita su aceptación y se espera su confirmación. Productos De entrada Descripción General del Sistema (EVS 1.2) Catálogo de Usuarios (EVS 1.3) Descripción de la Situación Actual (EVS 2.1) De salida Catálogo de Usuarios Prácticas Catalogación Sesiones de Trabajo Participantes Jefe de Proyecto Directores de Usuarios Tarea EVS 2.3: Descripción de los sistemas de información existentes En esta tarea se describen los sistemas de información existentes afectados, según el alcance y nivel de detalle establecido en la tarea Valoración del Estudio de la Situación Actual (EVS 2.1), mediante sesiones de trabajo con los usuarios designados para este estudio. Si se ha decidido describir los sistemas a nivel lógico, y si existe un conocimiento suficiente de los sistemas de información a especificar, puede hacerse directamente, aplicando las técnicas de modelización y siguiendo un método descendente. Si no se dispone del conocimiento suficiente, se construyen los modelos a partir de la descripción del modelo físico, es decir, de forma ascendente. Si se tiene que describir el modelo físico, se puede hacer mediante un Diagrama de Representación en el que se recojan todos los componentes físicos y sus referencias cruzadas. Otra opción es describir el modelo físico de forma más detallada, para lo que es necesaria la utilización de herramientas de tipo scanner. Es conveniente indicar la localización geográfica y física actual de los módulos y datos de los sistemas de información afectados, evaluando al mismo tiempo la redundancia en las distintas unidades organizativas. Productos De entrada Descripción de la Situación Actual (EVS 2.1) Catálogo de Usuarios (EVS 2.2) De salida Descripción de la Situación Actual: Descripción Lógica del Sistema Actual Modelo Físico del Sistema Actual (opcional) Matriz de Localización Geográfica y Física de Módulos y Datos, incluidas las redundancias Técnicas Diagrama de Flujo de Datos Modelo Entidad / Relación extendido Diagrama de Clases Diagrama de Interacción de Objetos Matricial Prácticas Diagrama de Representación Sesiones de Trabajo Participantes Analistas Usuarios Expertos Equipo de Soporte Técnico Tarea EVS 2.4: Realización del diagnóstico de la situación actual Con el fin de elaborar el diagnóstico de la situación actual se analiza la información de los sistemas de información existentes, obtenida en la tarea anterior y se identifican problemas, deficiencias y mejoras. Estas últimas deben tenerse en cuenta en la definición de los requisitos. En el caso de haber realizado un Plan de Sistemas de Información, se considera la valoración realizada sobre los sistemas de información actuales que pertenecen al ámbito de este estudio. Si se ha tomado la decisión de no describir la situación actual, se realiza un diagnóstico global justificando esta decisión Productos De entrada Descripción de la Situación Actual (EVS 2.3) Catálogo de Objetivos del EVS (EVS 1.3) Valoración de la Situación actual (EVS 5.3) De salida Descripción de la Situación Actual: Diagnóstico de Situación Actual Participantes Analistas Responsable de Mantenimiento Actividad EVS 3: Definición de requisitos del sistemaEsta actividad incluye la determinación de los requisitos generales, mediante una serie de sesiones de trabajo con los usuarios participantes, que hay que planificar y realizar. Una vez finalizadas, se analiza la información obtenida definiendo los requisitos y sus prioridades, que se añaden al catálogo de requisitos que servirá para el estudio y valoración de las distintas alternativas de solución que se propongan. Tarea EVS 3.1: Identificación de las directrices técnicas y de gestión La realización de esta tarea permite considerar los términos de referencia para el sistema en estudio desde el punto de vista de directrices tanto técnicas como de gestión. Si el sistema en estudio pertenece al ámbito de un Plan de Sistemas de Información vigente, éste proporciona un marco de referencia a considerar en esta tarea. Con este fin, se recoge información sobre los estándares y procedimientos que deben considerarse al proponer una solución, relativos a: Políticas técnicas: Gestión de Proyectos (seguimiento, revisión y aprobación final). Desarrollo de Sistemas (existencia de normativas, metodologías y técnicas de programación). Arquitectura de Sistemas (centralizada, distribuida). Política de Seguridad (control de accesos, integridad de datos, disponibilidad de aplicaciones). Directrices de Planificación. Directrices de Gestión de Cambios. Directrices de Gestión de Calidad. Productos De entrada Catálogo de Normas del PSI (PSI 3.2) Recopilación de Directrices Técnicas y de Gestión (externo) De salida Catálogo de Normas Prácticas Catalogación Participantes Jefe de Proyecto Analistas Usuarios Expertos Tarea EVS 3.2: Identificación de Requisitos Para la obtención de las necesidades que ha de cubrir el sistema en estudio, se debe decidir qué tipo de sesiones de trabajo se realizarán y con qué frecuencia tendrán lugar, en función de la disponibilidad de los usuarios participantes. Si se ha realizado el Estudio de la Situación Actual (EVS 2), puede ser conveniente seleccionar la información de los sistemas de información existentes que resulte de interés para el desarrollo de dichas sesiones de trabajo. Una vez establecidos los puntos anteriores, se planifican las sesiones de trabajo con los usuarios participantes identificados al estudiar el alcance del Estudio de Viabilidad del Sistema (EVS 1.3), y se realizan de acuerdo al plan previsto. La información obtenida depende del tipo de sesión de trabajo seleccionado. Productos De entrada Descripción General del Sistema (EVS 1.2) Catálogo de Requisitos (EVS 1.2) Equipo de Trabajo del EVS (EVS 1.3) Catálogo de Usuarios (EVS 2.2/1.3) Descripción de la Situación Actual (EVS 2.4) De salida Identificación de Requisitos Prácticas Sesiones de Trabajo Participantes Jefe de Proyecto Analistas Usuarios Expertos Tarea EVS 3.3: Catalogación de Requisitos Se analiza la información obtenida en las sesiones de trabajo para la Identificación de Requisitos, definiendo y catalogando los requisitos (funcionales y no funcionales) que debe satisfacer el sistema, indicando sus prioridades. Se incluirán también requisitos relativos a distribución geográfica y entorno tecnológico. Productos De entrada Identificación de Requisitos (EVS 3.2) Catálogo de Requisitos (EVS 1.2) De salida Catálogo de Requisitos Prácticas Catalogación Participantes Jefe de Proyecto Analistas Usuarios Expertos Actividad EVS 4: Estudio de alternativas de soluciónEste estudio se centra en proponer diversas alternativas que respondan satisfactoriamente a los requisitos planteados, considerando también los resultados obtenidos en el Estudio de la Situación Actual (EVS 2), en el caso de que se haya realizado. Teniendo en cuenta el ámbito y funcionalidad que debe cubrir el sistema, puede ser conveniente realizar, previamente a la definición de cada alternativa, una descomposición del sistema en subsistemas. En la descripción de las distintas alternativas de solución propuestas, se debe especificar si alguna de ellas está basada, total o parcialmente, en un producto existente en el mercado. Si la alternativa incluye un desarrollo a medida, se debe incorporar en la descripción de la misma un modelo abstracto de datos y un modelo de procesos, y en orientación a objetos, un modelo de negocio y un modelo de dominio. Tarea EVS 4.1: Preselección de Alternativas de Solución Una vez definidos los requisitos a cubrir por el sistema, se estudian las diferentes opciones que hay para configurar la solución. Entre ellas, hay que considerar la adquisición de productos software estándar del mercado, desarrollos a medida o soluciones mixtas. Dependiendo del alcance del sistema y las posibles opciones, puede ser conveniente realizar inicialmente una descomposición del sistema en subsistemas. Se establecen las posibles alternativas sobre las que se va a centrar el estudio de la solución, combinando las opciones que se consideren más adecuadas. Productos De entrada Información de Productos Software del Mercado (externo) Descripción General del Sistema (EVS 1.2) Descripción de la Situación Actual (EVS 2.4) Catálogo de Requisitos (EVS 3.3) De salida Descomposición Inicial del Sistema en Subsistemas (opcional) Alternativas de Solución a Estudiar Prácticas Diagrama de Representación Participantes Jefe de Proyecto Analistas Técnicos de Sistemas Tarea EVS 4.2: Descripción de las Alternativas de Solución Para cada alternativa propuesta, se identifican los subsistemas que cubre y los requisitos a los que se da respuesta. Se deben considerar también aspectos relativos a la cobertura geográfica (ámbito y limitaciones) de procesos y datos, teniendo en cuenta a su vez la gestión de comunicaciones y control de red. En la definición de cada alternativa, se propone una estrategia de implantación teniendo en cuenta tanto la cobertura global del sistema como la cobertura geográfica. Si la alternativa incluye desarrollo se describe el modelo abstracto de datos y el modelo de procesos, y en el caso de Orientación a Objetos, el modelo de negocio y, opcionalmente, el modelo de dominio. Se propone el entorno tecnológico que se considera más apropiado para la parte del sistema basada en desarrollo y se describen los procesos manuales. Si la alternativa incluye una solución basada en producto se analiza su evolución prevista, adaptabilidad y portabilidad, así como los costes ocasionados por licencias, y los estándares del producto. Igualmente se valora y determina su entorno tecnológico. Productos De entrada Descripción General del Sistema (EVS 1.2) Descripción de la Situación Actual (EVS 2.4) Catálogo de Requisitos (EVS 3.3) Descomposición Inicial del Sistema en Subsistemas (EVS 4.1) (opcional) Alternativas de Solución a Estudiar (EVS 4.1) De salida Catálogo de Requisitos (actualizado) Alternativas de Solución a Estudiar: Catálogo de Requisitos (cobertura) Modelo de Descomposición en Subsistemas Matriz Procesos / Localización Geográfica Matriz Datos / Localización Geográfica Entorno Tecnológico y Comunicaciones Estrategia de Implantación Global del Sistema Descripción de Procesos Manuales Si la alternativa incluye desarrollo: Modelo Abstracto de Datos / Modelo de Procesos Modelo de Negocio / Modelo de Dominio (en caso de Orientación a Objetos) Si la alternativa incluye un producto software estándar de mercado: Descripción del Producto Evolución del Producto Costes Ocasionados por Producto Estándares del Producto Descripción de Adaptación (si es necesaria) Técnicas Matricial Diagrama de Flujo de Datos Modelo Entidad / Relación extendido Diagrama de Clases Casos de Uso Prácticas Catalogación Diagrama de Representación Participantes Jefe de Proyecto Analistas Usuarios Expertos Técnicos de Sistemas Responsable de Seguridad Especialistas en Comunicaciones Actividad EVS 5: Valoración de la alternativasUna vez descritas las alternativas se realiza una valoración de las mismas, considerando el impacto en la organización, tanto desde el punto de vista tecnológico y organizativo como de operación, y los posibles beneficios que se esperan contrastados con los costes asociados. Se realiza también un análisis de los riesgos, decidiendo cómo enfocar el plan de acción para minimizar los mismos y cuantificando los recursos y plazos precisos para planificar cada alternativa. Tarea EVS 5.1: Estudio de la Inversión Para cada alternativa de solución propuesta, se valora el impacto y se establece su viabilidad económica. Para ello, se realiza un análisis coste/beneficio que determina los costes del sistema y los pondera con los beneficios tangibles, cuantificables directamente, y con los beneficios intangibles, buscando el modo de cuantificarlos. Productos De entrada Alternativas de Solución a Estudiar (EVS 4.2) De salida Valoración de Alternativas: Impacto en la Organización de Alternativas Coste / beneficio de Alternativas Técnicas Análisis Coste / Beneficio Participantes Jefe de Proyecto Analistas Tarea EVS 5.2: Estudio de los Riesgos Para cada alternativa se seleccionan los factores de situación que habrá que considerar, relativos tanto a la incertidumbre como a la complejidad del sistema. Se identifican y valoran los riesgos asociados y se determinan las medidas a tomar para minimizarlos. Productos De entrada Alternativas de Solución a Estudiar (EVS 4.2) Valoración de Alternativas (EVS 5.1) De salida Valoración de Alternativas: Valoración de Riesgos Prácticas Impacto en la Organización Participantes Jefe de Proyecto Analistas Tarea EVS 5.3: Planificación de Alternativas En función del análisis de riesgos realizado en la tarea anterior, y para cada una de las alternativas existentes: Se determina el enfoque más adecuado para llevar a buen fin la solución propuesta en cada alternativa. Se realiza una planificación, teniendo en cuenta los puntos de sincronismo con otros proyectos en desarrollo o que esté previsto desarrollar, según se ha recogido en el catálogo de requisitos. De esta manera se garantiza el cumplimiento del plan de trabajo en los restantes procesos del ciclo de vida. Productos De entrada Catálogo de Requisitos (EVS 4.2) Alternativas de Solución a Estudiar (EVS 4.2) Valoración de Alternativas (EVS 5.2) De salida Plan de Trabajo de Cada Alternativa: Enfoque del Plan de Trabajo de Cada Alternativa Planificación de Cada Alternativa Técnicas Planificación Participantes Jefe de Proyecto Analistas Actividad EVS 6: Selección de la soluciónAntes de finalizar el Estudio de Viabilidad del Sistema, se convoca al Comité de Dirección para la presentación de las distintas alternativas de solución, resultantes de la actividad anterior. En dicha presentación, se debaten las ventajas de cada una de ellas, incorporando las modificaciones que se consideren oportunas, con el fin de seleccionar la más adecuada. Finalmente, se aprueba la solución o se determina su inviabilidad. Tarea EVS 6.1: Convocatoria de la Presentación Se efectúa la convocatoria de la presentación de las distintas alternativas propuestas, adjuntando los productos de la actividad anterior con el fin de que el Comité de Dirección pueda estudiar previamente su contenido. Se espera confirmación por parte del Comité de Dirección de las alternativas a presentar. Productos De entrada Catálogo de Usuarios (EVS 2.2/1.3) Alternativas de Solución a Estudiar (EVS 4.2) Valoración de Alternativas (EVS 5.2) Plan de Trabajo de Cada Alternativa (EVS 5.3) De salida Plan de Presentación de Alternativas Prácticas Presentación Participantes Jefe de Proyecto Tarea EVS 6.2: Evaluación de las Alternativas y Selección Una vez recibida la confirmación de qué alternativas van a ser presentadas para su valoración, se efectúa su presentación al Comité de Dirección, debatiendo sobre las ventajas e inconvenientes de cada una de ellas y realizando las modificaciones que sugiera dicho Comité, hasta la selección de la solución final. Productos De entrada Descripción General del Sistema (Contexto del Sistema) (EVS 1.2) Catálogo de Requisitos (EVS 4.2) Alternativas de Solución a Estudiar (EVS 4.2) Valoración de Alternativas (EVS 5.2) Plan de Trabajo de Cada Alternativa (EVS 5.3) Plan de Presentación de Alternativas (EVS 6.1) De salida Plan de Presentación de Alternativas Catálogo de Requisitos (Actualizado en Función de la Cobertura de la Solución) Solución Propuesta: Descripción de la Solución: Modelo de Descomposición en Subsistemas Matriz Procesos / Localización Geográfica Matriz Datos / Localización Geográfica Entorno Tecnológico y Comunicaciones Estrategia de Implantación Global del Sistema Descripción de Procesos Manuales Si la alternativa incluye desarrollo: Modelo Abstracto de Datos / Modelo de Procesos Modelo de Negocio / Modelo de Dominio Si la alternativa incluye un producto software estándar del mercado: Descripción del Producto Evolución del Producto Costes Ocasionados por Producto Estándares del Producto Descripción de Adaptación (si es necesaria) Contexto del Sistema (con la Definición de las Interfaces en Función de la Solución) Impacto en la Organización de la Solución Coste / Beneficio de la Solución Valoración de Riesgos de la Solución Enfoque del Plan de Trabajo de la Solución Planificación de la Solución Prácticas Presentación Sesiones de Trabajo Participantes Comité de Dirección Jefe de Proyecto Analistas Tarea EVS 6.3: Aprobación de la Solución El Comité de Dirección da su aprobación formal o determina la inviabilidad del sistema, por motivos económicos, de funcionalidad como resultado del incumplimiento de los requisitos identificados en plazos razonables o de cobertura de los mismos, etc. Productos De entrada Catálogo de Requisitos (EVS 6.2) Solución Propuesta (EVS 6.2) De salida Aprobación de la Solución Participantes Comité de Dirección Jefe de Proyecto Técnicas de Evaluación de AlternativasTécnicas de Análisis Coste / BeneficioObjetivos La técnica de análisis coste/beneficio tiene como objetivo fundamental proporcionar una medida de los costes en que se incurre en la realización de un proyecto y comparar dichos costes previstos con los beneficios esperados de la realización de dicho proyecto. Esta medida o estimación servirá para: Valorar la necesidad y oportunidad de acometer la realización del proyecto. Seleccionar la alternativa más beneficiosa para la realización del proyecto. Estimar adecuadamente los recursos económicos necesarios en el plazo de realización del proyecto. Es de destacar la necesidad cada vez mayor de guiarse por criterios económicos y no sólo técnicos para la planificación de trabajos y proyectos. Por ello se hace una primera introducción sobre las técnicas y métodos de evaluación de conceptos económicos, con el fin de proporcionar a los profesionales criterios que les ayuden en la planificación de proyectos y evaluación de alternativas. Conceptos Punto de amortización (Break-Even Point) Es el momento en el tiempo en que el conjunto de beneficios obtenidos por la explotación del nuevo sistema iguala al conjunto de costes de todo tipo que ha ocasionado. A partir del punto de amortización (Break-Even Point), el sistema entra en fase de aportar beneficios netos a la organización. Periodo de amortización (PayBack) Es el periodo de tiempo que transcurre desde que los costes con máximos hasta que se alcanza el punto de amortización (Break-Even Point), es decir, en cuanto el sistema empieza a aportar beneficios. Cuanto menor sea el periodo de amortización (Payback) de un Sistema, más atractivo será para la organización acometer su implantación. Retorno de Inversión – ROI (Return of Investment) Es el rendimiento de la inversión expresada en términos de procentaje. Se calcula mediante la fórmula siguiente: ROI = 100 x (Beneficio Neto Anual - Coste Desarrollo Anualizado) / Inversión Promedio Siendo: Beneficio Neto Anual : Beneficio neto que aporta el sistema como consecuencia de su uso, es decir los beneficios obtenidos más los gastos no incurridos. Deben restársele los gastos operacionales anuales y los de mantenimiento del sistema. Coste Desarrollo Anualizado : Total del coste inicial de desarrollo del sistema, dividido por los años que se supone que va a ser operativo. Inversión Promedio : Total de la inversión realizada (costes de desarrollo, hardware, software, etc.) dividido por dos. Descripción Para la realización del análisis coste/beneficio se seguirán los siguientes pasos: Producir estimaciones de costes/beneficios Se realizará una lista de todo lo que es necesario para implementar el sistema y una lista de los beneficios esperados del nuevo sistema. En un análisis de costes y beneficios se deberán considerar aquellos aspectos tangibles, es decir, medibles en valores como dinero, tiempo, etc, y no tangibles, es decir, no ponderables de una forma objetiva. En general, los costes suelen ser medibles y estimables en unidades económicas, no así en cuanto a los beneficios, los cuales pueden ser tangibles o no tangibles. Entre los beneficios no tangibles pueden estar: El aumento de cuentas debido a un mejor servicio a los clientes. La mejora en la toma de decisiones debido a una mejora en el soporte informático. La valoración de dichos beneficios se deberá estimar de una forma subjetiva y será realizada por las áreas correspondientes. A menudo es conveniente dividir los costes estimados a lo largo del proyecto, para ofrecer una información más detallada de la distribución de los recursos de cara a la dirección. En la estimación de costes se considerarán, los siguientes aspectos: Adquisición de hardware y software : El que sea preciso para el desarrollo, implantación y normal funcionamiento del sistema. Se debe considerar la saturación de máquinas o sistemas actuales como consecuencia de la entrada en vigor del nuevo sistema. Gastos de mantenimiento de hardware y software anteriores. Gastos de comunicaciones : Líneas, teléfonos, correo, etc. Gastos de instalación : Cableado, acondicionamiento de sala, recursos humanos y materiales, gastos de viaje, etc. Coste de desarrollo del sistema. Gastos del mantenimiento del sistema : Coste anual. Gastos de consultoría : En caso de requerirse algún consultor externo en cualquier etapa del proyecto. Gastos de formación : De todo tipo (Desarrolladores, Operadores, Implantadores, Usuario Final, etc). Gastos de material : Papel, tóner, etc Costes derivados de la curva de aprendizaje : De todo el personal involucrado: Desarrolladores, Técnicos de Sistemas, Operadores, y desde luego, Usuarios. Costes financieros , de publicidad, etc En la estimación de beneficios se pueden considerar cuestiones como las siguientes: Incremento de la productividad : Ahorro o mejor utilización de recursos humanos. Ahorro de gastos de mantenimiento del sistema actual. Ahorros de adquisición y mantenimiento de hardware y software , o reutilización de plataformas sustituidas. Incremento de ventas o resultados, disminución de costes : Producidos por una mejora de la gestión (rotación de stock “ just in time “, analítica de clientes, etc). Ahorro de material de todo tipo : Sustituido por datos electrónicos que proporciona el sistema, como por ejemplo: papel correo, etc. Beneficios financieros . Otros beneficios tangibles : Ahorro de recursos externos, consultoría, formación, etc. Beneficios intangibles : Incremento de la calidad del producto o servicio, mejora de la imagen de la compañía, mejora en la atención al cliente, mejora en la explotación, etc. Determinar la viabilidad del proyecto y su aceptación Se basará en uno de los métodos siguientes: Retorno de la inversión Este método consisten en calcular el coste y el beneficio anual, conociendo el coste total al inicio del proyecto “C0”, para determinar en qué año se recupera el coste total inicialmente estimado. El año de recuperación de la inversión se produce cuando ∑ Beneficio Neto = C0 . Valor Actual Este método permite tener en cuenta que un gasto invertido durante un cierto tiempo produce un beneficio. El método consiste en determinar el dinero que es viable invertir inicialmente para que se recupere la inversión en un periodo de tiempo definido previamente. El resultado depende del tipo de interés (r) utilizado en la evaluación. Se debe calcular, en primer lugar, el beneficio neto que se obtendrá cada año. Dicho beneficio no es real, ya que se debe estimar el valor real de dicha cantidad en el año n. Para ello se aplica la fórmula: Valor Actual = Beneficio neto / (1 + r/100)n n = año 1,…,i Se debe estudiar en cuántos años se recupera la inversión realizada inicialmente, o bien, si en un periodo de años fijado previamente se retorna la inversión y, por tanto, es viable el proyecto. Si la inversión es el C0, se determinará la viabilidad del proyecto consultando la siguiente tabla: El proyecto será viable si ∑ VAi &gt; C0 a lo largo del periodo fijado. Técnicas basadas en la teoría de la decisión multicriterio discretaLa Selección de un sistema informático, entre varias alternativas posibles, a fin de cubrir unas necesidades previas es un factor crítico de éxito de un sistema informático. Esta tarea puede y suele ser bastante compleja en la realidad debido a múltiples razones. En primer lugar por las sutilezas técnicas de la materia, que impiden una fácil y nítida visión global del conjunto. En segundo lugar, por la dispersión y variedad de fuentes de los datos que han de constituir la información de base del problema. Por último por la dificultad de estructurar todo ello, junto con las frecuentemente diversas opiniones de expertos y directivos, de forma que pueda tomarse una decisión final. Existen diferentes metodologías de análisis para afrontar el problema de manera más o menos cuantitativa, en un intento de hacerlo más racional y objetivo. Tradicionalmente se ha propuesto el análisis Coste-Beneficio, pero sus limitaciones son ya bien conocidas (necesidad de traducción a unidades monetarias, criterio único de evaluación), como para seguir utilizándolo. Otra metodología muy utilizada últimamente es la denominada metodología multicriterio, que recogen la multiplicidad de aspectos y de puntos de vista que inciden en la evaluación de los sistemas que compiten por ser seleccionados. Este es el marco natural de la denominada Decisión Multicriterio Discreta (DMD). Definición de criterios El objetivo final de cualquier proceso de evaluación de bienes y/o servicios informáticos es la selección de la mejor alternativa posible escogida entre las existentes. Debe partirse de una enumeración y enunciado de las alternativas (a efectos operativos estas serán las ofertas presentadas por las empresas,los proyectos candidatos o las soluciones posibles). Las alternativas son completamente disjuntas y exhaustivas, es decir no caben en principio soluciones mixtas mezcla de otras alternativas (no obstante podemos considerar variantes dentro de la alternativa presentada por una empresa, que se introducirán en el proceso evaluativo como una oferta más). Llamaremos A, a una alternativa genérica, con una variación entre 1 y m, (i =I,M). Por otra parte tenemos los criterios (también denominados atributos o características) que son los elementos en los que se basará el proceso de decisión, con su selección y posterior ponderación el decisor esta definiendo qué características de las alternativas le resultan importantes y en qué medida. Constituyen un conjunto discreto (C, 0=1, n). Los criterios deben ser ilustrativos de la característica que se quiera medir, cuando su número es muy grande hay que establecer un árbol de jerarquías entre ellos. Por último tenemos las evaluaciones o puntuaciones Xi, de cada alternativa i respecto a cada criterio i, constituyendo en su conjunto la denominada matriz de decisión y que sirve para definir las alternativas en función de sus criterios. Por otro lado tenemos los pesos W, agrupados en el llamado vector de pesos (W …. W) que representa la importancia que el decisor otorga a cada criterio. Asignación de pesos Desde el punto de vista de la DMD, estimar unos pesos W, que reflejen la importancia relativa de cada criterio j para el decisor, es una cuestión bastante delicada. La naturaleza de los pesos Wi como una cuantificación de la estructura de preferencias del decisor hace necesario “extraerlos” del mismo por algún procedimiento. Esto plantea, más en unos métodos que en otros, importantes problemas ya que, como es bien conocido, las inercias psicológicas del ser humano producen peligrosos sesgos e inconsistencias. Los principales métodos son: Método de las utilidades relativas: Partiendo de unas estimaciones provisionales, ir afinando dichas estimaciones mediante comparaciones binarias de subgrupos de criterios. Método AHP (Analitic Hierarchy Process – Proceso Jerárquico Analítico): Comparaciones binarias de todos los criterios detallados. Método Delphi: Es el método del consenso. Consiste en consensuar entre todos los participantes decidores. Método de la entropía: Se utiliza cuando se quiere disminuir la subjetividad de los métodos anteriores. Determinar cual es la importancia relativa que tiene un determinado criterio. La importancia relativa se determina al estar directamente relacionada con la información intrínseca promedio generada por el conjunto de alternativas y por la asignación subjetiva que le otorgue el decisor. Información intrínseca promedio del criterio Cj es: Puntuación de las ofertas Una vez puntuadas los criterios de las diferentes alternativas, se hace preciso en muchos métodos (como el de ponderación lineal) el trasladar las puntuaciones brutas otorgadas a una escala normalizada por dos motivos fundamentales: Como estamos manejando un espacio multivariable hay que homogeneizar las puntuaciones para su comparación: esto es, considerarlas todas sobre la misma escala. Es razonable trabajar con escalas de dimensión suficientemente pequeña para simplificación de cálculos. Con la normalización buscamos que las evaluaciones m de cada alternativa i correspondientes a un cierto criterio j sean comparables con las correspondientes a otros criterios. Llamaremos (Xj… Xi… Xm) al vector de puntuaciones de todas las alternativas sobre un criterio, el cual queremos transformar a uno normalizado (Yj, Yi, Ym). Los métodos de normalización más utilizados son los siguientes: Se otorga un cero a la mínima puntuación y un 1 a la máxima y el resto de las puntuaciones proporcionales a su valor en ese rango que es muy amplio. La alternativa con valor máximo alcanza el 1 en esta escala, pero la mínima no alcanza el cero si ella misma no es cero. Este método es el más utilizado. Este método mantiene la proporcionalidad pre y postnormalización: Selección de las alternativas Lexicográfico: Considerar el criterio de mayor peso y elegir aquella alternativa que para ese criterio tenga mayor puntuación. Si hay igualdad se toma el siguiente criterio en peso y así sucesivamente. Es un método sencillo, teniendo además la ventaja de no requerir comparabilidad intercriterios, un inconveniente es que no utiliza toda la información disponible. Prometheè (pertenece al conjunto de métodos “relaciones de superación”): Ignora la cuantía de la diferencia sólo señala si existe o no, y al trabajar con los pesos de los criterios, considera si esa diferencia se ha hallado en un Criterio más o menos importante para el decisor. Concordancia: comparaciones binarias de las alternativas, como información del decisor exigen tan sólo un preorden en las evaluaciones por cada criterio, y unos pesos en escala cardinal o incluso ordinal en algunas variantes. El procedimiento esencial de todos ellos gira alrededor de construir un coeficiente de concordancia cik para cada par de alternativas i,k. Dicho cik suele definirse como la suma de pesos de los criterios en que la alternativa i es superior a la k más la mitad de los pesos en los que ambas sean consideradas iguales. Permutación: La idea básica es la de comparar cada permutación posible de las alternativas, considerada como una ordenación de las mismas, con la información (ordinal) que aporta para cada criterio la matriz de decisión. Para cada permutación se calcula un llamado índice de evaluación, atendiendo a lo bien que concuerda con la información que proporcionan los datos, y aquella permutación que lo tenga máximo es la elegida. Entre sus ventajas figuran su flexibilidad cara al decisor (método cualitativo), y entre sus inconvenientes el que su dificultad de cálculo crece con m. Ponderación lineal (pertenece al conjunto de métodos “utilidad multiatributo”): consiste en calcular cual es el valor de cada alternativa y se elige la que tenga mayor valor. Para calcular el valor se emplea la fórmula: V(Ai) = ∑ XijWj. El problema fundamental es una buena estimación de los pesos. Necesita normalización previa de las puntuaciones. Entre sus ventajas podemos citar las siguientes: Procesa bien los fenómenos económicos, ya que suelen ser lineales, es un método muy intuitivo (el decisor lo comprende bien, ha demostrado su utilidad en otros contextos de decisión financieros, comerciales) y es el primer método para implantar en organizaciones poco tecnificadas. En cuanto a sus inconvenientes deben citarse: El ser de relativa facilidad en su manipulación vía pesos o vía evaluaciones, tener un enfoque absolutamente compensatorio lo que tiende a favorecer a las alternativas que son medianías y los resultados no son significativos sin una cuidadosa elección de escalas de medida de las evaluaciones. Bibliografía Scribd (Ibiza Ales) Documática. Gestión y archivo electrónico de documentos. Sistemas de gestión documental y de contenidos. Sindicación de contenido. Sistemas de gestión de flujos de trabajos. Búsqueda de información: robots, spiders, otros. Posicionamiento y buscadores (SEO)IntroducciónEn una organización, la información susceptible de almacenamiento crece a un ritmo exponencial. Dicho crecimiento hace necesario solucionar el problema de su adecuada gestión, ya que a partir de un cierto volumen se hace imprescindible un sistema organizativo que posibilite la localización de la información que se precise en cualquier momento. Podemos clasificar la información que es necesario manejar de la siguiente manera: Información estructurada: se trata de información que se puede subdividir en campos. Nos estamos refiriendo por ejemplo a los registros de las tablas de las BDR. Información no estructurada: es información en la que no se puede encontrar una estructura interna. Hablamos por ejemplo de fotos, archivos de texto, archivos de vídeo, páginas web, etc. Incluimos en este apartado los documentos de cualquier tipo. El ámbito de este tema se circunscribe al segundo tipo de información. El desarrollo de los sistemas automatizados de recuperación de información se inició con el objetivo de facilitar el manejo de la enorme cantidad de literatura científica surgida des de los años 40; posteriormente esta disciplina se extendió a otros ámbitos fuera de los científicos. Otlet es considerado el precursor de la gestión de documentación automática (documática) con su obra Traité de Documentation , publicada en 1934, en la que expone los principios y relaciones de la Tecnología documental. Otlet identifica los componentes fundamentales del moderno concepto de Documentación Automática (o Automatizada), distinguiendo estas tres premisas principales: Establece una teoría sobre la organización, las herramientas y los soportes tecnológicos para sustentar esta nueva disciplina. Aplicación práctica del proceso documental: la Documentación ocupa un lugar preponderante en la organización. Objetivo: satisfacer las necesidades informativas del usuario. Posteriormente en los años 50, los especialistas se centran en el problema de la búsqueda y recuperación de información, acuñándose el término Information Retrieval (recuperación de información). La recuperación de información es el conjunto de tareas mediante las cuales el usuario localiza y accede a los recursos de información que son pertinentes para la resolución del problema planteado. En un sistema documático, el proceso de recuperación de la información sigue en general el esquema siguiente: El usuario formula una necesidad de conocimiento. Se interroga al sistema gestor documental (SGD). El SGD devuelve una lista de referencias. Si lo que buscamos no está en la lista se realiza una segunda búsqueda y empieza el proceso de nuevo. A finales de los años 60 se da un nuevo paso en la evolución de la documática, con la introducción de la Information Science (Ciencia de la Información) como ciencia integradora de la teoría, proceso y práctica documental con otras ciencias complementarias, como la cibernética, la informática, la teoría de la información y la comunicación, etc. El desarrollo de nuevas teorías ha traído, de la mano de la Ciencia de la Información, la aparición de la disciplina Information Management (Gestión de la Información y la Documentación en las Organizaciones), en la que desempeñan un papel fundamental las telecomunicaciones y la informática, íntimamente relacionadas con los sistemas de información, en el marco de redes complejas de información. Archivo Electrónico de DocumentosComo ya hemos visto, el archivo electrónico de documentos o documentación automática consiste en la gestión de grandes volúmenes de información no estructurada (texto, imágenes, gráficos, sonidos, etc). Adicionalmente, será necesario gestionar cierta información que permita localizar el documento cuando sea necesario; así, los documentos han de ser sometidos a un proceso de indización . El otro gran proceso involucrado en un sistema de gestión documental es la recuperación de la información . Abarca el conjunto de tareas mediante las que un usuario recupera la información relevante en respuesta de una necesidad cognitiva. IndizaciónConsiste en extraer los conceptos clave del texto de un documento. Su objetivo es definir el contenido de un documento mediante un conjunto de conceptos que especifican el tema o temas de que trata. La indización conlleva dos procesos fundamentales: Extraer los conceptos informativos de cada documento. Traducirlos a un lenguaje documental. El lenguaje documental es el que se usa para la interrogación del SGD. En función del lenguaje documental que utilice, podemos clasificar los SGD en dos grandes grupos: Sistemas de lenguaje libre o free-text. Permiten hacer búsquedas en lenguaje natural. Un ejemplo es el buscador de Internet Google. Sistemas basados en lenguajes controlados. En este caso, los términos que contiene un lenguaje documental son de dos clases: Términos preferentes o descriptores (descriptors, keywords): son aquellos que deben utilizarse en la indización y en la recuperación. Representan términos precios y unívocos. Términos no preferentes (no-descriptors): no pueden asignarse a los documentos ni la indización, ni realizar consultas utilizándolos. En cuanto a la indización, hay que tener en cuenta que la cantidad de términos que representen a un documento no indica la calidad de la indización; no por muchos términos es más precisa, cuántos más términos representan a un documento aumenta la exhaustividad (mayor probabilidad de que se seleccione ese documento) y disminuye la precisión (conceptos que realmente identifican al documento). Si se cae en excesiva exhaustividad o precisión, se pueden producir dos fallos a la hora de realizar una búsqueda documental: Ruido : documentos que el sistema ha seleccionado y que en realidad no responden a la pregunta. Esto es consecuencia de indicar los documentos con más términos de los que se debiera. Silencio : documentos que al hacer la búsqueda no han sido seleccionados y sin embargo responden a la pregunta formulada. Es consecuencia de la falta de precisión, es decir, no indizar los términos correctos. Etapas de la indización Hablamos de sistema indizador como el encargado de realizar el proceso de indización. Existen aplicaciones en que este proceso es manual, realizado por un operador, pero en otras el operador es ayudado por un sistema informático, por ser un proceso totalmente automático. Las distintas fases de las que consta el proceso de indización son las siguientes: 1. Examen del documento . El examen será más o menos extenso según el tipo de documento y su forma física; en general, el sistema indizador tendrá que asegurarse de leer toda la información y no olvidar ninguna parte. En el caso de un documento de texto, éstas son las partes del texto que habrá de tener en cuenta por orden de importancia: titulo resumen introducción, capítulos y conclusiones ilustraciones y gráficos palabras subrayadas o impresas en otra tipografía 2. Identificación del documento . El sistema indizador aplicará una serie de criterios para identificar los conceptos esenciales para la descripción del tema, eligiendo los más acordes con las necesidades del centro o servicio en que se esté indizando. En la selección de los conceptos se persiguen dos objetivos principales: Exhaustividad: no dejar de indizar nada que pueda ser importante. Pertinencia: la información ha de ser representativa del documento. Para la identificación de los conceptos esenciales se pueden emplear los siguientes métodos: Sistema full-text: consiste en extraer todas las palabras clave, a excepción de aquellas que se encuentren en una lista de palabras vacías (aquellas que no aportan información, como los determinantes, preposiciones, etc). Es el sistema que se utiliza habitualmente para los sistemas documentales free-text. Indización mediante lenguajes controlados: el universo de las palabras a indizar está restringido, utilizándose una lista de descriptores. El método estadístico: seleccionar los conceptos más significativos mediante el análisis de las frecuencias de los términos del documento. El método sintáctico: utiliza técnicas de análisis morfológico y semántico para captar la estructura del texto. Utilizado sobre todo en la investigación sobre el procesamiento de lenguaje natural. 3. Traducción de los términos . Consiste en la traducción de los conceptos extraídos del documento al lenguaje documental utilizado, es decir, a términos de indización: Si utilizamos un lenguaje documental controlado, habrán de traducirse a los convenientes descriptores. Si utilizamos texto libre, habrá que comprobar que los conceptos extraídos están aceptados en las distintas fuentes de referencia: diccionarios y enciclopedias libros de texto y manuales tesauros etc Los tesauros Los tesauros que se acaban de citar son diccionarios que muestran la equivalencia entre los términos o expresiones del lenguaje natural ylos términos normalizados del lenguaje documental, así como las relaciones semánticas que existen entre ellos. Los tesauros en España están definidos en la norma UNE 50-106-90, la cual no es de obligado cumplimiento, pero proporciona un marco para la comunicación entre centros y para facilitar el trabajo en equipo. Los elementos principales de un tesauro son los siguientes: Unidades lexicales. A su vez de subdividen en varios tipos: descriptores términos equivalentes o sinónimos. Son aquellos cuya presencia es útil en el tesauro, pero que no se pueden utilizar en la indización, pues remiten o envían a un descriptor. Pueden ser de dos clases: sinónimos lingüisticos: se traducen directamente por un descriptor y tienen exactamente el mismo significado que el descriptor elegido. sinónimos documentales o cuasi-sinónimos: agrupan en un solo descriptor varios términos que tienen un significado próximo, aunque no es exactamente el mismo. infraconceptos: términos que no tienen sentido por sí solos y que se añaden a los descriptores para formar nuevos descriptores. Ejemplo: infra, multi, super, etc. palabras herrmienta o instrumento: descriptores que no tienen significado exacto si van solos. Son términos como: comparación, evaluación, método. Relaciones entre unidades lexicales. Existen las siguientes clases de relaciones: Relaciones de equivalencia o sustitución: son aquellas que relacionan un sinónimo con un descriptor. Relaciones de jerarquía: expresan relaciones de superioridad y subordinación entre descriptores. A su vez pueden ser: relaciones genéricas: en las que existe un término genérico que representa un concepto en el que están contenidos los términos específicos. relaciones partitivas o relaciones todo-parte: en las que se expresa que un término se compone de otros. Relaciones asociativas o de vecindad: indican las analogías que pueden existir entre dos descriptores. Relaciones de definición: que relacionan un descriptor con su uso o aplicación. Los tesauros se utilizan para eliminar ambigüedades y facilitar la indización, pero también son utilizdos en el proceso de recuperación de la información que se verá posteriormente. Sistemas de Indización En función de cuál es el resultado de la indización, es decir, cómo se organiza la información resultado de la indización de los documentos, podemos establecer las siguientes categorías: Ficheros planos : (a) la información referente a la indización de uno o más documentos son almacenados en un fichero (generalmente en formato de texto ASCII). La búsqueda sobre estos ficheros planos se llevan a cabo generalmente por medio de la localización de patrones de texto. Ficheros inversos : (b) son un tipo de fichero índice donde la estructura de cada ítem (emtrada) del fichero es, generalmente: descriptor, identificador de documento, identificador de campo, donde el identificador de documento es único para cada documento y el identificador de campo es un término que nos indica dentro de qué campo del documento aparece el descriptor. Algunos sistemas incluyen también información acerca de la localización en el documento del párrafo y frase de los términos utilizados para proceder a interrogar la BD. La búsqueda se realiza, corrientemente, por medio de la localización de los términos solicitados en el fichero inverso. Los ficheros de patrones de bits contienen hileras de dígitos binarios, patrones de bits que representan a los documentos. Existen varias formas de construir estos patrones de bits. Un método común consiste en la división de los documentos en bloques lógicos, e identificar los términos de indización que contiene cada bloque. Cada palabra es desglosada para traducirse en una hilera de bits (es decir, un patrón de bits con algunos de los bits “puesto a 1”). Los patrones de bits de cada palabra en un bloque son agrupados para crear un bloque de patrones. Los bloques de signaturas se concatenen posteriormente para producir el patrón de bits del documento. La búsqueda se lleva a cabo por medio de la comparación entre los patrones de bits de las interrogaciones con los patrones de bits de los documentos de la BD. Los grafos (redes) son colecciones ordenads de nodos conectados por arcos y se usan para representar documentos de diversas formas y maneras. Un ejemplo es el grafo denominado red semántica , que representa las relaciones semánticas que se establecen en el texto, relaciones que se pierden a menudo en otros sistemas de indización. Aunque constituyen un campo interesante para el estudio, resultan bastante difíciles de llevar a la práctica y requieren excesivo esfuerzo manual para el proceso de la representación de las colecciones de documentos. Recuperación de la InformaciónLa recuperación de la información es el conjunto de tareas mediante las cuales un usuario recupera la información relevante , para dar respuesta a su necesidad cognitiva. Es decir, un documento será relevante, si satisface la necesidad de conocimiento del usuario. Esto supone una gran diferencia con los sistemas gestores de BD, en los que el criterio de éxito de una interrogación a la BD es la exactitud y corrección de los datos, en ningún caso depende de las subjetividad del usuario. Uno de los problemas con los que nos encontramos, al interrogar un SGD, es que el usuario concibe su necesidad de conocimiento en “lenguaje natural”, el cual ha de ser traducido al lenguaje documental que entiende el sistema. Por lo tanto, puede producirse una pérdida de eficiencia en la traducción. Por ello se dice que el tipo de recuperación que se puede producir en la interrogación a un SGD es aproximada o probabilística , es decir, ante una misma necesidad de conocimiento se pueden obtener múltiples respuestas dependiendo de la habilidad ante una misma necesidad de conocimiento se pueden obtener múltiples respuestas dependiendo de la habilidad del usuario para traducirla al lenguaje documental que entiende el sistema. Hay que hacer notar que esto supone otra diferencia relevante con los SGBD tradicionales, en los que la información que devuelve el sistema es determinista , ya que ante una misma necesidad de información siempre devolverá el mismo resultado. Métricas de Eficiencia Al igual que ocurría en el proceso de indización, a la hora de la recuperación de la información no se puede ser exhaustivo y preciso al mismo tiempo, ya que si uno de los parámetros aumenta el otro disminuye, como podemos representar gráficamente de la siguiente manera: Por ello, para medir la eficiencia de un sistema de recuperación de la información se establecen una serie de parámetros, que enunciaremos a continuación basándonos en la tabla siguiente: La tabla pretende reflejar, para una consulta a un SGD: A: documentos relevantes que han sido devueltos por el SGD. B: documentos no relevantes que han sido devueltos por el SGD,lo que hemos definido anteriormente como ruido. C: documentos relevantes que no han sido devueltos y que deberían haber sido extraídos, lo que hemos llamado silencio. D: documentos no relevantes y que no han sido extraidos. Definimos entonces las siguientes métricas: Indice de pertinencia o precisión: mide cuantos documentos devueltos son los considerados relevantes por el usuario: A / (A + B). Es en definitiva una medida de la calidad de la información obtenida. Indice de exhaustividad o de respueta: mide el porcentaje de documentos que han sido devueltos sobre el total de la base documental: A / (A + C). Es una medida e la cantidad de la información obtenida. Tasa de ruido: mide el porcentaje de documentos que carecen de interés y han sido devueltos por el sistema: B / (A + B). El Proceso de Recuperación de la Información Un proceso de recuperación, al que podríamos considerar “genérico”, seguiría las siguientes fases: Definición de las necesidades informativas del usuario. Selección y ordenación de las fuentes a utilizar. Traslación de las necesidades del usuario al lenguaje documental propio de la fuente a utilizar en cada caso. Es posible, además, encontrar fuentes en las que no se utilice ningún tipo de vocabulario controlado, en cuyo caso resultará necesario afinar el trabajo terminológico. Traducción de la expresión de lenguaje documental al lenguaje de interrogación propio de cada sistema. Ejecución de las expresiones del lenguaje de interrogación obtenidas. Consulta de las respuesta obtenidas, para analizar su pertinencia o no a la cuestión planteada. Replanteamiento, si procede, de las expresiones utilizadas, si los resultados obtenidos no son pertinentes. Selección y obtención de los documentos que respondan a las necesidades manifestadas por el usuario. Transmisión del resultado, preparado adecuadamente, al usuario. Este proceso se puede plasmar gráficamente como aparece en la figura: Organización Funcional de los Sistemas DocumáticosEn los Sistemas de Gestión Documental (SGD) se pueden identificar una serie de subsistemas funcionales. Un SGD puede incorporar todos ellos o sólo algunos. Además, hay SGD’s que permiten integrar subsistemas de otros fabricantes: Sistemas de Gestión de Bases de Datos Documentales (SGBDD) : son sistemas que incorporan todas las características de los SGBD tradicionales, incluyendo la creación y mantenimiento de BD Documentales (adecuadas para información no estructurada), usuarios, controles de seguridad, e incluso lenguajes propios de programación. Estos sistemas están basados en sistemas de archivo y ficheros inversos, los cuales son una modalidad de organización de los datos especialmente apropiada para la información documental. Los rasgos más característicos de un SGBDD son: capacidad para almacenar información textual de longitud grande y variable. capacidad para recuperar con rapidez registros que responden a un criterio de búsqueda. capacidad para realizar búsquedas multicriterio sobre ficheros inversos utilizando lógica booleana. capacidad para administrar tesauros y diccionarios terminológicos. Como ejemplos de sistemas de gestión de BD más representativos, podemos citar: BRS/Search de BRS Information Techonologies (uno de los más completos),Inmagic, CDS-Isis y su interfaz Winslsis, … Sistemas de indización : anteriormente hemos visto el proceso de indización documental. Estos sistemas por lo tanto son aquellos encargados de realizar dicho proceso. Sistemas de exploración o escáneres : se trata de aplicaciones que son capaces de acceder a ficheros con diferentes formatos y buscar dentro de los mismos las cadenas de caracteres que respondan a lo expresado en la ecuación de búsqueda. Pueden encontrarse aplicaciones que combinen la exploración con la indexación, como dtSearch. Sistemas de gestión bibliográfica : sistema especializado para la gestión y mantenimiento de bibliografías especializadas. Es una aplicación específica de los sistemas de gestión de bases documentales que permite, no sólo el almacenamiento y la recuperación de referencias bibliográficas, sino también la exportación de estas referencias en diferentes formatos de cita bibliográfica a diferentes procesadores de textos, sistemas de gestión de BD, etc. Sistemas de recuperación de información (SRI) : son aplicaciones que se encargan exclusivamente de recuperar información de BD documentales no modificables. Ponen a disposición del usuario potentes herramientas de búsqueda y de apoyo a la búsqueda, pero su funcionalidad queda reducida a la consulta y exportación de documentos. Los SRI incorporan un gestor de interrogación o motor de búsqueda , el cual realiza búsquedas dentro de una BD de documentos. El motor de búsqueda recibe la interrogación del usuario (query), que consiste en una o varias palabras, realiza la búsqueda en la BD y extrae una lista ordenada de documentos que cumplen entera o parcialmente con la interrogación. El orden depende de una puntuación (score) que asocia el programa a cada documento cuando realiza la búsqueda y en cada caso varía. Un criterio para puntuar los resultados que usualmente se aplica es que cuanto más próximos en el documento aparecen los términos de búsqueda, mayor es la puntuación del documento. Un SRI debe permitir la recuperación de la información contenida en los documentos de la BD a la que accede, a través de cualquier término existente en ella, mediante la formulación de ecuaciones de búsqueda que permitan combinar los términos según diferentes criterios. Existen sistemas que ofrecen la posibilidad de ejecutar las consultas sobre una o varias BD simultáneamente. Los documentos resultantes se agrupan en sets o conjuntos, susceptibles de combinación posterior. El SRI ha de poseer algún tipo de mecanismo para la salida de la información, generalmente mediante edición en pantalla, impresión y redirección a ficheros de los documentos de interés para el usuario. Las órdenes de salida de información deben ofrecer la posibilidad de enviar ésta a diferentes destinos, así como los formatos de presentación de los datos a utilizar (tamaño, campos, …). Deben incluirse aquí las capacidades para ordenar, según diferentes criterios, los documentos resultantes. Otra función a considerar es la posibilidad de crear nuevas BD, tomando como base los documentos recuperados en un búsqueda previa. Es interesante que el SRI incluya también herramientas que permitan analizar y procesar la respuesta obtenida, utilizando herramientas de análisis de frecuencias de los términos (es decir, cuántas veces aparece el término buscado en los documentos recuperados) o de coocurrencias (frecuencia con la que aparecen dos o más términos de búsqueda en los documentos recuperados). Otro posible subsistema de un SRI es aquel que permita definir los perfiles de búsqueda de los usuarios, así como realizar un seguimiento de las ecuaciones que ejecuten. Por ejemplo, la posibilidad de almacenar las ecuaciones de búsqueda que usualmente ejecutan, de manera que puedan ejecutarse en cualquier momento, se les llama normalmente “macros”. Estas macros son ficheros susceptibles de edición y modificación, lo que facilita la recuperación de información con un mínimo esfuerzo de tiempo y coste. Un elemento fundamental de un SRI es que incluya algún mecanismo de control terminológico, tanto para la entrada de datos como para su recuperación. Puede tratarse de un tesauro, de un glosario o de un diccionario terminológico. Además se puede incluir una ayuda al usuario en todo momento, a través de mensajes y líneas de estado, especialmente durante el proceso de interrogación (interrogación asistida). En sistemas de recuperación en línea (teledocumentación), el sistema informa al usuario del tiempo de conexión, tareas ejecutadas, coste de la sesión, etc. Los mecanismos de ayuda al usuario, especialmente aquellos referidos a la evaluación y refinamiento de las búsquedas, son una de las principales áreas de investigación. Por último, dependiendo de la configuración del sistema, éste puede ofrecer opciones de acceso multiusuario, niveles de seguridad, reorganización y recuperación de ficheros, etc. Sistemas hipertextuales: en su origen, los hipertextos e hipermedias eran una forma de organizar, acceder y explorar documentos de diferentes tipos, que posteriormente se han popularizado como motor y parte de tutoriales y presentaciones. Actualmente estos sistemas están volviendo a ser considerados como una forma válida y muy avanzada de gestionar documentación. Para que sea posible una existencia real de los conceptos de hipertexto e hipermedia, deben utilizarse aplicaciones que sean capaces de crear los vínculos y asociaciones entre los documentos. Las aplicaciones ofrecen unos elementos particulares que facilitan la creación y navegación por las estructuras hipertextuales: Un conjunto de ficheros que contienen los documentos relacionados. Ventanas de presentación de los documentos, las cuales son modificables en tamaño y posición. Punteros o enlaces, que generalmente utilizan una representación gráfica distinta a la del resto del material informativo, en forma de color, iconos, botones… Así como dispositivos señaladores, que facilitan la selección y el acceso a los documentos mostrados en las ventanas. Herramientas de creación de enlaces y anotación de la navegación, lo que da al usuario la posibilidad de crear sus propias asociaciones y documentos. Estas funcionalidades se integran en una herramienta que en el entorno hipertextual es conocida como “browser”, navegador o visualizador. El visualizador actúa como una interfaz, que muestra al usuario el contenido informativo de los documentos que selecciona, mediante la selección de enlaces. Suele completarse con la posibilidad de ejecutar búsquedas en el texto completo que contienen los documentos y/o búsquedas más rígidas utilizando lenguajes clásicos de interrogación. La interrogación, sea de texto, imágenes o sonidos, suele realizarse a través de la ejecución de patrones, que representan una necesidad dada de información por parte del usuario. Además, una completa aplicación para este ámbito debería ser capaz de generar mapas gráficos de la estructura hipertextual y utilizar estas representaciones para acceder directamente a los documentos deseados. La visión que obtiene el usuario mediante el visualizador es una visión transparente, integrada, en la que no resulta complicado navegar de un documento a otro. Esta aparente facilidad no debe ocultar que los documentos pueden encontrarse en diferentes ficheros informáticos, e incluso en diferentes ordenadores, formando lo que se llama repositorio de información, que será tratado con más detalle en el próximo capítulo, por su relación con las BD multimedia. Los sistemas y estructuras de hipermedia pueden además incorporar inteligencia embebida, es decir, ser capaces de ejecutar otras aplicaciones o de tomar decisiones con la actividad desarrollada por el usuario, tanto en la utilización de los enlaces como en el acceso a los contenedores. Sistemas de Gestión Documental o de Gestión Electrónica de Documentos (GED) : se trata de sistemas que pretenden ofrecer una solución integral para la documentación, especialmente administrativa y de gestión, que se utiliza en una organización dada (PRAX, 1994; LASSOURY, 1994). Incorporan funciones clásicas de gestión de BD y utilizan esquemas de obtención de una copia del documento original mediante escáner, almacenamiento óptico o magneto-óptico y un nivel básico de descripción textual del documento y de su contenido. Sistemas o Gestores de Información Personal (Personal Infromation Systems/Managers) : son aquellos que integran, en un único entorno, todos los documentos, ficheros y relaciones entre ellos que son de interés para el trabajo de un usuario. Numerosos sistemas integrados de informatización ofrecen a sus usuarios un acceso homogéneo a los diferentes tipos de documentos y ficheros que manejan en su trabajo diario. Sistemas compuestos : se denomina así a aquellos que dan soporte a todas las tares que se realizan en una unidad informativa, sea ésta un archivo, biblioteca o centro de documentación. Esto significa que cubren tanto la cadena documental como la gestión administrativa. Sirvan como ejemplo las aplicaciones de automatización de bibliotecas, como Absys o Libertas, o las aplicaciones de automatización de archivos, como la desarrollada para el Archivo de Indias de Sevilla. Normalmente, integran un motor documental, encargado de gestionar las BD documentales que cubren los catálogos, y un motor relacional, que cubre las tareas administrativas. Optimización de Consultas y Recuperación de la InformaciónLenguajes de Interrogación y OperadoresUn lenguaje de interrogación puede definirse como un conjunto de órdenes, operadores y estructuras que, organizados conforme a unas normas lógicas, permiten la consulta de fuentes y recursos de información electrónica. El resultado de la combinación de estos elementos, siguiendo las normas establecidas, es una expresión a la que se conoce con el nombre “ecuación”, capaz de interrogar el contenido de la fuente de información. La definición mínima de un lenguaje de interrogación y de sus componentes puede encontrarse en el borrador del la norma ISO 8777-1988. Las normas lógicas que rigen un lenguaje de interrogación responden a cuestiones relacionadas con la coordinación de los elementos, es decir, con la formulación de ecuaciones. Estas normas funcionan como la sintaxis del lenguaje, es decir, especificarán el orden de los elementos, la disposición de las estructuras, sus posibilidades combinatorias, las prioridades en la ejecución y todo tipo de posibles funciones. Las órdenes serán aquellas palabras o abreviaturas que le indicarán al sistema las acciones a ejecutar (buscar la expresión, mostrar los documentos o registros resultantes, consultar el tesauro o los ficheros inversos, ejecutar un perfil de usuario, …). Sin embargo, no todos los lenguajes de interrogación utilizan las mismas palabras como órdenes, aunque las órdenes ejecuten las mismas funciones. Existen intentos para homogeneizar la interrogación de las BD, como el lenguaje CCL (Common Command Language) promovido por la Unión Europea, que aún no han alcanzado el objetivo para el que fueron desarrollados. A este panorama se une la proliferación de interfaces gráficos de usuario, que sustituyen a las órdenes y las sintaxis tradicional, dejando al usuario (si éste lo desea) sólo la labor de introducir los términos y los operadores que expresan las relaciones existentes entre ellos. En un lenguaje de interrogación, los operadores son los encargados de expresar las relaciones que mantienen entre sí los términos que definen (más adecuado sería decir que pueden definir) las necesidades informativas del usuario. Pueden distinguirse diferentes tipos de operadores que se analizan a continuación. Operadores Lógicos o Booleanos Los operadores lógicos, también llamados booleanos en honor a George Boole, precursor de la lógica simbólica y del álgebra de conjuntos, son los más utilizados en numerosos sistemas. El principio que rige la utilización de este tipo de operadores es que las relaciones entre conceptos pueden expresarse como relaciones entre conjuntos. Las ecuaciones de búsqueda pueden transformarse en ecuaciones matemáticas, que ejecutan operaciones sobre los conjuntos, lo que da como resultado otro conjunto. Los tres operadores básicos son el operador suma/unión (generalmente identificado como O/OR), el operador producto/intersección (identificado como Y/AND) y el operador resta/negación (identificado como NO/NOT). A su vez, estos operadores pueden combinarse entre sí generando operaciones más complejas, como el O exclusivo (elimina la intersección), etc. No deben obviarse los problemas que plantean los operadores booleanos, independientemente de su potencia. En primer lugar, siempre se plantean en términos absolutos (es decir, selecciona el documento en función de si las palabras de búsqueda están o no están presentes, sin considerar el peso específico de cada término en el contexto). Por esa misma razón, es necesario un alto valor de precisión en los términos de búsqueda utilizados. En segundo lugar, requieren claridad en la composición de las expresiones a buscar. Operadores posicionales La utilización de operadores posicionales pretende superar algunas de las limitaciones anteriormente citadas que ofrecen los operadores booleanos. Toman como punto de partida la consideración del valor de cada término dentro del contexto, es decir, de su relación con el resto. En definitiva lo que quiere decir es que la posición de los términos de búsqueda dentro del documento es significativa para valorar su utilidad. Los operadores posicionales pueden dividirse en dos tipos: Posicionales absolutos: Son aquellos que permiten buscar un término en un lugar dado del documento o registro. Por regla general, son operadores de campo, es decir, permiten al usuario fijar en qué campo o campos presentes en la estructura de BD debe aparecer el término buscado. La presencia del término en un campo dado (por ejemplo, en el campo título) puede ser una garantía de la adecuación del documento a los objetivos, en la mayor parte de las situaciones. Posicionales relativos: También llamados de proximidad, se trata de operadores que permiten establecer la posición de un término respecto a otro dado. Se considera que la cercanía entre los dos términos puede reflejar una íntima relación entre los conceptos reflejados por los mismos. Estos operadores permiten definir el nivel de proximidad entre los términos (mismo campo, línea, frase, número de términos significativos que los separa …). Operadores de Comparación Especifican el rango de búsqueda, fijando unos límites para la misma. Estos límites pueden ser tanto numéricos como alfabéticos, correspondiendo los operadores a formas del tipo “mayor que”, “menor o igual que”. Se utilizan principalmente en documentos que pueden contener datos numéricos. Operadores de Truncamiento Pueden darse situaciones en las cuales sea necesario utilizar no un término simple, sino también sus derivados, determinados por prefijación o sufijación, mínimas variantes léxicas, etc. Para facilitar este tipo de búsqueda se han introducido operadores de truncamiento, a los que también se llama máscaras. Se trata de operadores (normalmente se emplean símbolos como *, $) cuya presencia puede sustituir a un carácter o a un conjunto de caracteres, situados a la izquierda, dentro o a la derecha del término en cuestión. En los actuales sistemas de recuperación de información es posible encontrar todos estos tipos de operadores, que pueden combinarse entre sí, permitiendo crear ecuaciones complejas que reflejan con bastante precisión los conceptos y sus relaciones. La combinación de los operadores debe respetar un conjunto de reglas básicas en todos los sistemas, que establecen las prioridades y formas de ejecución de ecuaciones complejas, cuando éstas combinan más de dos conceptos. En primer lugar, los sistemas tienden a resolver, o ejecutar en primer lugar, aquellas expresiones que se relacionan utilizando el operador más restrictivo o prioritario. Por ejemplo, un operador posicional absoluto posee un nivel de restricción (una prioridad) mayor que un operador booleano, lo que significa que el sistema ejecutará antes la expresión cuyo operador es el posicional absoluto, combinando posteriormente el resultado con el operador booleano y su término relacionado. Sin embargo, pueden darse expresiones en las cuales sea necesario variar estas prioridades y ordenar al sistema que ejecute en primer lugar expresiones con operadores de menor nivel de restricción, relacionando luego su resultado con términos a través de operadores más restrictivos. Para estas situaciones, se utilizan paréntesis, los cuales engloban a las expresiones que deben ejecutarse en primer lugar, independientemente de las prioridades fijadas por el sistema. La utilización de expresiones entre paréntesis hace posible, por ejemplo, que el resultado de una expresión con un operador booleano pueda ser combinada con un operador posicional absoluto. Además, los paréntesis pueden anidarse, resolviéndose las ecuaciones planteadas desde dentro hacia fuera, de la misma forma que las igualdades y polinomios matemáticos. Estrategia de la InterrogaciónLos lenguajes, sus órdenes y operadores son utilizados dentro del proceso de recuperación de información, la cual se encuentra almacenada en un repositorio, que suele ofrecer la forma de BD. La BD es consultada mediante la ejecución de búsquedas, expresiones que reúnen los elementos citados con anterioridad, y cuya resolución da como resultado aquellos elementos que responden a la lógica expresada en la búsqueda. Con el concepto “estrategia de la interrogación” nos referimos a los posibles enfoques que se le puede dar a la planificación del proceso de recuperación de la información, tanto de la visión general de cómo se va a afrontar la búsqueda hasta la formulación de la ecuación concreta. La estrategia debe ser un plan ideal de interrogación de la BD que incluya el objetivo de la búsqueda, el plan general y el plan específico de operación. El objetivo de la búsqueda se obtiene identificando qué tipo de información se necesita y sus características. Una vez definido el objetivo, debe establecerse un plan general de operación, que incluya una selección de la base o BD a consultar, las primeras aproximaciones a los términos a utilizar en las ecuaciones, así como las posibles relaciones lógicas. El plan específico de operación se pone en marcha una vez obtenidos los resultados del anterior y debe formular ecuaciones y utilizar términos con el mayor grado de precisión, establecer una secuencia lógica con todo ello y redefinirlo si es preciso. Independientemente de ambos planes, resulta necesario conocer con anterioridad la respuesta a varias cuestiones que afectan a la interrogación de la BD, tales como el contenido y alcance de la BD, coste de consulta, lenguaje y operadores a utilizar durante las consultas, límites preestablecidos (por el usuario o el sistema)… Todas ellas afectan y modifican el enfoque del interrogador. Tipos de Estrategia En el momento actual, parece más adecuado utilizar el término para identificar el plan general de búsqueda. No existe una única ni perfecta aproximación a las estrategias de interrogación de BD. En la mayor parte de las ocasiones depende de la experiencia del usuario y de la calidad del contenido de los registros existentes en la BD, especialmente en lo que corresponde a su control terminológico. La estrategia depende, en gran manera, de la formación, intuición y experiencia del usuario. Tomando en consideración la intención del interrogador, la bibliografía señala que pueden existir varios tipos principales de búsqueda, que pueden clasificarse en dos grandes grupos, sin perjuicio de que puedan darse situaciones en las que se combinen: Categorización por objetivo: Búsqueda de elemento conocido: se trata de búsquedas en las cuales el interrogador sabe cuál será la respuesta. Por ejemplo, en una biblioteca en la que estamos buscando un libro concreto (documento respuesta conocido) y realizamos la búsqueda por su ISBN. Búsqueda de información específica: el interrogador busca una información específica dada, generalmente sobre un tema concreto y limitado, como trabajos publicados en un año o por un autor. Búsqueda de información general: intenta buscar la información sobre una materia o asunto, de forma general, que obtenga una visión global del estado de la misma. Exploración de la BD: se trata de conocer qué tipos de información y/o documentos se encuentran almacenados en la BD, a qué pueden responder y cómo pueden utilizarse. Categorización por plan de operación: Búsqueda directa: se trata de una aproximación expeditiva, en la que se intenta resolver el problema con la formulación de una única consulta. Como puede deducirse, resulta difícil obtener buenos resultados con la misma. Búsqueda “breve”: es una evolución de la anterior, en la que se trata de recuperar unos ítems significativos entre un gran número obtenido tras una sola ecuación. Ampliación: comienza con ecuaciones muy restrictivas, que ofrezcan documentos pertinentes. Tras analizar la respuesta, el usuario puede ampliar o expandir las ecuaciones de búsqueda hasta recuperar toda la información existente. Puede ofrecer problemas si la ecuación inicial no es adecuada. Restricción: opuesta a la anterior, formula ecuaciones que ofrecen resultados muy amplios, para posteriormente utilizar ecuaciones más restrictivas, hasta delimitar los documentos pertinentes. Construcción de bloques: intenta establecer bloques de información que se correspondan con el objetivo de la búsqueda, para combinarlos entre sí de manera que se responda a la necesidad planteada de manera óptima. La Exploración como Mecanismo de RecuperaciónLas limitaciones inherentes al proceso de recuperación mediante ecuaciones han conducido a experimentar otras aproximaciones. Una de las más utilizadas es aquella que emplea la exploración, es decir, el acceso a los documentos mediante técnicas de visualización de parte de su contenido que puede ser relevante, y la posterior asociación con otros documentos de perfil similar. El usuario accede a un listado o enumeración de elementos descriptivos y, mediante un proceso de selección de elementos, va centrando el objetivo de su búsqueda. Los criterios utilizados por el usuario se basan en la deducción y la asociación de conceptos (aproximación ésta similar a la que utiliza un sistema hipertextual) frente a la lógica de conjuntos que se plantea en un sistema de ecuaciones. Este tipo de representación es más adecuada para reflejar la polirepresentación que un concepto puede tener para un usuario individual. En cambio, la utilización de la exploración suele realizarse en entornos en los cuales el usuario no posee una idea clara de cuál debería ser la mejor táctica para aproximarse a la información que precisa. Por lo tanto, la cuestión clave a considerar en un sistema de exploración es combinar las ideas y esquemas del usuario con el esquema de organización de la información que ofrece el sistema. Ésta es la aproximación que pretenden desarrollar los enfoques cognitivos,poniendo su énfasis en el intermediario que debe existir entre el modelo del usuario y el modelo del sistema. Revisión y Análisis de ResultadosEl resultado de la ejecución de una ecuación de búsqueda es un conjunto de documentos que cumplen las condiciones expresadas en la ecuación. Se trata, a su vez, de un subconjunto del conjunto total de documentos existentes en el recurso o fuente de información consultado. Sin embargo, puede darse el caso de que la respuesta sea un número excesivamente elevado de documentos, o un número mínimo. Por otra parte, los documentos resultantes responden a la lógica y a las condiciones expresadas en la ecuación de búsqueda, lo cual no supone, como ya se ha señalado, que sean pertinentes a las necesidades del usuario. En realidad, es posible ejecutar ecuaciones perfectas,desde un punto de vista funcional (operadores, términos, …), sin que los documentos resultantes reúnan las características que los harían deseables para el usuario. Para superar esta posible distorsión en los resultados es necesario valorar y evaluar la respuesta a las ecuaciones planteadas. La primera modificación a realizar en la formulación de las ecuaciones afecta al número de respuestas obtenidas. En el caso de un excesivo número, se utilizan técnicas de restricción mediante la introducción de términos más específicos, se desechan términos generalistas o se limitan los truncamientos. En el caso de un número muy reducido, las acciones a tomar son las contrarias, es decir, utilización de términos más generales, incluyendo derivados y relacionados, limitación de los operadores más restrictivos, introducción de truncamientos, etc. Si se da la situación de ecuaciones correctas funcionalmente, pero sin respuesta adecuada, sería necesario replantear el proceso de recuperación, especialmente en la utilización de los lenguajes documentales y en la selección de fuentes. Gestores de ContenidoUn CMS (Content Management System), Sistema de Gestión de Contenidos o Gestor de Contenidos , es una aplicación web a la que podremos acceder a través de un navegador tras ser instalada en un servidor. A través de su panel de administración podremos crear, eliminar, modificar y en definitiva, gestionar el contenido de la “página web” (sitio web). Por lo que también podríamos definirlo como una herramienta que nos permite la creación de una “página web” (sitio web) y su gestión por perfiles no técnicos. ¿Por qué surgieron los CMS?No hace muchos años los sitios web estaban formados por páginas web estáticas codificadas en html. Existía la figura del webmaster, que era un técnico que se encargaba del mantenimiento de las páginas web del sitio. Por fortuna, las páginas web se modificaban pocas veces al año ya que todavía no se hacían blogs ni periódicos online que requieren una alta frecuencia de gestión del contenido. Además, modificar el contenido era tedioso, pues había que abrir el archivo html correspondiente a la página web en cuestión que había que modificar y “bucear” entre el código html para realizar los oportunos cambios. Un día surgió la necesidad de crear blogs, periódicos online y otros tipos de páginas web (sitios web) que requerían de frecuentes modificaciones. No se podían encargar todas las modificaciones al webmaster, había que encontrar alguna manera de que personas no técnicas pudieran crear y gestionar contenido de la página web (sitio web). Así aparecieron los CMS o Gestores de Contenidos. Problemas, Beneficios y Ventajas de un CMSProblemas de no usar un CMS Poca usabilidad de la interfaz. Pérdida de tiempo. Los tiempos para encontrar y editar una página son más largos. Solo pueden modificar contenidos personal con conocimientos HTML. Desorganización: Con una página sin CMS y con muchos contenidos puede ser un desastre localizar una página concreta de forma rápida. Necesidad de usar manuales de Dreamweaver, Frontpage, … Gracias al CMS podemos solucionar todos estos problemas, agilizando nuestro trabajo y permitiendo, sin muchos conocimientos técnicos, a cualquier persona a poder hacer uso de la página web de la empresa. Beneficios del uso de un CMS Proceso de creación rápido y dinámico. Tiempo de ejecución más rápido para crear nuevas páginas y editar contenidos. Mayor consistencia del sitio web. Todo al alcance de tu mano. Mejora de la navegación del sitio. Mayor flexibilidad. Mayor seguridad. Menos contenido duplicado. Facilidad en la escalabilidad de la página web. Reducción de los costes de mantenimiento. Ventajas Ahorro de tiempo : una de las mejores ventajas del uso de estos gestores es que tenemos la oportunidad de ahorrar tiempo en la creación, edición y administración de los contenidos. Sin necesidad de emplear otras herramientas para poder hacerlo. Facilidad : los gestores de contenido tienen la enorme ventaja de que pueden ser utilizados por las personas sin la necesidad de que tengan conocimientos en áreas del lenguaje de programación o diseño. La interfaz está hecha para que los usuarios empleen una herramienta con la cual puedan encontrar todo lo que necesitan al alcance de un solo click y de la forma más sencilla. Creación : los CMS permiten que las personas aún sin conocimientos en programación tengan la oportunidad de crear desde cero sus contenidos sin ayuda de nadie y de la forma que desean. Diseño : otra de las muchas ventajas que te ofrecen los gestores de contenidos es que tienes la posibilidad de elegir plantillas de diseño. Entre muchas que se encuentran para elegir según sean tus necesidades o gustos. No es necesario tampoco conocer sobre programación o diseño para tener un espacio web realmente estético e impactante, lo cual es muy importante. Otra de las muchas ventajas que ofrecen, es que tienes la posibilidad de trabajar el SEO con ellos. Recordemos que para que un sitio web sea visible requiere de trabajo y posicionamiento para lograr el tráfico que necesita. Front Office y Back Office del CMSLos CMS se caracterizan por tener dos entornos: Front Office : es la parte pública de la página web (sitio web), a la que accedemos escribiendo la URL del sitio en la barra de direcciones del navegador web. Back Office : es la parte privada de la página web (sitio web) o lo que también se conoce como el panel de administración del sitio web. Desde aquí se puede gestionar el contenido del sitio web, su estructura, diseño y los diferentes elementos de configuración. Para acceder al Back Office de un CMS habrá que escribir una url especial que dependerá del CMS utilizado. En el caso de WordPress habrá que añadir al nombre de dominio la palabra “wp-admin”, por ejemplo: http://www.mipagina.es/wp-admin . Clasificación y Características de los CMS o Gestores de ContenidosLos Gestores de Contenidos o CMS son aplicaciones web Los Gestores de Contenidos son aplicaciones web especialmente diseñadas para crear páginas web. Las aplicaciones web son aquellas aplicaciones a las que se accede a través de un navegador web. Los Gestores de Contenidos o CMS como aplicaciones web que son, habitualmente necesitan de la compañía de una serie de elementos: Un servidor web : Encargado de recibir las peticiones de los navegadores web de los clientes cuando solicitan una página web, de comunicarse con el módulo encargado de la ejecución del código y de enviar las páginas web resultado de la ejecución del código al navegador del cliente. El servidor web más utilizado es Apache . Módulo encargado de ejecutar el código escrito en un lenguaje de programación y de enviar la página web resultante al servidor web (para la mayoría de CMS se utiliza el módulo PHP del servidor Apache). Un servidor de base de datos . Encargado de almacenar los datos del sitio web. El más utilizado en los Gestores de Contenidos es sin duda el servidor de BD MySQL . Un lenguaje de programación . El lenguaje de programación más utilizado para los Gestores de contenido más populares es PHP . Clasificación de los CMS o Gestores de Contenidos por sus características Según el lenguaje de programación empleado por el CMS para crear la página web, como por ejemplo Java, PHP, ASP.NET, Python, PERL. Tanto WordPress como los más conocidos gestores de contenidos están codificados en el Lenguaje de programación del lado del servidor PHP. Según la licencia : Código abierto o Software propietario. Tanto el CMS WordPress como el resto de aplicaciones para crear páginas web más conocidas (Drupal, Joomla, Prestashop, etc) son Software abierto y gratuito. Clasificación de los CMS o Gestores de Contenidos por su uso y funcionalidad Genéricos : Tienen muchos posibles usos. Crear una página web corporativa, un blog, una tienda online, etc. Aquí podemos incluir CMS como: Joomla, Drupal, … Y desde hace algún tiempo WordPress (comenzó siendo un Gestor de contenidos específico para la creación de Blogs). Blogs : Son los CMS especialmente creados para la gestión de diarios personales. Son CMS de blogs WordPress, B2Evolution, Movable Type, Blogger, … Comercio electrónico : Son CMS creados específicamente para crear tiendas online. Algunos ejemplos son Magento, PrestaShop, Opencart, etc. Existen CMS específicos para crear Foros, Wikis, CMS para cursos online como Moodle, etc. Lista de los mejores CMS más utilizados CMS WordPress : Es el CMS más utilizado y mejor valorado para creación de blogs y webs. Está hecho en PHP y es gratuito. CMS Drupal : Es uno de los CMS más conocidos, es gratuito y open source. Está construido en PHP. CMS Joomla : Es otro CMS popular de código abierto y también creado en PHP. Es una evolución del CMS Mambo. Prestashop CMS : Es el CMS de ecommerce más conocido y mejor valorado. Podemos decir que es el WordPress de los ecommerce. Magento CMS : Es otro CMS para ecommerce de los más populares y mejor valorados. Ofrece muchos niveles de configuración. A diferencia de Prestashop, se requiere de conocimientos técnicos avanzados para utilizarlo. Blogger : aun hoy se sigue utilizando esta plataforma de gestión de contenidos, fue una de las primeras en hacer presencia en la red. Su forma de uso es gratuita y bastante sencilla, por lo que crear contenidos no genera ningún tipo de problemas. LiveJournal : dedicado a todas las personas que no cuentan con toda la expericencia requerida en el manejo de sitios web. Este gestor de contenidos permite que se puedan conectar blogs dependiendo de su temática, así como la clasificación de los mismos. Sindicación de ContenidoSe denomina Sindicación a la distribución masiva de contenidos en la web. A partir de la inclusión de algún nuevo contenido en un sitio, lo que se distribuye es una lista de enlaces junto con cierta cantidad de información adicional o metadata. Los enlaces apuntarán a esos nuevos contenidos y la información adicional permitirá a los receptores evaluar si los contenidos son de su interés, en cuyo caso accederá a la versión completa simplemente siguiendo el enlace. Los primeros sindicadores de contenido en línea fueron mega sitios de la magnitud de Yahoo y Excite. Su propuesta era muy clara: que sus visitantes pudieran acceder a información de orígenes muy diversos desde un lugar único. Durante un tiempo, la sindicación resultó demasiado cara y trabajosa ya que se realizaba en base a la recuperación del título de cada página y la revisión de todo el HTML (que está concebido para mostrar contenidos pero no para organizarlos) para detectar los encabezados y enlaces para luego categorizarlos. Semejante tarea no estaba al alcance de cualquiera. La gran novedad para la sindicación surgió de la utilización de archivos XML . Conceptos RSS : Se corresponde con las siglas de Really Simply Syndication. Es un formato XML para la sindicación de contenidos. Es el más extendido, y permite distribuir contenidos sin necesidad de un navegador, utilizando un agregador de contenidos. Agregador de contenidos : Es el software que permite suscribirse a fuentes de noticias en RSS, por ello es también conocido como lector RSS o agregador de noticias. Feed : Es la fuente o canal web propiamente dicho, al que pueden suscribirse los usuarios. Los archivos RSSUn archivo RSS es la descripción estructural de un sitio web en formato XML. RSS es un lenguaje surgido de la aplicación del metalenguaje XML. Por lo tanto, un archivo RSS no será más que un documento de texto compuesto por etiquetas acotadas entre los símbolos de mayor y menor, similares a las utilizadas XHTML. El término RSS corresponde a Rich Site Summary o Really Simple Syndication . Sindicación de Contenidos : Es el término técnico utilizado para designar un método o proceso que permite la notificación y envío de información recientemente publicada en la web. Por tanto, su principal objetivo es la organización y difusión de esta nueva información de un modo rápido y fiable. Parte del principio de suscripción, y se apoya en un conjunto de programas que permiten interpretar sus formatos. Wikipedia dice : RSS son las siglas Really Simple Syndication (Sindicación Realmente Simple), un formato XML para sindicar o compartir contenido en la web. Se utiliza para difundir información actualizada frecuentemente a usuarios que se han suscrito a la fuente de contenidos. El formato permite distribuir contenidos sin necesidad de un navegador, utilizando un software diseñado para leer estos contenidos RSS tales como Internet Explorer, entre otros (agregador). Es interesante destacar que se trata de un formato que no está concebido para su visualización (como el HTML) sino para la interacción entre computadoras, ofreciendo la información en un formato estandarizado. Para que este proceso resulte posible, un sitio web debe generar un feed o canal (el archivo RSS) que permanecerá alojado en el servidor tal como los demás archivos que lo componen. Una vez que el feed está disponible, otros sistemas podrán accederlo y así enterarse de los nuevos contenidos que el sitio ofrece. Hoy en día los sitios que permiten la creación y mantenimiento de blogs personales como Blogger y las aplicaciones que lo facilitan en cualquier dominio como WordPress han automatizado la generación de feeds, por lo que los usuarios solo deben manejar sus contenidos. Sin demasiado misterio, los contenidos estarán entonces sindicados. Para leer los feeds o canales RSS es necesario utilizar un tipo de programa denominado genéricamente agregador. Los Lectores o Agregadores de feedsLos archivos RSS, a diferencia de los XHTML, no son interpretados por los navegadores web y al abrirlos lo que hacen es mostrar el código XML que los compone. Para visualizar directamente un feed es necesario utilizar un programa lector o agregador de feeds. Hay distintos tipos de agregadores. Las basados en web (usualmente denominados Portales) permiten la visualización en una página web. Un ejemplo típico de este tipo de agregador es Yahoo con su agregador MiYahoo! o el agregador de Bloglines. Otros agregadores están integrados a clientes de correo o son clientes RSS exclusivamente. Los agregadores ofrecen variedad de prestaciones especiales, tales como la inclusión de varios feeds relacionados en una única vista, el ocultamiento de entradas que ya han sido leídas y la categorización de feeds en áreas temáticas. Sistemas de Gestión de Flujos de TrabajosWorkflow o flujo de trabajo consiste en el estudio de aspectos operacionales de una actividad de trabajo, esto es, cómo se realizan y estructuran las tareas, cuál es su orden correlativo, cómo se sincronizan, cómo fluye la información y cómo se hace su seguimiento. Una de las aplicaciones de workflow consiste en automatizar la secuencia de tareas, acciones o actividades para ejecutar el proceso, con el consiguiente seguimiento del estado de las etapas y las herramientas que son necesarias para gestionar esto. Esto a nivel real es muy sencillo y por eso es muy utilizado por las empresas. Existen tres tipos de actividad en los flujos de trabajo : actividades cooperativas, actividades colaborativas y actividades de coordinación . También existen dos tipos de workflow principales: workflow ad hoc y workflow procedimental . El principal objetivo de los flujos de trabajo consiste en reducir el tiempo y acelerar la realización de un trabajo mediante el acercamiento de procesos, personas y máquinas, incluso permitiendo trabajar en equipo desde diferentes lugares. Además de esto, puede facilitar la movilidad del personal, mecanizar y automatizar métodos y organización en la información, ofrecer mecanismos de control y seguimiento de procedimientos de la empresa, agilizar el proceso de intercambio de información y toma de decisiones de la empresa, independizar el flujo de trabajo y método de quien lo realiza, etc. Puede ser muy interesante en el trabajo de gestión de stocks o control de existencias así como también en la gestión documental . Principalmente, el workflow busca seguir la realización y consecución de las tareas o trabajos por medio de una secuencia de tareas del proceso de negocio. De esta manera organiza y controla recursos, tareas y las reglas para completar este proceso buscando una mayor agilidad y la descentralización de actividades comerciales y administrativas principalmente. Con esto se puede conseguir un control de todas las etapas a la vez que la automatización de los procesos de trabajo , por lo cual las tareas, información y documentos pasan por los participantes mediante unos procedimientos que se han establecido. Para ello en muchos casos se recurre a muchas aplicaciones informáticas y software que ayudan a controlar el flujo de trabajo en todos sus aspectos. ¿Qué es el flujo de trabajo y por qué es importante en un gestor documental?En el contexto de los gestores documentales, se refiere al movimiento automatizado de documentos a través de una correlación de acciones relacionadas con el proceso empresarial. Dicho de una forma más sencilla, con un gestor documental que controla los flujos de trabajo cada documento queda ligado al estado en el que se encuentre en todo momento. Por ejemplo, una factura puede estar en diversos estados (recibida, aprobada, pagada, etc) y el administrador determinado podrá controlar en todo momento la situación de la misma. El control de los flujos de trabajo supone la máxima automatización de los procesos empresariales y el control de las etapas, durante las cuales los documentos pasan de un empleado a otro, según procedimientos previamente definidos. La etapa previa al control de flujos de trabajo es el control de flujos de información. Las empresas deben analizar cómo la información llega, se almacena y se distribuye por la compañía para generar un flujo de trabajo eficiente. Beneficios del workflow managementNo existen flujos de trabajo que funcionen de igual manera para todas las empresas. Sin embargo, muchas experimentan beneficios similares derivados. Mejora de la productividad del trabajo de los empleados con la automatización de procesos. Normalización de los métodos de trabajo mediante procedimientos preestablecidos. Optimización de la circulación de información interna. Ahorro de tiempo en tareas poco necesarias u obsoletas. Sistemas de flujo de trabajo o workflow management systemDel mismo modo que el workflow management puede encontrarse dentro de un gestor documental, también se ha desarrollado como un sistema individual. Los sistemas de flujo de trabajo permiten automatizar y mejorar los procesos empresariales con el propósito de ahorrar tiempo y eliminar errores. Entre las características esenciales que suelen presentar este tipo de sistemas destacan: Notificaciones por email: a través de la notificaciones por email, los administradores reciben información detallada del punto en el que se encuentra una tarea. SLA control status: se trata de una representación gráfica del estado de una tarea. Gracias a un código de colores se enfoca la importancia en aquellas etapas del proceso que necesitan de mayor atención o están experimentando algún problema. Formularios pre-completados: con el fin de evitar las pérdidas de tiempo a la hora de rellenar formularios repetitivos, se aconseja la distribución de formularios parcialmente completos. Reasignación de tareas: no todos los procesos terminan funcionando de la forma en la que se planean. Por ello, y para evitar gastos económicos, el software de flujo de trabajo permite la reasignación de tareas. Objetivos de los sistemas de flujo de trabajo (workflow)Métodos y organización en el sistema de información Uno de los principales objetivos de los sistemas de flujo de trabajo es reflejar, mecanizar y automatizar los métodos y la organización en el sistema de información. Y es que hay que tener en cuenta que hoy en día es esencial poder acceder a la información de forma fácil y eficaz y lo normal es que ésta esté en diferentes formatos, lo que puede provocar un problema de accesibilidad. Procedimientos organizativos El segundo objetivo de este tipo de sistemas es establecer los mecanismos de control y seguimiento de los procedimientos organizativos, algo que se consigue gracias a una normalización en la metodología de trabajo. Método y flujo de trabajo Por otro lado, los sistemas de flujo de trabajo tienen el objetivo de independizar el método y el flujo de trabajo de las personas que lo ejecutan. Movilidad del personal El cuarto objetivo de los sistemas de flujo de trabajo es facilitar la movilidad del personal. De hecho, permiten trabajar en equipo desde distintos lugares físicos. Reingeniería de negocio Otro de los objetivos es soportar procesos de reingeniería de negocio que es un método mediante el cual, en función de las necesidades del cliente, se rediseñan radicalmente los procesos principales de negocios, de principio a fin, con el objetivo de alcanzar mejoras espectaculares en medidas críticas de rendimiento, tales como costes, calidad, servicio y rapidez. Toma de decisiones El sexto objetivo es agilizar el proceso de intercambio de información y agilizar la toma de decisiones de una empresa, organización o institución. De hecho, con la implementación de este tipo de sistemas las decisiones son rápidas, ágiles y oportunas. Servicio También es importante tener en cuenta que con este tipo de sistemas se optimiza el servicio. En este sentido, hay que señalar que supone dar una respuesta más rápida a los clientes, además de transmitir una sensación de apuesta por la tecnología, lo que contribuye a motivar a los trabajadores. Gestión del conocimiento El último objetivo es la mejora de la gestión del conocimiento, una nueva cultura empresarial que se basa en gestionar las organizaciones situando los recursos humanos como el principal activo. Aplicaciones/Sistemas Workflow, flujos de trabajoLas aplicaciones Workflow automatizan la secuencia de acciones, actividades o tareas en la ejecución del proceso, permiten realizar un seguimiento de cada etapa del mismo y aportan las herramientas necesarias para su control o gestión del flujo de trabajo. Un sistema Workflow va más allá y se caracteriza, principalmente, por una adecuada integración con sistemas de información actuales: BD, gestión documental, mensajería, ERP, etc, permitiendo la ampliación de un workflow, de un simple proceso a la integración de varios procesos de negocio interrelacionados. En el mercado existen diversos tipos de herramientas Workflow, las principales son: Workflow Corporativo, Workflow de Aplicación, Workflow Documental y Workflow de Producción . Algunos de ellas se limitan a su área en particular y otras permiten la comunicación con aplicaciones externas de manera síncrona (esperando la respuesta antes de proseguir) y/o asíncrona (solamente deja un “mensaje” y recupera la respuesta más adelante). Lenguajes de especificación de workflow BPMN (Business Proccess Model and Notation): Modelo y Notación de Procesos de Negocio. BPEL / WS-BPEL ( Web Services Business Process Execution Language): Lenguaje de ejecución de Procesos de Negocio con Servicios Web . XPDL (XML Process Definition Language): Lenguaje para la Definición de un Flujo de Trabajo. YAML (Yet Another Workflow Language): Lenguaje de workflow basado en patrones de Workflow. Búsqueda de información: robots, spiders, otrosUn motor de búsqueda, también conocido como buscador, es un sistema informático que busca archivos almacenados en servidores web gracias a su “spider” (Web crawler). Un ejemplo son los buscadores de Internet (algunos buscan únicamente en la web, pero otros lo hacen además en noticias, servicios como Gopher, FTP, etc) cuando se pide información sobre algún tema. Las búsquedas se hacen con palabras clave o con árboles jerárquicos por temas; el resultado de la búsqueda es un listado de direcciones web en los que se mencionan temas relacionados con las palabras claves buscadas. Como operan de forma automática, los motores de búsqueda contienen generalmente más información que los directorios. Sin embargo, estos últimos también han de construirse a partir de búsquedas (no automatizadas) o bien partir de avisos dados por los creadores de páginas (lo cual puede ser muy limitante). Los buenos directorios combinan ambos sistemas. Hoy en día Internet se ha convertido en una herramienta, para la búsqueda de información, rápida, para ello han surgido los buscadores que son un motor de búsqueda que nos facilita encontrar información rápida de cualquier tema de interés, en cualquier área de las ciencias, y de cualquier parte del mundo. Se pueden clasificar en dos tipos: Índices temáticos : Son sistemas de búsqueda por temas o categorías jerarquizados (aunque también suelen incluir sistemas de búsqueda por palabras clave). Se trata de BD de direcciones Web elaboradas “manualmente”, es decir, hay personas que se encargan de asignar cada página web a una categoría o tema determinado. Por ejemplo existen buscadores de fauna, flora, educación, música y de diferentes áreas. Motores de búsqueda : Son sistemas de búsqueda por palabras clave. Son BD que incorporan automáticamente páginas web mediante “robots” de búsqueda en la red. Clases de buscadores: Buscadores jerárquicos (Arañas o Spiders) Recorren las páginas recopilando información sobre los contenidos de las páginas. Cuando se busca una información en los motores, ellos consultan su BD y presentan resultados clasificados por su relevancia. De las webs, los buscadores pueden almacenar desde la página de entrada, a todas las páginas que residan en el servidor. Si se busca una palabra, por ejemplo, “ordenadores”. En los resultados que ofrecerá el motor de búsqueda, aparecerán las páginas que contengan esta palabra en alguna parte de su texto. Si consideran que un sitio web es importante para el usuario, tienden a registrarlas todas. Si no la consideran importante, sólo almacenan una o más páginas. Cada cierto tiempo, los motores revisan los sitios, para actualizar los contenidos de su BD, por tanto puede que los resultados de la búsqueda estén desactualizados. Los buscadores jerárquicos tienen una colección de programas simples y potentes con diferentes cometidos. Se suelen dividir en tres partes. Los programas que exploran la red -arañas (spiders)-, los que construyen la BD y los que utiliza el usuario, el programa que explota la BD. Si se paga, se puede aparecer en las primeras páginas de resultados, aunque los principales buscadores delimitan estos resultados e indican al usuario que se trata de resultados esponsorizados o patrocinados. Hasta el momento, aparentemente, esta forma de publicidad es indicada explícitamente. Los buscadores jerárquicos se han visto obligados a comercializar este tipo de publicidad para poder seguir ofreciendo a los usuarios el servicio de forma gratuita. Ejemplo de arañas: Google, Bing, Hotbot. Directorios Una tecnología barata, ampliamente utilizada por gran cantidad de scripts en el mercado. No se requieren muchos recursos de informática. En cambio, se requiere más soporte humano y mantenimiento. Los algoritmos son muchos más sencillos, presentando la información sobre los sitios registrados como una colección de directorios. No recorren los sitios web ni almacenan sus contenidos. Solo registran algunos de los datos de nuestra página, como el título y la descripción que se introduzcan al momento de registrar el sitio en el directorio. Los resultados de la búsqueda, estarán determinados por la información que se haya suministrado al directorio cuando se registra el sitio. En cambio, a diferencia de los motores, son revisadas por operadores humanos, y clasificadas según categorías, de forma que es más fácil encontrar páginas del tema de nuestro interés. Más que buscar información sobre contenidos de la página, los resultados serán presentados haciendo referencia a los contenidos y temática del sitio. Su tecnología es muy barata y sencilla. Ejemplo de directorios: Antiguos directorios, Open Directory Project, Yahoo!, Terra (antiguo Olé). Ahora, ambos utilizan tecnología de búsqueda jerárquica, y Yahoo! conserva su directorio. Metabuscador Permite lanzar varias búsquedas en motores seleccionados respetando el formato original de los buscadores. Lo que hacen, es realizar búsquedas en auténticos buscadores, analizan los resultados de la página, y presentan sus propios resultados, según un orden definido por el sistema estructural del metabuscador. FFA – Enlaces gratuitos para todos FFA (Free For All). Cualquiera puede inscribir su página durante un tiempo limitado en estos pequeños directorios. Los enlaces no son permanentes. Buscadores verticales Buscadores especializados en un sector concreto, lo que les permite analizar la información con mayor profundidad, disponer de resultados más actualizados y ofrecer al usuario herramientas de búsqueda avanzadas. Es importante resaltar que utilizan índices especializados de esta manera para acceder a la información de una manera más específica y fácil. Ejemplos de este tipo de buscadores son: Trovit, Nestoria. ¿Qué es un crawler o arañas de la web y qué hacen?¿Qué es un crawler? El crawler, también conocido como araña de la web, es un software o webbot que se encarga de recorrer los enlaces de las páginas webs de una forma automática y sistemática. ¿Qué hace un crawler y cómo funciona? Normalmente, un crawler dispone de un conjunto de inicial de URLs, conocidas como semillas, y va descargando las páginas web asociadas a las semillas y buscando dentro de éstas otras URLs. Cada nueva URL encontrada se añade a la lista de URLs que la araña web debe visitar. Es decir, recoleta URL’s para posteriormente procesarlas. Así, el motor de búsqueda creará un índice de las páginas descargadas para proporcionar búsquedas más rápidas. Cuando un crawler visita un sitio web opta por una de estas dos alternativas: Buscar el archivo robots.txt y la meta etiqueta robots para ver las reglas que se han estipulado. Elaborar un índice de las páginas web que hay en su sitio. ¿Cómo? Explorando el contenido del texto visible, de varias etiquetas HTML y los hipervínculos en listados en la página. Ejemplo: Googlebot Diferencia entre los robots, spider y crawlerEl ranking de los motores de búsqueda está basado en robots (arañas o crawlers). Crawler Se trata de un software desarrollado para realizar una exploración en Internet de una manera sistemática a través de la información percibida como relevante para su función. Capturan el texto de las páginas y los enlaces encontrados y por lo tanto permiten encontrar nuevas páginas. Es una de las bases de los motores de búsqueda, que son responsables de la indexación de sitios web, almacenarlos en la BD de los buscadores. Es también conocio como araña o Bot (robot). El proceso que ejecuta un rastreador web se llama Web Crawler o rastreador . Muchos sitios, en particular los motores de búsqueda utilizan rastreadores para mantener una BD actualizada. Los rastreadores web son usados básicamente para realizar una copia de todas las páginas visitadas para post-procesamiento por un motor de búsqueda que indexa las páginas descargadas para proporcionar búsquedas rápidas. Los rastreadores también se pueden utilizar para tareas de mantenimiento automatizadas en un sitio web, como la comprobación de enlaces o la validación de código HTML. Las spiders también pueden ser utilizadas para obtener los tipos específicos de información de páginas web, como direcciones de correo electrónico (más comúnmente como spam). Los rastreadores de motores de búsqueda por lo general buscan información acerca de los permisos sobre el contenido. En especial hay dos maneras de bloquear un rastreador que indexe una página en particular (y los enlaces contenidos en ella). La primera, y más común, es a través del archivo robots.txt. La otra forma es a través de la etiqueta meta robots con el valor “noindex” o “nofollow”, que sirve para no indexar (la página sí) y no por debajo (los enlaces en la página), respectivamente. También hay una tercera posibilidad, mucho menos explotado, que está utilizando el ‘rel=”nofollow”‘ para los enlaces, lo que indica que el rastreador que enlazan, en particular, no se debe seguir. Araña También conocido como Robot, Bot o Cadenas . Estos son los programas utilizados por los motores de búsqueda para navegar por Internet y descargar automáticamente contenido de sitios web. Metódicamente, expone el contenido que estime pertinente en el código fuente de los sitios, y almacena el resto en su BD. Por lo tanto, los motores de búsqueda robots basados (arañas o crawlers) buscan en Internet después de la búsqueda de información y lo clasifican de acuerdo a los vínculos y también al contenido que se encuentra en las páginas de búsqueda, como el principal portal de búsqueda web, Google. Por lo tanto, cualquier página necesita ser trazada por el robot y por lo tanto pueden aparecer los resultados de búsqueda de los mecanismos implicados. Posicionamiento y Buscadores (SEO)¿Qué es? Posicionamiento web, posicionamiento en buscadores o posicionamiento SEO se refiere a las técnicas que buscan que una página web aparezca en las primeras posiciones de los resultados en buscadores (Google, Yahoo, …) para una serie de palabras o frases. Conceptos SEO (Search Engine Optimization) o posicionamiento orgánico/natural. SEM (Search Engine Marketing) o posicionamiento de pago/publicitario. SERPs (Search Engine Results Page) o Página de resultados del Buscador. Posicionamiento web natural u orgánico Los buscadores proporcionan dos tipos de resultados: enlaces patrocinados o anuncios y resultados orgánicos o naturales: Resultados Orgánicos, Posicionamiento “Gratuito” o Posicionamiento Natural en Buscadores (SEO) : los buscadores como Google aplican cierto criterio para decidir en qué orden deben aparecer los resultados de una búsqueda. Algunas de las características valoradas por los buscadores son, por ejemplo, la popularidad de la página web, su contenido, su velocidad de carga y otras cuestiones técnicas. Enlaces Patrocinados, Posicionamiento “de Pago” o Marketing en Buscadores (SEM) : la presencia de una página web en los resultados patrocinados se consigue con la compra de palabras clave al buscador (Google, Yahoo!, Bing, …). Es importante destacar que el anunciante no paga por mostrar su anuncio, sólo paga cuando el usuario hace clic en él. A este tipo de publicidad se le llama también PPC (Pago Por Clic) . Diferencias entre SEO y SEM Funcionamiento: El criterio utilizado por los buscadores para mostrar los resultados naturales (SEO) es desconocido y las técnicas para mejorar el SEO están en las recomendaciones que de vez en cuando dan los propios buscadores y en la experiencia de quienes trabajan haciendo SEO. Existen certificaciones oficiales de SEM, expedidas por los propios buscadores, que permiten formarse oficialmente en esta disciplina. Tiempo en obtener resultados: Los resultados de las acciones para mejorar el SEO son observables a largo plazo. Con el SEM se obtienen resultados de forma más inmediata. Garantías en la obtención de resultados: En el SEO es imposible estimar, y mucho menos garantizar resultados. En el SEM se puede estimar resultados. Costes: La competencia en el SEO es tan alta que intentar aparecer en la primera página de resultados puede ser inútil, sobre todo para términos genéricos. En el SEM, el precio de las palabras clave cambia en cada instante dependiendo de varios factores: competencia, país, idioma, … Medición de resultados: Es difícil medir con rigurosidad los resultados de las acciones para mejorar el SEO. Los resultados del SEM se pueden medir con total precisión. Objetivos del SEO Definir las palabras claves que son importantes para nuestra página, pues serán lo términos utilizados por los usuarios para buscar información sobre contenido y soluciones que nosotros proveemos. Tenemos que tener en cuenta la Teoría del Long Tail aplicada a las búsquedas en la red; ya que, a pesar de que exista un número de búsquedas muy frecuentes, la mayoría de ellas son muy diferentes entre sí, y buscadores como Google se centraron en las pequeñas pero variadas búsquedas para obtener beneficios y componer su sistema de búsqueda. Mejorar la visibilidad de la página web . Los algoritmos empleados que emplean los buscadores para posicionar las páginas webs no son conocidos y van modificándose continuamente; consecuentemente, nadie puede tener la certeza de saber cómo posicionar en primer lugar una web en los SERPs, aunque se pueda trabajar para intentar aparecer en los primeros puestos. No obstante, se conocen algunos de los aspectos que influyen en los algoritmos y que darán visibilidad a la web: Los propios de la programación de la página, que son “manipulables” y tenidos en cuenta para valorar la relevancia de la web, llamados factores de relevancia on-page . Los que están relacionados con otras páginas webs a través de una estructura de vínculos que permiten navegar por toda la red de internet, llamados f actores de relevancia off-page . En estos se incluye el Social SEO , que mayoritariamente está centrado en la capacidad de aportar enlaces entrantes desde los medios sociales hacia la web. Aumentar el número de visitas que están buscando lo que puede ofrecerle nuestra página; es decir, incrementar el tráfico cualificado que llega de los buscadores a la web. Link BuildingLinkbuilding o construcción de enlaces, es una técnica de SEO que consiste en conseguir que otras páginas web enlacen a la página que interesa que los buscadores consideren relevantes y la posicionen mejor en sus rankings. La técnica puede hacerse de manera natural, cuando otras webs enlazan sin previo acuerdo por algún hecho o dicho, o bien de manera artificial, cuando se simula que los enlaces se han conseguido de manera natural. Esta se basa en el concepto de que uno de los factores que se incluyen dentro de la evaluación del ranking de una página es la cantidad de enlaces entrantes que tiene una página, concepto basado en el hecho de que el número de enlaces entrantes constituía uno de los factores evaluados en PageRank en 1999 por Google. Las ventajas son: Posibilidad de medir la demanda y cantidad de personas que están buscando a través de una palabra clave. Efectividad del posicionamiento. Posicionamiento de la marca o branding. Técnicas Alta en directorios : consiste en dar de alta la web en diferentes directorios, ya sean generales o temáticos. Directorios de artículos : consiste en escribir artículos para publicarlos en directorios que, a cambio del contenido, permiten incluir enlaces hacia una web. Bookmarking : se trata de guardar aquello que interesa posicionar en los buscadores en las diferentes webs de bookmarking. Link baiting : es una de las técnicas más valoradas por los buscadores pero una de las más difíciles de conseguir, ya que solo se consiguen cientos de enlaces a un artículo si este realmente aporta valor. Intercambio de enlaces : una buena forma de conseguir enlaces y una de las primeras que se empezaron a utilizar. Compra de enlaces : Más efectiva que el intercambio de enlaces pero también más cara. Para Google esta forma de conseguir enlaces es penalizable. Enlaces desde foros : otra forma para construir enlaces es de foros, agregando el link o enlace desde la firma del foro. Otras técnicas : envío de enlaces a bloggers, redes sociales, escribir revisiones, notas de prensa, entre otros. Pasos para SEO y posicionamiento webPosicionamiento a través de las Palabras clave Elige bien tus palabras clave. Comprueba la competencia. Mide la densidad de las palabras. Usa las palabras clave. Palabras clave en títulos y negrita. Mide y analiza tu posicionamiento natural para distintas palabras clave. Configuración del Sitio Meta description y title. Url amigables y editadas. Creación y envío de sitemap a buscadores. Automatiza el envío de sitemaps. Transcribe el contenido audiovisual. Favicon. Evita el uso de cookies. Utiliza rel=”autor”. Las imágenes Título y descripción. Especifica su tamaño. No escalar imágenes en Html. Optimización de imágenes para la web. Combinar imágenes usando CSS sprites. Cuida los enlaces Anchor text diversificados. Comprueba los links rotos. Evita las redirecciones. Automatizar la búsqueda de links rotos. Conoce el PageRank. No enlaces contenido malicioso o ilegal. Busca en donde enlazan a tu competencia. Utiliza enlaces internos. Evitar contenido duplicado Textos originales. No-index a nuestro contenido duplicado. Página inicial con sólo muestra. Descripción y Título Meta sin repetir. URL Canónica. Un mismo diseño para web y móvil. Guía de Estilo Crear contenido divertidos y originales. Contenidos largos. Cuida a tus visitantes desde dispositivos. Publica periódicamente. Participa y conecta con tu comunidad. Protocolo después de cada artículo. Guest Blogging. Ofrece algún contenido de valor. Landing Page. Cuida al lector. Evitar penalizaciones No poner palabras clave fuera de contexto. No poner texto escondido. Evita los errores de código que puedas. No te pases con el intercambio de enlaces. Reduce el tiempo de carga de tu página Mide y mejora la velocidad de tu página. No abusar de los códigos en javascript. Elimina plugins de WordPress que no utilices. Pon javascript al final del código. Retrasar o diferir la carga de javascript. Ahorra y limpia tu código. Minimiza tu Css y Javascript. Combina tus Javascript. Usa la paginación. Reduce el número de consultas de DNS. Pocas llamadas http. Comprimir en gzip. Usar cache de la página. Usar cache para Javascript. Determinar una fecha de caducidad de la cache. Usar un CDN. No usar tablas anidadas en html. CSS externo. Javascript externo. Comprueba los tiempos de carga de cada página. Herramientas imprescindibles Woorank. Web Ceo. Screaming Frog. All in One SEO Pack. SEO by Yoast. W3 Total Cache. SEO Chat Seo Tools. Bibliografía Scribd (Ibiza Ales) Horizonweb NocionesUnidas Coguan Neosoft DesarrolloWeb mariaelena biblipos gestion.org tic.portal EAE Business School Pixelware wikipedia Tiendas Virtuales y Comercio Web Seo Coaching Seo Básico Arume mongemalo Marketing Digital desde 0 Bruno VD"}],"posts":[{"title":"Primer post","date":"2019-01-16T10:22:00.000Z","path":"wiki/post/","text":"","tags":[],"categories":[]},{"title":"Hello World","date":"2019-01-16T10:17:13.973Z","path":"wiki/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[],"categories":[]}]}